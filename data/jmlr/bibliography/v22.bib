@article{JMLR:v22:16-179,
 abstract = {Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification.},
 author = {Andreas Damianou and Neil D. Lawrence and Carl Henrik Ek},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W4293549471},
 pages = {1--51},
 title = {Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis},
 url = {http://jmlr.org/papers/v22/16-179.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:17-298,
 abstract = {This work initiates a general study of learning and generalization without the i.i.d. assumption, starting from first principles. While the traditional approach to statistical learning theory typically relies on standard assumptions from probability theory (e.g., i.i.d. or stationary ergodic), in this work we are interested in developing a theory of learning based only on the most fundamental and necessary assumptions implicit in the requirements of the learning problem itself. We specifically study universally consistent function learning, where the objective is to obtain low long-run average loss for any target function, when the data follow a given stochastic process. We are then interested in the question of whether there exist learning rules guaranteed to be universally consistent given only the assumption that universally consistent learning is possible for the given data process. The reasoning that motivates this criterion emanates from a kind of optimist's decision theory, and so we refer to such learning rules as being optimistically universal. We study this question in three natural learning settings: inductive, self-adaptive, and online. Remarkably, as our strongest positive result, we find that optimistically universal learning rules do indeed exist in the self-adaptive learning setting. Establishing this fact requires us to develop new approaches to the design of learning algorithms. Along the way, we also identify concise characterizations of the family of processes under which universally consistent learning is possible in the inductive and self-adaptive settings. We additionally pose a number of enticing open problems, particularly for the online learning setting.},
 author = {Steve Hanneke},
 journal = {Journal of Machine Learning Research},
 number = {130},
 openalex = {W2623739609},
 pages = {1--116},
 title = {Learning Whenever Learning is Possible: Universal Learning under General Stochastic Processes},
 url = {http://jmlr.org/papers/v22/17-298.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:17-474,
 author = {Patrick Kreitzberg and Oliver Serang},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W3165937638},
 pages = {1--24},
 title = {On Solving Probabilistic Linear Diophantine Equations},
 url = {http://jmlr.org/papers/v22/17-474.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:17-570,
 abstract = {The reproducing kernel Hilbert space (RKHS) embedding of distributions offers a general and flexible framework for testing problems in arbitrary domains and has attracted considerable amount of attention in recent years. To gain insights into their operating characteristics, we study here the statistical performance of such approaches within a minimax framework. Focusing on the case of goodness-of-fit tests, our analyses show that a vanilla version of the kernel-embedding based test could be suboptimal, and suggest a simple remedy by moderating the embedding. We prove that the moderated approach provides optimal tests for a wide range of deviations from the null and can also be made adaptive over a large collection of interpolation spaces. Numerical experiments are presented to further demonstrate the merits of our approach.},
 author = {Krishnakumar Balasubramanian and Tong Li and Ming Yuan},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2760093735},
 pages = {1--45},
 title = {On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests},
 url = {http://jmlr.org/papers/v22/17-570.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:17-679,
 abstract = {In the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced (Blanchard et al., 2011). We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets.},
 author = {Gilles Blanchard and Aniket Anand Deshmukh and Urun Dogan and Gyemin Lee and Clayton Scott},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2768783441},
 pages = {1--55},
 title = {Domain Generalization by Marginal Transfer Learning},
 url = {http://jmlr.org/papers/v22/17-679.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:17-720,
 abstract = {In retail, there are predictable yet dramatic time-dependent patterns in customer behavior, such as periodic changes in the number of visitors, or increases in customers just before major holidays. The current paradigm of multi-armed bandit analysis does not take these known patterns into account. This means that for applications in retail, where prices are fixed for periods of time, current bandit algorithms will not suffice. This work provides a remedy that takes the time-dependent patterns into account, and we show how this remedy is implemented for the UCB, $\varepsilon$-greedy, and UCB-L algorithms, and also through a new policy called the variable arm pool algorithm. In the corrected methods, exploitation (greed) is regulated over time, so that more exploitation occurs during higher reward periods, and more exploration occurs in periods of low reward. In order to understand why regret is reduced with the corrected methods, we present a set of bounds that provide insight into why we would want to exploit during periods of high reward, and discuss the impact on regret. Our proposed methods perform well in experiments, and were inspired by a high-scoring entry in the Exploration and Exploitation 3 contest using data from Yahoo$!$ Front Page. That entry heavily used time-series methods to regulate greed over time, which was substantially more effective than other contextual bandit methods.},
 author = {Stefano Trac√† and Cynthia Rudin and Weiyu Yan},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W3126427931},
 pages = {1--99},
 title = {Regulating Greed Over Time in Multi-Armed Bandits},
 url = {http://jmlr.org/papers/v22/17-720.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-056,
 abstract = {MushroomRL is an open-source Python library developed to simplify the process of implementing and running Reinforcement Learning (RL) experiments. Compared to other available libraries, MushroomRL has been created with the purpose of providing a comprehensive and flexible framework to minimize the effort in implementing and testing novel RL methodologies. Indeed, the architecture of MushroomRL is built in such a way that every component of an RL problem is already provided, and most of the time users can only focus on the implementation of their own algorithms and experiments. The result is a library from which RL researchers can significantly benefit in the critical phase of the empirical analysis of their works. MushroomRL stable code, tutorials and documentation can be found at https://github.com/MushroomRL/mushroom-rl.},
 author = {Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and Jan Peters},
 journal = {Journal of Machine Learning Research},
 number = {131},
 openalex = {W2998544442},
 pages = {1--5},
 title = {MushroomRL: Simplifying Reinforcement Learning Research},
 url = {http://jmlr.org/papers/v22/18-056.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-105,
 abstract = {Suppose that we wish to estimate a vector $\mathbf{x}$ from a set of binary paired comparisons of the form "$\mathbf{x}$ is closer to $\mathbf{p}$ than to $\mathbf{q}$" for various choices of vectors $\mathbf{p}$ and $\mathbf{q}$. The problem of estimating $\mathbf{x}$ from this type of observation arises in a variety of contexts, including nonmetric multidimensional scaling, "unfolding," and ranking problems, often because it provides a powerful and flexible model of preference. We describe theoretical bounds for how well we can expect to estimate $\mathbf{x}$ under a randomized model for $\mathbf{p}$ and $\mathbf{q}$. We also present results for the case where the comparisons are noisy and subject to some degree of error. Additionally, we show that under a randomized model for $\mathbf{p}$ and $\mathbf{q}$, a suitable number of binary paired comparisons yield a stable embedding of the space of target vectors. Finally, we also show that we can achieve significant gains by adaptively changing the distribution for choosing $\mathbf{p}$ and $\mathbf{q}$.},
 author = {Andrew K. Massimino and Mark A. Davenport},
 journal = {Journal of Machine Learning Research},
 number = {186},
 openalex = {W2789169447},
 pages = {1--39},
 title = {As you like it: Localization via paired comparisons},
 url = {http://jmlr.org/papers/v22/18-105.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-220,
 author = {Erich Merrill and Alan Fern and Xiaoli Fern and Nima Dolatnia},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W3128458662},
 pages = {1--25},
 title = {An Empirical Study of Bayesian Optimization: Acquisition Versus Partition},
 url = {http://jmlr.org/papers/v22/18-220.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-240,
 author = {Can M. Le},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W3165893786},
 pages = {1--29},
 title = {Edge Sampling Using Local Network Information},
 url = {http://jmlr.org/papers/v22/18-240.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-332,
 author = {Feifei Wang and Junni L. Zhang and Yichao Li and Ke Deng and Jun S. Liu},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W3166374891},
 pages = {1--48},
 title = {Bayesian Text Classification and Summarization via A Class-Specified Topic Model},
 url = {http://jmlr.org/papers/v22/18-332.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-401,
 abstract = {Biological processes underlying the basic functions of a cell involve complex interactions between genes. From a technical point of view, these interactions can be represented through a graph where genes and their connections are, respectively, nodes and edges. The main objective of this paper is to develop a statistical framework for modelling the interactions between genes when the activity of genes is measured on a discrete scale. In detail, we define a new algorithm for learning the structure of undirected graphs, PC-LPGM, proving its theoretical consistence in the limit of infinite observations. The proposed algorithm shows promising results when applied to simulated data as well as to real data.},
 author = {Nguyen Thi Kim Hue and Monica Chiogna},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W2898102668},
 pages = {1--53},
 title = {Structure learning of undirected graphical models for count data},
 url = {http://jmlr.org/papers/v22/18-401.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-407,
 author = {Junlong Zhu and Qingtao Wu and Mingchuan Zhang and Ruijuan Zheng and Keqin Li},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W3157756024},
 pages = {1--42},
 title = {Projection-free Decentralized Online Learning for Submodular Maximization over Time-Varying Networks},
 url = {http://jmlr.org/papers/v22/18-407.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-417,
 abstract = {Motivated by the needs of online large-scale recommender systems, we specialize the decoupled extended Kalman filter (DEKF) to factorization models, including factorization machines, matrix and tensor factorization, and illustrate the effectiveness of the approach through numerical experiments on synthetic and on real-world data. Online learning of model parameters through the DEKF makes factorization models more broadly useful by (i) allowing for more flexible observations through the entire exponential family, (ii) modeling parameter drift, and (iii) producing parameter uncertainty estimates that can enable explore/exploit and other applications. We use a different parameter dynamics than the standard DEKF, allowing parameter drift while encouraging reasonable values. We also present an alternate derivation of the extended Kalman filter and DEKF that highlights the role of the Fisher information matrix in the EKF.},
 author = {Carlos A. Gomez-Uribe and Brian Karrer},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2886639315},
 pages = {1--25},
 title = {The decoupled extended Kalman filter for dynamic exponential-family factorization models},
 url = {http://jmlr.org/papers/v22/18-417.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-431,
 author = {Rasmus Bonnevie and Mikkel N. Schmidt},
 journal = {Journal of Machine Learning Research},
 number = {187},
 openalex = {W3206574577},
 pages = {1--48},
 title = {Matrix Product States for Inference in Discrete Probabilistic Models},
 url = {http://jmlr.org/papers/v22/18-431.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-485,
 author = {Takayuki Okuno and Akiko Takeda and Akihiro Kawana and Motokazu Watanabe},
 journal = {Journal of Machine Learning Research},
 number = {245},
 openalex = {W3215151450},
 pages = {1--47},
 title = {On lp-hyperparameter Learning via Bilevel Nonsmooth Optimization},
 url = {http://jmlr.org/papers/v22/18-485.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-489,
 abstract = {The recent empirical success of unsupervised cross-domain mapping algorithms, between two domains that share common characteristics, is not well-supported by theoretical justifications. This lacuna is especially troubling, given the clear ambiguity in such mappings. 
We work with adversarial training methods based on IPMs and derive a novel risk bound, which upper bounds the risk between the learned mapping $h$ and the target mapping $y$, by a sum of three terms: (i) the risk between $h$ and the most distant alternative mapping that was learned by the same cross-domain mapping algorithm, (ii) the minimal discrepancy between the target domain and the domain obtained by applying a hypothesis $h^*$ on the samples of the source domain, where $h^*$ is a hypothesis selectable by the same algorithm. The bound is directly related to Occam's razor and encourages the selection of the minimal architecture that supports a small mapping discrepancy and (iii) an approximation error term that decreases as the complexity of the class of discriminators increases and is empirically shown to be small. 
The bound leads to multiple algorithmic consequences, including a method for hyperparameters selection and for early stopping in cross-domain mapping GANs. We also demonstrate a novel capability for unsupervised learning of estimating confidence in the mapping of every specific sample.},
 author = {Tomer Galanti and Sagie Benaim and Lior Wolf},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W3095888939},
 pages = {1--42},
 title = {Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs},
 url = {http://jmlr.org/papers/v22/18-489.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-534,
 abstract = {Consider an (observable) random sample of size n from an infinite population of individuals, each individual being endowed with a finite set of features from a collection of features (Fj)j‚â•1 with unknown probabilities (pj)j‚â•1, i.e., pj is the probability that an individual displays feature Fj. Under this feature sampling framework, in recent years there has been a growing interest in estimating the sum of the probability masses pj's of features observed with frequency r‚â•0 in the sample, here denoted by Mn,r. This is the natural feature sampling counterpart of the classical problem of estimating small probabilities in the species sampling framework, where each individual is endowed with only one feature (or ‚Äúspecies). In this paper we study the problem of consistent estimation of the small mass Mn,r. We first show that there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass Mn,0. Then, we introduce an estimator of Mn,r and identify sufficient conditions under which the estimator is consistent. In particular, we propose a nonparametric estimator M^n,r of Mn,r which has the same analytic form of the celebrated Good--Turing estimator for small probabilities, with the sole difference that the two estimators have different ranges (supports). Then, we show that M^n,r is strongly consistent, in the multiplicative sense, under the assumption that (pj)j‚â•1 has regularly varying heavy tails.},
 author = {Fadhel Ayed and Marco Battiston and Federico Camerlenghi and Stefano Favaro},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W3128325655},
 pages = {1--28},
 title = {Consistent estimation of small masses in feature sampling},
 url = {http://jmlr.org/papers/v22/18-534.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-546,
 abstract = {In machine learning, the notion of multi-armed bandits refers to a class of online learning problems, in which an agent is supposed to simultaneously explore and exploit a given set of choice alternatives in the course of a sequential decision process. In the standard setting, the agent learns from stochastic feedback in the form of real-valued rewards. In many applications, however, numerical reward signals are not readily available -- instead, only weaker information is provided, in particular relative preferences in the form of qualitative comparisons between pairs of alternatives. This observation has motivated the study of variants of the multi-armed bandit problem, in which more general representations are used both for the type of feedback to learn from and the target of prediction. The aim of this paper is to provide a survey of the state of the art in this field, referred to as preference-based multi-armed bandits or dueling bandits. To this end, we provide an overview of problems that have been considered in the literature as well as methods for tackling them. Our taxonomy is mainly based on the assumptions made by these methods about the data-generating process and, related to this, the properties of the preference-based feedback.},
 author = {Viktor Bengs and R√≥bert Busa-Fekete and Adil El Mesaoudi-Paul and Eyke H√ºllermeier},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2883737864},
 pages = {1--108},
 title = {Preference-based Online Learning with Dueling Bandits: A Survey},
 url = {http://jmlr.org/papers/v22/18-546.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-558,
 abstract = {We introduce a unified framework for random forest prediction error estimation based on a novel estimator of the conditional prediction error distribution function. Our framework enables simple plug-in estimation of key prediction uncertainty metrics, including conditional mean squared prediction errors, conditional biases, and conditional quantiles, for random forests and many variants. Our approach is especially well-adapted for prediction interval estimation; we show via simulations that our proposed prediction intervals are competitive with, and in some settings outperform, existing methods. To establish theoretical grounding for our framework, we prove pointwise uniform consistency of a more stringent version of our estimator of the conditional prediction error distribution function. The estimators introduced here are implemented in the R package forestError.},
 author = {Benjamin Lu and Johanna Hardin},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W3126852964},
 pages = {1--41},
 title = {A Unified Framework for Random Forest Prediction Error Estimation},
 url = {http://jmlr.org/papers/v22/18-558.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-651,
 abstract = {Sampling the parameters of high-dimensional Continuous Time Markov Chains (CTMC) is a challenging problem with important applications in many fields of applied statistics. In this work a recently proposed type of non-reversible rejection-free Markov Chain Monte Carlo (MCMC) sampler, the Bouncy Particle Sampler (BPS), is brought to bear to this problem. BPS has demonstrated its favorable computational efficiency compared with state-of-the-art MCMC algorithms, however to date applications to real-data scenario were scarce. An important aspect of the practical implementation of BPS is the simulation of event times. Default implementations use conservative thinning bounds. Such bounds can slow down the algorithm and limit the computational performance. Our paper develops an algorithm with an exact analytical solution to the random event times in the context of CTMCs. Our local version of BPS algorithm takes advantage of the sparse structure in the target factor graph and we also provide a framework for assessing the computational complexity of local BPS algorithms.},
 author = {Tingting Zhao and Alexandre Bouchard-C√¥t√©},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2947115254},
 pages = {1--41},
 title = {Analysis of high-dimensional Continuous Time Markov Chains using the Local Bouncy Particle Sampler},
 url = {http://jmlr.org/papers/v22/18-651.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-694,
 abstract = {Clustering is a fundamental problem in unsupervised learning. Popular methods like K-means, may suffer from poor performance as they are prone to get stuck in its local minima. Recently, the sum-of-norms (SON) model (also known as the clustering path) has been proposed in Pelckmans et al. (2005), Lindsten et al. (2011) and Hocking et al. (2011). The perfect recovery properties of the convex clustering model with uniformly weighted all pairwise-differences regularization have been proved by Zhu et al. (2014) and Panahi et al. (2017). However, no theoretical guarantee has been established for the general weighted convex clustering model, where better empirical results have been observed. In the numerical optimization aspect, although algorithms like the alternating direction method of multipliers (ADMM) and the alternating minimization algorithm (AMA) have been proposed to solve the convex clustering model (Chi and Lange, 2015), it still remains very challenging to solve large-scale problems. In this paper, we establish sufficient conditions for the perfect recovery guarantee of the general weighted convex clustering model, which include and improve existing theoretical results as special cases. In addition, we develop a semismooth Newton based augmented Lagrangian method for solving large-scale convex clustering problems. Extensive numerical experiments on both simulated and real data demonstrate that our algorithm is highly efficient and robust for solving large-scale problems. Moreover, the numerical results also show the superior performance and scalability of our algorithm comparing to the existing first-order methods. In particular, our algorithm is able to solve a convex clustering problem with 200,000 points in $\mathbb{R}^3$ in about 6 minutes.},
 author = {Defeng Sun and Kim-Chuan Toh and Yancheng Yuan},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W3127907562},
 pages = {1--32},
 title = {Convex Clustering: Model, Theoretical Guarantee and Efficient Algorithm},
 url = {http://jmlr.org/papers/v22/18-694.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-726,
 abstract = {We consider a setup in which confidential i.i.d. samples $X_1,\dotsc,X_n$ from an unknown finite-support distribution $\boldsymbol{p}$ are passed through $n$ copies of a discrete privatization channel (a.k.a. mechanism) producing outputs $Y_1,\dotsc,Y_n$. The channel law guarantees a local differential privacy of $\epsilon$. Subject to a prescribed privacy level $\epsilon$, the optimal channel should be designed such that an estimate of the source distribution based on the channel outputs $Y_1,\dotsc,Y_n$ converges as fast as possible to the exact value $\boldsymbol{p}$. For this purpose we study the convergence to zero of three distribution distance metrics: $f$-divergence, mean-squared error and total variation. We derive the respective normalized first-order terms of convergence (as $n\to\infty$), which for a given target privacy $\epsilon$ represent a rule-of-thumb factor by which the sample size must be augmented so as to achieve the same estimation accuracy as that of a non-randomizing channel. We formulate the privacy-fidelity trade-off problem as being that of minimizing said first-order term under a privacy constraint $\epsilon$. We further identify a scalar quantity that captures the essence of this trade-off, and prove bounds and data-processing inequalities on this quantity. For some specific instances of the privacy-fidelity trade-off problem, we derive inner and outer bounds on the optimal trade-off curve.},
 author = {Adriano Pastore and Michael Gastpar},
 journal = {Journal of Machine Learning Research},
 number = {132},
 openalex = {W3191400578},
 pages = {1--56},
 title = {Locally Differentially-Private Randomized Response for Discrete Distribution Learning},
 url = {http://jmlr.org/papers/v22/18-726.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-745,
 abstract = {Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with $\ell_0$-"norm" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is, instead, to tackle their convex surrogates based on $\ell_1$-norm relaxations. In this paper, we propose a new iterative (convex) conic quadratic relaxations that exploit not only the $\ell_0$-"norm" terms, but also the fitness and smoothness functions. The iterative convexification approach substantially closes the gap between the $\ell_0$-"norm" and its $\ell_1$ surrogate. These stronger relaxations lead to significantly better estimators than $\ell_1$-norm approaches and also allow one to utilize affine sparsity priors. In addition, the parameters of the model and the resulting estimators are easily interpretable. Experiments with a tailored Lagrangian decomposition method indicate that the proposed iterative convex relaxations \rev{yield solutions within 1\% of the exact $\ell_0$ approach, and can tackle instances with up to 100,000 variables under one minute.},
 author = {Alper Atamturk and Andres Gomez and Shaoning Han},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2899782288},
 pages = {1--43},
 title = {Sparse and Smooth Signal Estimation: Convexification of L0 Formulations},
 url = {http://jmlr.org/papers/v22/18-745.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-770,
 abstract = {We study the computational complexity of a Metropolis-Hastings algorithm for Bayesian community detection. We first establish a posterior strong consistency result for a natural prior distribution on stochastic block models under the optimal signal-to-noise ratio condition in the literature. We then give a set of conditions that guarantee rapid mixing of a simple Metropolis-Hastings algorithm. The mixing time analysis is based on a careful study of posterior ratios and a canonical path argument to control the spectral gap of the Markov chain.},
 author = {Bumeng Zhuo and Chao Gao},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2899626244},
 pages = {1--89},
 title = {Mixing Time of Metropolis-Hastings for Bayesian Community Detection},
 url = {http://jmlr.org/papers/v22/18-770.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-780,
 abstract = {The problem of dimension reduction is of increasing importance in modern data analysis. In this paper, we consider modeling the collection of points in a high dimensional space as a union of low dimensional subspaces. In particular we propose a highly scalable sampling based algorithm that clusters the entire data via first spectral clustering of a small random sample followed by classifying or labeling the remaining out of sample points. The key idea is that this random subset borrows information across the entire data set and that the problem of clustering points can be replaced with the more efficient and robust problem of "clustering sub-clusters". We provide theoretical guarantees for our procedure. The numerical results indicate we outperform other state-of-the-art subspace clustering algorithms with respect to accuracy and speed.},
 author = {Weiwei Li and Jan Hannig and Sayan Mukherjee},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W3140866867},
 pages = {1--37},
 title = {Subspace Clustering through Sub-Clusters},
 url = {http://jmlr.org/papers/v22/18-780.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-798,
 abstract = {This paper presents new deviation inequalities that are valid uniformly in time under adaptive sampling in a multi-armed bandit model. The deviations are measured using the Kullback-Leibler divergence in a given one-dimensional exponential family, and may take into account several arms at a time. They are obtained by constructing for each arm a mixture martingale based on a hierarchical prior, and by multiplying those martingales. Our deviation inequalities allow us to analyze stopping rules based on generalized likelihood ratios for a large class of sequential identification problems, and to construct tight confidence intervals for some functions of the means of the arms.},
 author = {Emilie Kaufmann and Wouter M. Koolen},
 journal = {Journal of Machine Learning Research},
 number = {246},
 openalex = {W3215133059},
 pages = {1--44},
 title = {Mixture Martingales Revisited with Applications to Sequential Tests and Confidence Intervals},
 url = {http://jmlr.org/papers/v22/18-798.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-803,
 abstract = {Effective feature representation is key to the predictive performance of any algorithm. This paper introduces a meta-procedure, called Non-Euclidean Upgrading (NEU), which learns feature maps that are expressive enough to embed the universal approximation property (UAP) into most model classes while only outputting feature maps that preserve any model class's UAP. We show that NEU can learn any feature map with these two properties if that feature map is asymptotically deformable into the identity. We also find that the feature-representations learned by NEU are always submanifolds of the feature space. NEU's properties are derived from a new deep neural model that is universal amongst all orientation-preserving homeomorphisms on the input space. We derive qualitative and quantitative approximation guarantees for this architecture. We quantify the number of parameters required for this new architecture to memorize any set of input-output pairs while simultaneously fixing every point of the input space lying outside some compact set, and we quantify the size of this set as a function of our model's depth. Moreover, we show that no deep feed-forward network with commonly used activation function has all these properties. NEU's performance is evaluated against competing machine learning methods on various regression and dimension reduction tasks both with financial and simulated data.},
 author = {Anastasis Kratsios and Cody Hyndman},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W3172967783},
 pages = {1--51},
 title = {NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation},
 url = {http://jmlr.org/papers/v22/18-803.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-846,
 abstract = {Multidimensional unfolding methods are widely used for visualizing item response data. Such methods project respondents and items simultaneously onto a low-dimensional Euclidian space, in which respondents and items are represented by ideal points, with person-person, item-item, and person-item similarities being captured by the Euclidian distances between the points. In this paper, we study the visualization of multidimensional unfolding from a statistical perspective. We cast multidimensional unfolding into an estimation problem, where the respondent and item ideal points are treated as parameters to be estimated. An estimator is then proposed for the simultaneous estimation of these parameters. Asymptotic theory is provided for the recovery of the ideal points, shedding lights on the validity of model-based visualization. An alternating projected gradient descent algorithm is proposed for the parameter estimation. We provide two illustrative examples, one on users' movie rating and the other on senate roll call voting.},
 author = {Yunxiao Chen and Zhiliang Ying and Haoran Zhang},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W3198819368},
 pages = {1--51},
 title = {Unfolding-Model-Based Visualization: Theory, Method and Applications},
 url = {http://jmlr.org/papers/v22/18-846.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:18-863,
 abstract = {Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to empirically evaluate contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate various components of contextual bandit algorithm design such as loss estimators. Overall, this is a thorough study and review of contextual bandit methodology.},
 author = {Alberto Bietti and Alekh Agarwal and John Langford},
 journal = {Journal of Machine Learning Research},
 number = {133},
 openalex = {W2807644309},
 pages = {1--49},
 title = {A Contextual Bandit Bake-off},
 url = {http://jmlr.org/papers/v22/18-863.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-017,
 abstract = {A continuing challenge for machine learning is providing methods to perform computation on data while ensuring the data remains private. In this paper we build on the provable privacy guarantees of differential privacy which has been combined with Gaussian processes through the previously published \emph{cloaking method}. In this paper we solve several shortcomings of this method, starting with the problem of predictions in regions with low data density. We experiment with the use of inducing points to provide a sparse approximation and show that these can provide robust differential privacy in outlier areas and at higher dimensions. We then look at classification, and modify the Laplace approximation approach to provide differentially private predictions. We then combine this with the sparse approximation and demonstrate the capability to perform classification in high dimensions. We finally explore the issue of hyperparameter selection and develop a method for their private selection. This paper and associated libraries provide a robust toolkit for combining differential privacy and GPs in a practical manner.},
 author = {Michael Thomas Smith and Mauricio A. Alvarez and Neil D. Lawrence},
 journal = {Journal of Machine Learning Research},
 number = {188},
 openalex = {W2974658117},
 pages = {1--41},
 title = {Differentially Private Regression and Classification with Sparse Gaussian Processes},
 url = {http://jmlr.org/papers/v22/19-017.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-026,
 abstract = {Algorithms based on the hard thresholding principle have been well studied with sounding theoretical guarantees in the compressed sensing and more general sparsity-constrained optimization. It is widely observed in existing empirical studies that when a restricted Newton step was used (as the debiasing step), the hard-thresholding algorithms tend to meet halting conditions in a significantly low number of iterations and are very efficient. Hence, the thus obtained Newton hard-thresholding algorithms call for stronger theoretical guarantees than for their simple hard-thresholding counterparts. This paper provides a theoretical justification for the use of the restricted Newton step. We build our theory and algorithm, Newton Hard-Thresholding Pursuit (NHTP), for the sparsity-constrained optimization. Our main result shows that NHTP is quadratically convergent under the standard assumption of restricted strong convexity and smoothness. We also establish its global convergence to a stationary point under a weaker assumption. In the special case of the compressive sensing, NHTP effectively reduces to some of the existing hard-thresholding algorithms with a Newton step. Consequently, our fast convergence result justifies why those algorithms perform better than without the Newton step. The efficiency of NHTP was demonstrated on both synthetic and real data in compressed sensing and sparse logistic regression.},
 author = {Shenglong Zhou and Naihua Xiu and Hou-Duo Qi},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W3126727012},
 pages = {1--45},
 title = {Global and Quadratic Convergence of Newton Hard-Thresholding Pursuit},
 url = {http://jmlr.org/papers/v22/19-026.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-042,
 abstract = {Signal denoising-also known as non-parametric regression-is often performed through shrinkage estimation in a transformed (e.g., wavelet) domain; shrinkage in the transformed domain corresponds to smoothing in the original domain. A key question in such applications is how much to shrink, or, equivalently, how much to smooth. Empirical Bayes shrinkage methods provide an attractive solution to this problem; they use the data to estimate a distribution of underlying "effects," hence automatically select an appropriate amount of shrinkage. However, most existing implementations of empirical Bayes shrinkage are less flexible than they could be-both in their assumptions on the underlying distribution of effects, and in their ability to handle heteroskedasticity-which limits their signal denoising applications. Here we address this by adopting a particularly flexible, stable and computationally convenient empirical Bayes shrinkage method and applying it to several signal denoising problems. These applications include smoothing of Poisson data and heteroskedastic Gaussian data. We show through empirical comparisons that the results are competitive with other methods, including both simple thresholding rules and purpose-built empirical Bayes procedures. Our methods are implemented in the R package smashr, "SMoothing by Adaptive SHrinkage in R," available at https://www.github.com/stephenslab/smashr.},
 author = {Zhengrong Xing and Peter Carbonetto and Matthew Stephens},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W2914760527},
 pages = {1--28},
 title = {Flexible signal denoising via flexible empirical Bayes shrinkage},
 url = {http://jmlr.org/papers/v22/19-042.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-081,
 abstract = {Semi-supervised Laplacian regularization, a standard graph-based approach for learning from both labelled and unlabelled data, was recently demonstrated to have an insignificant high dimensional learning efficiency with respect to unlabelled data (Mai and Couillet 2018), causing it to be outperformed by its unsupervised counterpart, spectral clustering, given sufficient unlabelled data. Following a detailed discussion on the origin of this inconsistency problem, a novel regularization approach involving centering operation is proposed as solution, supported by both theoretical analysis and empirical results.},
 author = {Xiaoyi Mai and Romain Couillet},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W3034785407},
 pages = {1--48},
 title = {Consistent Semi-Supervised Graph Regularization for High Dimensional Data},
 url = {http://jmlr.org/papers/v22/19-081.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1004,
 abstract = {We propose a novel algorithm for large-scale regression problems named histogram transform ensembles (HTE), composed of random rotations, stretchings, and translations. First of all, we investigate the theoretical properties of HTE when the regression function lies in the H\"{o}lder space $C^{k,\alpha}$, $k \in \mathbb{N}_0$, $\alpha \in (0,1]$. In the case that $k=0, 1$, we adopt the constant regressors and develop the na\"{i}ve histogram transforms (NHT). Within the space $C^{0,\alpha}$, although almost optimal convergence rates can be derived for both single and ensemble NHT, we fail to show the benefits of ensembles over single estimators theoretically. In contrast, in the subspace $C^{1,\alpha}$, we prove that if $d \geq 2(1+\alpha)/\alpha$, the lower bound of the convergence rates for single NHT turns out to be worse than the upper bound of the convergence rates for ensemble NHT. In the other case when $k \geq 2$, the NHT may no longer be appropriate in predicting smoother regression functions. Instead, we apply kernel histogram transforms (KHT) equipped with smoother regressors such as support vector machines (SVMs), and it turns out that both single and ensemble KHT enjoy almost optimal convergence rates. Then we validate the above theoretical results by numerical experiments. On the one hand, simulations are conducted to elucidate that ensemble NHT outperform single NHT. On the other hand, the effects of bin sizes on accuracy of both NHT and KHT also accord with theoretical analysis. Last but not least, in the real-data experiments, comparisons between the ensemble KHT, equipped with adaptive histogram transforms, and other state-of-the-art large-scale regression estimators verify the effectiveness and accuracy of our algorithm.},
 author = {Hanyuan Hang and Zhouchen Lin and Xiaoyu Liu and Hongwei Wen},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W3168072388},
 pages = {1--87},
 title = {Histogram Transform Ensembles for Large-scale Regression},
 url = {http://jmlr.org/papers/v22/19-1004.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1006,
 author = {Xinming Yang and Lingrui Gan and Naveen N. Narisetty and Feng Liang},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W3151591352},
 pages = {1--48},
 title = {GemBag: Group Estimation of Multiple Bayesian Graphical Models},
 url = {http://jmlr.org/papers/v22/19-1006.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1012,
 abstract = {In mixed multi-view data, multiple sets of diverse features are measured on the same set of samples. By integrating all available data sources, we seek to discover common group structure among the samples that may be hidden in individualistic cluster analyses of a single data view. While several techniques for such integrative clustering have been explored, we propose and develop a convex formalization that enjoys strong empirical performance and inherits the mathematical properties of increasingly popular convex clustering methods. Specifically, our Integrative Generalized Convex Clustering Optimization (iGecco) method employs different convex distances, losses, or divergences for each of the different data views with a joint convex fusion penalty that leads to common groups. Additionally, integrating mixed multi-view data is often challenging when each data source is high-dimensional. To perform feature selection in such scenarios, we develop an adaptive shifted group-lasso penalty that selects features by shrinking them towards their loss-specific centers. Our so-called iGecco+ approach selects features from each data view that are best for determining the groups, often leading to improved integrative clustering. To solve our problem, we develop a new type of generalized multi-block ADMM algorithm using sub-problem approximations that more efficiently fits our model for big data sets. Through a series of numerical experiments and real data examples on text mining and genomics, we show that iGecco+ achieves superior empirical performance for high-dimensional mixed multi-view data.},
 author = {Minjie Wang and Genevera I. Allen},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W3148595015},
 pages = {1--73},
 title = {Integrative Generalized Convex Clustering Optimization and Feature Selection for Mixed Multi-View Data},
 url = {http://jmlr.org/papers/v22/19-1012.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1016,
 abstract = {We study the Personalized PageRank (PPR) algorithm, a local spectral method for clustering, which extracts clusters using locally-biased random walks around a given seed node. In contrast to previous work, we adopt a classical statistical learning setup, where we obtain samples from an unknown nonparametric distribution, and aim to identify sufficiently salient clusters. We introduce a trio of population-level functionals -- the normalized cut, conductance, and local spread, analogous to graph-based functionals of the same name -- and prove that PPR, run on a neighborhood graph, recovers clusters with small population normalized cut and large conductance and local spread. We apply our general theory to establish that PPR identifies connected regions of high density (density clusters) that satisfy a set of natural geometric conditions. We also show a converse result, that PPR can fail to recover geometrically poorly-conditioned density clusters, even asymptotically. Finally, we provide empirical support for our theory.},
 author = {Alden Green and Sivaraman Balakrishnan and Ryan J. Tibshirani},
 journal = {Journal of Machine Learning Research},
 number = {247},
 openalex = {W4288020404},
 pages = {1--71},
 title = {Statistical Guarantees for Local Spectral Clustering on Random Neighborhood Graphs},
 url = {http://jmlr.org/papers/v22/19-1016.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1018,
 author = {Di Xiao and Yuan Ke and Runze Li},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W3127128442},
 pages = {1--42},
 title = {Homogeneity Structure Learning in Large-scale Panel Data with Heavy-tailed Errors},
 url = {http://jmlr.org/papers/v22/19-1018.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1023,
 abstract = {We study a robust alternative to empirical risk minimization called distributionally robust learning (DRL), in which one learns to perform against an adversary who can choose the data distribution from a specified set of distributions. We illustrate a problem with current DRL formulations, which rely on an overly broad definition of allowed distributions for the adversary, leading to learned classifiers that are unable to predict with any confidence. We propose a solution that incorporates unlabeled data into the DRL problem to further constrain the adversary. We show that this new formulation is tractable for stochastic gradient-based optimization and yields a computable guarantee on the future performance of the learned classifier, analogous to -- but tighter than -- guarantees from conventional DRL. We examine the performance of this new formulation on 14 real datasets and find that it often yields effective classifiers with nontrivial performance guarantees in situations where conventional DRL produces neither. Inspired by these results, we extend our DRL formulation to active learning with a novel, distributionally-robust version of the standard model-change heuristic. Our active learning algorithm often achieves superior learning performance to the original heuristic on real datasets.},
 author = {Charlie Frogner and Sebastian Claici and Edward Chien and Justin Solomon},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2995472116},
 pages = {1--46},
 title = {Incorporating Unlabeled Data into Distributionally Robust Learning},
 url = {http://jmlr.org/papers/v22/19-1023.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1024,
 abstract = {We devise a learning algorithm for possibly nonsmooth deep neural networks featuring inertia and Newtonian directional intelligence only by means of a back-propagation oracle. Our algorithm, called INDIAN, has an appealing mechanical interpretation, making the role of its two hyperparameters transparent. An elementary phase space lifting allows both for its implementation and its theoretical study under very general assumptions. We handle in particular a stochastic version of our method (which encompasses usual mini-batch approaches) for nonsmooth activation functions (such as ReLU). Our algorithm shows high efficiency and reaches state of the art on image classification problems.},
 author = {Camille Castera and J√©r√¥me Bolte and C√©dric F√©votte and Edouard Pauwels},
 journal = {Journal of Machine Learning Research},
 number = {134},
 openalex = {W2947930523},
 pages = {1--31},
 title = {An Inertial Newton Algorithm for Deep Learning},
 url = {http://jmlr.org/papers/v22/19-1024.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1028,
 abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
 author = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W3150807214},
 pages = {1--64},
 title = {Normalizing Flows for Probabilistic Modeling and Inference},
 url = {http://jmlr.org/papers/v22/19-1028.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1048,
 abstract = {We consider distributed statistical optimization in one-shot setting, where there are $m$ machines each observing $n$ i.i.d. samples. Based on its observed samples, each machine sends a $B$-bit-long message to a server. The server then collects messages from all machines, and estimates a parameter that minimizes an expected convex loss function. We investigate the impact of communication constraint, $B$, on the expected error and derive a tight lower bound on the error achievable by any algorithm. We then propose an estimator, which we call Multi-Resolution Estimator (MRE), whose expected error (when $B\ge\log mn$) meets the aforementioned lower bound up to poly-logarithmic factors, and is thereby order optimal. We also address the problem of learning under tiny communication budget, and present lower and upper error bounds when $B$ is a constant. The expected error of MRE, unlike existing algorithms, tends to zero as the number of machines ($m$) goes to infinity, even when the number of samples per machine ($n$) remains upper bounded by a constant. This property of the MRE algorithm makes it applicable in new machine learning paradigms where $m$ is much larger than $n$.},
 author = {Saber Salehkaleybar and Arsalan Sharifnassab and S. Jamaloddin Golestani},
 journal = {Journal of Machine Learning Research},
 number = {189},
 openalex = {W3207236107},
 pages = {1--47},
 title = {One-Shot Federated Learning: Theoretical Limits and Algorithms to Achieve Them},
 url = {http://jmlr.org/papers/v22/19-1048.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-1049,
 abstract = {We consider a discrete optimization formulation for learning sparse classifiers, where the outcome depends upon a linear combination of a small subset of features. Recent work has shown that mixed integer programming (MIP) can be used to solve (to optimality) $\ell_0$-regularized regression problems at scales much larger than what was conventionally considered possible. Despite their usefulness, MIP-based global optimization approaches are significantly slower compared to the relatively mature algorithms for $\ell_1$-regularization and heuristics for nonconvex regularized problems. We aim to bridge this gap in computation times by developing new MIP-based algorithms for $\ell_0$-regularized classification. We propose two classes of scalable algorithms: an exact algorithm that can handle $p\approx 50,000$ features in a few minutes, and approximate algorithms that can address instances with $p\approx 10^6$ in times comparable to the fast $\ell_1$-based algorithms. Our exact algorithm is based on the novel idea of \textsl{integrality generation}, which solves the original problem (with $p$ binary variables) via a sequence of mixed integer programs that involve a small number of binary variables. Our approximate algorithms are based on coordinate descent and local combinatorial search. In addition, we present new estimation error bounds for a class of $\ell_0$-regularized estimators. Experiments on real and synthetic data demonstrate that our approach leads to models with considerably improved statistical performance (especially, variable selection) when compared to competing methods.},
 author = {Antoine Dedieu and Hussein Hazimeh and Rahul Mazumder},
 journal = {Journal of Machine Learning Research},
 number = {135},
 openalex = {W3187377294},
 pages = {1--47},
 title = {Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives},
 url = {http://jmlr.org/papers/v22/19-1049.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-132,
 abstract = {The focus of modern biomedical studies has gradually shifted to explanation and estimation of joint effects of high dimensional predictors on disease risks. Quantifying uncertainty in these estimates may provide valuable insight into prevention strategies or treatment decisions for both patients and physicians. High dimensional inference, including confidence intervals and hypothesis testing, has sparked much interest. While much work has been done in the linear regression setting, there is lack of literature on inference for high dimensional generalized linear models. We propose a novel and computationally feasible method, which accommodates a variety of outcome types, including normal, binomial, and Poisson data. We use a "splitting and smoothing" approach, which splits samples into two parts, performs variable selection using one part and conducts partial regression with the other part. Averaging the estimates over multiple random splits, we obtain the smoothed estimates, which are numerically stable. We show that the estimates are consistent, asymptotically normal, and construct confidence intervals with proper coverage probabilities for all predictors. We examine the finite sample performance of our method by comparing it with the existing methods and applying it to analyze a lung cancer cohort study.},
 author = {Zhe Fei and Yi Li},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W3150539639},
 pages = {1--32},
 title = {Estimation and Inference for High Dimensional Generalized Linear Models: A Splitting and Smoothing Approach},
 url = {http://jmlr.org/papers/v22/19-132.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-149,
 abstract = {We provide high-probability sample complexity guarantees for exact structure recovery and accurate predictive learning using noise-corrupted samples from an acyclic (tree-shaped) graphical model. The hidden variables follow a tree-structured Ising model distribution, whereas the observable variables are generated by a binary symmetric channel taking the hidden variables as its input (flipping each bit independently with some constant probability $q\in [0,1/2)$). In the absence of noise, predictive learning on Ising models was recently studied by Bresler and Karzand (2020); this paper quantifies how noise in the hidden model impacts the tasks of structure recovery and marginal distribution estimation by proving upper and lower bounds on the sample complexity. Our results generalize state-of-the-art bounds reported in prior work, and they exactly recover the noiseless case ($q=0$). In fact, for any tree with $p$ vertices and probability of incorrect recovery $\delta>0$, the sufficient number of samples remains logarithmic as in the noiseless case, i.e., $\mathcal{O}(\log(p/\delta))$, while the dependence on $q$ is $\mathcal{O}\big( 1/(1-2q)^{4} \big)$, for both aforementioned tasks. We also present a new equivalent of Isserlis' Theorem for sign-valued tree-structured distributions, yielding a new low-complexity algorithm for higher-order moment estimation.},
 author = {Konstantinos E. Nikolakakis and Dionysios S. Kalogerias and Anand D. Sarwate},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W3158548902},
 pages = {1--82},
 title = {Predictive Learning on Hidden Tree-Structured Ising Models},
 url = {http://jmlr.org/papers/v22/19-149.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-228,
 abstract = {We study the problem of finding the optimal dosage in early stage clinical trials through the multi-armed bandit lens. We advocate the use of the Thompson Sampling principle, a flexible algorithm that can accommodate different types of monotonicity assumptions on the toxicity and efficacy of the doses. For the simplest version of Thompson Sampling, based on a uniform prior distribution for each dose, we provide finite-time upper bounds on the number of sub-optimal dose selections, which is unprecedented for dose-finding algorithms. Through a large simulation study, we then show that variants of Thompson Sampling based on more sophisticated prior distributions outperform state-of-the-art dose identification algorithms in different types of dose-finding studies that occur in phase I or phase I/II trials.},
 author = {Maryam Aziz and Emilie Kaufmann and Marie-Karelle Riviere},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W3015364209},
 pages = {1--38},
 title = {On Multi-Armed Bandit Designs for Dose-Finding Trials},
 url = {http://jmlr.org/papers/v22/19-228.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-292,
 abstract = {For sampling from a log-concave density, we study implicit integrators resulting from $\theta$-method discretization of the overdamped Langevin diffusion stochastic differential equation. Theoretical and algorithmic properties of the resulting sampling methods for $ \theta \in [0,1] $ and a range of step sizes are established. Our results generalize and extend prior works in several directions. In particular, for $\theta\ge1/2$, we prove geometric ergodicity and stability of the resulting methods for all step sizes. We show that obtaining subsequent samples amounts to solving a strongly-convex optimization problem, which is readily achievable using one of numerous existing methods. Numerical examples supporting our theoretical analysis are also presented.},
 author = {Liam Hodgkinson and Robert Salomone and Fred Roosta},
 journal = {Journal of Machine Learning Research},
 number = {136},
 openalex = {W2930583881},
 pages = {1--30},
 title = {Implicit Langevin Algorithms for Sampling From Log-concave Densities},
 url = {http://jmlr.org/papers/v22/19-292.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-325,
 abstract = {Interpretable machine learning has become a strong competitor for traditional black-box models. However, the possible loss of the predictive performance for gaining interpretability is often inevitable, putting practitioners in a dilemma of choosing between high accuracy (black-box models) and interpretability (interpretable models). In this work, we propose a novel framework for building a Hybrid Predictive Model (HPM) that integrates an interpretable model with any black-box model to combine their strengths. The interpretable model substitutes the black-box model on a subset of data where the black-box is overkill or nearly overkill, gaining transparency at no or low cost of the predictive accuracy. We design a principled objective function that considers predictive accuracy, model interpretability, and model transparency (defined as the percentage of data processed by the interpretable substitute.) Under this framework, we propose two hybrid models, one substituting with association rules and the other with linear models, and we design customized training algorithms for both models. We test the hybrid models on structured data and text data where interpretable models collaborate with various state-of-the-art black-box models. Results show that hybrid models obtain an efficient trade-off between transparency and predictive performance, characterized by our proposed efficient frontiers.},
 author = {Tong Wang and Qihang Lin},
 journal = {Journal of Machine Learning Research},
 number = {137},
 openalex = {W2943982549},
 pages = {1--38},
 title = {Hybrid Predictive Model: When an Interpretable Model Collaborates with a Black-box Model},
 url = {http://jmlr.org/papers/v22/19-325.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-345,
 abstract = {Stratified models are models that depend in an arbitrary way on a set of selected categorical features, and depend linearly on the other features. In a basic and traditional formulation a separate model is fit for each value of the categorical feature, using only the data that has the specific categorical value. To this formulation we add Laplacian regularization, which encourages the model parameters for neighboring categorical values to be similar. Laplacian regularization allows us to specify one or more weighted graphs on the stratification feature values. For example, stratifying over the days of the week, we can specify that the Sunday model parameter should be close to the Saturday and Monday model parameters. The regularization improves the performance of the model over the traditional stratified model, since the model for each value of the categorical `borrows strength' from its neighbors. In particular, it produces a model even for categorical values that did not appear in the training data set. 
We propose an efficient distributed method for fitting stratified models, based on the alternating direction method of multipliers (ADMM). When the fitting loss functions are convex, the stratified model fitting problem is convex, and our method computes the global minimizer of the loss plus regularization; in other cases it computes a local minimizer. The method is very efficient, and naturally scales to large data sets or numbers of stratified feature values. We illustrate our method with a variety of examples.},
 author = {Jonathan Tuck and Shane Barratt and Stephen Boyd},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W3145959995},
 pages = {1--37},
 title = {A Distributed Method for Fitting Laplacian Regularized Stratified Models},
 url = {http://jmlr.org/papers/v22/19-345.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-364,
 abstract = {Efficient explorative data analysis systems must take into account both what a user knows and wants to know. This paper proposes a principled framework for interactive visual exploration of relations in data, through views most informative given the user's current knowledge and objectives. The user can input pre-existing knowledge of relations in the data and also formulate specific exploration interests, which are then taken into account in the exploration. The idea is to steer the exploration process towards the interests of the user, instead of showing uninteresting or already known relations. The user's knowledge is modelled by a distribution over data sets parametrised by subsets of rows and columns of data, called tile constraints. We provide a computationally efficient implementation of this concept based on constrained randomisation. Furthermore, we describe a novel dimensionality reduction method for finding the views most informative to the user, which at the limit of no background knowledge and with generic objectives reduces to PCA. We show that the method is suitable for interactive use and is robust to noise, outperforms standard projection pursuit visualisation methods, and gives understandable and useful results in analysis of real-world data. We provide an open-source implementation of the framework.},
 author = {Kai Puolam√§ki and Emilia Oikarinen and Andreas Henelius},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W3167423243},
 pages = {1--32},
 title = {Guided Visual Exploration of Relations in Data Sets},
 url = {http://jmlr.org/papers/v22/19-364.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-372,
 abstract = {This work describes simple and efficient algorithms for interactively learning non-binary concepts in the learning from random counter-examples (LRC) model. Here, learning takes place from random counter-examples that the learner receives in response to their proper equivalence queries. In this context, the learning time is defined as the number of counter-examples needed by the learner to identify the target concept. Such learning is particularly suited for online ranking, classification, clustering, etc., where machine learning models must be used before they are fully trained. We provide two simple LRC algorithms, deterministic and randomized, for exactly learning non-binary target concepts for any concept class $H$. We show that both of these algorithms have an $\mathcal{O}(\log{}|H|)$ asymptotically optimal average learning time. This solves an open problem on the existence of an efficient LRC randomized algorithm while simplifying and generalizing previous results. We also show that the expected learning time of any arbitrary LRC algorithm can be upper bounded by $\mathcal{O}(\frac{1}Œµ\log{\frac{|H|}Œ¥})$, where $Œµ$ and $Œ¥$ are the allowed learning error and failure probability respectively. This shows that LRC interactive learning is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning. Our simulations show that in practice, these algorithms outperform their theoretical bounds.},
 author = {Jagdeep Singh Bhatia},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2997227286},
 pages = {1--30},
 title = {Simple and Fast Algorithms for Interactive Machine Learning with Random Counter-examples},
 url = {http://jmlr.org/papers/v22/19-372.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-373,
 author = {Changyue Song and Kaibo Liu and Xi Zhang},
 journal = {Journal of Machine Learning Research},
 number = {190},
 openalex = {W3205711048},
 pages = {1--45},
 title = {Collusion Detection and Ground Truth Inference in Crowdsourcing for Labeling Tasks},
 url = {http://jmlr.org/papers/v22/19-373.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-418,
 abstract = {In this paper we consider the problem of maximizing the Area under the ROC curve (AUC) which is a widely used performance metric in imbalanced classification and anomaly detection. Due to the pairwise nonlinearity of the objective function, classical SGD algorithms do not apply to the task of AUC maximization. We propose a novel stochastic proximal algorithm for AUC maximization which is scalable to large scale streaming data. Our algorithm can accommodate general penalty terms and is easy to implement with favorable $O(d)$ space and per-iteration time complexities. We establish a high-probability convergence rate $O(1/\sqrt{T})$ for the general convex setting, and improve it to a fast convergence rate $O(1/T)$ for the cases of strongly convex regularizers and no regularization term (without strong convexity). Our proof does not need the uniform boundedness assumption on the loss function or the iterates which is more fidelity to the practice. Finally, we perform extensive experiments over various benchmark data sets from real-world application domains which show the superior performance of our algorithm over the existing AUC maximization algorithms.},
 author = {Yunwen Lei and Yiming Ying},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W3148827437},
 pages = {1--45},
 title = {Stochastic Proximal AUC Maximization},
 url = {http://jmlr.org/papers/v22/19-418.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-433,
 abstract = {Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at https://github.com/Sujit-O/pykg2vec.},
 author = {Shih-Yuan Yu and Sujit Rokka Chhetri and Arquimedes Canedo and Palash Goyal and Mohammad Abdullah Al Faruque},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W4288336232},
 pages = {1--6},
 title = {Pykg2vec: A Python Library for Knowledge Graph Embedding},
 url = {http://jmlr.org/papers/v22/19-433.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-466,
 abstract = {Gradient descent-based optimization methods underpin the parameter training of neural networks, and hence comprise a significant component in the impressive test results found in a number of applications. Introducing stochasticity is key to their success in practical problems, and there is some understanding of the role of stochastic gradient descent in this context. Momentum modifications of gradient descent such as Polyak's Heavy Ball method (HB) and Nesterov's method of accelerated gradients (NAG), are also widely adopted. In this work our focus is on understanding the role of momentum in the training of neural networks, concentrating on the common situation in which the momentum contribution is fixed at each step of the algorithm. To expose the ideas simply we work in the deterministic setting. 
Our approach is to derive continuous time approximations of the discrete algorithms; these continuous time approximations provide insights into the mechanisms at play within the discrete algorithms. We prove three such approximations. Firstly we show that standard implementations of fixed momentum methods approximate a time-rescaled gradient descent flow, asymptotically as the learning rate shrinks to zero; this result does not distinguish momentum methods from pure gradient descent, in the limit of vanishing learning rate. We then proceed to prove two results aimed at understanding the observed practical advantages of fixed momentum methods over gradient descent. We achieve this by proving approximations to continuous time limits in which the small but fixed learning rate appears as a parameter. Furthermore in a third result we show that the momentum methods admit an exponentially attractive invariant manifold on which the dynamics reduces, approximately, to a gradient flow with respect to a modified loss function.},
 author = {Nikola B. Kovachki and Andrew M. Stuart},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W3129088206},
 pages = {1--40},
 title = {Continuous Time Analysis of Momentum Methods},
 url = {http://jmlr.org/papers/v22/19-466.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-477,
 author = {Yunzhang Zhu and Renxiong Liu},
 journal = {Journal of Machine Learning Research},
 number = {138},
 openalex = {W3187404668},
 pages = {1--62},
 title = {An algorithmic view of L2 regularization and some path-following algorithms},
 url = {http://jmlr.org/papers/v22/19-477.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-479,
 abstract = {This paper establishes Hoeffding's lemma and inequality for bounded functions of general-state-space and not necessarily reversible Markov chains. The sharpness of these results is characterized by the optimality of the ratio between variance proxies in the Markov-dependent and independent settings. The boundedness of functions is shown necessary for such results to hold in general. To showcase the usefulness of the new results, we apply them for non-asymptotic analyses of MCMC estimation, respondent-driven sampling and high-dimensional covariance matrix estimation on time series data with a Markovian nature. In addition to statistical problems, we also apply them to study the time-discounted rewards in econometric models and the multi-armed bandit problem with Markovian rewards arising from the field of machine learning.},
 author = {Jianqing Fan and Bai Jiang and Qiang Sun},
 journal = {Journal of Machine Learning Research},
 number = {139},
 openalex = {W3192653763},
 pages = {1--35},
 title = {Hoeffding's inequality for general Markov chains with its applications to statistical learning.},
 url = {http://jmlr.org/papers/v22/19-479.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-482,
 abstract = {This paper generalizes regularized regression problems in a hyper-reproducing kernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning and out-of-sample extensions, and proves asymptotic convergence results for the introduced regression models in an approximation theory view. Algorithmically, we consider two regularized regression models with bivariate forms in this space, including kernel ridge regression (KRR) and support vector regression (SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with Nystr\"{o}m approximation for scalability in large sample cases. This framework is general: the underlying kernel is learned from a broad class, and can be positive definite or not, which adapts to various requirements in kernel learning. Theoretically, we study the convergence behavior of regularized regression algorithms in hyper-RKHS and derive the learning rates, which goes beyond the classical analysis on RKHS due to the non-trivial independence of pairwise samples and the characterisation of hyper-RKHS. Experimentally, results on several benchmarks suggest that the employed framework is able to learn a general kernel function form an arbitrary similarity matrix, and thus achieves a satisfactory performance on classification tasks.},
 author = {Fanghui Liu and Lei Shi and Xiaolin Huang and Jie Yang and Johan A.K. Suykens},
 journal = {Journal of Machine Learning Research},
 number = {140},
 openalex = {W3193015206},
 pages = {1--38},
 title = {Generalization Properties of hyper-RKHS and its Applications},
 url = {http://jmlr.org/papers/v22/19-482.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-486,
 abstract = {Bayesian inference in the presence of an intractable likelihood function is computationally challenging. When following a Markov chain Monte Carlo (MCMC) approach to approximate the posterior distribution in this context, one typically either uses MCMC schemes which target the joint posterior of the parameters and some auxiliary latent variables, or pseudo-marginal Metropolis--Hastings (MH) schemes. The latter mimic a MH algorithm targeting the marginal posterior of the parameters by approximating unbiasedly the intractable likelihood. However, in scenarios where the parameters and auxiliary variables are strongly correlated under the posterior and/or this posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme, will be inefficient for high dimensional parameters. We propose here an original MCMC algorithm, termed pseudo-marginal HMC, which combines the advantages of both HMC and pseudo-marginal schemes. Specifically, the pseudo-marginal HMC method is controlled by a precision parameter N, controlling the approximation of the likelihood and, for any N, it samples the marginal posterior of the parameters. Additionally, as N tends to infinity, its sample trajectories and acceptance probability converge to those of an ideal, but intractable, HMC algorithm which would have access to the marginal posterior of parameters and its gradient. We demonstrate through experiments that pseudo-marginal HMC can outperform significantly both standard HMC and pseudo-marginal MH schemes.},
 author = {Johan Alenl√∂v and Arnoud Doucet and Fredrik Lindsten},
 journal = {Journal of Machine Learning Research},
 number = {141},
 openalex = {W2503209911},
 pages = {1--45},
 title = {Pseudo-Marginal Hamiltonian Monte Carlo},
 url = {http://jmlr.org/papers/v22/19-486.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-498,
 author = {Gaoxia Jiang and Wenjian Wang and Yuhua Qian and Jiye Liang},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W3126316690},
 pages = {1--66},
 title = {A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective},
 url = {http://jmlr.org/papers/v22/19-498.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-531,
 abstract = {Detecting when the underlying distribution changes for the observed time series is a fundamental problem arising in a broad spectrum of applications. In this paper, we study multiple change-point localization in the high-dimensional regression setting, which is particularly challenging as no direct observations of the parameter of interest is available. Specifically, we assume we observe $\{ x_t, y_t\}_{t=1}^n$ where $ \{ x_t\}_{t=1}^n $ are $p$-dimensional covariates, $\{y_t\}_{t=1}^n$ are the univariate responses satisfying $\mathbb{E}(y_t) = x_t^\top \beta_t^* \text{ for } 1\le t \le n $ and $\{\beta_t^*\}_{t=1}^n $ are the unobserved regression coefficients that change over time in a piecewise constant manner. We propose a novel projection-based algorithm, Variance Projected Wild Binary Segmentation~(VPWBS), which transforms the original (difficult) problem of change-point detection in $p$-dimensional regression to a simpler problem of change-point detection in mean of a one-dimensional time series. VPWBS is shown to achieve sharp localization rate $O_p(1/n)$ up to a log factor, a significant improvement from the best rate $O_p(1/\sqrt{n})$ known in the existing literature for multiple change-point localization in high-dimensional regression. Extensive numerical experiments are conducted to demonstrate the robust and favorable performance of VPWBS over two state-of-the-art algorithms, especially when the size of change in the regression coefficients $\{\beta_t^*\}_{t=1}^n $ is small.},
 author = {Daren Wang and Zifeng Zhao and Kevin Z. Lin and Rebecca Willett},
 journal = {Journal of Machine Learning Research},
 number = {248},
 openalex = {W2955344082},
 pages = {1--46},
 title = {Statistically and Computationally Efficient Change Point Localization in Regression Settings},
 url = {http://jmlr.org/papers/v22/19-531.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-542,
 abstract = {Given a measurement graph $G= (V,E)$ and an unknown signal $r \in \mathbb{R}^n$, we investigate algorithms for recovering $r$ from pairwise measurements of the form $r_i - r_j$; $\{i,j\} \in E$. This problem arises in a variety of applications, such as ranking teams in sports data and time synchronization of distributed networks. Framed in the context of ranking, the task is to recover the ranking of $n$ teams (induced by $r$) given a small subset of noisy pairwise rank offsets. We propose a simple SVD-based algorithmic pipeline for both the problem of time synchronization and ranking. We provide a detailed theoretical analysis in terms of robustness against both sampling sparsity and noise perturbations with outliers, using results from matrix perturbation and random matrix theory. Our theoretical findings are complemented by a detailed set of numerical experiments on both synthetic and real data, showcasing the competitiveness of our proposed algorithms with other state-of-the-art methods.},
 author = {Alexandre d'Aspremont and Mihai Cucuringu and Hemant Tyagi},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2948065064},
 pages = {1--63},
 title = {Ranking and synchronization from pairwise measurements via SVD},
 url = {http://jmlr.org/papers/v22/19-542.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-558,
 abstract = {The development of models and methodology for the analysis of data from multiple heterogeneous networks is of importance both in statistical network theory and across a wide spectrum of application domains. Although single-graph analysis is well-studied, multiple graph inference is largely unexplored, in part because of the challenges inherent in appropriately modeling graph differences and yet retaining sufficient model simplicity to render estimation feasible. This paper addresses exactly this gap, by introducing a new model, the common subspace independent-edge multiple random graph model, which describes a heterogeneous collection of networks with a shared latent structure on the vertices but potentially different connectivity patterns for each graph. The model encompasses many popular network representations, including the stochastic blockmodel. The model is both flexible enough to meaningfully account for important graph differences, and tractable enough to allow for accurate inference in multiple networks. In particular, a joint spectral embedding of adjacency matrices-the multiple adjacency spectral embedding-leads to simultaneous consistent estimation of underlying parameters for each graph. Under mild additional assumptions, the estimates satisfy asymptotic normality and yield improvements for graph eigenvalue estimation. In both simulated and real data, the model and the embedding can be deployed for a number of subsequent network inference tasks, including dimensionality reduction, classification, hypothesis testing, and community detection. Specifically, when the embedding is applied to a data set of connectomes constructed through diffusion magnetic resonance imaging, the result is an accurate classification of brain scans by human subject and a meaningful determination of heterogeneity across scans of different individuals.},
 author = {Jes√∫s Arroyo and Avanti Athreya and Joshua Cape and Guodong Chen and Carey E. Priebe and Joshua T. Vogelstein},
 journal = {Journal of Machine Learning Research},
 number = {142},
 openalex = {W3189983173},
 pages = {1--49},
 title = {Inference for Multiple Heterogeneous Networks with a Common Invariant Subspace},
 url = {http://jmlr.org/papers/v22/19-558.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-586,
 abstract = {Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e., non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks of decreasing width after the wide layer, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.},
 author = {Henning Petzka and Cristian Sminchisescu},
 journal = {Journal of Machine Learning Research},
 number = {143},
 openalex = {W3191053428},
 pages = {1--34},
 title = {Non-attracting Regions of Local Minima in Deep and Wide Neural Networks},
 url = {http://jmlr.org/papers/v22/19-586.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-600,
 author = {Mariusz Kubkowski and Jan Mielniczuk and Pawe≈Ç Teisseyre},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W3144981492},
 pages = {1--57},
 title = {How to Gain on Power: Novel Conditional Independence Tests Based on Short Expansion of Conditional Mutual Information},
 url = {http://jmlr.org/papers/v22/19-600.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-624,
 abstract = {Aggregated hold-out (Agghoo) is a method which averages learning rules selected by hold-out (that is, cross-validation with a single split). We provide the first theoretical guarantees on Agghoo, ensuring that it can be used safely: Agghoo performs at worst like the hold-out when the risk is convex. The same holds true in classification with the 0-1 risk, with an additional constant factor. For the hold-out, oracle inequalities are known for bounded losses, as in binary classification. We show that similar results can be proved, under appropriate assumptions, for other risk-minimization problems. In particular, we obtain an oracle inequality for regularized kernel regression with a Lip-schitz loss, without requiring that the Y variable or the regressors be bounded. Numerical experiments show that aggregation brings a significant improvement over the hold-out and that Agghoo is competitive with cross-validation.},
 author = {Guillaume Maillard and Sylvain Arlot and Matthieu Lerasle},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W3127360146},
 pages = {1--55},
 title = {Aggregated Hold-Out},
 url = {http://jmlr.org/papers/v22/19-624.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-629,
 abstract = {We consider the problem of computing a Wasserstein barycenter for a set of discrete probability distributions with finite supports, which finds many applications in areas such as statistics, machine learning and image processing. When the support points of the barycenter are pre-specified, this problem can be modeled as a linear programming (LP) problem whose size can be extremely large. To handle this large-scale LP, we analyse the structure of its dual problem, which is conceivably more tractable and can be reformulated as a well-structured convex problem with 3 kinds of block variables and a coupling linear equality constraint. We then adapt a symmetric Gauss-Seidel based alternating direction method of multipliers (sGS-ADMM) to solve the resulting dual problem and establish its global convergence and global linear convergence rate. As a critical component for efficient computation, we also show how all the subproblems involved can be solved exactly and efficiently. This makes our method suitable for computing a Wasserstein barycenter on a large-scale data set, without introducing an entropy regularization term as is commonly practiced. In addition, our sGS-ADMM can be used as a subroutine in an alternating minimization method to compute a barycenter when its support points are not pre-specified. Numerical results on synthetic data sets and image data sets demonstrate that our method is highly competitive for solving large-scale Wasserstein barycenter problems, in comparison to two existing representative methods and the commercial software Gurobi.},
 author = {Lei Yang and Jia Li and Defeng Sun and Kim-Chuan Toh},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2890430524},
 pages = {1--37},
 title = {A Fast Globally Linearly Convergent Algorithm for the Computation of Wasserstein Barycenters},
 url = {http://jmlr.org/papers/v22/19-629.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-630,
 abstract = {Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple two-class Stochastic Blockmodel (SBM) with equal class sizes. Using batch co-ordinate ascent (BCAVI) for updates, we show different convergence behavior with respect to different initializations. When the parameters are known or estimated within a reasonable range and held fixed, we characterize conditions under which an initialization can converge to the ground truth. On the other hand, when the parameters need to be estimated iteratively, a random initialization will converge to an uninformative local optimum.},
 author = {Purnamrita Sarkar and Y. X. Rachel Wang and Soumendu S. Mukherjee},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2945836068},
 pages = {1--46},
 title = {When random initializations help: a study of variational inference for community detection},
 url = {http://jmlr.org/papers/v22/19-630.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-632,
 author = {Giulio Galvan and Matteo Lapucci and Chih-Jen Lin and Marco Sciandrone},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W3128312899},
 pages = {1--38},
 title = {A Two-Level Decomposition Framework Exploiting First and Second Order Information for SVM Training Problems},
 url = {http://jmlr.org/papers/v22/19-632.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-658,
 abstract = {Since many critical decisions impacting human lives are increasingly being made by algorithms, it is important to ensure that the treatment of individuals under such algorithms is demonstrably fair under reasonable notions of fairness. One compelling notion proposed in the literature is that of individual fairness (IF), which advocates that similar individuals should be treated similarly (Dwork et al. 2012). Originally proposed for offline decisions, this notion does not, however, account for temporal considerations relevant for online decision-making. In this paper, we extend the notion of IF to account for the time at which a decision is made, in settings where there exists a notion of conduciveness of decisions as perceived by the affected individuals. We introduce two definitions: (i) fairness-across-time (FT) and (ii) fairness-in-hindsight (FH). FT is the simplest temporal extension of IF where treatment of individuals is required to be individually fair relative to the past as well as future, while in FH, we require a one-sided notion of individual fairness that is defined relative to only the past decisions. We show that these two definitions can have drastically different implications in the setting where the principal needs to learn the utility model. Linear regret relative to optimal individually fair decisions is inevitable under FT for non-trivial examples. On the other hand, we design a new algorithm: Cautious Fair Exploration (CAFE), which satisfies FH and achieves sub-linear regret guarantees for a broad range of settings. We characterize lower bounds showing that these guarantees are order-optimal in the worst case. FH can thus be embedded as a primary safeguard against unfair discrimination in algorithmic deployments, without hindering the ability to take good decisions in the long-run.},
 author = {Swati Gupta and Vijay Kamble},
 journal = {Journal of Machine Learning Research},
 number = {144},
 openalex = {W2911749777},
 pages = {1--35},
 title = {Individual Fairness in Hindsight},
 url = {http://jmlr.org/papers/v22/19-658.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-665,
 abstract = {We consider the problem of operator-valued kernel learning and investigate the possibility of going beyond the well-known separable kernels. Borrowing tools and concepts from the field of quantum computing, such as partial trace and entanglement, we propose a new view on operator-valued kernels and define a general family of kernels that encompasses previously known operator-valued kernels, including separable and transformable kernels. Within this framework, we introduce another novel class of operator-valued kernels called entangled kernels that are not separable. We propose an efficient two-step algorithm for this framework, where the entangled kernel is learned based on a novel extension of kernel alignment to operator-valued kernels. We illustrate our algorithm with an application to supervised dimensionality reduction, and demonstrate its effectiveness with both artificial and real data for multi-output regression.},
 author = {Riikka Huusari and Hachem Kadri},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W4300003842},
 pages = {1--40},
 title = {Entangled Kernels -- Beyond Separability},
 url = {http://jmlr.org/papers/v22/19-665.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-683,
 abstract = {We analyze the spectral clustering procedure for identifying coarse structure in a data set $x_1, \dots, x_n$, and in particular study the geometry of graph Laplacian embeddings which form the basis for spectral clustering algorithms. More precisely, we assume that the data is sampled from a mixture model supported on a manifold $\mathcal{M}$ embedded in $\mathbb{R}^d$, and pick a connectivity length-scale $\varepsilon&gt;0$ to construct a kernelized graph Laplacian. We introduce a notion of a well-separated mixture model which only depends on the model itself, and prove that when the model is well separated, with high probability the embedded data set concentrates on cones that are centered around orthogonal vectors. Our results are meaningful in the regime where $\varepsilon = \varepsilon(n)$ is allowed to decay to zero at a slow enough rate as the number of data points grows. This rate depends on the intrinsic dimension of the manifold on which the data is supported.},
 author = {Nicol√°s Garc√≠a Trillos and Franca Hoffmann and Bamdad Hosseini},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2935826133},
 pages = {1--55},
 title = {Geometric structure of graph Laplacian embeddings},
 url = {http://jmlr.org/papers/v22/19-683.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-707,
 author = {Alberto Maria Metelli and Matteo Pirotta and Daniele Calandriello and Marcello Restelli},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W3104139512},
 pages = {1--83},
 title = {Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach},
 url = {http://jmlr.org/papers/v22/19-707.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-716,
 author = {Yunwen Lei and Ting Hu and Ke Tang},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W3128680160},
 pages = {1--41},
 title = {Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions},
 url = {http://jmlr.org/papers/v22/19-716.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-725,
 author = {Tuhin Sarkar and Alexander Rakhlin and Munther A. Dahleh},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W3128922944},
 pages = {1--61},
 title = {Finite Time LTI System Identification.},
 url = {http://jmlr.org/papers/v22/19-725.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-736,
 abstract = {Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution or how they cope with approximation error due to using a restricted class of parametric policies. This work provides provable characterizations of the computational, approximation, and sample size properties of policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: tabular policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy; and parametric policy classes (considering both log-linear and neural policy classes), which may not contain the optimal policy and where we provide agnostic learning results. One central contribution of this work is in providing approximation guarantees that are average case -- which avoid explicit worst-case dependencies on the size of state space -- by making a formal connection to supervised learning under distribution shift. This characterization shows an important interplay between estimation error, approximation error, and exploration (as characterized through a precisely defined condition number).},
 author = {Alekh Agarwal and Sham M. Kakade and Jason D. Lee and Gaurav Mahajan},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W3039845099},
 pages = {1--76},
 title = {On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift},
 url = {http://jmlr.org/papers/v22/19-736.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-744,
 abstract = {We consider the problem of statistical inference for a finite number of covariates in a generalized single-index model with p > n covariates and unknown (potentially random) link function under an elliptically symmetric design. Under elliptical symmetry, the problem can be reformulated as a proxy linear model in terms of an identifiable parameter, which characterization is then used to construct estimates of the regression coefficients of interest that are similar to the de-biased lasso estimates in the standard linear model and exhibit similar properties: square-root consistency and asymptotic normality. The procedure is agnostic in the sense that it completely bypasses the estimation of the link function, which can be extremely challenging depending on the underlying structure of the problem. Our method allows testing for the importance of pre-fixed covariates in the single-index model, as well as testing for the relative importance of coefficients via straightforward application of the delta method. Furthermore, under Gaussianity, we extend our approach to prescribe improved, i.e., more efficient estimates of the coefficients using a sieved strategy that involves an expansion of the true regression function in terms of Hermite polynomials. Finally, we illustrate our approach via carefully designed simulation experiments.},
 author = {Hamid Eftekhari and Moulinath Banerjee and Ya'acov Ritov},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W3127194541},
 pages = {1--63},
 title = {Inference In High-dimensional Single-Index Models Under Symmetric Designs},
 url = {http://jmlr.org/papers/v22/19-744.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-747,
 abstract = {Network complexity has been studied for over half a century and has found a wide range of applications. Many methods have been developed to characterize and estimate the complexity of networks. However, there has been little research with statistical guarantees. In this paper, we develop a statistical theory of graph complexity in a general model of random graphs, the so-called graphon model. Given a graphon, we endow the latent space of the nodes with the neighborhood distance that measures the propensity of two nodes to be connected with similar nodes. Our complexity index is then based on the covering number and the Minkowski dimension of (a purified version of) this metric space. Although the latent space is not identifiable, these indices turn out to be identifiable. This notion of complexity has simple interpretations on popular examples of random graphs: it matches the number of communities in stochastic block models; the dimension of the Euclidean space in random geometric graphs; the regularity of the link function in H√∂lder graphon models. From a single observation of the graph, we construct an estimator of the neighborhood-distance and show universal non-asymptotic bounds for its risk, matching minimax lower bounds. Based on this estimated distance, we compute the corresponding covering number and Minkowski dimension and we provide optimal non-asymptotic error bounds for these two plug-in estimators.},
 author = {Yann Issartel},
 journal = {Journal of Machine Learning Research},
 number = {191},
 openalex = {W3206766348},
 pages = {1--62},
 title = {On the Estimation of Network Complexity: Dimension of Graphons},
 url = {http://jmlr.org/papers/v22/19-747.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-753,
 abstract = {We derive an algorithm that achieves the optimal (within constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon. The algorithm is based on online mirror descent (OMD) with Tsallis entropy regularization with power $\alpha=1/2$ and reduced-variance loss estimators. More generally, we define an adversarial regime with a self-bounding constraint, which includes stochastic regime, stochastically constrained adversarial regime (Wei and Luo), and stochastic regime with adversarial corruptions (Lykouris et al.) as special cases, and show that the algorithm achieves logarithmic regret guarantee in this regime and all of its special cases simultaneously with the adversarial regret guarantee.} The algorithm also achieves adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it significantly outperforms UCB1 and EXP3 in stochastic environments. We also provide examples of adversarial environments, where UCB1 and Thompson Sampling exhibit almost linear regret, whereas our algorithm suffers only logarithmic regret. To the best of our knowledge, this is the first example demonstrating vulnerability of Thompson Sampling in adversarial environments. Last, but not least, we present a general stochastic analysis and a general adversarial analysis of OMD algorithms with Tsallis entropy regularization for $\alpha\in[0,1]$ and explain the reason why $\alpha=1/2$ works best.},
 author = {Julian Zimmert and Yevgeny Seldin},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W3126160212},
 pages = {1--49},
 title = {Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits},
 url = {http://jmlr.org/papers/v22/19-753.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-769,
 abstract = {Tensors are becoming prevalent in modern applications such as medical imaging and digital marketing. In this paper, we propose a sparse tensor additive regression (STAR) that models a scalar response as a flexible nonparametric function of tensor covariates. The proposed model effectively exploits the sparse and low-rank structures in the tensor additive regression. We formulate the parameter estimation as a non-convex optimization problem, and propose an efficient penalized alternating minimization algorithm. We establish a non-asymptotic error bound for the estimator obtained from each iteration of the proposed algorithm, which reveals an interplay between the optimization error and the statistical rate of convergence. We demonstrate the efficacy of STAR through extensive comparative simulation studies, and an application to the click-through-rate prediction in online advertising.},
 author = {Botao Hao and Boxiang Wang and Pengyuan Wang and Jingfei Zhang and Jian Yang and Will Wei Sun},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W3128411150},
 pages = {1--43},
 title = {Sparse Tensor Additive Regression},
 url = {http://jmlr.org/papers/v22/19-769.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-770,
 author = {Wanrong Zhang and Sara Krehbiel and Rui Tuo and Yajun Mei and Rachel Cummings},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W3127214541},
 pages = {1--36},
 title = {Single and Multiple Change-Point Detection with Differential Privacy},
 url = {http://jmlr.org/papers/v22/19-770.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-776,
 abstract = {Simultaneous inference after model selection is of critical importance to address scientific hypotheses involving a set of parameters. In this paper, we consider high-dimensional linear regression model in which a regularization procedure such as LASSO is applied to yield a sparse model. To establish a simultaneous post-model selection inference, we propose a method of contraction and expansion (MOCE) along the line of debiasing estimation that enables us to balance the bias-and-variance trade-off so that the super-sparsity assumption may be relaxed. We establish key theoretical results for the proposed MOCE procedure from which the expanded model can be selected with theoretical guarantees and simultaneous confidence regions can be constructed by the joint asymptotic normal distribution. In comparison with existing methods, our proposed method exhibits stable and reliable coverage at a nominal significance level with substantially less computational burden, and thus it is trustworthy for its application in solving real-world problems.},
 author = {Fei Wang and Ling Zhou and Lu Tang and Peter X.K. Song},
 journal = {Journal of Machine Learning Research},
 number = {192},
 openalex = {W3206411490},
 pages = {1--32},
 title = {Method of Contraction-Expansion (MOCE) for Simultaneous Inference in Linear Models},
 url = {http://jmlr.org/papers/v22/19-776.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-782,
 abstract = {We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics. We propose several variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters. Consistency properties are established for the estimates of both local and global clusters. Finally, experimental results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.},
 author = {Viet Huynh and Nhat Ho and Nhan Dam and XuanLong Nguyen and Mikhail Yurochkin and Hung Bui and Dinh Phung},
 journal = {Journal of Machine Learning Research},
 number = {145},
 openalex = {W3192466798},
 pages = {1--43},
 title = {On Efficient Multilevel Clustering via Wasserstein Distances},
 url = {http://jmlr.org/papers/v22/19-782.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-792,
 abstract = {Recommender systems have been extensively used by the entertainment industry, business marketing and the biomedical industry. In addition to its capacity of providing preference-based recommendations as an unsupervised learning methodology, it has been also proven useful in sales forecasting, product introduction and other production related businesses. Since some consumers and companies need a recommendation or prediction for future budget, labor and supply chain coordination, dynamic recommender systems for precise forecasting have become extremely necessary. In this article, we propose a new recommendation method, namely the dynamic tensor recommender system (DTRS), which aims particularly at forecasting future recommendation. The proposed method utilizes a tensor-valued function of time to integrate time and contextual information, and creates a time-varying coefficient model for temporal tensor factorization through a polynomial spline approximation. Major advantages of the proposed method include competitive future recommendation predictions and effective prediction interval estimations. In theory, we establish the convergence rate of the proposed tensor factorization and asymptotic normality of the spline coefficient estimator. The proposed method is applied to simulations and IRI marketing data. Numerical studies demonstrate that the proposed method outperforms existing methods in terms of future time forecasting.},
 author = {Yanqing Zhang and Xuan Bi and Niansheng Tang and Annie Qu},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W3011248981},
 pages = {1--35},
 title = {Dynamic Tensor Recommender Systems},
 url = {http://jmlr.org/papers/v22/19-792.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-804,
 abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
 author = {Oliver Kroemer and Scott Niekum and George Konidaris},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W3127352841},
 pages = {1--82},
 title = {A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms},
 url = {http://jmlr.org/papers/v22/19-804.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-835,
 abstract = {In the present paper we study a sparse stochastic network enabled with a block structure. The popular Stochastic Block Model (SBM) and the Degree Corrected Block Model (DCBM) address sparsity by placing an upper bound on the maximum probability of connections between any pair of nodes. As a result, sparsity describes only the behavior of network as a whole, without distinguishing between the block-dependent sparsity patterns. To the best of our knowledge, the recently introduced Popularity Adjusted Block Model (PABM) is the only block model that allows to introduce a {\it structural sparsity} where some probabilities of connections are identically equal to zero while the rest of them remain above a certain threshold. The latter presents a more nuanced view of the network.},
 author = {Majid Noroozi and Marianna Pensky and Ramchandra Rimal},
 journal = {Journal of Machine Learning Research},
 number = {193},
 openalex = {W2977642958},
 pages = {1--36},
 title = {Sparse Popularity Adjusted Stochastic Block Model},
 url = {http://jmlr.org/papers/v22/19-835.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-852,
 abstract = {Graph embeddings, a class of dimensionality reduction techniques designed for relational data, have proven useful in exploring and modeling network structure. Most dimensionality reduction methods allow out-of-sample extensions, by which an embedding can be applied to observations not present in the training set. Applied to graphs, the out-of-sample extension problem concerns how to compute the embedding of a vertex that is added to the graph after an embedding has already been computed. In this paper, we consider the out-of-sample extension problem for two graph embedding procedures: the adjacency spectral embedding and the Laplacian spectral embedding. In both cases, we prove that when the underlying graph is generated according to a latent space model called the random dot product graph, which includes the popular stochastic block model as a special case, an out-of-sample extension based on a least-squares objective obeys a central limit theorem about the true latent position of the out-of-sample vertex. In addition, we prove a concentration inequality for the out-of-sample extension of the adjacency spectral embedding based on a maximum-likelihood objective. Our results also yield a convenient framework in which to analyze trade-offs between estimation accuracy and computational expense, which we explore briefly.},
 author = {Keith D. Levin and Fred Roosta and Minh Tang and Michael W. Mahoney and Carey E. Priebe},
 journal = {Journal of Machine Learning Research},
 number = {194},
 openalex = {W3205726959},
 pages = {1--59},
 title = {Limit theorems for out-of-sample extensions of the adjacency and Laplacian spectral embeddings},
 url = {http://jmlr.org/papers/v22/19-852.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-853,
 abstract = {A classical problem in causal inference is that of matching, where treatment units need to be matched to control units based on covariate information. In this work, we propose a method that computes high quality almost-exact matches for high-dimensional categorical datasets. This method, called FLAME (Fast Large-scale Almost Matching Exactly), learns a distance metric for matching using a hold-out training data set. In order to perform matching efficiently for large datasets, FLAME leverages techniques that are natural for query processing in the area of database management, and two implementations of FLAME are provided: the first uses SQL queries and the second uses bit-vector techniques. The algorithm starts by constructing matches of the highest quality (exact matches on all covariates), and successively eliminates variables in order to match exactly on as many variables as possible, while still maintaining interpretable high-quality matches and balance between treatment and control groups. We leverage these high quality matches to estimate conditional average treatment effects (CATEs). Our experiments show that FLAME scales to huge datasets with millions of observations where existing state-of-the-art methods fail, and that it achieves significantly better performance than other matching methods.},
 author = {Tianyu Wang and Marco Morucci and M. Usaid Awan and Yameng Liu and Sudeepa Roy and Cynthia Rudin and Alexander Volfovsky},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2737061368},
 pages = {1--41},
 title = {FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference},
 url = {http://jmlr.org/papers/v22/19-853.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-861,
 abstract = {Systems of interacting particles or agents have wide applications in many disciplines such as Physics, Chemistry, Biology and Economics. These systems are governed by interaction laws, which are often unknown: estimating them from observation data is a fundamental task that can provide meaningful insights and accurate predictions of the behaviour of the agents. In this paper, we consider the inverse problem of learning interaction laws given data from multiple trajectories, in a nonparametric fashion, when the interaction kernels depend on pairwise distances. We establish a condition for learnability of interaction kernels, and construct estimators that are guaranteed to converge in a suitable $L^2$ space, at the optimal min-max rate for 1-dimensional nonparametric regression. We propose an efficient learning algorithm based on least squares, which can be implemented in parallel for multiple trajectories and is therefore well-suited for the high dimensional, big data regime. Numerical simulations on a variety examples, including opinion dynamics, predator-swarm dynamics and heterogeneous particle dynamics, suggest that the learnability condition is satisfied in models used in practice, and the rate of convergence of our estimator is consistent with the theory. These simulations also suggest that our estimators are robust to noise in the observations, and produce accurate predictions of dynamics in relative large time intervals, even when they are learned from data collected in short time intervals.},
 author = {Fei Lu and Mauro Maggioni and Sui Tang},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W3128929428},
 pages = {1--67},
 title = {Learning interaction kernels in heterogeneous systems of agents from multiple trajectories},
 url = {http://jmlr.org/papers/v22/19-861.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-870,
 author = {Haishan Ye and Luo Luo and Zhihua Zhang},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W1969800610},
 pages = {1--41},
 title = {Global approximate Newton methods},
 url = {http://jmlr.org/papers/v22/19-870.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-873,
 abstract = {We develop a Nonparametric Empirical Bayes (NEB) framework for compound estimation in the discrete linear exponential family, which includes a wide class of discrete distributions frequently arising from modern big data applications. We propose to directly estimate the Bayes shrinkage factor in the generalized Robbins' formula via solving a scalable convex program, which is carefully developed based on a RKHS representation of the Stein's discrepancy measure. The new NEB estimation framework is flexible for incorporating various structural constraints into the data driven rule, and provides a unified approach to compound estimation with both regular and scaled squared error losses. We develop theory to show that the class of NEB estimators enjoys strong asymptotic properties. Comprehensive simulation studies as well as analyses of real data examples are carried out to demonstrate the superiority of the NEB estimator over competing methods.},
 author = {Trambak Banerjee and Qiang Liu and Gourab Mukherjee and Wengunag Sun},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2980775349},
 pages = {1--46},
 title = {A General Framework for Empirical Bayes Estimation in Discrete Linear Exponential Family},
 url = {http://jmlr.org/papers/v22/19-873.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-892,
 abstract = {We provide general adaptive upper bounds for estimating nonparametric functionals based on second order U-statistics arising from finite dimensional approximation of the infinite dimensional models. We then provide examples of functionals for which the theory produces rate optimally matching adaptive upper and lower bounds. Our results are automatically adaptive in both parametric and nonparametric regimes of estimation and are automatically adaptive and semiparametric efficient in the regime of parametric convergence rate.},
 author = {Lin Liu and Rajarshi Mukherjee and James M. Robins and Eric Tchetgen Tchetgen},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2496371581},
 pages = {1--66},
 title = {Adaptive Estimation of Nonparametric Functionals},
 url = {http://jmlr.org/papers/v22/19-892.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-910,
 abstract = {We consider the problem of asynchronous online testing, aimed at providing control of the false discovery rate (FDR) during a continual stream of data collection and testing, where each test may be a sequential test that can start and stop at arbitrary times. This setting increasingly characterizes real-world applications in science and industry, where teams of researchers across large organizations may conduct tests of hypotheses in a decentralized manner. The overlap in time and space also tends to induce dependencies among test statistics, a challenge for classical methodology, which either assumes (overly optimistically) independence or (overly pessimistically) arbitrary dependence between test statistics. We present a general framework that addresses both of these issues via a unified computational abstraction that we refer to as "conflict sets." We show how this framework yields algorithms with formal FDR guarantees under a more intermediate, local notion of dependence. We illustrate our algorithms in simulations by comparing to existing algorithms for online FDR control.},
 author = {Tijana Zrnic and Aaditya Ramdas and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W3128529018},
 pages = {1--39},
 title = {Asynchronous Online Testing of Multiple Hypotheses},
 url = {http://jmlr.org/papers/v22/19-910.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-920,
 abstract = {OpenML is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper we introduce OpenML-Python, a client API for Python, opening up the OpenML platform for a wide range of Python-based tools. It provides easy access to all datasets, tasks and experiments on OpenML from within Python. It also provides functionality to conduct machine learning experiments, upload the results to OpenML, and reproduce results which are stored on OpenML. Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to easily integrate other machine learning libraries written in Python into the OpenML ecosystem. Source code and documentation is available at this https URL.},
 author = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas M√ºller and Joaquin Vanschoren and Frank Hutter},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2983758708},
 pages = {1--5},
 title = {OpenML-Python: an extensible Python API for OpenML},
 url = {http://jmlr.org/papers/v22/19-920.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-924,
 abstract = {Dimensionality reduction is considered as an important step for ensuring competitive performance in unsupervised learning such as anomaly detection. Non-negative matrix factorization (NMF) is a popular and widely used method to accomplish this goal. But NMF do not have the provision to include the neighborhood structure information and, as a result, may fail to provide satisfactory performance in presence of nonlinear manifold structure. To address that shortcoming, we propose to consider and incorporate the neighborhood structural similarity information within the NMF framework by modeling the data through a minimum spanning tree. We label the resulting method as the neighborhood structure assisted NMF. We further devise both offline and online algorithmic versions of the proposed method. Empirical comparisons using twenty benchmark datasets as well as an industrial dataset extracted from a hydropower plant demonstrate the superiority of the neighborhood structure assisted NMF and support our claim of merit. Looking closer into the formulation and properties of the neighborhood structure assisted NMF with other recent, enhanced versions of NMF reveals that inclusion of the neighborhood structure information using MST plays a key role in attaining the enhanced performance in anomaly detection.},
 author = {Imtiaz Ahmed and Xia Ben Hu and Mithun P. Acharya and Yu Ding},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W3127128818},
 pages = {1--32},
 title = {Neighborhood Structure Assisted Non-negative Matrix Factorization and its Application in Unsupervised Point-wise Anomaly Detection},
 url = {http://jmlr.org/papers/v22/19-924.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-941,
 abstract = {We study statistical and algorithmic aspects of using hypergraphons, that are limits of large hypergraphs, for modeling higher-order interactions. Although hypergraphons are extremely powerful from a modeling perspective, we consider a restricted class of Simple Lipschitz Hypergraphons (SLH), that are amenable to practically efficient estimation. We also provide rates of convergence for our estimator that are optimal for the class of SLH. Simulation results are provided to corroborate the theory.},
 author = {Krishnakumar Balasubramanian},
 journal = {Journal of Machine Learning Research},
 number = {146},
 openalex = {W4287180059},
 pages = {1--35},
 title = {Nonparametric Modeling of Higher-Order Interactions via Hypergraphons},
 url = {http://jmlr.org/papers/v22/19-941.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-944,
 author = {Pierre Humbert and Batiste Le Bars and Laurent Oudre and Argyris Kalogeratos and Nicolas Vayatis},
 journal = {Journal of Machine Learning Research},
 number = {195},
 openalex = {W3205137593},
 pages = {1--47},
 title = {Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation},
 url = {http://jmlr.org/papers/v22/19-944.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-969,
 author = {Meiling Hao and Lianqiang Qu and Dehan Kong and Liuquan Sun and Hongtu Zhu},
 journal = {Journal of Machine Learning Research},
 number = {147},
 openalex = {W3189021302},
 pages = {1--39},
 title = {Optimal Minimax Variable Selection for Large-Scale Matrix Linear Regression Model},
 url = {http://jmlr.org/papers/v22/19-969.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-979,
 abstract = {We derive bounds on the path length $Œ∂$ of gradient descent (GD) and gradient flow (GF) curves for various classes of smooth convex and nonconvex functions. Among other results, we prove that: (a) if the iterates are linearly convergent with factor $(1-c)$, then $Œ∂$ is at most $\mathcal{O}(1/c)$; (b) under the Polyak-Kurdyka-Lojasiewicz (PKL) condition, $Œ∂$ is at most $\mathcal{O}(\sqrtŒ∫)$, where $Œ∫$ is the condition number, and at least $\widetildeŒ©(\sqrt{d} \wedge Œ∫^{1/4})$; (c) for quadratics, $Œ∂$ is $Œò(\min\{\sqrt{d},\sqrt{\log Œ∫}\})$ and in some cases can be independent of $Œ∫$; (d) assuming just convexity, $Œ∂$ can be at most $2^{4d\log d}$; (e) for separable quasiconvex functions, $Œ∂$ is $Œò(\sqrt{d})$. Thus, we advance current understanding of the properties of GD and GF curves beyond rates of convergence. We expect our techniques to facilitate future studies for other algorithms.},
 author = {Chirag Gupta and Sivaraman Balakrishnan and Aaditya Ramdas},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2964467559},
 pages = {1--63},
 title = {Path Length Bounds for Gradient Descent and Flow},
 url = {http://jmlr.org/papers/v22/19-979.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-006,
 abstract = {This paper proposes a formal approach to online learning and planning for agents operating in a priori unknown, time-varying environments. The proposed method computes the maximally likely model of the environment, given the observations about the environment made by an agent earlier in the system run and assuming knowledge of a bound on the maximal rate of change of system dynamics. Such an approach generalizes the estimation method commonly used in learning algorithms for unknown Markov decision processes with time-invariant transition probabilities, but is also able to quickly and correctly identify the system dynamics following a change. Based on the proposed method, we generalize the exploration bonuses used in learning for time-invariant Markov decision processes by introducing a notion of uncertainty in a learned time-varying model, and develop a control policy for time-varying Markov decision processes based on the exploitation and exploration trade-off. We demonstrate the proposed methods on four numerical examples: a patrolling task with a change in system dynamics, a two-state MDP with periodically changing outcomes of actions, a wind flow estimation task, and a multi-armed bandit problem with periodically changing probabilities of different rewards.},
 author = {Melkior Ornik and Ufuk Topcu},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2990691216},
 pages = {1--40},
 title = {Learning and Planning for Time-Varying MDPs Using Maximum Likelihood Estimation},
 url = {http://jmlr.org/papers/v22/20-006.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-029,
 abstract = {Local graph clustering methods aim to find small clusters in very large graphs. These methods take as input a graph and a seed node, and they return as output a good cluster in a running time that depends on the size of the output cluster but that is independent of the size of the input graph. In this paper, we adopt a statistical perspective on local graph clustering, and we analyze the performance of the l1-regularized PageRank method~(Fountoulakis et. al.) for the recovery of a single target cluster, given a seed node inside the cluster. Assuming the target cluster has been generated by a random model, we present two results. In the first, we show that the optimal support of l1-regularized PageRank recovers the full target cluster, with bounded false positives. In the second, we show that if the seed node is connected solely to the target cluster then the optimal support of l1-regularized PageRank recovers exactly the target cluster. We also show empirically that l1-regularized PageRank has a state-of-the-art performance on many real graphs, demonstrating the superiority of the method. From a computational perspective, we show that the solution path of l1-regularized PageRank is monotonic. This allows for the application of the forward stagewise algorithm, which approximates the solution path in running time that does not depend on the size of the whole graph. Finally, we show that l1-regularized PageRank and approximate personalized PageRank (APPR), another very popular method for local graph clustering, are equivalent in the sense that we can lower and upper bound the output of one with the output of the other. Based on this relation, we establish for APPR similar results to those we establish for l1-regularized PageRank.},
 author = {Wooseok Ha and Kimon Fountoulakis and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {148},
 openalex = {W3196102972},
 pages = {1--54},
 title = {Statistical guarantees for local graph clustering},
 url = {http://jmlr.org/papers/v22/20-029.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-033,
 author = {Zhiqiang Xu and Ping Li},
 journal = {Journal of Machine Learning Research},
 number = {249},
 openalex = {W3216249545},
 pages = {1--46},
 title = {On the Riemannian Search for Eigenvector Computation},
 url = {http://jmlr.org/papers/v22/20-033.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-037,
 abstract = {We propose to estimate the number of communities in degree-corrected stochastic block models based on a pseudo likelihood ratio statistic. To this end, we introduce a method that combines spectral clustering with binary segmentation. This approach guarantees an upper bound for the pseudo likelihood ratio statistic when the model is over-fitted. We also derive its limiting distribution when the model is under-fitted. Based on these properties, we establish the consistency of our estimator for the true number of communities. Developing these theoretical properties require a mild condition on the average degrees -- growing at a rate no slower than log(n), where n is the number of nodes. Our proposed method is further illustrated by simulation studies and analysis of real-world networks. The numerical results show that our approach has satisfactory performance when the network is semi-dense.},
 author = {Shujie Ma and Liangjun Su and Yichong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W3163028856},
 pages = {1--63},
 title = {Determining the number of communities in degree-corrected stochastic block models},
 url = {http://jmlr.org/papers/v22/20-037.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-048,
 abstract = {Many modern data sets require inference methods that can estimate the shared and individual-specific components of variability in collections of matrices that change over time. Promising methods have been developed to analyze these types of data in static cases, but only a few approaches are available for dynamic settings. To address this gap, we consider novel models and inference methods for pairs of matrices in which the columns correspond to multivariate observations at different time points. In order to characterize common and individual features, we propose a Bayesian dynamic factor modeling framework called Time Aligned Common and Individual Factor Analysis (TACIFA) that includes uncertainty in time alignment through an unknown warping function. We provide theoretical support for the proposed model, showing identifiability and posterior concentration. The structure enables efficient computation through a Hamiltonian Monte Carlo (HMC) algorithm. We show excellent performance in simulations, and illustrate the method through application to a social mimicry experiment.},
 author = {Arkaprava Roy and Jana Schaich Borg and David B Dunson},
 journal = {Journal of Machine Learning Research},
 number = {250},
 openalex = {W2941738790},
 pages = {1--27},
 title = {Bayesian time-aligned factor analysis of paired multivariate time series},
 url = {http://jmlr.org/papers/v22/20-048.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-052,
 author = {Baoxun Wang and Zhen Xu and Huan Zhang and Kexin Qiu and Deyuan Zhang and Chengjie Sun},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2998590008},
 pages = {1--29},
 title = {LocalGAN: Modeling Local Distributions for Adversarial Response Generation},
 url = {http://jmlr.org/papers/v22/20-052.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-058,
 abstract = {Hyperparameter optimization (HPO) plays a central role in the automated machine learning (AutoML). It is a challenging task as the response surfaces of hyperparameters are generally unknown, hence essentially a global optimization problem. This paper reformulates HPO as a computer experiment and proposes a novel sequential uniform design (SeqUD) strategy with three-fold advantages: a) the hyperparameter space is adaptively explored with evenly spread design points, without the need of expensive meta-modeling and acquisition optimization; b) the batch-by-batch design points are sequentially generated with parallel processing support; c) a new augmented uniform design algorithm is developed for the efficient real-time generation of follow-up design points. Extensive experiments are conducted on both global optimization tasks and HPO applications. The numerical results show that the proposed SeqUD strategy outperforms benchmark HPO methods, and it can be therefore a promising and competitive alternative to existing AutoML tools.},
 author = {Zebin Yang and Aijun Zhang},
 journal = {Journal of Machine Learning Research},
 number = {149},
 openalex = {W3084079134},
 pages = {1--47},
 title = {Hyperparameter Optimization via Sequential Uniform Designs},
 url = {http://jmlr.org/papers/v22/20-058.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-070,
 abstract = {This paper studies the decentralized optimization and learning problem where multiple interconnected agents aim to learn an optimal decision function defined over a reproducing kernel Hilbert space by jointly minimizing a global objective function, with access to their own locally observed dataset. As a non-parametric approach, kernel learning faces a major challenge in distributed implementation: the decision variables of local objective functions are data-dependent and thus cannot be optimized under the decentralized consensus framework without any raw data exchange among agents. To circumvent this major challenge, we leverage the random feature (RF) approximation approach to enable consensus on the function modeled in the RF space by data-independent parameters across different agents. We then design an iterative algorithm, termed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we further develop a communication-censored kernel learning (COKE) algorithm that reduces the communication load of DKLA by preventing an agent from transmitting at every iteration unless its local updates are deemed informative. Theoretical results in terms of linear convergence guarantee and generalization performance analysis of DKLA and COKE are provided. Comprehensive tests on both synthetic and real datasets are conducted to verify the communication efficiency and learning effectiveness of COKE.},
 author = {Ping Xu and Yue Wang and Xiang Chen and Zhi Tian},
 journal = {Journal of Machine Learning Research},
 number = {196},
 openalex = {W3205776668},
 pages = {1--35},
 title = {COKE: Communication-Censored Decentralized Kernel Learning},
 url = {http://jmlr.org/papers/v22/20-070.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-082,
 abstract = {Bayesian feature allocation models are a popular tool for modelling data with a combinatorial latent structure. Exact inference in these models is generally intractable and so practitioners typically apply Markov Chain Monte Carlo (MCMC) methods for posterior inference. The most widely used MCMC strategies rely on an element wise Gibbs update of the feature allocation matrix. These element wise updates can be inefficient as features are typically strongly correlated. To overcome this problem we have developed a Gibbs sampler that can update an entire row of the feature allocation matrix in a single move. However, this sampler is impractical for models with a large number of features as the computational complexity scales exponentially in the number of features. We develop a Particle Gibbs sampler that targets the same distribution as the row wise Gibbs updates, but has computational complexity that only grows linearly in the number of features. We compare the performance of our proposed methods to the standard Gibbs sampler using synthetic data from a range of feature allocation models. Our results suggest that row wise updates using the PG methodology can significantly improve the performance of samplers for feature allocation models.},
 author = {Alexandre Bouchard-C√¥t√© and Andrew Roth},
 journal = {Journal of Machine Learning Research},
 number = {197},
 openalex = {W3001762958},
 pages = {1--105},
 title = {Particle-Gibbs Sampling For Bayesian Feature Allocation Models},
 url = {http://jmlr.org/papers/v22/20-082.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-084,
 abstract = {Data integration, or the strategic analysis of multiple sources of data simultaneously, can often lead to discoveries that may be hidden in individualistic analyses of a single data source. We develop a new unsupervised data integration method named Integrated Principal Components Analysis (iPCA), which is a model-based generalization of PCA and serves as a practical tool to find and visualize common patterns that occur in multiple data sets. The key idea driving iPCA is the matrix-variate normal model, whose Kronecker product covariance structure captures both individual patterns within each data set and joint patterns shared by multiple data sets. Building upon this model, we develop several penalized (sparse and non-sparse) covariance estimators for iPCA, and using geodesic convexity, we prove that our non-sparse iPCA estimator converges to the global solution of a non-convex problem. We also demonstrate the practical advantages of iPCA through extensive simulations and a case study application to integrative genomics for Alzheimer's disease. In particular, we show that the joint patterns extracted via iPCA are highly predictive of a patient's cognition and Alzheimer's diagnosis.},
 author = {Tiffany M. Tang and Genevera I. Allen},
 journal = {Journal of Machine Learning Research},
 number = {198},
 openalex = {W2893691638},
 pages = {1--71},
 title = {Integrated Principal Components Analysis},
 url = {http://jmlr.org/papers/v22/20-084.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1005,
 abstract = {This paper develops a new approach to learning high-dimensional linear structural equation models (SEMs) without the commonly assumed faithfulness, Gaussian error distribution, and equal error distribution conditions. A key component of the algorithm is componentwise ordering and parent estimations, where both problems can be efficiently addressed using '1-regularized regression. This paper proves that sample sizes n = (d2 log p) and n = (d2p2=m) are sufficient for the proposed algorithm to recover linear SEMs with sub- Gaussian and (4m)-th bounded-moment error distributions, respectively, where p is the number of nodes and d is the maximum degree of the moralized graph. Further shown is the worst-case computational complexity O(n(p3 + p2d2)), and hence, the proposed algorithm is statistically consistent and computationally feasible for learning a high-dimensional linear SEM when its moralized graph is sparse. Through simulations, we verify that the proposed algorithm is statistically consistent and computationally feasible, and it performs well compared to the state-of-the-art US, GDS, LISTEN and TD algorithms with our settings. We also demonstrate through real COVID-19 data that the proposed algorithm is well-suited to estimating a virus-spread map in China. ¬© 2021 Microtome Publishing. All rights reserved.},
 author = {Gunwoong Park and Sang Jun Moon and Sion Park and Jong-June Jeon},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W3169180353},
 pages = {1--41},
 title = {Learning a High-dimensional Linear Structural Equation Model via l1-Regularized Regression},
 url = {http://jmlr.org/papers/v22/20-1005.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1006,
 abstract = {In this paper, we develop an alternating direction method of multipliers (ADMM) for deep neural networks training with sigmoid-type activation functions (called \textit{sigmoid-ADMM pair}), mainly motivated by the gradient-free nature of ADMM in avoiding the saturation of sigmoid-type activations and the advantages of deep neural networks with sigmoid-type activations (called deep sigmoid nets) over their rectified linear unit (ReLU) counterparts (called deep ReLU nets) in terms of approximation. In particular, we prove that the approximation capability of deep sigmoid nets is not worse than that of deep ReLU nets by showing that ReLU activation function can be well approximated by deep sigmoid nets with two hidden layers and finitely many free parameters but not vice-verse. We also establish the global convergence of the proposed ADMM for the nonlinearly constrained formulation of the deep sigmoid nets training from arbitrary initial points to a Karush-Kuhn-Tucker (KKT) point at a rate of order ${\cal O}(1/k)$. Besides sigmoid activation, such a convergence theorem holds for a general class of smooth activations. Compared with the widely used stochastic gradient descent (SGD) algorithm for the deep ReLU nets training (called ReLU-SGD pair), the proposed sigmoid-ADMM pair is practically stable with respect to the algorithmic hyperparameters including the learning rate, initial schemes and the pro-processing of the input data. Moreover, we find that to approximate and learn simple but important functions the proposed sigmoid-ADMM pair numerically outperforms the ReLU-SGD pair.},
 author = {Jinshan Zeng and Shao-Bo Lin and Yuan Yao and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {199},
 openalex = {W3083491356},
 pages = {1--67},
 title = {On ADMM in Deep Learning: Convergence and Saturation-Avoidance},
 url = {http://jmlr.org/papers/v22/20-1006.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1009,
 abstract = {In this paper, we propose an analytical method for performing tractable approximate Gaussian inference (TAGI) in Bayesian neural networks. The method enables the analytical Gaussian inference of the posterior mean vector and diagonal covariance matrix for weights and biases. The method proposed has a computational complexity of $\mathcal{O}(n)$ with respect to the number of parameters $n$, and the tests performed on regression and classification benchmarks confirm that, for a same network architecture, it matches the performance of existing methods relying on gradient backpropagation.},
 author = {James-A. Goulet and Luong Ha Nguyen and Saeid Amiri},
 journal = {Journal of Machine Learning Research},
 number = {251},
 openalex = {W4287813746},
 pages = {1--23},
 title = {Tractable Approximate Gaussian Inference for Bayesian Neural Networks},
 url = {http://jmlr.org/papers/v22/20-1009.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1019,
 abstract = {Blackwell's approachability is a framework where two players, the Decision Maker and the Environment, play a repeated game with vector-valued payoffs. The goal of the Decision Maker is to make the average payoff converge to a given set called the target. When this is indeed possible, simple algorithms which guarantee the convergence are known. This abstract tool was successfully used for the construction of optimal strategies in various repeated games, but also found several applications in online learning. By extending an approach proposed by (Abernethy et al., 2011), we construct and analyze a class of Follow the Regularized Leader algorithms (FTRL) for Blackwell's approachability which are able to minimize not only the Euclidean distance to the target set (as it is often the case in the context of Blackwell's approachability) but a wide range of distance-like quantities. This flexibility enables us to apply these algorithms to closely minimize the quantity of interest in various online learning problems. In particular, for regret minimization with $\ell_p$ global costs, we obtain the first bounds with explicit dependence in $p$ and the dimension $d$.},
 author = {Joon Kwon},
 journal = {Journal of Machine Learning Research},
 number = {200},
 openalex = {W3084208336},
 pages = {1--38},
 title = {Refined approachability algorithms and application to regret minimization with global costs},
 url = {http://jmlr.org/papers/v22/20-1019.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1023,
 abstract = {We propose and analyze batch greedy heuristics for cardinality constrained maximization of non-submodular non-decreasing set functions. Our theoretical guarantees are characterized by the combination of submodularity and supermodularity ratios. We argue how these parameters define tight modular bounds based on incremental gains, and provide a novel reinterpretation of the classical greedy algorithm using the minorize-maximize (MM) principle. Based on that analogy, we propose a new class of methods exploiting any plausible modular bound. In the context of optimal experimental design for linear Bayesian inverse problems, we bound the submodularity and supermodularity ratios when the underlying objective is based on mutual information. We also develop novel modular bounds for the mutual information in this setting, and describe certain connections to polyhedral combinatorics. We discuss how algorithms using these modular bounds relate to established statistical notions such as leverage scores and to more recent efforts such as volume sampling. We demonstrate our theoretical findings on synthetic problems and on a real-world climate monitoring example.},
 author = {Jayanth Jagalur-Mohan and Youssef Marzouk},
 journal = {Journal of Machine Learning Research},
 number = {252},
 openalex = {W3033289058},
 pages = {1--62},
 title = {Batch greedy maximization of non-submodular functions: Guarantees and applications to experimental design},
 url = {http://jmlr.org/papers/v22/20-1023.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1031,
 abstract = {Spiking neural networks (SNNs) has attracted much attention due to its great potential of modeling time-dependent signals. The firing rate of spiking neurons is decided by control rate which is fixed manually in advance, and thus, whether the firing rate is adequate for modeling actual time series relies on fortune. Though it is demanded to have an adaptive control rate, it is a non-trivial task because the control rate and the connection weights learned during the training process are usually entangled. In this paper, we show that the firing rate is related to the eigenvalue of the spike generation function. Inspired by this insight, by enabling the spike generation function to have adaptable eigenvalues rather than parametric control rates, we develop the Bifurcation Spiking Neural Network (BSNN), which has an adaptive firing rate and is insensitive to the setting of control rates. Experiments validate the effectiveness of BSNN on a broad range of tasks, showing that BSNN achieves superior performance to existing SNNs and is robust to the setting of control rates.},
 author = {Shao-Qun Zhang and Zhao-Yu Zhang and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {253},
 openalex = {W2974874736},
 pages = {1--21},
 title = {Bifurcation Spiking Neural Network},
 url = {http://jmlr.org/papers/v22/20-1031.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1050,
 abstract = {Labeling patients in electronic health records with respect to their statuses of having a disease or condition, i.e. case or control statuses, has increasingly relied on prediction models using high-dimensional variables derived from structured and unstructured electronic health record data. A major hurdle currently is a lack of valid statistical inference methods for the case probability. In this paper, considering high-dimensional sparse logistic regression models for prediction, we propose a novel bias-corrected estimator for the case probability through the development of linearization and variance enhancement techniques. We establish asymptotic normality of the proposed estimator for any loading vector in high dimensions. We construct a confidence interval for the case probability and propose a hypothesis testing procedure for patient case-control labelling. We demonstrate the proposed method via extensive simulation studies and application to real-world electronic health record data.},
 author = {Zijian Guo and Prabrisha Rakshit and Daniel S. Herman and Jinbo Chen},
 journal = {Journal of Machine Learning Research},
 number = {254},
 openalex = {W3215156956},
 pages = {1--54},
 title = {Inference for the Case Probability in High-dimensional Logistic Regression},
 url = {http://jmlr.org/papers/v22/20-1050.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1061,
 abstract = {Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMAP have demonstrated impressive visualization performance on many real world datasets. One tension that has always faced these methods is the trade-off between preservation of global structure and preservation of local structure: these methods can either handle one or the other, but not both. In this work, our main goal is to understand what aspects of DR methods are important for preserving both local and global structure: it is difficult to design a better method without a true understanding of the choices we make in our algorithms and their empirical impact on the lower-dimensional embeddings they produce. Towards the goal of local structure preservation, we provide several useful design principles for DR loss functions based on our new understanding of the mechanisms behind successful DR methods. Towards the goal of global structure preservation, our analysis illuminates that the choice of which components to preserve is important. We leverage these insights to design a new algorithm for DR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP), which preserves both local and global structure. Our work provides several unexpected insights into what design choices both to make and avoid when constructing DR algorithms.},
 author = {Yingfan Wang and Haiyang Huang and Cynthia Rudin and Yaron Shaposhnik},
 journal = {Journal of Machine Learning Research},
 number = {201},
 openalex = {W3111307227},
 pages = {1--73},
 title = {Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization},
 url = {http://jmlr.org/papers/v22/20-1061.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1065,
 abstract = {We frame the meta-learning of prediction procedures as a search for an optimal strategy in a two-player game. In this game, Nature selects a prior over distributions that generate labeled data consisting of features and an associated outcome, and the Predictor observes data sampled from a distribution drawn from this prior. The Predictor's objective is to learn a function that maps from a new feature to an estimate of the associated outcome. We establish that, under reasonable conditions, the Predictor has an optimal strategy that is equivariant to shifts and rescalings of the outcome and is invariant to permutations of the observations and to shifts, rescalings, and permutations of the features. We introduce a neural network architecture that satisfies these properties. The proposed strategy performs favorably compared to standard practice in both parametric and nonparametric experiments.},
 author = {Alex Luedtke and Incheoul Chung and Oleg Sofrygin},
 journal = {Journal of Machine Learning Research},
 number = {255},
 openalex = {W3007361404},
 pages = {1--67},
 title = {Adversarial Monte Carlo Meta-Learning of Optimal Prediction Procedures.},
 url = {http://jmlr.org/papers/v22/20-1065.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1067,
 abstract = {Low-rank matrix estimation is a canonical problem that finds numerous applications in signal processing, machine learning and imaging science. A popular approach in practice is to factorize the matrix into two compact low-rank factors, and then optimize these factors directly via simple iterative methods such as gradient descent and alternating minimization. Despite nonconvexity, recent literatures have shown that these simple heuristics in fact achieve linear convergence when initialized properly for a growing number of problems of interest. However, upon closer examination, existing approaches can still be computationally expensive especially for ill-conditioned matrices: the convergence rate of gradient descent depends linearly on the condition number of the low-rank matrix, while the per-iteration cost of alternating minimization is often prohibitive for large matrices. The goal of this paper is to set forth a competitive algorithmic approach dubbed Scaled Gradient Descent (ScaledGD) which can be viewed as pre-conditioned or diagonally-scaled gradient descent, where the pre-conditioners are adaptive and iteration-varying with a minimal computational overhead. With tailored variants for low-rank matrix sensing, robust principal component analysis and matrix completion, we theoretically show that ScaledGD achieves the best of both worlds: it converges linearly at a rate independent of the condition number of the low-rank matrix similar as alternating minimization, while maintaining the low per-iteration cost of gradient descent. Our analysis is also applicable to general loss functions that are restricted strongly convex and smooth over low-rank matrices. To the best of our knowledge, ScaledGD is the first algorithm that provably has such properties over a wide range of low-rank matrix estimation tasks.},
 author = {Tian Tong and Cong Ma and Yuejie Chi},
 journal = {Journal of Machine Learning Research},
 number = {150},
 openalex = {W3192692954},
 pages = {1--63},
 title = {Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent},
 url = {http://jmlr.org/papers/v22/20-1067.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1068,
 abstract = {The theory of integral quadratic constraints (IQCs) allows the certification of exponential convergence of interconnected systems containing nonlinear or uncertain elements. In this work, we adapt the IQC theory to study first-order methods for smooth and strongly-monotone games and show how to design tailored quadratic constraints to get tight upper bounds of convergence rates. Using this framework, we recover the existing bound for the gradient method~(GD), derive sharper bounds for the proximal point method~(PPM) and optimistic gradient method~(OG), and provide \emph{for the first time} a global convergence rate for the negative momentum method~(NM) with an iteration complexity $\mathcal{O}(Œ∫^{1.5})$, which matches its known lower bound. In addition, for time-varying systems, we prove that the gradient method with optimal step size achieves the fastest provable worst-case convergence rate with quadratic Lyapunov functions. Finally, we further extend our analysis to stochastic games and study the impact of multiplicative noise on different algorithms. We show that it is impossible for an algorithm with one step of memory to achieve acceleration if it only queries the gradient once per batch (in contrast with the stochastic strongly-convex optimization setting, where such acceleration has been demonstrated). However, we exhibit an algorithm which achieves acceleration with two gradient queries per batch.},
 author = {Guodong Zhang and Xuchan Bao and Laurent Lessard and Roger Grosse},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W3089190316},
 pages = {1--39},
 title = {A Unified Analysis of First-Order Methods for Smooth Games via Integral Quadratic Constraints},
 url = {http://jmlr.org/papers/v22/20-1068.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-107,
 abstract = {It is a common practice in the machine learning community to assume that the observed data are noise-free in the input attributes. Nevertheless, scenarios with input noise are common in real problems, as measurements are never perfectly accurate. If this input noise is not taken into account, a supervised machine learning method is expected to perform sub-optimally. In this paper, we focus on multi-class classification problems and use Gaussian processes (GPs) as the underlying classifier. Motivated by a data set coming from the astrophysics domain, we hypothesize that the observed data may contain noise in the inputs. Therefore, we devise several multi-class GP classifiers that can account for input noise. Such classifiers can be efficiently trained using variational inference to approximate the posterior distribution of the latent variables of the model. Moreover, in some situations, the amount of noise can be known before-hand. If this is the case, it can be readily introduced in the proposed methods. This prior information is expected to lead to better performance results. We have evaluated the proposed methods by carrying out several experiments, involving synthetic and real data. These include several data sets from the UCI repository, the MNIST data set and a data set coming from astrophysics. The results obtained show that, although the classification error is similar across methods, the predictive distribution of the proposed methods is better, in terms of the test log-likelihood, than the predictive distribution of a classifier based on GPs that ignores input noise.},
 author = {Carlos Villacampa-Calvo and Bryan Zald√≠var and Eduardo C. Garrido-Merch√°n and Daniel Hern√°ndez-Lobato},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W3003254083},
 pages = {1--52},
 title = {Multi-class Gaussian Process Classification with Noisy Inputs},
 url = {http://jmlr.org/papers/v22/20-107.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1074,
 abstract = {The partial copula provides a method for describing the dependence between two random variables $X$ and $Y$ conditional on a third random vector $Z$ in terms of nonparametric residuals $U_1$ and $U_2$. This paper develops a nonparametric test for conditional independence by combining the partial copula with a quantile regression based method for estimating the nonparametric residuals. We consider a test statistic based on generalized correlation between $U_1$ and $U_2$ and derive its large sample properties under consistency assumptions on the quantile regression procedure. We demonstrate through a simulation study that the resulting test is sound under complicated data generating distributions. Moreover, in the examples considered the test is competitive to other state-of-the-art conditional independence tests in terms of level and power, and it has superior power in cases with conditional variance heterogeneity of $X$ and $Y$ given $Z$.},
 author = {Lasse Petersen and Niels Richard Hansen},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W3013422023},
 pages = {1--47},
 title = {Testing Conditional Independence via Quantile Regression Based Partial Copulas},
 url = {http://jmlr.org/papers/v22/20-1074.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1078,
 abstract = {We consider a distributed learning setting where each agent/learner holds a specific parametric model and data source. The goal is to integrate information across a set of learners to enhance the prediction accuracy of a given learner. A natural way to integrate information is to build a joint model across a group of learners that shares common parameters of interest. However, the underlying parameter sharing patterns across a set of learners may not be a priori known. Misspecifying the parameter sharing patterns or the parametric model for each learner often yields a biased estimation and degrades the prediction accuracy. We propose a general method to integrate information across a set of learners that is robust against misspecifications of both models and parameter sharing patterns. The main crux is to sequentially incorporate additional learners that can enhance the prediction accuracy of an existing joint model based on user-specified parameter sharing patterns across a set of learners. Theoretically, we show that the proposed method can data-adaptively select the most suitable way of parameter sharing and thus enhance the predictive performance of any particular learner of interest. Extensive numerical studies show the promising performance of the proposed method.},
 author = {Jiaying Zhou and Jie Ding and Kean Ming Tan and Vahid Tarokh},
 journal = {Journal of Machine Learning Research},
 number = {256},
 openalex = {W3215921142},
 pages = {1--44},
 title = {Model Linkage Selection for Cooperative Learning},
 url = {http://jmlr.org/papers/v22/20-1078.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1081,
 abstract = {We study universal consistency and convergence rates of simple nearest-neighbor prototype rules for the problem of multiclass classification in metric paces. We first show that a novel data-dependent partitioning rule, named Proto-NN, is universally consistent in any metric space that admits a universally consistent rule. Proto-NN is a significant simplification of OptiNet, a recently proposed compression-based algorithm that, to date, was the only algorithm known to be universally consistent in such a general setting. Practically, Proto-NN is simpler to implement and enjoys reduced computational complexity. We then proceed to study convergence rates of the excess error probability. We first obtain rates for the standard $k$-NN rule under a margin condition and a new generalized-Lipschitz condition. The latter is an extension of a recently proposed modified-Lipschitz condition from $\mathbb R^d$ to metric spaces. Similarly to the modified-Lipschitz condition, the new condition avoids any boundness assumptions on the data distribution. While obtaining rates for Proto-NN is left open, we show that a second prototype rule that hybridizes between $k$-NN and Proto-NN achieves the same rates as $k$-NN while enjoying similar computational advantages as Proto-NN. However, as $k$-NN, this hybrid rule is not consistent in general.},
 author = {L√°szl√≥ Gy√∂rfi and Roi Weiss},
 journal = {Journal of Machine Learning Research},
 number = {151},
 openalex = {W3091238722},
 pages = {1--25},
 title = {Universal consistency and rates of convergence of multiclass prototype algorithms in metric spaces},
 url = {http://jmlr.org/papers/v22/20-1081.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1098,
 author = {Huafeng Liu and Liping Jing and Jingxuan Wen and Pengyu Xu and Jiaqi Wang and Jian Yu and Michael K. Ng},
 journal = {Journal of Machine Learning Research},
 number = {202},
 openalex = {W3206596835},
 pages = {1--54},
 title = {Interpretable Deep Generative Recommendation Models},
 url = {http://jmlr.org/papers/v22/20-1098.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1100,
 abstract = {Effective decision making requires understanding the uncertainty inherent in a prediction. In regression, this uncertainty can be estimated by a variety of methods; however, many of these methods are laborious to tune, generate overconfident uncertainty intervals, or lack sharpness (give imprecise intervals). We address these challenges by proposing a novel method to capture predictive distributions in regression by defining two neural networks with two distinct loss functions. Specifically, one network approximates the cumulative distribution function, and the second network approximates its inverse. We refer to this method as Collaborating Networks (CN). Theoretical analysis demonstrates that a fixed point of the optimization is at the idealized solution, and that the method is asymptotically consistent to the ground truth distribution. Empirically, learning is straightforward and robust. We benchmark CN against several common approaches on two synthetic and six real-world datasets, including forecasting A1c values in diabetic patients from electronic health records, where uncertainty is critical. In the synthetic data, the proposed approach essentially matches ground truth. In the real-world datasets, CN improves results on many performance metrics, including log-likelihood estimates, mean absolute errors, coverage estimates, and prediction interval widths.},
 author = {Tianhui Zhou and Yitong Li and Yuan Wu and David Carlson},
 journal = {Journal of Machine Learning Research},
 number = {257},
 openalex = {W3214984253},
 pages = {1--47},
 title = {Estimating Uncertainty Intervals from Collaborating Networks},
 url = {http://jmlr.org/papers/v22/20-1100.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1123,
 abstract = {How neural network behaves during the training over different choices of hyperparameters is an important question in the study of neural networks. In this work, inspired by the phase diagram in statistical mechanics, we draw the phase diagram for the two-layer ReLU neural network at the infinite-width limit for a complete characterization of its dynamical regimes and their dependence on hyperparameters related to initialization. Through both experimental and theoretical approaches, we identify three regimes in the phase diagram, i.e., linear regime, critical regime and condensed regime, based on the relative change of input weights as the width approaches infinity, which tends to $0$, $O(1)$ and $+\infty$, respectively. In the linear regime, NN training dynamics is approximately linear similar to a random feature model with an exponential loss decay. In the condensed regime, we demonstrate through experiments that active neurons are condensed at several discrete orientations. The critical regime serves as the boundary between above two regimes, which exhibits an intermediate nonlinear behavior with the mean-field model as a typical example. Overall, our phase diagram for the two-layer ReLU NN serves as a map for the future studies and is a first step towards a more systematical investigation of the training behavior and the implicit regularization of NNs of different structures.},
 author = {Tao Luo and Zhi-Qin John Xu and Zheng Ma and Yaoyu Zhang},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W3043000634},
 pages = {1--47},
 title = {Phase diagram for two-layer ReLU neural networks at infinite-width limit},
 url = {http://jmlr.org/papers/v22/20-1123.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1137,
 abstract = {We study the problem of recovering the structure underlying large Gaussian graphical models or, more generally, partial correlation graphs. In high-dimensional problems it is often too costly to store the entire sample covariance matrix. We propose a new input model in which one can query single entries of the covariance matrix. We prove that it is possible to recover the support of the inverse covariance matrix with low query and computational complexity. Our algorithms work in a regime when this support is represented by tree-like graphs and, more generally, for graphs of small treewidth. Our results demonstrate that for large classes of graphs, the structure of the corresponding partial correlation graphs can be determined much faster than even computing the empirical covariance matrix.},
 author = {G√°bor Lugosi and Jakub Truszkowski and Vasiliki Velona and Piotr Zwiernik},
 journal = {Journal of Machine Learning Research},
 number = {203},
 openalex = {W3207695190},
 pages = {1--41},
 title = {Learning partial correlation graphs and graphical models by covariance queries},
 url = {http://jmlr.org/papers/v22/20-1137.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1143,
 abstract = {This paper considers fair probabilistic binary classification where the outputs of primary interest are predicted probabilities, commonly referred to as scores. We formulate the problem of transforming scores to satisfy fairness constraints that are linear in conditional means of scores while minimizing a cross-entropy objective. The formulation can be applied directly to post-process classifier outputs and we also explore a pre-processing extension, thus allowing maximum freedom in selecting a classification algorithm. We derive a closed-form expression for the optimal transformed scores and a convex optimization problem for the transformation parameters. In the population limit, the transformed score function is the fairness-constrained minimizer of cross-entropy with respect to the true conditional probability of the outcome. In the finite sample setting, we propose a method called FairScoreTransformer to approach this solution using a combination of standard probabilistic classifiers and ADMM. We provide several consistency and finite-sample guarantees for FairScoreTransformer, relating to the transformation parameters and transformed score function that it obtains. Comprehensive experiments comparing to 10 existing methods show that FairScoreTransformer has advantages for score-based metrics such as Brier score and AUC while remaining competitive for binary label-based metrics such as accuracy.},
 author = {Dennis Wei and Karthikeyan Natesan Ramamurthy and Flavio P. Calmon},
 journal = {Journal of Machine Learning Research},
 number = {258},
 openalex = {W3216389579},
 pages = {1--78},
 title = {Optimized Score Transformation for Consistent Fair Classification},
 url = {http://jmlr.org/papers/v22/20-1143.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1162,
 abstract = {We study identity testing for restricted Boltzmann machines (RBMs), and more generally for undirected graphical models. Given sample access to the Gibbs distribution corresponding to an unknown or hidden model $M^*$ and given an explicit model $M$, can we distinguish if either $M = M^*$ or if they are (statistically) far apart? Daskalakis et al. (2018) presented a polynomial-time algorithm for identity testing for the ferromagnetic (attractive) Ising model. In contrast, for the antiferromagnetic (repulsive) Ising model, Bezakova et al. (2019) proved that unless $RP=NP$ there is no identity testing algorithm when $\beta d=\omega(\log{n})$, where $d$ is the maximum degree of the visible graph and $\beta$ is the largest edge weight in absolute value. 
We prove analogous hardness results for RBMs (i.e., mixed Ising models on bipartite graphs), even when there are no latent variables or an external field. Specifically, we show that if $RP \neq NP$, then when $\beta d=\omega(\log{n})$ there is no polynomial-time algorithm for identity testing for RBMs; when $\beta d =O(\log{n})$ there is an efficient identity testing algorithm that utilizes the structure learning algorithm of Klivans and Meka (2017). In addition, we prove similar lower bounds for purely ferromagnetic RBMs with inconsistent external fields, and for the ferromagnetic Potts model. Previous hardness results for identity testing of Bezakova et al. (2019) utilized the hardness of finding the maximum cuts, which corresponds to the ground states of the antiferromagnetic Ising model. Since RBMs are on bipartite graphs such an approach is not feasible. We instead introduce a general methodology to reduce from the corresponding approximate counting problem and utilize the phase transition that is exhibited by RBMs and the mean-field Potts model.},
 author = {Antonio Blanca and Zongchen Chen and Daniel ≈†tefankoviƒç and Eric Vigoda},
 journal = {Journal of Machine Learning Research},
 number = {152},
 openalex = {W3017771636},
 pages = {1--56},
 title = {Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models.},
 url = {http://jmlr.org/papers/v22/20-1162.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1164,
 abstract = {We consider bounds on the generalization performance of the least-norm linear regressor, in the over-parameterized regime where it can interpolate the data. We describe a sense in which any generalization bound of a type that is commonly proved in statistical learning theory must sometimes be very loose when applied to analyze the least-norm interpolant. In particular, for a variety of natural joint distributions on training examples, any valid generalization bound that depends only on the output of the learning algorithm, the number of training examples, and the confidence parameter, and that satisfies a mild condition (substantially weaker than monotonicity in sample size), must sometimes be very loose -- it can be bounded below by a constant when the true excess risk goes to zero.},
 author = {Peter L. Bartlett and Philip M. Long},
 journal = {Journal of Machine Learning Research},
 number = {204},
 openalex = {W3093129138},
 pages = {1--15},
 title = {Failures of model-dependent generalization bounds for least-norm interpolation},
 url = {http://jmlr.org/papers/v22/20-1164.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1170,
 abstract = {Factorization machines (FMs) are machine learning predictive models based on second-order feature interactions and FMs with sparse regularization are called sparse FMs. Such regularizations enable feature selection, which selects the most relevant features for accurate prediction, and therefore they can contribute to the improvement of the model accuracy and interpretability. However, because FMs use second-order feature interactions, the selection of features often causes the loss of many relevant feature interactions in the resultant models. In such cases, FMs with regularization specially designed for feature interaction selection trying to achieve interaction-level sparsity may be preferred instead of those just for feature selection trying to achieve feature-level sparsity. In this paper, we present a new regularization scheme for feature interaction selection in FMs. The proposed regularizer is an upper bound of the $\ell_1$ regularizer for the feature interaction matrix, which is computed from the parameter matrix of FMs. For feature interaction selection, our proposed regularizer makes the feature interaction matrix sparse without a restriction on sparsity patterns imposed by the existing methods. We also describe efficient proximal algorithms for the proposed FMs and present theoretical analyses of both existing and the new regularize. In addition, we will discuss how our ideas can be applied or extended to more accurate feature selection and other related models such as higher-order FMs and the all-subsets model. The analysis and experimental results on synthetic and real-world datasets show the effectiveness of the proposed methods.},
 author = {Kyohei Atarashi and Satoshi Oyama and Masahito Kurihara},
 journal = {Journal of Machine Learning Research},
 number = {153},
 openalex = {W3093210783},
 pages = {1--50},
 title = {Factorization Machines with Regularization for Sparse Feature Interactions},
 url = {http://jmlr.org/papers/v22/20-1170.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1176,
 abstract = {A crucial ability of human intelligence is to build up models of individual 3D objects from partial scene observations. Recent works achieve object-centric generation but without the ability to infer the representation, or achieve 3D scene representation learning but without object-centric compositionality. Therefore, learning to represent and render 3D scenes with object-centric compositionality remains elusive. In this paper, we propose a probabilistic generative model for learning to build modular and compositional 3D object models from partial observations of a multi-object scene. The proposed model can (i) infer the 3D object representations by learning to search and group object areas and also (ii) render from an arbitrary viewpoint not only individual objects but also the full scene by compositing the objects. The entire learning process is unsupervised and end-to-end. In experiments, in addition to generation quality, we also demonstrate that the learned representation permits object-wise manipulation and novel scene generation, and generalizes to various settings. Results can be found on our project website: https://sites.google.com/view/roots3d},
 author = {Chang Chen and Fei Deng and Sungjin Ahn},
 journal = {Journal of Machine Learning Research},
 number = {259},
 openalex = {W3217765097},
 pages = {1--36},
 title = {ROOTS: Object-Centric Representation and Rendering of 3D Scenes},
 url = {http://jmlr.org/papers/v22/20-1176.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1194,
 abstract = {Directional data consist of observations distributed on a (hyper)sphere, and appear in many applied fields, such as astronomy, ecology, and environmental science. This paper studies both statistical and computational problems of kernel smoothing for directional data. We generalize the classical mean shift algorithm to directional data, which allows us to identify local modes of the directional kernel density estimator (KDE). The statistical convergence rates of the directional KDE and its derivatives are derived, and the problem of mode estimation is examined. We also prove the ascending property of the directional mean shift algorithm and investigate a general problem of gradient ascent on the unit hypersphere. To demonstrate the applicability of the algorithm, we evaluate it as a mode clustering method on both simulated and real-world data sets.},
 author = {Yikun Zhang and Yen-Chi Chen},
 journal = {Journal of Machine Learning Research},
 number = {154},
 openalex = {W3094565554},
 pages = {1--92},
 title = {Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data},
 url = {http://jmlr.org/papers/v22/20-1194.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1205,
 abstract = {Langevin Monte Carlo (LMC) is a popular Bayesian sampling method. For the log-concave distribution function, the method converges exponentially fast, up to a controllable discretization error. However, the method requires the evaluation of a full gradient in each iteration, and for a problem on $\mathbb{R}^d$, this amounts to $d$ times partial derivative evaluations per iteration. The cost is high when $d\gg1$. In this paper, we investigate how to enhance computational efficiency through the application of RCD (random coordinate descent) on LMC. There are two sides of the theory: 1 By blindly applying RCD to LMC, one surrogates the full gradient by a randomly selected directional derivative per iteration. Although the cost is reduced per iteration, the total number of iteration is increased to achieve a preset error tolerance. Ultimately there is no computational gain; 2 We then incorporate variance reduction techniques, such as SAGA (stochastic average gradient) and SVRG (stochastic variance reduced gradient), into RCD-LMC. It will be proved that the cost is reduced compared with the classical LMC, and in the underdamped case, convergence is achieved with the same number of iterations, while each iteration requires merely one-directional derivative. This means we obtain the best possible computational cost in the underdamped-LMC framework.},
 author = {Zhiyan Ding and Qin Li},
 journal = {Journal of Machine Learning Research},
 number = {205},
 openalex = {W3045501363},
 pages = {1--51},
 title = {Langevin Monte Carlo: random coordinate descent and variance reduction},
 url = {http://jmlr.org/papers/v22/20-1205.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1211,
 abstract = {Modern machine learning methods are often overparametrized, allowing adaptation to the data at a fine level. This can seem puzzling; in the worst case, such models do not need to generalize. This puzzle inspired a great amount of work, arguing when overparametrization reduces test error, in a phenomenon called "double descent". Recent work aimed to understand in greater depth why overparametrization is helpful for generalization. This leads to discovering the unimodality of variance as a function of the level of parametrization, and to decomposing the variance into that arising from label noise, initialization, and randomness in the training data to understand the sources of the error. In this work we develop a deeper understanding of this area. Specifically, we propose using the analysis of variance (ANOVA) to decompose the variance in the test error in a symmetric way, for studying the generalization performance of certain two-layer linear and non-linear networks. The advantage of the analysis of variance is that it reveals the effects of initialization, label noise, and training data more clearly than prior approaches. Moreover, we also study the monotonicity and unimodality of the variance components. While prior work studied the unimodality of the overall variance, we study the properties of each term in variance decomposition. One key insight is that in typical settings, the interaction between training samples and initialization can dominate the variance; surprisingly being larger than their marginal effect. Also, we characterize "phase transitions" where the variance changes from unimodal to monotone. On a technical level, we leverage advanced deterministic equivalent techniques for Haar random matrices, that -- to our knowledge -- have not yet been used in the area. We also verify our results in numerical simulations and on empirical data examples.},
 author = {Licong Lin and Edgar Dobriban},
 journal = {Journal of Machine Learning Research},
 number = {155},
 openalex = {W3092477403},
 pages = {1--82},
 title = {What causes the test error? Going beyond bias-variance via ANOVA},
 url = {http://jmlr.org/papers/v22/20-1211.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1221,
 abstract = {We study the problem of decision-making in the setting of a scarcity of shared resources when the preferences of agents are unknown a priori and must be learned from data. Taking the two-sided matching market as a running example, we focus on the decentralized setting, where agents do not share their learned preferences with a central authority. Our approach is based on the representation of preferences in a reproducing kernel Hilbert space, and a learning algorithm for preferences that accounts for uncertainty due to the competition among the agents in the market. Under regularity conditions, we show that our estimator of preferences converges at a minimax optimal rate. Given this result, we derive optimal strategies that maximize agents' expected payoffs and we calibrate the uncertain state by taking opportunity costs into account. We also derive an incentive-compatibility property and show that the outcome from the learned strategies has a stability property. Finally, we prove a fairness property that asserts that there exists no justified envy according to the learned strategies.},
 author = {Xiaowu Dai and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {260},
 openalex = {W3096516557},
 pages = {1--50},
 title = {Learning Strategies in Decentralized Matching Markets under Uncertain Preferences},
 url = {http://jmlr.org/papers/v22/20-1221.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1223,
 abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain which features were most important to a model's prediction on a given input. However, for many tasks, simply knowing which features were important to a model's prediction may not provide enough insight to understand model behavior. The interactions between features within the model may better help us understand not only the model, but also why certain features are more important than others. In this work, we present Integrated Hessians, an extension of Integrated Gradients that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods to explain interactions, and unlike such previous methods is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks. Code available at this https URL},
 author = {Joseph D. Janizek and Pascal Sturmfels and Su-In Lee},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W3005535506},
 pages = {1--54},
 title = {Explaining Explanations: Axiomatic Feature Interactions for Deep Networks},
 url = {http://jmlr.org/papers/v22/20-1223.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1227,
 abstract = {Domain adaptation (DA) arises as an important problem in statistical machine learning when the source data used to train a model is different from the target data used to test the model. Recent advances in DA have mainly been application-driven and have largely relied on the idea of a common subspace for source and target data. To understand the empirical successes and failures of DA methods, we propose a theoretical framework via structural causal models that enables analysis and comparison of the prediction performance of DA methods. This framework also allows us to itemize the assumptions needed for the DA methods to have a low target error. Additionally, with insights from our theory, we propose a new DA method called CIRM that outperforms existing DA methods when both the covariates and label distributions are perturbed in the target data. We complement the theoretical analysis with extensive simulations to show the necessity of the devised assumptions. Reproducible synthetic and real data experiments are also provided to illustrate the strengths and weaknesses of DA methods when parts of the assumptions in our theory are violated.},
 author = {Yuansi Chen and Peter B√ºhlmann},
 journal = {Journal of Machine Learning Research},
 number = {261},
 openalex = {W3095575559},
 pages = {1--80},
 title = {Domain adaptation under structural causal models},
 url = {http://jmlr.org/papers/v22/20-1227.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1233,
 abstract = {We propose a new computationally efficient method for quantizing the weights of pre- trained neural networks that is general enough to handle both multi-layer perceptrons and convolutional neural networks. Our method deterministically quantizes layers in an iterative fashion with no complicated re-training required. Specifically, we quantize each neuron, or hidden unit, using a greedy path-following algorithm. This simple algorithm is equivalent to running a dynamical system, which we prove is stable for quantizing a single-layer neural network (or, alternatively, for quantizing the first layer of a multi-layer network) when the training data are Gaussian. We show that under these assumptions, the quantization error decays with the width of the layer, i.e., its level of over-parametrization. We provide numerical experiments, on multi-layer networks, to illustrate the performance of our methods on MNIST and CIFAR10 data, as well as for quantizing the VGG16 network using ImageNet data.},
 author = {Eric Lybrand and Rayan Saab},
 journal = {Journal of Machine Learning Research},
 number = {156},
 openalex = {W4287627788},
 pages = {1--38},
 title = {A Greedy Algorithm for Quantizing Neural Networks},
 url = {http://jmlr.org/papers/v22/20-1233.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1234,
 abstract = {We study the problem of prediction with expert advice with adversarial corruption where the adversary can at most corrupt one expert. Using tools from viscosity theory, we characterize the long-time behavior of the value function of the game between the forecaster and the adversary. We provide lower and upper bounds for the growth rate of regret without relying on a comparison result. We show that depending on the description of regret, the limiting behavior of the game can significantly differ.},
 author = {Erhan Bayraktar and Ibrahim Ekren and Xin Zhang},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W3170338947},
 pages = {1--33},
 title = {Prediction against a limited adversary},
 url = {http://jmlr.org/papers/v22/20-1234.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1235,
 abstract = {In this paper, we propose Q-learning algorithms for continuous-time deterministic optimal control problems with Lipschitz continuous controls. Our method is based on a new class of Hamilton-Jacobi-Bellman (HJB) equations derived from applying the dynamic programming principle to continuous-time Q-functions. A novel semi-discrete version of the HJB equation is proposed to design a Q-learning algorithm that uses data collected in discrete time without discretizing or approximating the system dynamics. We identify the condition under which the Q-function estimated by this algorithm converges to the optimal Q-function. For practical implementation, we propose the Hamilton-Jacobi DQN, which extends the idea of deep Q-networks (DQN) to our continuous control setting. This approach does not require actor networks or numerical solutions to optimization problems for greedy actions since the HJB equation provides a simple characterization of optimal controls via ordinary differential equations. We empirically demonstrate the performance of our method through benchmark tasks and high-dimensional linear-quadratic problems.},
 author = {Jeongho Kim and Jaeuk Shin and Insoon Yang},
 journal = {Journal of Machine Learning Research},
 number = {206},
 openalex = {W3096740192},
 pages = {1--34},
 title = {Hamilton-Jacobi Deep Q-Learning for Deterministic Continuous-Time Systems with Lipschitz Continuous Controls},
 url = {http://jmlr.org/papers/v22/20-1235.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1238,
 abstract = {In this paper, we propose a unified convergence analysis for a class of generic shuffling-type gradient methods for solving finite-sum optimization problems. Our analysis works with any sampling without replacement strategy and covers many known variants such as randomized reshuffling, deterministic or randomized single permutation, and cyclic and incremental gradient schemes. We focus on two different settings: strongly convex and nonconvex problems, but also discuss the non-strongly convex case. Our main contribution consists of new non-asymptotic and asymptotic convergence rates for a wide class of shuffling-type gradient methods in both nonconvex and convex settings. We also study uniformly randomized shuffling variants with different learning rates and model assumptions. While our rate in the nonconvex case is new and significantly improved over existing works under standard assumptions, the rate on the strongly convex one matches the existing best-known rates prior to this paper up to a constant factor without imposing a bounded gradient condition. Finally, we empirically illustrate our theoretical results via two numerical examples: nonconvex logistic regression and neural network training examples. As byproducts, our results suggest some appropriate choices for diminishing learning rates in certain shuffling variants.},
 author = {Lam M. Nguyen and Quoc Tran-Dinh and Dzung T. Phan and Phuong Ha Nguyen and Marten van Dijk},
 journal = {Journal of Machine Learning Research},
 number = {207},
 openalex = {W3008438679},
 pages = {1--44},
 title = {A Unified Convergence Analysis for Shuffling-Type Gradient Methods},
 url = {http://jmlr.org/papers/v22/20-1238.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1251,
 abstract = {The Private Aggregation of Teacher Ensembles (PATE) framework is one of the most promising recent approaches in differentially private learning. Existing theoretical analysis shows that PATE consistently learns any VC-classes in the realizable setting, but falls short in explaining its success in more general cases where the error rate of the optimal classifier is bounded away from zero. We fill in this gap by introducing the Tsybakov Noise Condition (TNC) and establish stronger and more interpretable learning bounds. These bounds provide new insights into when PATE works and improve over existing results even in the narrower realizable setting. We also investigate the compelling idea of using active learning for saving privacy budget, and empirical studies show the effectiveness of this new idea. The novel components in the proofs include a more refined analysis of the majority voting classifier - which could be of independent interest - and an observation that the synthetic "student" learning problem is nearly realizable by construction under the Tsybakov noise condition.},
 author = {Chong Liu and Yuqing Zhu and Kamalika Chaudhuri and Yu-Xiang Wang},
 journal = {Journal of Machine Learning Research},
 number = {262},
 openalex = {W3157430537},
 pages = {1--44},
 title = {Revisiting Model-Agnostic Private Learning: Faster Rates and Active Learning},
 url = {http://jmlr.org/papers/v22/20-1251.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1259,
 abstract = {We study the optimization landscape and the stability properties of training problems with squared loss for neural networks and general nonlinear conic approximation schemes. It is demonstrated that, if a nonlinear conic approximation scheme is considered that is (in an appropriately defined sense) more expressive than a classical linear approximation approach and if there exist unrealizable label vectors, then a training problem with squared loss is necessarily unstable in the sense that its solution set depends discontinuously on the label vector in the training data. We further prove that the same effects that are responsible for these instability properties are also the reason for the emergence of saddle points and spurious local minima, which may be arbitrarily far away from global solutions, and that neither the instability of the training problem nor the existence of spurious local minima can, in general, be overcome by adding a regularization term to the objective function that penalizes the size of the parameters in the approximation scheme. The latter results are shown to be true regardless of whether the assumption of realizability is satisfied or not. We demonstrate that our analysis in particular applies to training problems for free-knot interpolation schemes and deep and shallow neural networks with variable widths that involve an arbitrary mixture of various activation functions (e.g., binary, sigmoid, tanh, arctan, soft-sign, ISRU, soft-clip, SQNL, ReLU, leaky ReLU, soft-plus, bent identity, SILU, ISRLU, and ELU). In summary, the findings of this paper illustrate that the improved approximation properties of neural networks and general nonlinear conic approximation instruments are linked in a direct and quantifiable way to undesirable properties of the optimization problems that have to be solved in order to train them.},
 author = {Constantin Christof},
 journal = {Journal of Machine Learning Research},
 number = {263},
 openalex = {W3105651651},
 pages = {1--77},
 title = {On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes},
 url = {http://jmlr.org/papers/v22/20-1259.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1260,
 abstract = {As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning.},
 author = {James T. Wilson and Viacheslav Borovitskiy and Alexander Terenin and Peter Mostowsky and Marc Peter Deisenroth},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W3171411034},
 pages = {1--47},
 title = {Pathwise Conditioning of Gaussian Processes},
 url = {http://jmlr.org/papers/v22/20-1260.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1288,
 abstract = {Stochastic gradient descent (SGD) is a popular algorithm for optimization problems arising in high-dimensional inference tasks. Here one produces an estimator of an unknown parameter from independent samples of data by iteratively optimizing a loss function. This loss function is random and often non-convex. We study the performance of the simplest version of SGD, namely online SGD, from a random start in the setting where the parameter space is high-dimensional. We develop nearly sharp thresholds for the number of samples needed for consistent estimation as one varies the dimension. Our thresholds depend only on an intrinsic property of the population loss which we call the information exponent. In particular, our results do not assume uniform control on the loss itself, such as convexity or uniform derivative bounds. The thresholds we obtain are polynomial in the dimension and the precise exponent depends explicitly on the information exponent. As a consequence of our results, we find that except for the simplest tasks, almost all of the data is used simply in the initial search phase to obtain non-trivial correlation with the ground truth. Upon attaining non-trivial correlation, the descent is rapid and exhibits law of large numbers type behavior. We illustrate our approach by applying it to a wide set of inference tasks such as phase retrieval, and parameter estimation for generalized linear models, online PCA, and spiked tensor models, as well as to supervised learning for single-layer networks with general activation functions.},
 author = {Gerard Ben Arous and Reza Gheissari and Aukosh Jagannath},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W4381806233},
 pages = {1--51},
 title = {Online stochastic gradient descent on non-convex losses from high-dimensional inference},
 url = {http://jmlr.org/papers/v22/20-1288.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1289,
 abstract = {We study the problem of $k$-way clustering in signed graphs. Considerable attention in recent years has been devoted to analyzing and modeling signed graphs, where the affinity measure between nodes takes either positive or negative values. Recently, Cucuringu et al. [CDGT 2019] proposed a spectral method, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem), which casts the clustering task as a generalized eigenvalue problem optimizing a suitably defined objective function. This approach is motivated by social balance theory, where the clustering task aims to decompose a given network into disjoint groups, such that individuals within the same group are connected by as many positive edges as possible, while individuals from different groups are mainly connected by negative edges. Through extensive numerical simulations, SPONGE was shown to achieve state-of-the-art empirical performance. On the theoretical front, [CDGT 2019] analyzed SPONGE and the popular Signed Laplacian method under the setting of a Signed Stochastic Block Model (SSBM), for $k=2$ equal-sized clusters, in the regime where the graph is moderately dense. In this work, we build on the results in [CDGT 2019] on two fronts for the normalized versions of SPONGE and the Signed Laplacian. Firstly, for both algorithms, we extend the theoretical analysis in [CDGT 2019] to the general setting of $k \geq 2$ unequal-sized clusters in the moderately dense regime. Secondly, we introduce regularized versions of both methods to handle sparse graphs -- a regime where standard spectral methods underperform -- and provide theoretical guarantees under the same SSBM model. To the best of our knowledge, regularized spectral methods have so far not been considered in the setting of clustering signed graphs. We complement our theoretical results with an extensive set of numerical experiments on synthetic data.},
 author = {Mihai Cucuringu and Apoorv Vikram Singh and D√©borah Sulem and Hemant Tyagi},
 journal = {Journal of Machine Learning Research},
 number = {264},
 openalex = {W3097810218},
 pages = {1--79},
 title = {Regularized spectral methods for clustering signed networks},
 url = {http://jmlr.org/papers/v22/20-1289.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1292,
 abstract = {Recent progress in reinforcement learning has led to remarkable performance in a range of applications, but its deployment in high-stakes settings remains quite rare. One reason is a limited understanding of the behavior of reinforcement algorithms, both in terms of their regret and their ability to learn the underlying system dynamics---existing work is focused almost exclusively on characterizing rates, with little attention paid to the constants multiplying those rates that can be critically important in practice. To start to address this challenge, we study perhaps the simplest non-bandit reinforcement learning problem: linear quadratic adaptive control (LQAC). By carefully combining recent finite-sample performance bounds for the LQAC problem with a particular (less-recent) martingale central limit theorem, we are able to derive asymptotically-exact expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations on both stable and unstable systems, we find that our asymptotic theory also describes the algorithm's finite-sample behavior remarkably well.},
 author = {Feicheng Wang and Lucas Janson},
 journal = {Journal of Machine Learning Research},
 number = {265},
 openalex = {W3215639537},
 pages = {1--112},
 title = {Exact Asymptotics for Linear Quadratic Adaptive Control},
 url = {http://jmlr.org/papers/v22/20-1292.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1300,
 abstract = {Bayesian neural networks attempt to combine the strong predictive performance of neural networks with formal quantification of uncertainty associated with the predictive output in the Bayesian framework. However, it remains unclear how to endow the parameters of the network with a prior distribution that is meaningful when lifted into the output space of the network. A possible solution is proposed that enables the user to posit an appropriate Gaussian process covariance function for the task at hand. Our approach constructs a prior distribution for the parameters of the network, called a ridgelet prior, that approximates the posited Gaussian process in the output space of the network. In contrast to existing work on the connection between neural networks and Gaussian processes, our analysis is non-asymptotic, with finite sample-size error bounds provided. This establishes the universality property that a Bayesian neural network can approximate any Gaussian process whose covariance function is sufficiently regular. Our experimental assessment is limited to a proof-of-concept, where we demonstrate that the ridgelet prior can out-perform an unstructured prior on regression problems for which a suitable Gaussian process prior can be provided.},
 author = {Takuo Matsubara and Chris J. Oates and Fran√ßois-Xavier Briol},
 journal = {Journal of Machine Learning Research},
 number = {157},
 openalex = {W3093463114},
 pages = {1--57},
 title = {The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks},
 url = {http://jmlr.org/papers/v22/20-1300.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1307,
 abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric by training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open source a training dataset that covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems of WMT. We open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.},
 author = {Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Michael Auli and Armand Joulin},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W3093871477},
 pages = {1--48},
 title = {Beyond English-Centric Multilingual Machine Translation},
 url = {http://jmlr.org/papers/v22/20-1307.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1311,
 abstract = {We investigate the problem of algorithmic fairness in the case where sensitive and non-sensitive features are available and one aims to generate new, `oblivious', features that closely approximate the non-sensitive features, and are only minimally dependent on the sensitive ones. We study this question in the context of kernel methods. We analyze a relaxed version of the Maximum Mean Discrepancy criterion which does not guarantee full independence but makes the optimization problem tractable. We derive a closed-form solution for this relaxed optimization problem and complement the result with a study of the dependencies between the newly generated features and the sensitive ones. Our key ingredient for generating such oblivious features is a Hilbert-space-valued conditional expectation, which needs to be estimated from data. We propose a plug-in approach and demonstrate how the estimation errors can be controlled. While our techniques help reduce the bias, we would like to point out that no post-processing of any dataset could possibly serve as an alternative to well-designed experiments.},
 author = {Steffen Gr√ºnew√§lder and Azadeh Khaleghi},
 journal = {Journal of Machine Learning Research},
 number = {208},
 openalex = {W3005529192},
 pages = {1--36},
 title = {Oblivious Data for Fairness with Kernels},
 url = {http://jmlr.org/papers/v22/20-1311.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1316,
 abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.},
 author = {Ian Covert and Scott Lundberg and Su-In Lee},
 journal = {Journal of Machine Learning Research},
 number = {209},
 openalex = {W4287593075},
 pages = {1--90},
 title = {Explaining by Removing: A Unified Framework for Model Explanation},
 url = {http://jmlr.org/papers/v22/20-1316.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1329,
 abstract = {We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.},
 author = {Amin Rakhsha and Goran Radanovic and Rati Devidze and Xiaojin Zhu and Adish Singla},
 journal = {Journal of Machine Learning Research},
 number = {210},
 openalex = {W3109594330},
 pages = {1--45},
 title = {Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks},
 url = {http://jmlr.org/papers/v22/20-1329.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1338,
 abstract = {Bayesian networks are a powerful framework for studying the dependency structure of variables in a complex system. The problem of learning Bayesian networks is tightly associated with the given data type. Ordinal data, such as stages of cancer, rating scale survey questions, and letter grades for exams, are ubiquitous in applied research. However, existing solutions are mainly for continuous and nominal data. In this work, we propose an iterative score-and-search method - called the Ordinal Structural EM (OSEM) algorithm - for learning Bayesian networks from ordinal data. Unlike traditional approaches designed for nominal data, we explicitly respect the ordering amongst the categories. More precisely, we assume that the ordinal variables originate from marginally discretizing a set of Gaussian variables, whose structural dependence in the latent space follows a directed acyclic graph. Then, we adopt the Structural EM algorithm and derive closed-form scoring functions for efficient graph searching. Through simulation studies, we illustrate the superior performance of the OSEM algorithm compared to the alternatives and analyze various factors that may influence the learning accuracy. Finally, we demonstrate the practicality of our method with a real-world application on psychological survey data from 408 patients with co-morbid symptoms of obsessive-compulsive disorder and depression.},
 author = {Xiang Ge Luo and Giusi Moffa and Jack Kuipers},
 journal = {Journal of Machine Learning Research},
 number = {266},
 openalex = {W4287627215},
 pages = {1--44},
 title = {Learning Bayesian Networks from Ordinal Data},
 url = {http://jmlr.org/papers/v22/20-1338.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1346,
 abstract = {Kernel methods have been among the most popular techniques in machine learning, where learning tasks are solved using the property of reproducing kernel Hilbert space (RKHS). In this paper, we propose a novel data analysis framework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean embedding (KME) in RKHM. Since RKHM contains richer information than RKHS or vector-valued RKHS (vvRKHS), analysis with RKHM enables us to capture and extract structural properties in such as functional data. We show a branch of theories for RKHM to apply to data analysis, including the representer theorem, and the injectivity and universality of the proposed KME. We also show RKHM generalizes RKHS and vvRKHS. Then, we provide concrete procedures for employing RKHM and the proposed KME to data analysis.},
 author = {Yuka Hashimoto and Isao Ishikawa and Masahiro Ikeda and Fuyuta Komura and Takeshi Katsura and Yoshinobu Kawahara},
 journal = {Journal of Machine Learning Research},
 number = {267},
 openalex = {W3217442333},
 pages = {1--56},
 title = {Reproducing kernel Hilbert C*-module and kernel mean embeddings},
 url = {http://jmlr.org/papers/v22/20-1346.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-136,
 author = {Zhao Tang Luo and Huiyan Sang and Bani Mallick},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W3127218335},
 pages = {1--52},
 title = {A Bayesian Contiguous Partitioning Method for Learning Clustered Latent Variables},
 url = {http://jmlr.org/papers/v22/20-136.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1364,
 author = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
 journal = {Journal of Machine Learning Research},
 number = {268},
 openalex = {W3216772467},
 pages = {1--8},
 title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
 url = {http://jmlr.org/papers/v22/20-1364.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1366,
 abstract = {Many statistical models are given in the form of non-normalized densities with an intractable normalization constant. Since maximum likelihood estimation is computationally intensive for these models, several estimation methods have been developed which do not require explicit computation of the normalization constant, such as noise contrastive estimation (NCE) and score matching. However, model selection methods for general non-normalized models have not been proposed so far. In this study, we develop information criteria for non-normalized models estimated by NCE or score matching. They are approximately unbiased estimators of discrepancy measures for non-normalized models. Simulation results and applications to real data demonstrate that the proposed criteria enable selection of the appropriate non-normalized model in a data-driven manner.},
 author = {Takeru Matsuda and Masatoshi Uehara and Aapo Hyvarinen},
 journal = {Journal of Machine Learning Research},
 number = {158},
 openalex = {W3187336638},
 pages = {1--33},
 title = {Information criteria for non-normalized models},
 url = {http://jmlr.org/papers/v22/20-1366.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1369,
 abstract = {Random Fourier features is a widely used, simple, and effective technique for scaling up kernel methods. The existing theoretical analysis of the approach, however, remains focused on specific learning tasks and typically gives pessimistic bounds which are at odds with the empirical results. We tackle these problems and provide the first unified risk analysis of learning with random Fourier features using the squared error and Lipschitz continuous loss functions. In our bounds, the trade-off between the computational cost and the expected risk convergence rate is problem specific and expressed in terms of the regularization parameter and the \emph{number of effective degrees of freedom}. We study both the standard random Fourier features method for which we improve the existing bounds on the number of features required to guarantee the corresponding minimax risk convergence rate of kernel ridge regression, as well as a data-dependent modification which samples features proportional to \emph{ridge leverage scores} and further reduces the required number of features. As ridge leverage scores are expensive to compute, we devise a simple approximation scheme which provably reduces the computational cost without loss of statistical efficiency.},
 author = {Zhu Li and Jean-Francois Ton and Dino Oglic and Dino Sejdinovic},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W2953256123},
 pages = {1--51},
 title = {Towards A Unified Analysis of Random Fourier Features},
 url = {http://jmlr.org/papers/v22/20-1369.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1370,
 abstract = {As data are generated more and more from multiple disparate sources, multiview data sets, where each sample has features in distinct views, have ballooned in recent years. However, no comprehensive package exists that enables non-specialists to use these methods easily. mvlearn is a Python library which implements the leading multiview machine learning methods. Its simple API closely follows that of scikit-learn for increased ease-of-use. The package can be installed from Python Package Index (PyPI) and the conda package manager and is released under the MIT open-source license. The documentation, detailed examples, and all releases are available at https://mvlearn.github.io/.},
 author = {Ronan Perry and Gavin Mischler and Richard Guo and Theodore Lee and Alexander Chang and Arman Koul and Cameron Franz and Hugo Richard and Iain Carmichael and Pierre Ablin and Alexandre Gramfort and Joshua T. Vogelstein},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W4287775886},
 pages = {1--7},
 title = {mvlearn: Multiview Machine Learning in Python},
 url = {http://jmlr.org/papers/v22/20-1370.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1372,
 abstract = {We study the training of finite-width two-layer smoothed ReLU networks for binary classification using the logistic loss. We show that gradient descent drives the training loss to zero if the initial loss is small enough. When the data satisfies certain cluster and separation conditions and the network is wide enough, we show that one step of gradient descent reduces the loss sufficiently that the first result applies.},
 author = {Niladri S. Chatterji and Philip M. Long and Peter L. Bartlett},
 journal = {Journal of Machine Learning Research},
 number = {159},
 openalex = {W3112962553},
 pages = {1--48},
 title = {When does gradient descent with logistic loss find interpolating two-layer networks?},
 url = {http://jmlr.org/papers/v22/20-1372.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1374,
 abstract = {Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline) with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper at https://github.com/CAT-teams/CAT},
 author = {Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},
 journal = {Journal of Machine Learning Research},
 number = {269},
 openalex = {W2976783886},
 pages = {1--20},
 title = {CAT: Compression-Aware Training for bandwidth reduction},
 url = {http://jmlr.org/papers/v22/20-1374.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1380,
 abstract = {River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of the two most popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.},
 author = {Jacob Montiel and Max Halford and Saulo Martiello Mastelini and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse and Adil Zouitine and Heitor Murilo Gomes and Jesse Read and Talel Abdessalem and Albert Bifet},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W3111828510},
 pages = {1--8},
 title = {River: machine learning for streaming data in Python},
 url = {http://jmlr.org/papers/v22/20-1380.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1413,
 author = {Sammy Khalife and Douglas Gon√ßalves and Youssef Allouah and Leo Liberti},
 journal = {Journal of Machine Learning Research},
 number = {270},
 openalex = {W3217414905},
 pages = {1--36},
 title = {Further results on latent discourse models and word embeddings},
 url = {http://jmlr.org/papers/v22/20-1413.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1422,
 abstract = {Bayesian optimisation presents a sample-efficient methodology for global optimisation. Within this framework, a crucial performance-determining subroutine is the maximisation of the acquisition function, a task complicated by the fact that acquisition functions tend to be non-convex and thus nontrivial to optimise. In this paper, we undertake a comprehensive empirical study of approaches to maximise the acquisition function. Additionally, by deriving novel, yet mathematically equivalent, compositional forms for popular acquisition functions, we recast the maximisation task as a compositional optimisation problem, allowing us to benefit from the extensive literature in this field. We highlight the empirical advantages of the compositional approach to acquisition function maximisation across 3958 individual experiments comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given the generality of the acquisition function maximisation subroutine, we posit that the adoption of compositional optimisers has the potential to yield performance improvements across all domains in which Bayesian optimisation is currently being applied.},
 author = {Antoine Grosnit and Alexander I. Cowen-Rivers and Rasul Tutunov and Ryan-Rhys Griffiths and Jun Wang and Haitham Bou-Ammar},
 journal = {Journal of Machine Learning Research},
 number = {160},
 openalex = {W3112604776},
 pages = {1--78},
 title = {Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?},
 url = {http://jmlr.org/papers/v22/20-1422.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1429,
 abstract = {We study two-sided matching markets in which one side of the market (the players) does not have a priori knowledge about its preferences for the other side (the arms) and is required to learn its preferences from experience. Also, we assume the players have no direct means of communication. This model extends the standard stochastic multi-armed bandit framework to a decentralized multiple player setting with competition. We introduce a new algorithm for this setting that, over a time horizon $T$, attains $\mathcal{O}(\log(T))$ stable regret when preferences of the arms over players are shared, and $\mathcal{O}(\log(T)^2)$ regret when there are no assumptions on the preferences on either side. Moreover, in the setting where a single player may deviate, we show that the algorithm is incentive compatible whenever the arms' preferences are shared, but not necessarily so when preferences are fully general.},
 author = {Lydia T. Liu and Feng Ruan and Horia Mania and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {211},
 openalex = {W3110807238},
 pages = {1--34},
 title = {Bandit Learning in Decentralized Matching Markets},
 url = {http://jmlr.org/papers/v22/20-1429.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1444,
 abstract = {We provide a new adaptive method for online convex optimization, MetaGrad, that is robust to general convex losses but achieves faster rates for a broad class of special functions, including exp-concave and strongly convex functions, but also various types of stochastic and non-stochastic functions without any curvature. We prove this by drawing a connection to the Bernstein condition, which is known to imply fast rates in offline statistical learning. MetaGrad further adapts automatically to the size of the gradients. Its main feature is that it simultaneously considers multiple learning rates, which are weighted directly proportional to their empirical performance on the data using a new meta-algorithm. We provide three versions of MetaGrad. The full matrix version maintains a full covariance matrix and is applicable to learning tasks for which we can afford update time quadratic in the dimension. The other two versions provide speed-ups for high-dimensional learning tasks with an update time that is linear in the dimension: one is based on sketching, the other on running a separate copy of the basic algorithm per coordinate. We evaluate all versions of MetaGrad on benchmark online classification and regression tasks, on which they consistently outperform both online gradient descent and AdaGrad.},
 author = {Tim van Erven and Wouter M. Koolen and Dirk van der Hoeven},
 journal = {Journal of Machine Learning Research},
 number = {161},
 openalex = {W3131719325},
 pages = {1--61},
 title = {MetaGrad: Adaptation using Multiple Learning Rates in Online Learning},
 url = {http://jmlr.org/papers/v22/20-1444.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1447,
 abstract = {We develop a convex analytic approach to analyze finite width two-layer ReLU networks. We first prove that an optimal solution to the regularized training problem can be characterized as extreme points of a convex set, where simple solutions are encouraged via its convex geometrical properties. We then leverage this characterization to show that an optimal set of parameters yield linear spline interpolation for regression problems involving one dimensional or rank-one data. We also characterize the classification decision regions in terms of a kernel matrix and minimum $\ell_1$-norm solutions. This is in contrast to Neural Tangent Kernel which is unable to explain predictions of finite width networks. Our convex geometric characterization also provides intuitive explanations of hidden neurons as auto-encoders. In higher dimensions, we show that the training problem can be cast as a finite dimensional convex problem with infinitely many constraints. Then, we apply certain convex relaxations and introduce a cutting-plane algorithm to globally optimize the network. We further analyze the exactness of the relaxations to provide conditions for the convergence to a global optimum. Our analysis also shows that optimal network parameters can be also characterized as interpretable closed-form formulas in some practically relevant special cases.},
 author = {Tolga Ergen and Mert Pilanci},
 journal = {Journal of Machine Learning Research},
 number = {212},
 openalex = {W3006926186},
 pages = {1--63},
 title = {Convex Geometry and Duality of Over-parameterized Neural Networks},
 url = {http://jmlr.org/papers/v22/20-1447.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1462,
 abstract = {Quantile regression is a statistical method for estimating conditional quantiles of a response variable. In addition, for mean estimation, it is well known that quantile regression is more robust to outliers than $l_2$-based methods. By using the fused lasso penalty over a $K$-nearest neighbors graph, we propose an adaptive quantile estimator in a non-parametric setup. We show that the estimator attains optimal rate of $n^{-1/d}$ up to a logarithmic factor, under mild assumptions on the data generation mechanism of the $d$-dimensional data. We develop algorithms to compute the estimator and discuss methodology for model selection. Numerical experiments on simulated and real data demonstrate clear advantages of the proposed estimator over state of the art methods.},
 author = {Steven Siwei Ye and Oscar Hernan Madrid Padilla},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W3110271943},
 pages = {1--38},
 title = {Non-parametric Quantile Regression via the K-NN Fused Lasso},
 url = {http://jmlr.org/papers/v22/20-1462.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1468,
 abstract = {This paper develops a new mathematical framework that enables nonparametric joint semantic and geometric representation of continuous functions using data. The joint embedding is modeled by representing the processes in a reproducing kernel Hilbert space. The functions can be defined on arbitrary smooth manifolds where the action of a Lie group aligns them. The continuous functions allow the registration to be independent of a specific signal resolution. The framework is fully analytical with a closed-form derivation of the Riemannian gradient and Hessian. We study a more specialized but widely used case where the Lie group acts on functions isometrically. We solve the problem by maximizing the inner product between two functions defined over data, while the continuous action of the rigid body motion Lie group is captured through the integration of the flow in the corresponding Lie algebra. Low-dimensional cases are derived with numerical examples to show the generality of the proposed framework. The high-dimensional derivation for the special Euclidean group acting on the Euclidean space showcases the point cloud registration and bird's-eye view map registration abilities. An implementation of this framework for RGB-D cameras outperforms the state-of-the-art robust visual odometry and performs well in texture and structure-scarce environments.},
 author = {William Clark and Maani Ghaffari and Anthony Bloch},
 journal = {Journal of Machine Learning Research},
 number = {271},
 openalex = {W3217193728},
 pages = {1--50},
 title = {Nonparametric Continuous Sensor Registration},
 url = {http://jmlr.org/papers/v22/20-1468.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-147,
 author = {Jianyu Wang and Gauri Joshi},
 journal = {Journal of Machine Learning Research},
 number = {213},
 openalex = {W3205564238},
 pages = {1--50},
 title = {Cooperative SGD: A Unified Framework for the Design and Analysis of Local-Update SGD Algorithms},
 url = {http://jmlr.org/papers/v22/20-147.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1473,
 abstract = {The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at https://python.drwhy.ai/.},
 author = {Hubert Baniecki and Wojciech Kretowicz and Piotr PiƒÖtyszek and Jakub Wi≈õniewski and Przemys≈Çaw Biecek},
 journal = {Journal of Machine Learning Research},
 number = {214},
 openalex = {W3206100932},
 pages = {1--7},
 title = {dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python},
 url = {http://jmlr.org/papers/v22/20-1473.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-156,
 abstract = {We develop and analyze a new family of {\em nonaccelerated and accelerated loopless variance-reduced methods} for finite sum optimization problems. Our convergence analysis relies on a novel expected smoothness condition which upper bounds the variance of the stochastic gradient estimation by a constant times a distance-like function. This allows us to handle with ease {\em arbitrary sampling schemes} as well as the nonconvex case. We perform an in-depth estimation of these expected smoothness parameters and propose new importance samplings which allow {\em linear speedup} when the expected minibatch size is in a certain range. Furthermore, a connection between these expected smoothness parameters and expected separable overapproximation (ESO) is established, which allows us to exploit data sparsity as well. Our results recover as special cases the recently proposed loopless SVRG and loopless Katyusha.},
 author = {Xun Qian and Zheng Qu and Peter Richt√°rik},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W2948628224},
 pages = {1--47},
 title = {L-SVRG and L-Katyusha with Arbitrary Sampling},
 url = {http://jmlr.org/papers/v22/20-156.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-168,
 author = {Umit K√∂se and Andrzej Ruszczy≈Ñski},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W3128949466},
 pages = {1--34},
 title = {Risk-Averse Learning by Temporal Difference Methods with Markov Risk Measures},
 url = {http://jmlr.org/papers/v22/20-168.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-185,
 abstract = {Counterfactual inference has become a ubiquitous tool in online advertisement, recommendation systems, medical diagnosis, and econometrics. Accurate modeling of outcome distributions associated with different interventions -- known as counterfactual distributions -- is crucial for the success of these applications. In this work, we propose to model counterfactual distributions using a novel Hilbert space representation called counterfactual mean embedding (CME). The CME embeds the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, which allows us to perform causal inference over the entire landscape of the counterfactual distribution. Based on this representation, we propose a distributional treatment effect (DTE) that can quantify the causal effect over entire outcome distributions. Our approach is nonparametric as the CME can be estimated under the unconfoundedness assumption from observational data without requiring any parametric assumption about the underlying distributions. We also establish a rate of convergence of the proposed estimator which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Furthermore, our framework allows for more complex outcomes such as images, sequences, and graphs. Our experimental results on synthetic data and off-policy evaluation tasks demonstrate the advantages of the proposed estimator.},
 author = {Krikamol Muandet and Motonobu Kanagawa and Sorawit Saengkyongam and Sanparith Marukatat},
 journal = {Journal of Machine Learning Research},
 number = {162},
 openalex = {W3006957101},
 pages = {1--71},
 title = {Counterfactual Mean Embeddings},
 url = {http://jmlr.org/papers/v22/20-185.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-190,
 abstract = {We consider the problem of automated assignment of papers to reviewers in conference peer review, with a focus on fairness and statistical accuracy. Our fairness objective is to maximize the review quality of the most disadvantaged paper, in contrast to the commonly used objective of maximizing the total quality over all papers. We design an assignment algorithm based on an incremental max-flow procedure that we prove is near-optimally fair. Our statistical accuracy objective is to ensure correct recovery of the papers that should be accepted. We provide a sharp minimax analysis of the accuracy of the peer-review process for a popular objective-score model as well as for a novel subjective-score model that we propose in the paper. Our analysis proves that our proposed assignment algorithm also leads to a near-optimal statistical accuracy. Finally, we design a novel experiment that allows for an objective comparison of various assignment algorithms, and overcomes the inherent difficulty posed by the absence of a ground truth in experiments on peer-review. The results of this experiment as well as of other experiments on synthetic and real data corroborate the theoretical guarantees of our algorithm.},
 author = {Ivan Stelmakh and Nihar Shah and Aarti Singh},
 journal = {Journal of Machine Learning Research},
 number = {163},
 openalex = {W2808674837},
 pages = {1--66},
 title = {PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review.},
 url = {http://jmlr.org/papers/v22/20-190.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-195,
 author = {Ashia C. Wilson and Ben Recht and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W3171123992},
 pages = {1--34},
 title = {A Lyapunov Analysis of Accelerated Methods in Optimization},
 url = {http://jmlr.org/papers/v22/20-195.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-207,
 abstract = {We analyze the convergence rate of various momentum-based optimization algorithms from a dynamical systems point of view. Our analysis exploits fundamental topological properties, such as the continuous dependence of iterates on their initial conditions, to provide a simple characterization of convergence rates. In many cases, closed-form expressions are obtained that relate algorithm parameters to the convergence rate. The analysis encompasses discrete time and continuous time, as well as time-invariant and time-variant formulations, and is not limited to a convex or Euclidean setting. In addition, the article rigorously establishes why symplectic discretization schemes are important for momentum-based optimization algorithms, and provides a characterization of algorithms that exhibit accelerated convergence.},
 author = {Michael Muehlebach and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W4287863640},
 pages = {1--50},
 title = {Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives},
 url = {http://jmlr.org/papers/v22/20-207.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-213,
 abstract = {This paper focuses on spectral graph convolutional neural networks (ConvNets), where filters are defined as elementwise multiplication in the frequency domain of a graph. In machine learning settings where the dataset consists of signals defined on many different graphs, the trained ConvNet should generalize to signals on graphs unseen in the training set. It is thus important to transfer ConvNets between graphs. Transferability, which is a certain type of generalization capability, can be loosely defined as follows: if two graphs describe the same phenomenon, then a single filter or ConvNet should have similar repercussions on both graphs. This paper aims at debunking the common misconception that spectral filters are not transferable. We show that if two graphs discretize the same "continuous" space, then a spectral filter or ConvNet has approximately the same repercussion on both graphs. Our analysis is more permissive than the standard analysis. Transferability is typically described as the robustness of the filter to small graph perturbations and re-indexing of the vertices. Our analysis accounts also for large graph perturbations. We prove transferability between graphs that can have completely different dimensions and topologies, only requiring that both graphs discretize the same underlying space in some generic sense.},
 author = {Ron Levie and Wei Huang and Lorenzo Bucci and Michael Bronstein and Gitta Kutyniok},
 journal = {Journal of Machine Learning Research},
 number = {272},
 openalex = {W2966121357},
 pages = {1--59},
 title = {Transferability of Spectral Graph Convolutional Neural Networks},
 url = {http://jmlr.org/papers/v22/20-213.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-225,
 abstract = {TensorHive is a tool for organizing work of research and engineering teams that use servers with GPUs for machine learning workloads. In a comprehensive web interface, it supports reservation of GPUs for exclusive usage, hardware monitoring, as well as configuring, executing and queuing distributed computational jobs. Focusing on easy installation and simple configuration, the tool automatically detects the available computing resources and monitors their utilization. Reservations granted on the basis of flexible access control settings are protected by pluggable violation hooks. The job execution module includes auto-configuration templates for distributed neural network training jobs in frameworks such as TensorFlow and PyTorch. Documentation, source code, usage examples and issue tracking are available at the project page: https://github.com/roscisz/TensorHive/},
 author = {Pawe≈Ç Ro≈õciszewski and Micha≈Ç Martyniak and Filip Schodowski},
 journal = {Journal of Machine Learning Research},
 number = {215},
 openalex = {W3205622964},
 pages = {1--5},
 title = {TensorHive: Management of Exclusive GPU Access for Distributed Machine Learning Workloads},
 url = {http://jmlr.org/papers/v22/20-225.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-244,
 author = {Lili Zheng and Garvesh Raskutti and Rebecca Willett and Benjamin Mark},
 journal = {Journal of Machine Learning Research},
 number = {216},
 openalex = {W3205298872},
 pages = {1--88},
 title = {Context-dependent Networks in Multivariate Time Series: Models, Methods, and Risk Bounds in High Dimensions},
 url = {http://jmlr.org/papers/v22/20-244.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-255,
 abstract = {As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel SGD is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods.},
 author = {Ali Ramezani-Kebrya and Fartash Faghri and Ilya Markov and Vitalii Aksenov and Dan Alistarh and Daniel M. Roy},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W3168853077},
 pages = {1--43},
 title = {NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization},
 url = {http://jmlr.org/papers/v22/20-255.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-261,
 abstract = {This article considers spectral community detection in the regime of sparse networks with heterogeneous degree distributions, for which we devise an algorithm to efficiently retrieve communities. Specifically, we demonstrate that a conveniently parametrized form of regularized Laplacian matrix can be used to perform spectral clustering in sparse networks, without suffering from its degree heterogeneity. Besides, we exhibit important connections between this proposed matrix and the now popular non-backtracking matrix, the Bethe-Hessian matrix, as well as the standard Laplacian matrix. Interestingly, as opposed to competitive methods, our proposed improved parametrization inherently accounts for the hardness of the classification problem. These findings are summarized under the form of an algorithm capable of both estimating the number of communities and achieving high-quality community reconstruction.},
 author = {Lorenzo Dall'Amico and Romain Couillet and Nicolas Tremblay},
 journal = {Journal of Machine Learning Research},
 number = {217},
 openalex = {W3012795134},
 pages = {1--56},
 title = {A unified framework for spectral clustering in sparse graphs},
 url = {http://jmlr.org/papers/v22/20-261.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-275,
 abstract = {The KeOps library provides a fast and memory-efficient GPU support for tensors whose entries are given by a mathematical formula, such as kernel and distance matrices. KeOps alleviates the major bottleneck of tensor-centric libraries for kernel and geometric applications: memory consumption. It also supports automatic differentiation and outperforms standard GPU baselines, including PyTorch CUDA tensors or the Halide and TVM libraries. KeOps combines optimized C++/CUDA schemes with binders for high-level languages: Python (Numpy and PyTorch), Matlab and GNU R. As a result, high-level "quadratic" codes can now scale up to large data sets with millions of samples processed in seconds. KeOps brings graphics-like performances for kernel methods and is freely available on standard repositories (PyPi, CRAN). To showcase its versatility, we provide tutorials in a wide range of settings online at \url{www.kernel-operations.io}.},
 author = {Benjamin Charlier and Jean Feydy and Joan Alexis Glaun√®s and Fran√ßois-David Collin and Ghislain Durif},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W3020430839},
 pages = {1--6},
 title = {Kernel Operations on the GPU, with Autodiff, without Memory Overflows},
 url = {http://jmlr.org/papers/v22/20-275.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-285,
 abstract = {It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity. In particular, our definition of robust learnability requires polynomial sample complexity. We start with two negative results. We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit. We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb $\omega(\log n)$ input bits. However if the adversary is restricted to perturbing $O(\log n)$ bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework.},
 author = {Pascale Gourdeau and Varun Kanade and Marta Kwiatkowska and James Worrell},
 journal = {Journal of Machine Learning Research},
 number = {273},
 openalex = {W2970273067},
 pages = {1--29},
 title = {On the Hardness of Robust Classification},
 url = {http://jmlr.org/papers/v22/20-285.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-287,
 abstract = {This paper focuses on stochastic proximal gradient methods for optimizing a smooth non-convex loss function with a non-smooth non-convex regularizer and convex constraints. To the best of our knowledge we present the first non-asymptotic convergence results for this class of problem. We present two simple stochastic proximal gradient algorithms, for general stochastic and finite-sum optimization problems, which have the same or superior convergence complexities compared to the current best results for the unconstrained problem setting. In a numerical experiment we compare our algorithms with the current state-of-the-art deterministic algorithm and find our algorithms to exhibit superior convergence.},
 author = {Michael R. Metel and Akiko Takeda},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W3167742250},
 pages = {1--36},
 title = {Stochastic Proximal Methods for Non-Smooth Non-Convex Constrained Sparse Optimization},
 url = {http://jmlr.org/papers/v22/20-287.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-302,
 author = {Jorge P√©rez and Pablo Barcel√≥ and Javier Marinkovic},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W3149261839},
 pages = {1--35},
 title = {Attention is Turing-Complete},
 url = {http://jmlr.org/papers/v22/20-302.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-303,
 abstract = {One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.},
 author = {Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and Vincent Lariviere and Alina Beygelzimer and Florence d'Alche-Buc and Emily Fox and Hugo Larochelle},
 journal = {Journal of Machine Learning Research},
 number = {164},
 openalex = {W3013688454},
 pages = {1--20},
 title = {Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)},
 url = {http://jmlr.org/papers/v22/20-303.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-325,
 abstract = {We introduce giotto-tda, a Python library that integrates high-performance topological data analysis with machine learning via a scikit-learn-compatible API and state-of-the-art C++ implementations. The library's ability to handle various types of data is rooted in a wide range of preprocessing techniques, and its strong focus on data exploration and interpretability is aided by an intuitive plotting API. Source code, binaries, examples, and documentation can be found at https://github.com/giotto-ai/giotto-tda.},
 author = {Guillaume Tauzin and Umberto Lupo and Lewis Tunstall and Julian Burella P√©rez and Matteo Caorsi and Anibal M. Medina-Mardones and Alberto Dassatti and Kathryn Hess},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W4287817216},
 pages = {1--6},
 title = {giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration},
 url = {http://jmlr.org/papers/v22/20-325.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-326,
 abstract = {Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.},
 author = {Anton Bakhtin and Yuntian Deng and Sam Gross and Myle Ott and Marc'Aurelio Ranzato and Arthur Szlam},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W3018305985},
 pages = {1--41},
 title = {Residual Energy-Based Models for Text Generation},
 url = {http://jmlr.org/papers/v22/20-326.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-327,
 author = {Bin Liu and Xinsheng Zhang and Yufeng Liu},
 journal = {Journal of Machine Learning Research},
 number = {274},
 openalex = {W3216166990},
 pages = {1--62},
 title = {Simultaneous Change Point Inference and Structure Recovery for High Dimensional Gaussian Graphical Models},
 url = {http://jmlr.org/papers/v22/20-327.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-358,
 abstract = {We investigate the construction of early stopping rules in the nonparametric regression problem where iterative learning algorithms are used and the optimal iteration number is unknown. More precisely, we study the discrepancy principle, as well as modifications based on smoothed residuals, for kernelized spectral filter learning algorithms including gradient descent. Our main theoretical bounds are oracle inequalities established for the empirical estimation error (fixed design), and for the prediction error (random design). From these finite-sample bounds it follows that the classical discrepancy principle is statistically adaptive for slow rates occurring in the hard learning scenario, while the smoothed discrepancy principles are adaptive over ranges of faster rates (resp. higher smoothness parameters). Our approach relies on deviation inequalities for the stopping rules in the fixed design setting, combined with change-of-norm arguments to deal with the random design setting.},
 author = {Alain Celisse and Martin Wahl},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W4287814149},
 pages = {1--59},
 title = {Analyzing the discrepancy principle for kernelized spectral filter learning algorithms},
 url = {http://jmlr.org/papers/v22/20-358.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-366,
 author = {Victor Hamer and Pierre Dupont},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W3169682108},
 pages = {1--57},
 title = {An Importance Weighted Feature Selection Stability Measure},
 url = {http://jmlr.org/papers/v22/20-366.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-376,
 abstract = {In this paper, we introduce ChainerRL, an open-source deep reinforcement learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: https://github.com/chainer/chainerrl.},
 author = {Yasuhiro Fujita and Prabhat Nagarajan and Toshiki Kataoka and Takahiro Ishikawa},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2994240296},
 pages = {1--14},
 title = {ChainerRL: A Deep Reinforcement Learning Library},
 url = {http://jmlr.org/papers/v22/20-376.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-391,
 abstract = {Spectral clustering has become one of the most popular algorithms in data clustering and community detection. We study the performance of classical two-step spectral clustering via the graph Laplacian to learn the stochastic block model. Our aim is to answer the following question: when is spectral clustering via the graph Laplacian able to achieve strong consistency, i.e., the exact recovery of the underlying hidden communities? Our work provides an entrywise analysis (an $\ell_{\infty}$-norm perturbation bound) of the Fielder eigenvector of both the unnormalized and the normalized Laplacian associated with the adjacency matrix sampled from the stochastic block model. We prove that spectral clustering is able to achieve exact recovery of the planted community structure under conditions that match the information-theoretic limits.},
 author = {Shaofeng Deng and Shuyang Ling and Thomas Strohmer},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W3168808485},
 pages = {1--44},
 title = {Strong Consistency, Graph Laplacians, and the Stochastic Block Model},
 url = {http://jmlr.org/papers/v22/20-391.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-406,
 abstract = {We propose spectral methods for long-term forecasting of temporal signals stemming from linear and nonlinear quasi-periodic dynamical systems. For linear signals, we introduce an algorithm with similarities to the Fourier transform but which does not rely on periodicity assumptions, allowing for forecasting given potentially arbitrary sampling intervals. We then extend this algorithm to handle nonlinearities by leveraging Koopman theory. The resulting algorithm performs a spectral decomposition in a nonlinear, data-dependent basis. The optimization objective for both algorithms is highly non-convex. However, expressing the objective in the frequency domain allows us to compute global optima of the error surface in a scalable and efficient manner, partially by exploiting the computational properties of the Fast Fourier Transform. Because of their close relation to Bayesian Spectral Analysis, uncertainty quantification metrics are a natural byproduct of the spectral forecasting methods. We extensively benchmark these algorithms against other leading forecasting methods on a range of synthetic experiments as well as in the context of real-world power systems and fluid flows.},
 author = {Henning Lange and Steven L. Brunton and J. Nathan Kutz},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W3015146375},
 pages = {1--38},
 title = {From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction},
 url = {http://jmlr.org/papers/v22/20-406.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-410,
 abstract = {Random Matrix Theory (RMT) is applied to analyze weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization. The empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization. Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned DNNs. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a "size scale" separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This results from correlations arising at all size scales, which arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. This demonstrates that---all else being equal---DNN optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena.},
 author = {Charles H. Martin and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {165},
 openalex = {W2895616758},
 pages = {1--73},
 title = {Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning},
 url = {http://jmlr.org/papers/v22/20-410.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-416,
 abstract = {We overview the ensmallen numerical optimization library, which provides a flexible C++ framework for mathematical optimization of user-supplied objective functions. Many types of objective functions are supported, including general, differentiable, separable, constrained, and categorical. A diverse set of pre-built optimizers is provided, including Quasi-Newton optimizers and many variants of Stochastic Gradient Descent. The underlying framework facilitates the implementation of new optimizers. Optimization of an objective function typically requires supplying only one or two C++ functions. Custom behavior can be easily specified via callback functions. Empirical comparisons show that ensmallen outperforms other frameworks while providing more functionality. The library is available at https://ensmallen.org and is distributed under the permissive BSD license.},
 author = {Ryan R. Curtin and Marcus Edel and Rahul Ganesh Prabhu and Suryoday Basak and Zhihao Lou and Conrad Sanderson},
 journal = {Journal of Machine Learning Research},
 number = {166},
 openalex = {W3196826520},
 pages = {1--6},
 title = {The ensmallen library for flexible numerical optimization},
 url = {http://jmlr.org/papers/v22/20-416.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-429,
 abstract = {There is tremendous interest in precision medicine as a means to improve patient outcomes by tailoring treatment to individual characteristics. An individualized treatment rule formalizes precision medicine as a map from patient information to a recommended treatment. A treatment rule is defined to be optimal if it maximizes the mean of a scalar outcome in a population of interest, e.g., symptom reduction. However, clinical and intervention scientists often seek to balance multiple and possibly competing outcomes, e.g., symptom reduction and the risk of an adverse event. One approach to precision medicine in this setting is to elicit a composite outcome which balances all competing outcomes; unfortunately, eliciting a composite outcome directly from patients is difficult without a high-quality instrument, and an expert-derived composite outcome may not account for heterogeneity in patient preferences. We propose a new paradigm for the study of precision medicine using observational data that relies solely on the assumption that clinicians are approximately (i.e., imperfectly) making decisions to maximize individual patient utility. Estimated composite outcomes are subsequently used to construct an estimator of an individualized treatment rule which maximizes the mean of patient-specific composite outcomes. The estimated composite outcomes and estimated optimal individualized treatment rule provide new insights into patient preference heterogeneity, clinician behavior, and the value of precision medicine in a given domain. We derive inference procedures for the proposed estimators under mild conditions and demonstrate their finite sample performance through a suite of simulation experiments and an illustrative application to data from a study of bipolar depression.},
 author = {Daniel J. Luckett and Eric B. Laber and Siyeon Kim and Michael R. Kosorok},
 journal = {Journal of Machine Learning Research},
 number = {167},
 openalex = {W4287434650},
 pages = {1--40},
 title = {Estimation and Optimization of Composite Outcomes},
 url = {http://jmlr.org/papers/v22/20-429.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-445,
 abstract = {Robust Markov decision processes (MDPs) allow to compute reliable solutions for dynamic decision problems whose evolution is modeled by rewards and partially-known transition probabilities. Unfortunately, accounting for uncertainty in the transition probabilities significantly increases the computational complexity of solving robust MDPs, which severely limits their scalability. This paper describes new efficient algorithms for solving the common class of robust MDPs with s- and sa-rectangular ambiguity sets defined by weighted $L_1$ norms. We propose partial policy iteration, a new, efficient, flexible, and general policy iteration scheme for robust MDPs. We also propose fast methods for computing the robust Bellman operator in quasi-linear time, nearly matching the linear complexity the non-robust Bellman operator. Our experimental results indicate that the proposed methods are many orders of magnitude faster than the state-of-the-art approach which uses linear programming solvers combined with a robust value iteration.},
 author = {Chin Pang Ho and Marek Petrik and Wolfram Wiesemann},
 journal = {Journal of Machine Learning Research},
 number = {275},
 openalex = {W3036165382},
 pages = {1--46},
 title = {Partial Policy Iteration for L1-Robust Markov Decision Processes},
 url = {http://jmlr.org/papers/v22/20-445.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-447,
 abstract = {We design and analyze TS-Cascade, a Thompson sampling algorithm for the cascading bandit problem. In TS-Cascade, Bayesian estimates of the click probability are constructed using a univariate Gaussian; this leads to a more efficient exploration procedure vis-a-vis existing UCB-based approaches. We also incorporate the empirical variance of each item's click probability into the Bayesian updates. These two novel features allow us to prove an expected regret bound of the form O(‚àöKLT) where L and K are the number of ground items and the number of items in the chosen list respectively and T‚Äé‚â•L is the number of Thompson sampling update steps. This matches the state-of-the-art regret bounds for UCB-based algorithms. More importantly, it is the first theoretical guarantee on a Thompson sampling algorithm for any stochastic combinatorial bandit problem model with partial feedback. Empirical experiments demonstrate superiority of TS-Cascade compared to existing UCB-based procedures in terms of the expected cumulative regret and the time complexity.},
 author = {Zixin Zhong and Wang Chi Chueng and Vincent Y. F. Tan},
 journal = {Journal of Machine Learning Research},
 number = {218},
 openalex = {W2921465029},
 pages = {1--66},
 title = {A Thompson Sampling Algorithm for Cascading Bandits},
 url = {http://jmlr.org/papers/v22/20-447.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-451,
 author = {R√©mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur√©lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L√©o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W4287245321},
 pages = {1--8},
 title = {POT : Python Optimal Transport},
 url = {http://jmlr.org/papers/v22/20-451.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-469,
 abstract = {Generalized likelihoods are commonly used to obtain consistent estimators with attractive computational and robustness properties. Formally, any generalized likelihood can be used to define a generalized distribution, but an arbitrarily defined posterior cannot be expected to appropriately quantify uncertainty in any meaningful sense. In this article, we provide sufficient conditions under which generalized posteriors exhibit concentration, asymptotic normality (Bernstein-von Mises), an asymptotically correct Laplace approximation, and asymptotically correct frequentist coverage. We apply our results in detail to generalized posteriors for a wide array of generalized likelihoods, including pseudolikelihoods in general, the Gaussian Markov random field pseudolikelihood, the fully observed Boltzmann machine pseudolikelihood, the Ising model pseudolikelihood, the Cox proportional hazards partial likelihood, and a median-based likelihood for robust inference of location. Further, we show how our results can be used to easily establish the asymptotics of standard posteriors for exponential families and generalized linear models. We make no assumption of model correctness so that our results apply with or without misspecification.},
 author = {Jeffrey W. Miller},
 journal = {Journal of Machine Learning Research},
 number = {168},
 openalex = {W2963520603},
 pages = {1--53},
 title = {Asymptotic normality, concentration, and coverage of generalized posteriors},
 url = {http://jmlr.org/papers/v22/20-469.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-476,
 abstract = {Statistical methods relating tensor predictors to scalar outcomes in a regression model generally vectorize the tensor predictor and estimate the coefficients of its entries employing some form of regularization, use summaries of the tensor covariate, or use a low dimensional approximation of the coefficient tensor. However, low rank approximations of the coefficient tensor can suffer if the true rank is not small. We propose a tensor regression framework which assumes a soft version of the parallel factors (PARAFAC) approximation. In contrast to classic PARAFAC, where each entry of the coefficient tensor is the sum of products of row-specific contributions across the tensor modes, the soft tensor regression (Softer) framework allows the row-specific contributions to vary around an overall mean. We follow a Bayesian approach to inference, and show that softening the PARAFAC increases model flexibility, leads to improved estimation of coefficient tensors, more accurate identification of important predictor entries, and more precise predictions, even for a low approximation rank. From a theoretical perspective, we show that employing Softer leads to a weakly consistent posterior distribution of the coefficient tensor, irrespective of the true or approximation tensor rank, a result that is not true when employing the classic PARAFAC for tensor regression. In the context of our motivating application, we adapt Softer to symmetric and semi-symmetric tensor predictors and analyze the relationship between brain network characteristics and human traits.soft},
 author = {Georgia Papadogeorgou and Zhengwu Zhang and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {219},
 openalex = {W2981716445},
 pages = {1--53},
 title = {Soft Tensor Regression.},
 url = {http://jmlr.org/papers/v22/20-476.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-513,
 author = {Xi Chen and Victor Chernozhukov and Ivan Fernandez-Val and Scott Kostyshak and Ye Luo},
 journal = {Journal of Machine Learning Research},
 number = {220},
 openalex = {W3206877543},
 pages = {1--42},
 title = {Shape-Enforcing Operators for Generic Point and Interval Estimators of Functions},
 url = {http://jmlr.org/papers/v22/20-513.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-533,
 abstract = {In this paper, we consider first-order convergence theory and algorithms for solving a class of non-convex non-concave min-max saddle-point problems, whose objective function is weakly convex in the variables of minimization and weakly concave in the variables of maximization. It has many important applications in machine learning including training Generative Adversarial Nets (GANs). We propose an algorithmic framework motivated by the inexact proximal point method, where the weakly monotone variational inequality (VI) corresponding to the original min-max problem is solved through approximately solving a sequence of strongly monotone VIs constructed by adding a strongly monotone mapping to the original gradient mapping. We prove first-order convergence to a nearly stationary solution of the original min-max problem of the generic algorithmic framework and establish different rates by employing different algorithms for solving each strongly monotone VI. Experiments verify the convergence theory and also demonstrate the effectiveness of the proposed methods on training GANs.},
 author = {Mingrui Liu and Hassan Rafique and Qihang Lin and Tianbao Yang},
 journal = {Journal of Machine Learning Research},
 number = {169},
 openalex = {W3029957720},
 pages = {1--34},
 title = {First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems},
 url = {http://jmlr.org/papers/v22/20-533.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-537,
 author = {Chidubem Arachie and Bert Huang},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W3166226836},
 pages = {1--33},
 title = {A General Framework for Adversarial Label Learning},
 url = {http://jmlr.org/papers/v22/20-537.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-539,
 abstract = {Much of the theory for the lasso in the linear model $Y = X \beta^* + \varepsilon$ hinges on the quantity $2 \| X^\top \varepsilon \|_{\infty} / n$, which we call the lasso's effective noise. Among other things, the effective noise plays an important role in finite-sample bounds for the lasso, the calibration of the lasso's tuning parameter, and inference on the parameter vector $\beta^*$. In this paper, we develop a bootstrap-based estimator of the quantiles of the effective noise. The estimator is fully data-driven, that is, does not require any additional tuning parameters. We equip our estimator with finite-sample guarantees and apply it to tuning parameter calibration for the lasso and to high-dimensional inference on the parameter vector $\beta^*$.},
 author = {Johannes Lederer and Michael Vogt},
 journal = {Journal of Machine Learning Research},
 number = {276},
 openalex = {W4285687721},
 pages = {1--32},
 title = {Estimating the Lasso's Effective Noise},
 url = {http://jmlr.org/papers/v22/20-539.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-553,
 abstract = {Generative Adversarial Networks (GANs) have been successful in producing outstanding results in areas as diverse as image, video, and text generation. Building on these successes, a large number of empirical studies have validated the benefits of the cousin approach called Wasserstein GANs (WGANs), which brings stabilization in the training process. In the present paper, we add a new stone to the edifice by proposing some theoretical advances in the properties of WGANs. First, we properly define the architecture of WGANs in the context of integral probability metrics parameterized by neural networks and highlight some of their basic mathematical features. We stress in particular interesting optimization properties arising from the use of a parametric 1-Lipschitz discriminator. Then, in a statistically-driven approach, we study the convergence of empirical WGANs as the sample size tends to infinity, and clarify the adversarial effects of the generator and the discriminator by underlining some trade-off properties. These features are finally illustrated with experiments using both synthetic and real-world datasets.},
 author = {G√©rard Biau and Maxime Sangnier and Ugo Tanielian},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W3033050199},
 pages = {1--45},
 title = {Some Theoretical Insights into Wasserstein GANs},
 url = {http://jmlr.org/papers/v22/20-553.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-567,
 abstract = {Since the discovery of adversarial examples - the ability to fool modern CNN classifiers with tiny perturbations of the input, there has been much discussion whether they are a "bug" that is specific to current neural architectures and training methods or an inevitable "feature" of high dimensional geometry. In this paper, we argue for examining adversarial examples from the perspective of Bayes-Optimal classification. We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and derive analytic conditions on the distributions under which these classifiers are provably robust against any adversarial attack even in high dimensions. Our results show that even when these "gold standard" optimal classifiers are robust, CNNs trained on the same datasets consistently learn a vulnerable classifier, indicating that adversarial examples are often an avoidable "bug". We further show that RBF SVMs trained on the same data consistently learn a robust classifier. The same trend is observed in experiments with real images in different datasets.},
 author = {Eitan Richardson and Yair Weiss},
 journal = {Journal of Machine Learning Research},
 number = {221},
 openalex = {W2995592330},
 pages = {1--28},
 title = {A Bayes-Optimal View on Adversarial Examples},
 url = {http://jmlr.org/papers/v22/20-567.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-576,
 abstract = {We propose a Markov chain Monte Carlo (MCMC) algorithm based on third-order Langevin dynamics for sampling from distributions with log-concave and smooth densities. The higher-order dynamics allow for more flexible discretization schemes, and we develop a specific method that combines splitting with more accurate integration. For a broad class of $d$-dimensional distributions arising from generalized linear models, we prove that the resulting third-order algorithm produces samples from a distribution that is at most $\varepsilon > 0$ in Wasserstein distance from the target distribution in $O\left(\frac{d^{1/4}}{ \varepsilon^{1/2}} \right)$ steps. This result requires only Lipschitz conditions on the gradient. For general strongly convex potentials with $\alpha$-th order smoothness, we prove that the mixing time scales as $O \left(\frac{d^{1/4}}{\varepsilon^{1/2}} + \frac{d^{1/2}}{\varepsilon^{1/(\alpha - 1)}} \right)$.},
 author = {Wenlong Mou and Yi-An Ma and Martin J. Wainwright and Peter L. Bartlett and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W3128089927},
 pages = {1--41},
 title = {High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm},
 url = {http://jmlr.org/papers/v22/20-576.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-583,
 abstract = {We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.},
 author = {Rahul Parhi and Robert D. Nowak},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W3108216709},
 pages = {1--40},
 title = {Banach Space Representer Theorems for Neural Networks and Ridge Splines},
 url = {http://jmlr.org/papers/v22/20-583.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-588,
 abstract = {Computing Wasserstein barycenters is a fundamental geometric problem with widespread applications in machine learning, statistics, and computer graphics. However, it is unknown whether Wasserstein barycenters can be computed in polynomial time, either exactly or to high precision (i.e., with $\textrm{polylog}(1/\varepsilon)$ runtime dependence). This paper answers these questions in the affirmative for any fixed dimension. Our approach is to solve an exponential-size linear programming formulation by efficiently implementing the corresponding separation oracle using techniques from computational geometry.},
 author = {Jason M Altschuler and Enric Boix-Adsera},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W4287757887},
 pages = {1--19},
 title = {Wasserstein barycenters can be computed in polynomial time in fixed dimension},
 url = {http://jmlr.org/papers/v22/20-588.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-589,
 abstract = {Matrix factorization methods - including Factor analysis (FA), and Principal Components Analysis (PCA) - are widely used for inferring and summarizing structure in multivariate data. Many matrix factorization methods exist, corresponding to different assumptions on the elements of the underlying matrix factors. For example, many recent methods use a penalty or prior distribution to achieve sparse representations (Sparse FA/PCA). Here we introduce a general Empirical Bayes approach to matrix factorization (EBMF), whose key feature is that it uses the observed data to estimate prior distributions on matrix elements. We derive a correspondingly-general variational fitting algorithm, which reduces fitting EBMF to solving a simpler problem - the so-called normal means problem. We implement this general algorithm, but focus particular attention on the use of sparsity-inducing priors that are uni-modal at 0. This yields a sparse EBMF approach - essentially a version of sparse FA/PCA - that automatically adapts the amount of sparsity to the data. We demonstrate the benefits of our approach through both numerical comparisons with competing methods and through analysis of data from the GTEx (Genotype Tissue Expression) project on genetic associations across 44 human tissues. In numerical comparisons EBMF often provides more accurate inferences than other methods. In the GTEx data, EBMF identifies interpretable structure that concords with known relationships among human tissues. Software implementing our approach is available at this https URL},
 author = {Wei Wang and Matthew Stephens},
 journal = {Journal of Machine Learning Research},
 number = {120},
 openalex = {W3172786606},
 pages = {1--40},
 title = {Empirical Bayes Matrix Factorization},
 url = {http://jmlr.org/papers/v22/20-589.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-600,
 abstract = {We propose a flexible ensemble classification framework, Random Subspace Ensemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate many weak learners, where each weak learner is a base classifier trained in a subspace optimally selected from a collection of random subspaces. To conduct subspace selection, we propose a new criterion, ratio information criterion (RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis includes the risk and Monte-Carlo variance of the RaSE classifier, establishing the screening consistency and weak consistency of RIC, and providing an upper bound for the misclassification rate of the RaSE classifier. In addition, we show that in a high-dimensional framework, the number of random subspaces needs to be very large to guarantee that a subspace covering signals is selected. Therefore, we propose an iterative version of the RaSE algorithm and prove that under some specific conditions, a smaller number of generated random subspaces are needed to find a desirable subspace through iteration. An array of simulations under various models and real-data applications demonstrate the effectiveness and robustness of the RaSE classifier and its iterative version in terms of low misclassification rate and accurate feature ranking. The RaSE algorithm is implemented in the R package RaSEn on CRAN.},
 author = {Ye Tian and Yang Feng},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W3128403374},
 pages = {1--93},
 title = {RaSE: Random Subspace Ensemble Classification},
 url = {http://jmlr.org/papers/v22/20-600.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-603,
 abstract = {We compare classification and regression tasks in the overparameterized linear model with Gaussian features. On the one hand, we show that with sufficient overparameterization all training points are support vectors: solutions obtained by least-squares minimum-norm interpolation, typically used for regression, are identical to those produced by the hard-margin support vector machine (SVM) that minimizes the hinge loss, typically used for training classifiers. On the other hand, we show that there exist regimes where these solutions are near-optimal when evaluated by the 0-1 test loss function, but do not generalize if evaluated by the square loss function, i.e. they achieve the null risk. Our results demonstrate the very different roles and properties of loss functions used at the training phase (optimization) and the testing phase (generalization).},
 author = {Vidya Muthukumar and Adhyyan Narang and Vignesh Subramanian and Mikhail Belkin and Daniel Hsu and Anant Sahai},
 journal = {Journal of Machine Learning Research},
 number = {222},
 openalex = {W3025991882},
 pages = {1--69},
 title = {Classification vs regression in overparameterized regimes: Does the loss function matter?},
 url = {http://jmlr.org/papers/v22/20-603.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-610,
 abstract = {Driven by a wide range of applications, many principal subspace estimation problems have been studied individually under different structural constraints. This paper presents a unified framework for the statistical analysis of a general structured principal subspace estimation problem which includes as special cases non-negative PCA/SVD, sparse PCA/SVD, subspace constrained PCA/SVD, and spectral clustering. General minimax lower and upper bounds are established to characterize the interplay between the information-geometric complexity of the structural set for the principal subspaces, the signal-to-noise ratio (SNR), and the dimensionality. The results yield interesting phase transition phenomena concerning the rates of convergence as a function of the SNRs and the fundamental limit for consistent estimation. Applying the general results to the specific settings yields the minimax rates of convergence for those problems, including the previous unknown optimal rates for non-negative PCA/SVD, sparse SVD and subspace constrained PCA/SVD.},
 author = {Tony Cai and Hongzhe Li and Rong Ma},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W3127945696},
 pages = {1--45},
 title = {Optimal Structured Principal Subspace Estimation: Metric Entropy and Minimax Rates},
 url = {http://jmlr.org/papers/v22/20-610.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-611,
 author = {Bin Gu and Xiyuan Wei and Shangqian Gao and Ziran Xiong and Cheng Deng and Heng Huang},
 journal = {Journal of Machine Learning Research},
 number = {170},
 openalex = {W3188875283},
 pages = {1--47},
 title = {Black-Box Reductions for Zeroth-Order Gradient Algorithms to Achieve Lower Query Complexity},
 url = {http://jmlr.org/papers/v22/20-611.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-618,
 abstract = {We study the Extended Kalman Filter in constant dynamics, offering a bayesian perspective of stochastic optimization. We obtain high probability bounds on the cumulative excess risk in an unconstrained setting. In order to avoid any projection step we propose a two-phase analysis. First, for linear and logistic regressions, we prove that the algorithm enters a local phase where the estimate stays in a small region around the optimum. We provide explicit bounds with high probability on this convergence time. Second, for generalized linear regressions, we provide a martingale analysis of the excess risk in the local phase, improving existing ones in bounded stochastic optimization. The EKF appears as a parameter-free online algorithm with O(d^2) cost per iteration that optimally solves some unconstrained optimization problems.},
 author = {Joseph de Vilmarest and Olivier Wintenberger},
 journal = {Journal of Machine Learning Research},
 number = {223},
 openalex = {W3005440362},
 pages = {1--55},
 title = {Stochastic Online Optimization using Kalman Recursion},
 url = {http://jmlr.org/papers/v22/20-618.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-620,
 abstract = {Recurrent neural networks (RNNs) are brain-inspired models widely used in machine learning for analyzing sequential data. The present work is a contribution towards a deeper understanding of how RNNs process input signals using the response theory from nonequilibrium statistical mechanics. For a class of continuous-time stochastic RNNs (SRNNs) driven by an input signal, we derive a Volterra type series representation for their output. This representation is interpretable and disentangles the input signal from the SRNN architecture. The kernels of the series are certain recursively defined correlation functions with respect to the unperturbed dynamics that completely determine the output. Exploiting connections of this representation and its implications to rough paths theory, we identify a universal feature -- the response feature, which turns out to be the signature of tensor product of the input signal and a natural support basis. In particular, we show that the SRNNs can be viewed as kernel machines operating on a reproducing kernel Hilbert space associated with the response feature.},
 author = {Soon Hoe Lim},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W3035804453},
 pages = {1--48},
 title = {Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory},
 url = {http://jmlr.org/papers/v22/20-620.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-625,
 abstract = {Inverse reinforcement learning (IRL) aims to estimate the reward function of optimizing agents by observing their response (estimates or actions). This paper considers IRL when noisy estimates of the gradient of a reward function generated by multiple stochastic gradient agents are observed. We present a generalized Langevin dynamics algorithm to estimate the reward function $R(Œ∏)$; specifically, the resulting Langevin algorithm asymptotically generates samples from the distribution proportional to $\exp(R(Œ∏))$. The proposed IRL algorithms use kernel-based passive learning schemes. We also construct multi-kernel passive Langevin algorithms for IRL which are suitable for high dimensional data. The performance of the proposed IRL algorithms are illustrated on examples in adaptive Bayesian learning, logistic regression (high dimensional problem) and constrained Markov decision processes. We prove weak convergence of the proposed IRL algorithms using martingale averaging methods. We also analyze the tracking performance of the IRL algorithms in non-stationary environments where the utility function $R(Œ∏)$ jump changes over time as a slow Markov chain.},
 author = {Vikram Krishnamurthy and George Yin},
 journal = {Journal of Machine Learning Research},
 number = {121},
 openalex = {W3166190477},
 pages = {1--49},
 title = {Langevin Dynamics for Adaptive Inverse Reinforcement Learning of Stochastic Gradient Algorithms},
 url = {http://jmlr.org/papers/v22/20-625.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-627,
 abstract = {Distributed machine learning systems have been receiving increasing attentions for their efficiency to process large scale data. Many distributed frameworks have been proposed for different machine learning tasks. In this paper, we study the distributed kernel regression via the divide and conquer approach. This approach has been proved asymptotically minimax optimal if the kernel is perfectly selected so that the true regression function lies in the associated reproducing kernel Hilbert space. However, this is usually, if not always, impractical because kernels that can only be selected via prior knowledge or a tuning process are hardly perfect. Instead it is more common that the kernel is good enough but imperfect in the sense that the true regression can be well approximated by but does not lie exactly in the kernel space. We show distributed kernel regression can still achieves capacity independent optimal rate in this case. To this end, we first establish a general framework that allows to analyze distributed regression with response weighted base algorithms by bounding the error of such algorithms on a single data set, provided that the error bounds has factored the impact of the unexplained variance of the response variable. Then we perform a leave one out analysis of the kernel ridge regression and bias corrected kernel ridge regression, which in combination with the aforementioned framework allows us to derive sharp error bounds and capacity independent optimal rates for the associated distributed kernel regression algorithms. As a byproduct of the thorough analysis, we also prove the kernel ridge regression can achieve rates faster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting which, to our best knowledge, are first observed and novel in regression learning.},
 author = {Hongwei Sun and Qiang Wu},
 journal = {Journal of Machine Learning Research},
 number = {171},
 openalex = {W3039270669},
 pages = {1--34},
 title = {Optimal Rates of Distributed Regression with Imperfect Kernels},
 url = {http://jmlr.org/papers/v22/20-627.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-633,
 author = {Carlo D'Eramo and Andrea Cini and Alessandro Nuara and Matteo Pirotta and Cesare Alippi and Jan Peters and Marcello Restelli},
 journal = {Journal of Machine Learning Research},
 number = {277},
 openalex = {W3215753636},
 pages = {1--51},
 title = {Gaussian Approximation for Bias Reduction in Q-Learning},
 url = {http://jmlr.org/papers/v22/20-633.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-653,
 abstract = {We propose a variance reduction framework for variational inference using the Multilevel Monte Carlo (MLMC) method. Our framework is built on reparameterized gradient estimators and "recycles" parameters obtained from past update history in optimization. In addition, our framework provides a new optimization algorithm based on stochastic gradient descent (SGD) that adaptively estimates the sample size used for gradient estimation according to the ratio of the gradient variance. We theoretically show that, with our method, the variance of the gradient estimator decreases as optimization proceeds and that a learning rate scheduler function helps improve the convergence. We also show that, in terms of the \textit{signal-to-noise} ratio, our method can improve the quality of gradient estimation by the learning rate scheduler function without increasing the initial sample size. Finally, we confirm that our method achieves faster convergence and reduces the variance of the gradient estimator compared with other methods through experimental comparisons with baseline methods using several benchmark datasets.},
 author = {Masahiro Fujisawa and Issei Sato},
 journal = {Journal of Machine Learning Research},
 number = {278},
 openalex = {W2914683866},
 pages = {1--44},
 title = {Multilevel Monte Carlo Variational Inference},
 url = {http://jmlr.org/papers/v22/20-653.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-661,
 abstract = {The goal of Sparse Convex Optimization is to optimize a convex function $f$ under a sparsity constraint $s\leq s^*\gamma$, where $s^*$ is the target number of non-zero entries in a feasible solution (sparsity) and $\gamma\geq 1$ is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number $\kappa$. The best known algorithms guarantee to find an approximate solution of value $f(x^*)+\epsilon$ with the sparsity bound of $\gamma = O\left(\kappa\min\left\{\log \frac{f(x^0)-f(x^*)}{\epsilon}, \kappa\right\}\right)$, where $x^*$ is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to $\gamma=O(\kappa)$, which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general $f$, under the condition $s > s^* \frac{\kappa^2}{4}$, which yields Compressed Sensing bounds under the Restricted Isometry Property (RIP). When compared to other Compressed Sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function $f$ that meets the RIP condition.},
 author = {Kyriakos Axiotis and Maxim Sviridenko},
 journal = {Journal of Machine Learning Research},
 number = {122},
 openalex = {W3037792412},
 pages = {1--47},
 title = {Sparse Convex Optimization via Adaptively Regularized Hard Thresholding},
 url = {http://jmlr.org/papers/v22/20-661.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-662,
 abstract = {Gaussian processes are ubiquitous in machine learning, statistics, and applied mathematics. They provide a flexible modelling framework for approximating functions, whilst simultaneously quantifying uncertainty. However, this is only true when the model is well-specified, which is often not the case in practice. In this paper, we study the properties of Gaussian process means when the smoothness of the model and the likelihood function are misspecified. In this setting, an important theoretical question of practial relevance is how accurate the Gaussian process approximations will be given the difficulty of the problem, our model and the extent of the misspecification. The answer to this problem is particularly useful since it can inform our choice of model and experimental design. In particular, we describe how the experimental design and choice of kernel and kernel hyperparameters can be adapted to alleviate model misspecification.},
 author = {George Wynne and Fran√ßois-Xavier Briol and Mark Girolami},
 journal = {Journal of Machine Learning Research},
 number = {123},
 openalex = {W3035032733},
 pages = {1--40},
 title = {Convergence Guarantees for Gaussian Process Means With Misspecified Likelihoods and Smoothness},
 url = {http://jmlr.org/papers/v22/20-662.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-673,
 abstract = {Despite the availability of numerous statistical and machine learning tools for joint feature modeling, many scientists investigate features marginally, i.e., one feature at a time. This is partly due to training and convention but also roots in scientists' strong interests in simple visualization and interpretability. As such, marginal feature ranking for some predictive tasks, e.g., prediction of cancer driver genes, is widely practiced in the process of scientific discoveries. In this work, we focus on marginal ranking for binary classification, one of the most common predictive tasks. We argue that the most widely used marginal ranking criteria, including the Pearson correlation, the two-sample t test, and two-sample Wilcoxon rank-sum test, do not fully take feature distributions and prediction objectives into account. To address this gap in practice, we propose two ranking criteria corresponding to two prediction objectives: the classical criterion (CC) and the Neyman-Pearson criterion (NPC), both of which use model-free nonparametric implementations to accommodate diverse feature distributions. Theoretically, we show that under regularity conditions, both criteria achieve sample-level ranking that is consistent with their population-level counterpart with high probability. Moreover, NPC is robust to sampling bias when the two class proportions in a sample deviate from those in the population. This property endows NPC good potential in biomedical research where sampling biases are ubiquitous. We demonstrate the use and relative advantages of CC and NPC in simulation and real data studies. Our model-free objective-based ranking idea is extendable to ranking feature subsets and generalizable to other prediction tasks and learning objectives.},
 author = {Jingyi Jessica Li and Yiling Elaine Chen and Xin Tong},
 journal = {Journal of Machine Learning Research},
 number = {124},
 openalex = {W3172098162},
 pages = {1--54},
 title = {A flexible model-free prediction-based framework for feature ranking.},
 url = {http://jmlr.org/papers/v22/20-673.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-676,
 abstract = {Deep neural networks (DNNs) generalise remarkably well in the overparameterised regime, suggesting a strong inductive bias towards functions with low generalisation error. We empirically investigate this bias by calculating, for a range of architectures and datasets, the probability PSGD(f‚à£S) that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function f consistent with a training set S. We also use Gaussian processes to estimate the Bayesian posterior probability PB(f‚à£S) that the DNN expresses f upon random sampling of its parameters, conditioned on S. Our main findings are that PSGD(f‚à£S) correlates remarkably well with PB(f‚à£S) and that PB(f‚à£S) is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines PB(f‚à£S)), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior PB(f‚à£S) is the first order determinant of PSGD(f‚à£S), there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on PSGD(f‚à£S) and/or PB(f‚à£S), can shed light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.},
 author = {Chris Mingard and Guillermo Valle-P√©rez and Joar Skalse and Ard A. Louis},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W3157484000},
 pages = {1--64},
 title = {Is SGD a Bayesian sampler? Well, almost},
 url = {http://jmlr.org/papers/v22/20-676.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-688,
 abstract = {Model-based clustering is widely used in a variety of application areas. However, fundamental concerns remain about robustness. In particular, results can be sensitive to the choice of kernel representing the within-cluster data density. Leveraging on properties of pairwise differences between data points, we propose a class of Bayesian distance clustering methods, which rely on modeling the likelihood of the pairwise distances in place of the original data. Although some information in the data is discarded, we gain substantial robustness to modeling assumptions. The proposed approach represents an appealing middle ground between distance- and model-based clustering, drawing advantages from each of these canonical approaches. We illustrate dramatic gains in the ability to infer clusters that are not well represented by the usual choices of kernel. A simulation study is included to assess performance relative to competitors, and we apply the approach to clustering of brain genome expression data.},
 author = {Leo L. Duan and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {224},
 openalex = {W2897573629},
 pages = {1--27},
 title = {Bayesian Distance Clustering.},
 url = {http://jmlr.org/papers/v22/20-688.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-689,
 abstract = {We consider so-called univariate unlinked (sometimes ``decoupled,'' or ``shuffled'') regression when the unknown regression curve is monotone. In standard monotone regression, one observes a pair $(X,Y)$ where a response $Y$ is linked to a covariate $X$ through the model $Y= m_0(X) + Œµ$, with $m_0$ the (unknown) monotone regression function and $Œµ$ the unobserved error (assumed to be independent of $X$). In the unlinked regression setting one gets only to observe a vector of realizations from both the response $Y$ and from the covariate $X$ where now $Y \stackrel{d}{=} m_0(X) + Œµ$. There is no (observed) pairing of $X$ and $Y$. Despite this, it is actually still possible to derive a consistent non-parametric estimator of $m_0$ under the assumption of monotonicity of $m_0$ and knowledge of the distribution of the noise $Œµ$. In this paper, we establish an upper bound on the rate of convergence of such an estimator under minimal assumption on the distribution of the covariate $X$. We discuss extensions to the case in which the distribution of the noise is unknown. We develop a second order algorithm for its computation, and we demonstrate its use on synthetic data. Finally, we apply our method (in a fully data driven way, without knowledge of the error distribution) on longitudinal data from the US Consumer Expenditure Survey.},
 author = {Fadoua Balabdaoui and Charles R. Doss and C√©cile Durot},
 journal = {Journal of Machine Learning Research},
 number = {172},
 openalex = {W3038495971},
 pages = {1--60},
 title = {Unlinked monotone regression},
 url = {http://jmlr.org/papers/v22/20-689.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-697,
 abstract = {Gradient descent (GD) is known to converge quickly for convex objective functions, but it can be trapped at local minima. On the other hand, Langevin dynamics (LD) can explore the state space and find global minima, but in order to give accurate estimates, LD needs to run with a small discretization step size and weak stochastic force, which in general slow down its convergence. This paper shows that these two algorithms and their non-swapping variants. can ``collaborate" through a simple exchange mechanism, in which they swap their current positions if LD yields a lower objective function. This idea can be seen as the singular limit of the replica-exchange technique from the sampling literature. We show that this new algorithm converges to the global minimum linearly with high probability, assuming the objective function is strongly convex in a neighborhood of the unique global minimum. By replacing gradients with stochastic gradients, and adding a proper threshold to the exchange mechanism, our algorithm can also be used in online settings. We also study non-swapping variants of the algorithm, which achieve similar performance. We further verify our theoretical results through some numerical experiments and observe superior performance of the proposed algorithm over running GD or LD alone.},
 author = {Jing Dong and Xin T. Tong},
 journal = {Journal of Machine Learning Research},
 number = {173},
 openalex = {W3191940995},
 pages = {1--59},
 title = {Replica Exchange for Non-Convex Optimization},
 url = {http://jmlr.org/papers/v22/20-697.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-704,
 abstract = {We study an interesting variant of the stochastic multi-armed bandit problem, which we call the Fair-MAB problem, where, in addition to the objective of maximizing the sum of expected rewards, the algorithm also needs to ensure that at any time, each arm is pulled at least a pre-specified fraction of times. We investigate the interplay between learning and fairness in terms of a pre-specified vector denoting the fractions of guaranteed pulls. We define a fairness-aware regret, which we call r-Regret, that takes into account the above fairness constraints and extends the conventional notion of regret in a natural way. Our primary contribution is to obtain a complete characterization of a class of Fair-MAB algorithms via two parameters: the unfairness tolerance and the learning algorithm used as a black-box. For this class of algorithms, we provide a fairness guarantee that holds uniformly over time, irrespective of the choice of the learning algorithm. Further, when the learning algorithm is UCB1, we show that our algorithm achieves constant r-Regret for a large enough time horizon. Finally, we analyze the cost of fairness in terms of the conventional notion of regret. We conclude by experimentally validating our theoretical results.},
 author = {Vishakha Patil and Ganesh Ghalme and Vineet Nair and Y. Narahari},
 journal = {Journal of Machine Learning Research},
 number = {174},
 openalex = {W2996787464},
 pages = {1--31},
 title = {Achieving Fairness in the Stochastic Multi-Armed Bandit Problem},
 url = {http://jmlr.org/papers/v22/20-704.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-705,
 author = {Zengfeng Huang and Xuemin Lin and Wenjie Zhang and Ying Zhang},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W3157873772},
 pages = {1--38},
 title = {Communication-Efficient Distributed Covariance Sketch, with Application to Distributed PCA},
 url = {http://jmlr.org/papers/v22/20-705.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-706,
 abstract = {Modern neural networks (NN) featuring a large number of layers (depth) and units per layer (width) have achieved a remarkable performance across many domains. While there exists a vast literature on the interplay between infinitely wide NNs and Gaussian processes, a little is known about analogous interplays with respect to infinitely deep NNs. NNs with independent and identically distributed (i.i.d.) initializations exhibit undesirable forward and backward propagation properties as the number of layers increases. To overcome these drawbacks, Peluchetti and Favaro (2020) considered fully-connected residual networks (ResNets) with network's parameters initialized by means of distributions that shrink as the number of layers increases, thus establishing an interplay between infinitely deep ResNets and solutions to stochastic differential equations, i.e. diffusion processes, and showing that infinitely deep ResNets does not suffer from undesirable forward-propagation properties. In this paper, we review the results of Peluchetti and Favaro (2020), extending them to convolutional ResNets, and we establish analogous backward-propagation results, which directly relate to the problem of training fully-connected deep ResNets. Then, we investigate the more general setting of doubly infinite NNs, where both network's width and network's depth grow unboundedly. We focus on doubly infinite fully-connected ResNets, for which we consider i.i.d. initializations. Under this setting, we show that the dynamics of quantities of interest converge, at initialization, to deterministic limits. This allow us to provide analytical expressions for inference, both in the case of weakly trained and fully trained ResNets. Our results highlight a limited expressive power of doubly infinite ResNets when the unscaled network's parameters are i.i.d. and the residual blocks are shallow.},
 author = {Stefano Peluchetti and Stefano Favaro},
 journal = {Journal of Machine Learning Research},
 number = {175},
 openalex = {W4287726622},
 pages = {1--48},
 title = {Doubly infinite residual neural networks: a diffusion process approach},
 url = {http://jmlr.org/papers/v22/20-706.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-721,
 abstract = {We design a new algorithm for the Euclidean $k$-means problem that operates in the local model of differential privacy. Unlike in the non-private literature, differentially private algorithms for the $k$-means incur both additive and multiplicative errors. Our algorithm significantly reduces the additive error while keeping the multiplicative error the same as in previous state-of-the-art results. Specifically, on a database of size $n$, our algorithm guarantees $O(1)$ multiplicative error and $\approx n^{1/2+a}$ additive error for an arbitrarily small constant $a$, whereas all previous algorithms in the local model on had additive error $\approx n^{2/3+a}$. 
We give a simple lower bound showing that additive error of $\approx\sqrt{n}$ is necessary for $k$-means algorithms in the local model (at least for algorithms with a constant number of interaction rounds, which is the setting we consider in this paper).},
 author = {Uri Stemmer},
 journal = {Journal of Machine Learning Research},
 number = {176},
 openalex = {W3191542450},
 pages = {1--30},
 title = {Locally Private k-Means Clustering},
 url = {http://jmlr.org/papers/v22/20-721.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-751,
 abstract = {Constructing or learning a function from a finite number of sampled data points (measurements) is a fundamental problem in science and engineering. This is often formulated as a minimum norm interpolation problem, regularized learning problem or, in general, a semi-discrete inverse problem, in certain functional spaces. The choice of an appropriate space is crucial for solutions of these problems. Motivated by sparse representations of the reconstructed functions such as compressed sensing and sparse learning, much of the recent research interest has been directed to considering these problems in certain Banach spaces in order to obtain their sparse solutions, which is a feasible approach to overcome challenges coming from the big data nature of most practical applications. It is the goal of this paper to provide a systematic study of the representer theorems for these problems in Banach spaces. There are a few existing results for these problems in a Banach space, with all of them regarding implicit representer theorems. We aim at obtaining explicit representer theorems based on which convenient solution methods will then be developed. For the minimum norm interpolation, the explicit representer theorems enable us to express the infimum in terms of the norm of the linear combination of the interpolation functionals. For the purpose of developing efficient computational algorithms, we establish the fixed-point equation formulation of solutions of these problems. We reveal that unlike in a Hilbert space, in general, solutions of these problems in a Banach space may not be able to be reduced to truly finite dimensional problems (with certain infinite dimensional components hidden). We demonstrate how this obstacle can be removed, reducing the original problem to a truly finite dimensional one, in the special case when the Banach space is $\ell_1(\mathbb{N})$.},
 author = {Rui Wang and Yuesheng Xu},
 journal = {Journal of Machine Learning Research},
 number = {225},
 openalex = {W3205761825},
 pages = {1--65},
 title = {Representer Theorems in Banach Spaces: Minimum Norm Interpolation, Regularized Learning and Semi-Discrete Inverse Problems},
 url = {http://jmlr.org/papers/v22/20-751.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-753,
 abstract = {We develop conformal prediction methods for constructing valid predictive confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods---which give marginal validity (coverage) guarantees---provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples. By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically optimal) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels. Our methods can be bolted on top of any classification model---neural network, random forest, boosted tree---to guarantee its validity. We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.},
 author = {Maxime Cauchois and Suyash Gupta and John C. Duchi},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W3157809193},
 pages = {1--42},
 title = {Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction},
 url = {http://jmlr.org/papers/v22/20-753.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-755,
 abstract = {A sparse regression approach for the computation of high-dimensional optimal feedback laws arising in deterministic nonlinear control is proposed. The approach exploits the control-theoretical link between Hamilton-Jacobi-Bellman PDEs characterizing the value function of the optimal control problems, and first-order optimality conditions via Pontryagin's Maximum Principle. The latter is used as a representation formula to recover the value function and its gradient at arbitrary points in the space-time domain through the solution of a two-point boundary value problem. After generating a dataset consisting of different state-value pairs, a hyperbolic cross polynomial model for the value function is fitted using a LASSO regression. An extended set of low and high-dimensional numerical tests in nonlinear optimal control reveal that enriching the dataset with gradient information reduces the number of training samples, and that the sparse polynomial regression consistently yields a feedback law of lower complexity.},
 author = {Behzad Azmi and Dante Kalise and Karl Kunisch},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W3115087407},
 pages = {1--32},
 title = {Optimal Feedback Law Recovery by Gradient-Augmented Sparse Polynomial Regression},
 url = {http://jmlr.org/papers/v22/20-755.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-763,
 abstract = {Bandit Convex Optimization (BCO) is a fundamental framework for modeling sequential decision-making with partial information, where the only feedback available to the player is the one-point or two-point function values. In this paper, we investigate BCO in non-stationary environments and choose the \emph{dynamic regret} as the performance measure, which is defined as the difference between the cumulative loss incurred by the algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path-length of the comparator sequence that reflects the non-stationarity of environments. We propose a novel algorithm that achieves $O(T^{3/4}(1+P_T)^{1/2})$ and $O(T^{1/2}(1+P_T)^{1/2})$ dynamic regret respectively for the one-point and two-point feedback models. The latter result is optimal, matching the $Œ©(T^{1/2}(1+P_T)^{1/2})$ lower bound established in this paper. Notably, our algorithm is more adaptive to non-stationary environments since it does not require prior knowledge of the path-length $P_T$ ahead of time, which is generally unknown.},
 author = {Peng Zhao and Guanghui Wang and Lijun Zhang and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {125},
 openalex = {W2964596703},
 pages = {1--45},
 title = {Bandit Convex Optimization in Non-stationary Environments},
 url = {http://jmlr.org/papers/v22/20-763.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-768,
 author = {Xin Bing and Florentina Bunea and Seth Strimas-Mackey and Marten Wegkamp},
 journal = {Journal of Machine Learning Research},
 number = {177},
 openalex = {W3188584319},
 pages = {1--50},
 title = {Prediction Under Latent Factor Regression: Adaptive PCR, Interpolating Predictors and Beyond},
 url = {http://jmlr.org/papers/v22/20-768.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-774,
 abstract = {Identifying informative predictors in a high dimensional regression model is a critical step for association analysis and predictive modeling. Signal detection in the high dimensional setting often fails due to the limited sample size. One approach to improving power is through meta-analyzing multiple studies which address the same scientific question. However, integrative analysis of high dimensional data from multiple studies is challenging in the presence of between-study heterogeneity. The challenge is even more pronounced with additional data sharing constraints under which only summary data can be shared across different sites. In this paper, we propose a novel data shielding integrative large-scale testing (DSILT) approach to signal detection allowing between-study heterogeneity and not requiring the sharing of individual level data. Assuming the underlying high dimensional regression models of the data differ across studies yet share similar support, the proposed method incorporates proper integrative estimation and debiasing procedures to construct test statistics for the overall effects of specific covariates. We also develop a multiple testing procedure to identify significant effects while controlling the false discovery rate (FDR) and false discovery proportion (FDP). Theoretical comparisons of the new testing procedure with the ideal individual-level meta-analysis (ILMA) approach and other distributed inference methods are investigated. Simulation studies demonstrate that the proposed testing procedure performs well in both controlling false discovery and attaining power. The new method is applied to a real example detecting interaction effects of the genetic variants for statins and obesity on the risk for type II diabetes.},
 author = {Molei Liu and Yin Xia and Kelly Cho and Tianxi Cai},
 journal = {Journal of Machine Learning Research},
 number = {126},
 openalex = {W3168097668},
 pages = {1--26},
 title = {Integrative High Dimensional Multiple Testing with Heterogeneity under Data Sharing Constraints},
 url = {http://jmlr.org/papers/v22/20-774.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-813,
 abstract = {This paper considers online optimization of a renewal-reward system. A controller performs a sequence of tasks back-to-back. Each task has a random vector of parameters, called the task type vector, that affects the task processing options and also affects the resulting reward and time duration of the task. The probability distribution for the task type vector is unknown and the controller must learn to make efficient decisions so that time average reward converges to optimality. Prior work on such renewal optimization problems leaves open the question of optimal convergence time. This paper develops an algorithm with an optimality gap that decays like $O(1/\sqrt{k})$, where $k$ is the number of tasks processed. The same algorithm is shown to have faster $O(\log(k)/k)$ performance when the system satisfies a strong concavity property. The proposed algorithm uses an auxiliary variable that is updated according to a classic Robbins-Monro iteration. It makes online scheduling decisions at the start of each renewal frame based on this variable and on the observed task type. A matching converse is obtained for the strongly concave case by constructing an example system for which all algorithms have performance at best $Œ©(\log(k)/k)$. A matching $Œ©(1/\sqrt{k})$ converse is also shown for the general case without strong concavity.},
 author = {Michael J. Neely},
 journal = {Journal of Machine Learning Research},
 number = {279},
 openalex = {W3043199428},
 pages = {1--44},
 title = {Fast Learning for Renewal Optimization in Online Task Scheduling},
 url = {http://jmlr.org/papers/v22/20-813.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-815,
 author = {Yang Liu and Tao Fan and Tianjian Chen and Qian Xu and Qiang Yang},
 journal = {Journal of Machine Learning Research},
 number = {226},
 openalex = {W3206887799},
 pages = {1--6},
 title = {FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection},
 url = {http://jmlr.org/papers/v22/20-815.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-821,
 abstract = {Standard results in stochastic convex optimization bound the number of samples that an algorithm needs to generate a point with small function value in expectation. More nuanced high probability guarantees are rare, and typically either rely on light-tail noise assumptions or exhibit worse sample complexity. In this work, we show that a wide class of stochastic optimization algorithms for strongly convex problems can be augmented with high confidence bounds at an overhead cost that is only logarithmic in the confidence level and polylogarithmic in the condition number. The procedure we propose, called proxBoost, is elementary and builds on two well-known ingredients: robust distance estimation and the proximal point method. We discuss consequences for both streaming (online) algorithms and offline algorithms based on empirical risk minimization.},
 author = {Damek Davis and Dmitriy Drusvyatskiy and Lin Xiao and Junyu Zhang},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2980686465},
 pages = {1--38},
 title = {From low probability to high confidence in stochastic convex optimization},
 url = {http://jmlr.org/papers/v22/20-821.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-825,
 abstract = {Recently, knowledge graph embeddings (KGEs) received significant attention, and several software libraries have been developed for training and evaluating KGEs. While each of them addresses specific needs, we re-designed and re-implemented PyKEEN, one of the first KGE libraries, in a community effort. PyKEEN 1.0 enables users to compose knowledge graph embedding models (KGEMs) based on a wide range of interaction models, training approaches, loss functions, and permits the explicit modeling of inverse relations. Besides, an automatic memory optimization has been realized in order to exploit the provided hardware optimally, and through the integration of Optuna extensive hyper-parameter optimization (HPO) functionalities are provided.},
 author = {Mehdi Ali and Max Berrendorf and Charles Tapley Hoyt and Laurent Vermue and Sahand Sharifzadeh and Volker Tresp and Jens Lehmann},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W3046075728},
 pages = {1--6},
 title = {PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings},
 url = {http://jmlr.org/papers/v22/20-825.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-837,
 abstract = {In the Tensor PCA problem introduced by Richard and Montanari (2014), one is given a dataset consisting of $n$ samples $\mathbf{T}_{1:n}$ of i.i.d. Gaussian tensors of order $k$ with the promise that $\mathbb{E}\mathbf{T}_1$ is a rank-1 tensor and $\|\mathbb{E} \mathbf{T}_1\| = 1$. The goal is to estimate $\mathbb{E} \mathbf{T}_1$. This problem exhibits a large conjectured hard phase when $k&gt;2$: When $d \lesssim n \ll d^{\frac{k}{2}}$ it is information theoretically possible to estimate $\mathbb{E} \mathbf{T}_1$, but no polynomial time estimator is known. We provide a sharp analysis of the optimal sample complexity in the Statistical Query (SQ) model and show that SQ algorithms with polynomial query complexity not only fail to solve Tensor PCA in the conjectured hard phase, but also have a strictly sub-optimal sample complexity compared to some polynomial time estimators such as the Richard-Montanari spectral estimator. Our analysis reveals that the optimal sample complexity in the SQ model depends on whether $\mathbb{E} \mathbf{T}_1$ is symmetric or not. For symmetric, even order tensors, we also isolate a sample size regime in which it is possible to test if $\mathbb{E} \mathbf{T}_1 = \mathbf{0}$ or $\mathbb{E}\mathbf{T}_1 \neq \mathbf{0}$ with polynomially many queries but not estimate $\mathbb{E}\mathbf{T}_1$. Our proofs rely on the Fourier analytic approach of Feldman, Perkins and Vempala (2018) to prove sharp SQ lower bounds.},
 author = {Rishabh Dudeja and Daniel Hsu},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W3162233198},
 pages = {1--51},
 title = {Statistical Query Lower Bounds for Tensor PCA},
 url = {http://jmlr.org/papers/v22/20-837.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-848,
 abstract = {Much work has been done recently to make neural networks more interpretable, and one obvious approach is to arrange for the network to use only a subset of the available features. In linear models, Lasso (or $\ell_1$-regularized) regression assigns zero weights to the most irrelevant or redundant features, and is widely used in data science. However the Lasso only applies to linear models. Here we introduce LassoNet, a neural network framework with global feature selection. Our approach enforces a hierarchy: specifically a feature can participate in a hidden unit only if its linear representative is active. Unlike other approaches to feature selection for neural nets, our method uses a modified objective function with constraints, and so integrates feature selection with the parameter learning directly. As a result, it delivers an entire regularization path of solutions with a range of feature sparsity. On systematic experiments, LassoNet significantly outperforms state-of-the-art methods for feature selection and regression. The LassoNet method uses projected proximal gradient descent, and generalizes directly to deep networks. It can be implemented by adding just a few lines of code to a standard neural network.},
 author = {Ismael Lemhadri and Feng Ruan and Louis Abraham and Robert Tibshirani},
 journal = {Journal of Machine Learning Research},
 number = {127},
 openalex = {W3004752032},
 pages = {1--29},
 title = {LassoNet: A Neural Network with Feature Sparsity},
 url = {http://jmlr.org/papers/v22/20-848.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-863,
 abstract = {Real-world systems are often modelled by sets of equations with exogenous random variables. What can we say about the probabilistic and causal aspects of variables that appear in these equations without explicitly solving for them? We prove that, under a solvability assumption, we can construct a Markov ordering graph that implies conditional independences and a causal ordering graph that encodes the effects of soft and perfect interventions by making use of Simon's causal ordering algorithm. Our results shed new light on discussions in causal discovery about the justification of using graphs to simultaneously represent conditional independences and causal relations in models with feedback.},
 author = {Tineke Blom and Mirthe M. van Diepen and Joris M. Mooij},
 journal = {Journal of Machine Learning Research},
 number = {178},
 openalex = {W3043261226},
 pages = {1--62},
 title = {Conditional Independences and Causal Relations implied by Sets of Equations},
 url = {http://jmlr.org/papers/v22/20-863.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-867,
 abstract = {The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are widely used to quantify the similarity between probability distributions. In this work, we systematically study the relationship between these two families from the perspective of convex duality. Starting from a tight variational representation of the $f$-divergence, we derive a generalization of the moment-generating function, which we show exactly characterizes the best lower bound of the $f$-divergence as a function of a given IPM. Using this characterization, we obtain new bounds while also recovering in a unified manner well-known results, such as Hoeffding's lemma, Pinsker's inequality and its extension to subgaussian functions, and the Hammersley-Chapman-Robbins bound. This characterization also allows us to prove new results on topological properties of the divergence which may be of independent interest.},
 author = {Rohit Agrawal and Thibaut Horel},
 journal = {Journal of Machine Learning Research},
 number = {128},
 openalex = {W3035624438},
 pages = {1--59},
 title = {Optimal Bounds between $f$-Divergences and Integral Probability Metrics},
 url = {http://jmlr.org/papers/v22/20-867.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-879,
 abstract = {This paper presents an empirical study regarding training probabilistic neural networks using training objectives derived from PAC-Bayes bounds. In the context of probabilistic neural networks, the output of training is a probability distribution over network weights. We present two training objectives, used here for the first time in connection with training neural networks. These two training objectives are derived from tight PAC-Bayes bounds. We also re-implement a previously used training objective based on a classical PAC-Bayes bound, to compare the properties of the predictors learned using the different training objectives. We compute risk certificates for the learnt predictors, based on part of the data used to learn the predictors. We further experiment with different types of priors on the weights (both data-free and data-dependent priors) and neural network architectures. Our experiments on MNIST and CIFAR-10 show that our training methods produce competitive test set errors and non-vacuous risk bounds with much tighter values than previous results in the literature, showing promise not only to guide the learning algorithm through bounding the risk but also for model selection. These observations suggest that the methods studied here might be good candidates for self-certified learning, in the sense of using the whole data set for learning a predictor and certifying its risk on any unseen data (from the same distribution as the training data) potentially without the need for holding out test data.},
 author = {Mar√≠a P√©rez-Ortiz and Omar Rivasplata and John Shawe-Taylor and Csaba Szepesv√°ri},
 journal = {Journal of Machine Learning Research},
 number = {227},
 openalex = {W3044132587},
 pages = {1--40},
 title = {Tighter risk certificates for neural networks},
 url = {http://jmlr.org/papers/v22/20-879.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-911,
 abstract = {This paper studies the rates of convergence for learning distributions implicitly with the adversarial framework and Generative Adversarial Networks (GANs), which subsume Wasserstein, Sobolev, MMD GAN, and Generalized/Simulated Method of Moments (GMM/SMM) as special cases. We study a wide range of parametric and nonparametric target distributions under a host of objective evaluation metrics. We investigate how to obtain valid statistical guarantees for GANs through the lens of regularization. On the nonparametric end, we derive the optimal minimax rates for distribution estimation under the adversarial framework. On the parametric end, we establish a theory for general neural network classes (including deep leaky ReLU networks) that characterizes the interplay on the choice of generator and discriminator pair. We discover and isolate a new notion of regularization, called the generator-discriminator-pair regularization, that sheds light on the advantage of GANs compared to classical parametric and nonparametric approaches for explicit distribution estimation. We develop novel oracle inequalities as the main technical tools for analyzing GANs, which are of independent interest.},
 author = {Tengyuan Liang},
 journal = {Journal of Machine Learning Research},
 number = {228},
 openalex = {W3081116588},
 pages = {1--41},
 title = {How Well Generative Adversarial Networks Learn Distributions},
 url = {http://jmlr.org/papers/v22/20-911.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-919,
 abstract = {In this paper, we develop novel perturbation bounds for the high-order orthogonal iteration (HOOI) [DLDMV00b]. Under mild regularity conditions, we establish blockwise tensor perturbation bounds for HOOI with guarantees for both tensor reconstruction in Hilbert-Schmidt norm $\|\widehat{\bcT} - \bcT \|_{\tHS}$ and mode-$k$ singular subspace estimation in Schatten-$q$ norm $\| \sin Œò(\widehat{\U}_k, \U_k) \|_q$ for any $q \geq 1$. We show the upper bounds of mode-$k$ singular subspace estimation are unilateral and converge linearly to a quantity characterized by blockwise errors of the perturbation and signal strength. For the tensor reconstruction error bound, we express the bound through a simple quantity $Œæ$, which depends only on perturbation and the multilinear rank of the underlying signal. Rate matching deterministic lower bound for tensor reconstruction, which demonstrates the optimality of HOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI with only a single iteration) is also optimal in terms of tensor reconstruction and can be used to lower the computational cost. The perturbation results are also extended to the case that only partial modes of $\bcT$ have low-rank structure. We support our theoretical results by extensive numerical studies. Finally, we apply the novel perturbation bounds of HOOI on two applications, tensor denoising and tensor co-clustering, from machine learning and statistics, which demonstrates the superiority of the new perturbation results.},
 author = {Yuetian Luo and Garvesh Raskutti and Ming Yuan and Anru R. Zhang},
 journal = {Journal of Machine Learning Research},
 number = {179},
 openalex = {W3047116450},
 pages = {1--48},
 title = {A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration},
 url = {http://jmlr.org/papers/v22/20-919.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-950,
 abstract = {This paper develops an efficient distributed inference algorithm, which is robust against a moderate fraction of Byzantine nodes, namely arbitrary and possibly adversarial machines in a distributed learning system. In robust statistics, the median-of-means (MOM) has been a popular approach to hedge against Byzantine failures due to its ease of implementation and computational efficiency. However, the MOM estimator has the shortcoming in terms of statistical efficiency. The first main contribution of the paper is to propose a variance reduced median-of-means (VRMOM) estimator, which improves the statistical efficiency over the vanilla MOM estimator and is computationally as efficient as the MOM. Based on the proposed VRMOM estimator, we develop a general distributed inference algorithm that is robust against Byzantine failures. Theoretically, our distributed algorithm achieves a fast convergence rate with only a constant number of rounds of communications. We also provide the asymptotic normality result for the purpose of statistical inference. To the best of our knowledge, this is the first normality result in the setting of Byzantine-robust distributed learning. The simulation results are also presented to illustrate the effectiveness of our method.},
 author = {Jiyuan Tu and Weidong Liu and Xiaojun Mao and Xi Chen},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W3133700352},
 pages = {1--67},
 title = {Variance Reduced Median-of-Means Estimator for Byzantine-Robust Distributed Inference},
 url = {http://jmlr.org/papers/v22/20-950.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-974,
 abstract = {We prove bounds on the population risk of the maximum margin algorithm for two-class linear classification. For linearly separable training data, the maximum margin algorithm has been shown in previous work to be equivalent to a limit of training with logistic loss using gradient descent, as the training error is driven to zero. We analyze this algorithm applied to random data including misclassification noise. Our assumptions on the clean data include the case in which the class-conditional distributions are standard normal distributions. The misclassification noise may be chosen by an adversary, subject to a limit on the fraction of corrupted labels. Our bounds show that, with sufficient over-parameterization, the maximum margin algorithm trained on noisy data can achieve nearly optimal population risk.},
 author = {Niladri S. Chatterji and Philip M. Long},
 journal = {Journal of Machine Learning Research},
 number = {129},
 openalex = {W3168158992},
 pages = {1--30},
 title = {Finite-sample Analysis of Interpolating Linear Classifiers in the Overparameterized Regime},
 url = {http://jmlr.org/papers/v22/20-974.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-997,
 abstract = {A line of recent works established that when training linear predictors over separable data, using gradient methods and exponentially-tailed losses, the predictors asymptotically converge in direction to the max-margin predictor. As a consequence, the predictors asymptotically do not overfit. However, this does not address the question of whether overfitting might occur non-asymptotically, after some bounded number of iterations. In this paper, we formally show that standard gradient methods (in particular, gradient flow, gradient descent and stochastic gradient descent) never overfit on separable data: If we run these methods for $T$ iterations on a dataset of size $m$, both the empirical risk and the generalization error decrease at an essentially optimal rate of $\tilde{\mathcal{O}}(1/\gamma^2 T)$ up till $T\approx m$, at which point the generalization error remains fixed at an essentially optimal level of $\tilde{\mathcal{O}}(1/\gamma^2 m)$ regardless of how large $T$ is. Along the way, we present non-asymptotic bounds on the number of margin violations over the dataset, and prove their tightness.},
 author = {Ohad Shamir},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W3155726402},
 pages = {1--20},
 title = {Gradient Methods Never Overfit On Separable Data},
 url = {http://jmlr.org/papers/v22/20-997.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0006,
 author = {Trambak Banerjee and Gourab Mukherjee and Debashis Paul},
 journal = {Journal of Machine Learning Research},
 number = {180},
 openalex = {W3191141750},
 pages = {1--40},
 title = {Improved Shrinkage Prediction under a Spiked Covariance Structure},
 url = {http://jmlr.org/papers/v22/21-0006.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0017,
 author = {Janis Klaise and Arnaud Van Looveren and Giovanni Vacanti and Alexandru Coca},
 journal = {Journal of Machine Learning Research},
 number = {181},
 openalex = {W3190528386},
 pages = {1--7},
 title = {Alibi Explain: Algorithms for Explaining Machine Learning Models},
 url = {http://jmlr.org/papers/v22/21-0017.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0019,
 abstract = {When seeing a new object, humans can immediately recognize it across different retinal locations: the internal object representation is invariant to translation. It is commonly believed that Convolutional Neural Networks (CNNs) are architecturally invariant to translation thanks to the convolution and/or pooling operations they are endowed with. In fact, several studies have found that these networks systematically fail to recognise new objects on untrained locations. In this work, we test a wide variety of CNNs architectures showing how, apart from DenseNet-121, none of the models tested was architecturally invariant to translation. Nevertheless, all of them could learn to be invariant to translation. We show how this can be achieved by pretraining on ImageNet, and it is sometimes possible with much simpler data sets when all the items are fully translated across the input canvas. At the same time, this invariance can be disrupted by further training due to catastrophic forgetting/interference. These experiments show how pretraining a network on an environment with the right `latent' characteristics (a more naturalistic environment) can result in the network learning deep perceptual rules which would dramatically improve subsequent generalization.},
 author = {Valerio Biscione and Jeffrey S. Bowers},
 journal = {Journal of Machine Learning Research},
 number = {229},
 openalex = {W3205650418},
 pages = {1--28},
 title = {Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be},
 url = {http://jmlr.org/papers/v22/21-0019.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0020,
 abstract = {Graph matching aims to find the latent vertex correspondence between two edge-correlated graphs and has found numerous applications across different fields. In this paper, we study a seeded graph matching problem, which assumes that a set of seeds, i.e., pre-mapped vertex-pairs, is given in advance. While most previous work requires all seeds to be correct, we focus on the setting where the seeds are partially correct. Specifically, consider two correlated graphs whose edges are sampled independently from a parent \ER graph $\mathcal{G}(n,p)$. A mapping between the vertices of the two graphs is provided as seeds, of which an unknown $Œ≤$ fraction is correct. We first analyze a simple algorithm that matches vertices based on the number of common seeds in the $1$-hop neighborhoods, and then further propose a new algorithm that uses seeds in the $2$-hop neighborhoods. We establish non-asymptotic performance guarantees of perfect matching for both $1$-hop and $2$-hop algorithms, showing that our new $2$-hop algorithm requires substantially fewer correct seeds than the $1$-hop algorithm when graphs are sparse. Moreover, by combining our new performance guarantees for the $1$-hop and $2$-hop algorithms, we attain the best-known results (in terms of the required fraction of correct seeds) across the entire range of graph sparsity and significantly improve the previous results in \cite{10.14778/2794367.2794371,lubars2018correcting} when $p\ge n^{-5/6}$. For instance, when $p$ is a constant or $p=n^{-3/4}$, we show that only $Œ©(\sqrt{n\log n})$ correct seeds suffice for perfect matching, while the previously best-known results demand $Œ©(n)$ and $Œ©(n^{3/4}\log n)$ correct seeds, respectively. Numerical experiments corroborate our theoretical findings, demonstrating the superiority of our $2$-hop algorithm on a variety of synthetic and real graphs.},
 author = {Liren Yu and Jiaming Xu and Xiaojun Lin},
 journal = {Journal of Machine Learning Research},
 number = {280},
 openalex = {W3015883877},
 pages = {1--54},
 title = {Graph Matching with Partially-Correct Seeds},
 url = {http://jmlr.org/papers/v22/21-0020.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0021,
 abstract = {Many statistical learning problems have recently been shown to be amenable to Semi-Definite Programming (SDP), with community detection and clustering in Gaussian mixture models as the most striking instances Javanmard et al. (2016). Given the growing range of applications of SDP-based techniques to machine learning problems, and the rapid progress in the design of efficient algorithms for solving SDPs, an intriguing question is to understand how the recent advances from empirical process theory and Statistical Learning Theory can be leveraged for providing a precise statistical analysis of SDP estimators. In the present paper, we borrow cutting edge techniques and concepts from the Learning Theory literature, such as fixed point equations and excess risk curvature arguments, which yield general estimation and prediction results for a wide class of SDP estimators. From this perspective, we revisit some classical results in community detection from Guedon and Vershynin (2016) and Fei and Chen (2019), and we obtain statistical guarantees for SDP estimators used in signed clustering, angular group synchronization (for both multiplicative and additive models) and MAX-CUT. Our theoretical findings are complemented by numerical experiments for each of the three problems considered, showcasing the competitiveness of the SDP estimators.},
 author = {St√©phane Chr√©tien and Mihai Cucuringu and Guillaume Lecu√© and Lucie Neirac},
 journal = {Journal of Machine Learning Research},
 number = {230},
 openalex = {W3205586389},
 pages = {1--64},
 title = {Learning with semi-definite programming: statistical bounds based on fixed point analysis and excess risk curvature},
 url = {http://jmlr.org/papers/v22/21-0021.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0029,
 abstract = {The sklvq package is an open-source Python implementation of a set of learning vector quantization (LVQ) algorithms. In addition to providing the core functionality for the GLVQ, GMLVQ, and LGMLVQ algorithms, sklvq is distinctive by putting emphasis on its modular and customizable design. Not only resulting in a feature-rich implementation for users but enabling easy extensions of the algorithms for researchers. The theory behind this design is described in this paper. To facilitate adoptions and inspire future contributions, sklvq is publicly available on Github (under the BSD license) and can be installed through the Python package index (PyPI). Next to being well-covered by automated testing to ensure code quality, it is accompanied by detailed online documentation. The documentation covers usage examples and provides an in-depth API including theory and scientific references.},
 author = {Rick van Veen and Michael Biehl and Gert-Jan de Vries},
 journal = {Journal of Machine Learning Research},
 number = {231},
 openalex = {W3206436985},
 pages = {1--6},
 title = {sklvq : Scikit Learning Vector Quantization},
 url = {http://jmlr.org/papers/v22/21-0029.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0031,
 abstract = {This paper presents a probabilistic perspective on iterative methods for approximating the solution $\mathbf{x}_* \in \mathbb{R}^d$ of a nonsingular linear system $\mathbf{A} \mathbf{x}_* = \mathbf{b}$. In the approach a standard iterative method on $\mathbb{R}^d$ is lifted to act on the space of probability distributions $\mathcal{P}(\mathbb{R}^d)$. Classically, an iterative method produces a sequence $\mathbf{x}_m$ of approximations that converge to $\mathbf{x}_*$. The output of the iterative methods proposed in this paper is, instead, a sequence of probability distributions $\mu_m \in \mathcal{P}(\mathbb{R}^d)$. The distributional output both provides a best guess for $\mathbf{x}_*$, for example as the mean of $\mu_m$, and also probabilistic uncertainty quantification for the value of $\mathbf{x}_*$ when it has not been exactly determined. Theoretical analysis is provided in the prototypical case of a stationary linear iterative method. In this setting we characterise both the rate of contraction of $\mu_m$ to an atomic measure on $\mathbf{x}_*$ and the nature of the uncertainty quantification being provided. We conclude with an empirical illustration that highlights the insight into solution uncertainty that can be provided by probabilistic iterative methods.},
 author = {Jon Cockayne and Ilse C.F. Ipsen and Chris J. Oates and Tim W. Reid},
 journal = {Journal of Machine Learning Research},
 number = {232},
 openalex = {W3116159773},
 pages = {1--34},
 title = {Probabilistic Iterative Methods for Linear Systems},
 url = {http://jmlr.org/papers/v22/21-0031.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0037,
 abstract = {Although variational autoencoders (VAE) are successfully used to obtain meaningful low-dimensional representations for high-dimensional data, the characterization of critical points of the loss function for general observation models is not fully understood. We introduce a theoretical framework that is based on a connection between $\beta$-VAE and generalized linear models (GLM). The equality between the activation function of a $\beta$-VAE and the inverse of the link function of a GLM enables us to provide a systematic generalization of the loss analysis for $\beta$-VAE based on the assumption that the observation model distribution belongs to an exponential dispersion family (EDF). As a result, we can initialize $\beta$-VAE nets by maximum likelihood estimates (MLE) that enhance the training performance on both synthetic and real world data sets. As a further consequence, we analytically describe the auto-pruning property inherent in the $\beta$-VAE objective and reason for posterior collapse.},
 author = {Robert Sicks and Ralf Korn and Stefanie Schwaar},
 journal = {Journal of Machine Learning Research},
 number = {233},
 openalex = {W4287758851},
 pages = {1--41},
 title = {A Generalised Linear Model Framework for $Œ≤$-Variational Autoencoders based on Exponential Dispersion Families},
 url = {http://jmlr.org/papers/v22/21-0037.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0072,
 author = {Jackson Loper and David Blei and John P. Cunningham and Liam Paninski},
 journal = {Journal of Machine Learning Research},
 number = {234},
 openalex = {W3206818852},
 pages = {1--36},
 title = {A general linear-time inference method for Gaussian Processes on one dimension},
 url = {http://jmlr.org/papers/v22/21-0072.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0089,
 abstract = {Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.},
 author = {Christopher Tosh and Akshay Krishnamurthy and Daniel Hsu},
 journal = {Journal of Machine Learning Research},
 number = {281},
 openalex = {W3009571263},
 pages = {1--31},
 title = {Contrastive estimation reveals topic posterior information to linear models},
 url = {http://jmlr.org/papers/v22/21-0089.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0112,
 abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
 author = {Pascal Klink and Hany Abdulsamad and Boris Belousov and Carlo D'Eramo and Jan Peters and Joni Pajarinen},
 journal = {Journal of Machine Learning Research},
 number = {182},
 openalex = {W4287323065},
 pages = {1--52},
 title = {A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning},
 url = {http://jmlr.org/papers/v22/21-0112.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0120,
 abstract = {This paper describes a general-purpose extension of max-value entropy search, a popular approach for Bayesian Optimisation (BO). A novel approximation is proposed for the information gain -- an information-theoretic quantity central to solving a range of BO problems, including noisy, multi-fidelity and batch optimisations across both continuous and highly-structured discrete spaces. Previously, these problems have been tackled separately within information-theoretic BO, each requiring a different sophisticated approximation scheme, except for batch BO, for which no computationally-lightweight information-theoretic approach has previously been proposed. GIBBON (General-purpose Information-Based Bayesian OptimisatioN) provides a single principled framework suitable for all the above, out-performing existing approaches whilst incurring substantially lower computational overheads. In addition, GIBBON does not require the problem's search space to be Euclidean and so is the first high-performance yet computationally light-weight acquisition function that supports batch BO over general highly structured input spaces like molecular search and gene design. Moreover, our principled derivation of GIBBON yields a natural interpretation of a popular batch BO heuristic based on determinantal point processes. Finally, we analyse GIBBON across a suite of synthetic benchmark tasks, a molecular search loop, and as part of a challenging batch multi-fidelity framework for problems with controllable experimental noise.},
 author = {Henry B. Moss and David S. Leslie and Javier Gonzalez and Paul Rayson},
 journal = {Journal of Machine Learning Research},
 number = {235},
 openalex = {W3127003427},
 pages = {1--49},
 title = {GIBBON: General-purpose Information-Based Bayesian OptimisatioN},
 url = {http://jmlr.org/papers/v22/21-0120.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0131,
 abstract = {We present Low Distortion Local Eigenmaps (LDLE), a manifold learning technique which constructs a set of low distortion local views of a dataset in lower dimension and registers them to obtain a global embedding. The local views are constructed using the global eigenvectors of the graph Laplacian and are registered using Procrustes analysis. The choice of these eigenvectors may vary across the regions. In contrast to existing techniques, LDLE can embed closed and non-orientable manifolds into their intrinsic dimension by tearing them apart. It also provides gluing instruction on the boundary of the torn embedding to help identify the topology of the original manifold. Our experimental results will show that LDLE largely preserved distances up to a constant scale while other techniques produced higher distortion. We also demonstrate that LDLE produces high quality embeddings even when the data is noisy or sparse.},
 author = {Dhruv Kohli and Alexander Cloninger and Gal Mishne},
 journal = {Journal of Machine Learning Research},
 number = {282},
 openalex = {W3122222197},
 pages = {1--64},
 title = {LDLE: Low Distortion Local Eigenmaps},
 url = {http://jmlr.org/papers/v22/21-0131.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0179,
 abstract = {Sparse optimization problems are ubiquitous in many fields such as statistics, signal/image processing and machine learning. This has led to the birth of many iterative algorithms to solve them. A powerful strategy to boost the performance of these algorithms is known as safe screening: it allows the early identification of zero coordinates in the solution, which can then be eliminated to reduce the problem's size and accelerate convergence. In this work, we extend the existing Gap Safe screening framework by relaxing the global strong-concavity assumption on the dual cost function. Instead, we exploit local regularity properties, that is, strong concavity on well-chosen subsets of the domain. The non-negativity constraint is also integrated to the existing framework. Besides making safe screening possible to a broader class of functions that includes beta-divergences (e.g., the Kullback-Leibler divergence), the proposed approach also improves upon the existing Gap Safe screening rules on previously applicable cases (e.g., logistic regression). The proposed general framework is exemplified by some notable particular cases: logistic function, beta = 1.5 and Kullback-Leibler divergences. Finally, we showcase the effectiveness of the proposed screening rules with different solvers (coordinate descent, multiplicative-update and proximal gradient algorithms) and different data sets (binary classification, hyperspectral and count data).},
 author = {Cassio F. Dantas and Emmanuel Soubies and C√©dric F√©votte},
 journal = {Journal of Machine Learning Research},
 number = {236},
 openalex = {W3129377187},
 pages = {1--57},
 title = {Expanding boundaries of Gap Safe screening},
 url = {http://jmlr.org/papers/v22/21-0179.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0199,
 abstract = {Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: ViMON, a video-extension of MONet, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.},
 author = {Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S. Ecker},
 journal = {Journal of Machine Learning Research},
 number = {183},
 openalex = {W4287758375},
 pages = {1--61},
 title = {Benchmarking Unsupervised Object Representations for Video Sequences},
 url = {http://jmlr.org/papers/v22/21-0199.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0203,
 abstract = {High-dimensional data sets are often analyzed and explored via the construction of a latent low-dimensional space which enables convenient visualization and efficient predictive modeling or clustering. For complex data structures, linear dimensionality reduction techniques like PCA may not be sufficiently flexible to enable low-dimensional representation. Non-linear dimension reduction techniques, like kernel PCA and autoencoders, suffer from loss of interpretability since each latent variable is dependent of all input dimensions. To address this limitation, we here present path lasso penalized autoencoders. This structured regularization enhances interpretability by penalizing each path through the encoder from an input to a latent variable, thus restricting how many input variables are represented in each latent dimension. Our algorithm uses a group lasso penalty and non-negative matrix factorization to construct a sparse, non-linear latent representation. We compare the path lasso regularized autoencoder to PCA, sparse PCA, autoencoders and sparse autoencoders on real and simulated data sets. We show that the algorithm exhibits much lower reconstruction errors than sparse PCA and parameter-wise lasso regularized autoencoders for low-dimensional representations. Moreover, path lasso representations provide a more accurate reconstruction match, i.e. preserved relative distance between objects in the original and reconstructed spaces.},
 author = {Oskar Allerbo and Rebecka J√∂rnsten},
 journal = {Journal of Machine Learning Research},
 number = {283},
 openalex = {W4287325028},
 pages = {1--28},
 title = {Non-linear, Sparse Dimensionality Reduction via Path Lasso Penalized Autoencoders},
 url = {http://jmlr.org/papers/v22/21-0203.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0259,
 abstract = {We investigate the implementation of a new stochastic Kuramoto-Vicsek-type model for global optimization of nonconvex functions on the sphere. This model belongs to the class of Consensus-Based Optimization. In fact, particles move on the sphere driven by a drift towards an instantaneous consensus point, which is computed as a convex combination of particle locations, weighted by the cost function according to Laplace's principle, and it represents an approximation to a global minimizer. The dynamics is further perturbed by a random vector field to favor exploration, whose variance is a function of the distance of the particles to the consensus point. In particular, as soon as the consensus is reached the stochastic component vanishes. The main results of this paper are about the proof of convergence of the numerical scheme to global minimizers provided conditions of well-preparation of the initial datum. The proof combines previous results of mean-field limit with a novel asymptotic analysis, and classical convergence results of numerical methods for SDE. We present several numerical experiments, which show that the algorithm proposed in the present paper scales well with the dimension and is extremely versatile. To quantify the performances of the new approach, we show that the algorithm is able to perform essentially as good as ad hoc state of the art methods in challenging problems in signal processing and machine learning, namely the phase retrieval problem and the robust subspace detection.},
 author = {Massimo Fornasier and Lorenzo Pareschi and Hui Huang and Philippe S√ºnnen},
 journal = {Journal of Machine Learning Research},
 number = {237},
 openalex = {W3004185045},
 pages = {1--55},
 title = {Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning},
 url = {http://jmlr.org/papers/v22/21-0259.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0277,
 abstract = {Linear bandit algorithms yield $\tilde{\mathcal{O}}(n\sqrt{T})$ pseudo-regret bounds on compact convex action sets $\mathcal{K}\subset\mathbb{R}^n$ and two types of structural assumptions lead to better pseudo-regret bounds. When $\mathcal{K}$ is the simplex or an $\ell_p$ ball with $p\in]1,2]$, there exist bandits algorithms with $\tilde{\mathcal{O}}(\sqrt{nT})$ pseudo-regret bounds. Here, we derive bandit algorithms for some strongly convex sets beyond $\ell_p$ balls that enjoy pseudo-regret bounds of $\tilde{\mathcal{O}}(\sqrt{nT})$, which answers an open question from [BCB12, ¬ß5.5.]. Interestingly, when the action set is uniformly convex but not necessarily strongly convex, we obtain pseudo-regret bounds with a dimension dependency smaller than $\mathcal{O}(\sqrt{n})$. However, this comes at the expense of asymptotic rates in $T$ varying between $\tilde{\mathcal{O}}(\sqrt{T})$ and $\tilde{\mathcal{O}}(T)$.},
 author = {Thomas Kerdreux and Christophe Roux and Alexandre d'Aspremont and Sebastian Pokutta},
 journal = {Journal of Machine Learning Research},
 number = {284},
 openalex = {W3134739396},
 pages = {1--23},
 title = {Linear Bandits on Uniformly Convex Sets},
 url = {http://jmlr.org/papers/v22/21-0277.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0281,
 author = {Martin Binder and Florian Pfisterer and Michel Lang and Lennart Schneider and Lars Kotthoff and Bernd Bischl},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W3189984135},
 pages = {1--7},
 title = {mlr3pipelines - Flexible Machine Learning Pipelines in R},
 url = {http://jmlr.org/papers/v22/21-0281.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0287,
 abstract = {Low rank tensor approximation is a fundamental tool in modern machine learning and data science. In this paper, we study the characterization, perturbation analysis, and an efficient sampling strategy for two primary tensor CUR approximations, namely Chidori and Fiber CUR. We characterize exact tensor CUR decompositions for low multilinear rank tensors. We also present theoretical error bounds of the tensor CUR approximations when (adversarial or Gaussian) noise appears. Moreover, we show that low cost uniform sampling is sufficient for tensor CUR approximations if the tensor has an incoherent structure. Empirical performance evaluations, with both synthetic and real-world datasets, establish the speed advantage of the tensor CUR approximations over other state-of-the-art low multilinear rank tensor approximations.},
 author = {HanQin Cai and Keaton Hamm and Longxiu Huang and Deanna Needell},
 journal = {Journal of Machine Learning Research},
 number = {185},
 openalex = {W3137950938},
 pages = {1--36},
 title = {Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions},
 url = {http://jmlr.org/papers/v22/21-0287.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0294,
 abstract = {In this article, we study the problem of high-dimensional conditional independence testing, a key building block in statistics and machine learning. We propose an inferential procedure based on double generative adversarial networks (GANs). Specifically, we first introduce a double GANs framework to learn two generators of the conditional distributions. We then integrate the two generators to construct a test statistic, which takes the form of the maximum of generalized covariance measures of multiple transformation functions. We also employ data-splitting and cross-fitting to minimize the conditions on the generators to achieve the desired asymptotic properties, and employ multiplier bootstrap to obtain the corresponding $p$-value. We show that the constructed test statistic is doubly robust, and the resulting test both controls type-I error and has the power approaching one asymptotically. Also notably, we establish those theoretical guarantees under much weaker and practically more feasible conditions compared to the existing tests, and our proposal gives a concrete example of how to utilize some state-of-the-art deep learning tools, such as GANs, to help address a classical but challenging statistical problem. We demonstrate the efficacy of our test through both simulations and an application to an anti-cancer drug dataset. A Python implementation of the proposed procedure is available at https://github.com/tianlinxu312/dgcit.},
 author = {Chengchun Shi and Tianlin Xu and Wicher Bergsma and Lexin Li},
 journal = {Journal of Machine Learning Research},
 number = {285},
 openalex = {W3213558745},
 pages = {1--32},
 title = {Double Generative Adversarial Networks for Conditional Independence Testing},
 url = {http://jmlr.org/papers/v22/21-0294.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0298,
 abstract = {Due to the rapid growth of smart agents such as weakly connected computational nodes and sensors, developing decentralized algorithms that can perform computations on local agents becomes a major research direction. This paper considers the problem of decentralized Principal components analysis (PCA), which is a statistical method widely used for data analysis. We introduce a technique called subspace tracking to reduce the communication cost, and apply it to power iterations. This leads to a decentralized PCA algorithm called \texttt{DeEPCA}, which has a convergence rate similar to that of the centralized PCA, while achieving the best communication complexity among existing decentralized PCA algorithms. \texttt{DeEPCA} is the first decentralized PCA algorithm with the number of communication rounds for each power iteration independent of target precision. Compared to existing algorithms, the proposed method is easier to tune in practice, with an improved overall communication cost. Our experiments validate the advantages of \texttt{DeEPCA} empirically.},
 author = {Haishan Ye and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {238},
 openalex = {W3127956932},
 pages = {1--27},
 title = {DeEPCA: Decentralized Exact PCA with Linear Convergence Rate},
 url = {http://jmlr.org/papers/v22/21-0298.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0307,
 abstract = {Stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo (SGHMC) are two popular Markov Chain Monte Carlo (MCMC) algorithms for Bayesian inference that can scale to large datasets, allowing to sample from the posterior distribution of the parameters of a statistical model given the input data and the prior distribution over the model parameters. However, these algorithms do not apply to the decentralized learning setting, when a network of agents are working collaboratively to learn the parameters of a statistical model without sharing their individual data due to privacy reasons or communication constraints. We study two algorithms: Decentralized SGLD (DE-SGLD) and Decentralized SGHMC (DE-SGHMC) which are adaptations of SGLD and SGHMC methods that allow scaleable Bayesian inference in the decentralized setting for large datasets. We show that when the posterior distribution is strongly log-concave and smooth, the iterates of these algorithms converge linearly to a neighborhood of the target distribution in the 2-Wasserstein distance if their parameters are selected appropriately. We illustrate the efficiency of our algorithms on decentralized Bayesian linear regression and Bayesian logistic regression problems.},
 author = {Mert G√ºrb√ºzbalaban and Xuefeng Gao and Yuanhan Hu and Lingjiong Zhu},
 journal = {Journal of Machine Learning Research},
 number = {239},
 openalex = {W3206384338},
 pages = {1--69},
 title = {Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo},
 url = {http://jmlr.org/papers/v22/21-0307.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0343,
 abstract = {Although there exist several libraries for deep learning on graphs, they are aiming at implementing basic operations for graph deep learning. In the research community, implementing and benchmarking various advanced tasks are still painful and time-consuming with existing libraries. To facilitate graph deep learning research, we introduce DIG: Dive into Graphs, a turnkey library that provides a unified testbed for higher level, research-oriented graph deep learning tasks. Currently, we consider graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs. For each direction, we provide unified implementations of data interfaces, common algorithms, and evaluation metrics. Altogether, DIG is an extensible, open-source, and turnkey library for researchers to develop new methods and effortlessly compare with common baselines using widely used datasets and evaluation metrics. Source code is available at https://github.com/divelab/DIG.},
 author = {Meng Liu and Youzhi Luo and Limei Wang and Yaochen Xie and Hao Yuan and Shurui Gui and Haiyang Yu and Zhao Xu and Jingtun Zhang and Yi Liu and Keqiang Yan and Haoran Liu and Cong Fu and Bora M Oztekin and Xuan Zhang and Shuiwang Ji},
 journal = {Journal of Machine Learning Research},
 number = {240},
 openalex = {W3206541895},
 pages = {1--9},
 title = {DIG: A Turnkey Library for Diving into Graph Deep Learning Research},
 url = {http://jmlr.org/papers/v22/21-0343.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0366,
 abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
 author = {Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
 journal = {Journal of Machine Learning Research},
 number = {241},
 openalex = {W4287363917},
 pages = {1--124},
 title = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
 url = {http://jmlr.org/papers/v22/21-0366.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0383,
 abstract = {Tech companies (e.g., Google or Facebook) often use randomized online experiments and/or A/B testing primarily based on the average treatment effects to compare their new product with an old one. However, it is also critically important to detect qualitative treatment effects such that the new one may significantly outperform the existing one only under some specific circumstances. The aim of this paper is to develop a powerful testing procedure to efficiently detect such qualitative treatment effects. We propose a scalable online updating algorithm to implement our test procedure. It has three novelties including adaptive randomization, sequential monitoring, and online updating with guaranteed type-I error control. We also thoroughly examine the theoretical properties of our testing procedure including the limiting distribution of test statistics and the justification of an efficient bootstrap method. Extensive empirical studies are conducted to examine the finite sample performance of our test procedure.},
 author = {Chengchun Shi and Shikai Luo and Hongtu Zhu and Rui Song},
 journal = {Journal of Machine Learning Research},
 number = {286},
 openalex = {W3214177370},
 pages = {1--51},
 title = {An Online Sequential Test for Qualitative Treatment Effects},
 url = {http://jmlr.org/papers/v22/21-0383.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0453,
 abstract = {We present a framework that allows for the non-asymptotic study of the $2$-Wasserstein distance between the invariant distribution of an ergodic stochastic differential equation and the distribution of its numerical approximation in the strongly log-concave case. This allows us to study in a unified way a number of different integrators proposed in the literature for the overdamped and underdamped Langevin dynamics. In addition, we analyse a novel splitting method for the underdamped Langevin dynamics which only requires one gradient evaluation per time step. Under an additional smoothness assumption on a $d$--dimensional strongly log-concave distribution with condition number $\kappa$, the algorithm is shown to produce with an $\mathcal{O}\big(\kappa^{5/4} d^{1/4}\epsilon^{-1/2} \big)$ complexity samples from a distribution that, in Wasserstein distance, is at most $\epsilon>0$ away from the target distribution.},
 author = {Jesus Maria Sanz-Serna and Konstantinos C. Zygalakis},
 journal = {Journal of Machine Learning Research},
 number = {242},
 openalex = {W3206017305},
 pages = {1--37},
 title = {Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations},
 url = {http://jmlr.org/papers/v22/21-0453.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0498,
 author = {Sifan Liu and Art B. Owen},
 journal = {Journal of Machine Learning Research},
 number = {243},
 openalex = {W3205048900},
 pages = {1--23},
 title = {Quasi-Monte Carlo Quasi-Newton in Variational Bayes},
 url = {http://jmlr.org/papers/v22/21-0498.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0575,
 abstract = {This paper develops a general framework for analyzing asymptotics of $V$-statistics. Previous literature on limiting distribution mainly focuses on the cases when $n \to \infty$ with fixed kernel size $k$. Under some regularity conditions, we demonstrate asymptotic normality when $k$ grows with $n$ by utilizing existing results for $U$-statistics. The key in our approach lies in a mathematical reduction to $U$-statistics by designing an equivalent kernel for $V$-statistics. We also provide a unified treatment on variance estimation for both $U$- and $V$-statistics by observing connections to existing methods and proposing an empirically more accurate estimator. Ensemble methods such as random forests, where multiple base learners are trained and aggregated for prediction purposes, serve as a running example throughout the paper because they are a natural and flexible application of $V$-statistics.},
 author = {Zhengze Zhou and Lucas Mentch and Giles Hooker},
 journal = {Journal of Machine Learning Research},
 number = {287},
 openalex = {W3021226561},
 pages = {1--48},
 title = {$V$-statistics and Variance Estimation},
 url = {http://jmlr.org/papers/v22/21-0575.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0641,
 abstract = {In this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. This approach was previously considered by the same authors of this work in Garatti and Campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper Garatti and Campi (2019)). This connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. In the present work we establish new results. First, we expand the scope of Garatti and Campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. Then, we study classical support vector methods - including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD (Support Vector Data Description) - and derive new results for the ability of these methods to generalize. All results are valid for any finite size of the data set. When the sample size tends to infinity, we establish the unprecedented result that the risk approaches the ratio between the complexity and the cardinality of the data sample, regardless of the value of the complexity.},
 author = {Marco C. Campi and Simone Garatti},
 journal = {Journal of Machine Learning Research},
 number = {288},
 openalex = {W4287815533},
 pages = {1--38},
 title = {A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines},
 url = {http://jmlr.org/papers/v22/21-0641.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0657,
 abstract = {Trading off exploration and exploitation in an unknown environment is key to maximising expected online return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but also on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn approximately Bayes-optimal policies for complex tasks. VariBAD simultaneously meta-learns a variational auto-encoder to perform approximate inference, and a policy that incorporates task uncertainty directly during action selection by conditioning on both the environment state and the approximate belief. In two toy domains, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo tasks widely used in meta-RL and show that it achieves higher online return than existing methods. On the recently proposed Meta-World ML1 benchmark, variBAD achieves state of the art results by a large margin, fully solving two out of the three ML1 tasks for the first time.},
 author = {Luisa Zintgraf and Sebastian Schulze and Cong Lu and Leo Feng and Maximilian Igl and Kyriacos Shiarlis and Yarin Gal and Katja Hofmann and Shimon Whiteson},
 journal = {Journal of Machine Learning Research},
 number = {289},
 openalex = {W3215905638},
 pages = {1--39},
 title = {VariBAD: Variational Bayes-Adaptive Deep RL via Meta-Learning},
 url = {http://jmlr.org/papers/v22/21-0657.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0806,
 abstract = {Fourier neural operators (FNOs) have recently been proposed as an effective framework for learning operators that map between infinite-dimensional spaces. We prove that FNOs are universal, in the sense that they can approximate any continuous operator to desired accuracy. Moreover, we suggest a mechanism by which FNOs can approximate operators associated with PDEs efficiently. Explicit error bounds are derived to show that the size of the FNO, approximating operators associated with a Darcy type elliptic PDE and with the incompressible Navier-Stokes equations of fluid dynamics, only increases sub (log)-linearly in terms of the reciprocal of the error. Thus, FNOs are shown to efficiently approximate operators arising in a large class of PDEs.},
 author = {Nikola Kovachki and Samuel Lanthaler and Siddhartha Mishra},
 journal = {Journal of Machine Learning Research},
 number = {290},
 openalex = {W4287077061},
 pages = {1--76},
 title = {On universal approximation and error bounds for Fourier Neural Operators},
 url = {http://jmlr.org/papers/v22/21-0806.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0853,
 author = {Peter Koepernik and Florian Pfaff},
 journal = {Journal of Machine Learning Research},
 number = {244},
 openalex = {W3206194323},
 pages = {1--27},
 title = {Consistency of Gaussian Process Regression in Metric Spaces},
 url = {http://jmlr.org/papers/v22/21-0853.html},
 volume = {22},
 year = {2021}
}
