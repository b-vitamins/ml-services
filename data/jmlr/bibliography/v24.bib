@article{JMLR:v24:18-080,
 abstract = {Hierarchical clustering is a data analysis method that has been used for decades. Despite its widespread use, the method has an underdeveloped analytical foundation. Having a well understood foundation would both support the currently used methods and help guide future improvements. The goal of this paper is to give an analytic framework to better understand observations seen in practice. This paper considers the dual of a problem framework for hierarchical clustering introduced by Dasgupta. The main result is that one of the most popular algorithms used in practice, average linkage agglomerative clustering, has a small constant approximation ratio for this objective. Furthermore, this paper establishes that using bisecting k-means divisive clustering has a very poor lower bound on its approximation ratio for the same objective. However, we show that there are divisive algorithms that perform well with respect to this objective by giving two constant approximation algorithms. This paper is some of the first work to establish guarantees on widely used hierarchical algorithms for a natural objective function. This objective and analysis give insight into what these popular algorithms are optimizing and when they will perform well.},
 author = {Benjamin Moseley and Joshua R. Wang},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2768467070},
 pages = {1--36},
 title = {Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search},
 url = {http://jmlr.org/papers/v24/18-080.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:18-508,
 abstract = {Abstraction plays a key role in concept learning and knowledge discovery; this paper is concerned with computational abstraction. In particular, we study the nature of abstraction through a group-theoretic approach, formalizing it as symmetry-driven---as opposed to data-driven---hierarchical clustering. Thus, the resulting clustering framework is data-free, feature-free, similarity-free, and globally hierarchical---the four key features that distinguish it from common data clustering models such as $k$-means. Beyond a theoretical foundation for abstraction, we also present a top-down and a bottom-up approach to establish an algorithmic foundation for practical abstraction-generating methods. Lastly, via both a theoretical explanation and a real-world application, we illustrate that further coupling of our abstraction framework with statistics realizes Shannon's information lattice and even further, brings learning into the picture. This not only presents one use case of our proposed computational abstraction, but also gives a first step towards a principled and cognitive way of automatic concept learning and knowledge discovery.},
 author = {Haizi Yu and Igor Mineyev and Lav R. Varshney},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W2963088080},
 pages = {1--61},
 title = {A Group-Theoretic Approach to Computational Abstraction: Symmetry-Driven Hierarchical Clustering},
 url = {http://jmlr.org/papers/v24/18-508.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:18-521,
 author = {Weijie J. Su and Yuancheng Zhu},
 journal = {Journal of Machine Learning Research},
 number = {124},
 pages = {1--53},
 title = {HiGrad: Uncertainty Quantification for Online Learning and Stochastic Approximation},
 url = {http://jmlr.org/papers/v24/18-521.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:19-094,
 author = {Shiliang Sun and Jingjing Fei and Jing Zhao and Liang Mao},
 journal = {Journal of Machine Learning Research},
 number = {258},
 pages = {1--32},
 title = {Multi-view Collaborative Gaussian Process Dynamical Systems},
 url = {http://jmlr.org/papers/v24/19-094.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:19-1009,
 abstract = {We investigate the stochastic gradient descent (SGD) method where the step size lies within a banded region instead of being given by a fixed formula. The optimal convergence rate under mild conditions and large initial step size is proved. Our analysis provides comparable theoretical error bounds for SGD associated with a variety of step sizes. In addition, the convergence rates for some existing step size strategies, e.g., triangular policy and cosine-wave, can be revealed by our analytical framework under the boundary constraints. The bandwidth-based step size provides efficient and flexible step size selection in optimization. We also propose a $1/t$ up-down policy and give several non-monotonic step sizes. Numerical experiments demonstrate the efficiency and significant potential of the bandwidth-based step-size in many applications.},
 author = {Xiaoyu Wang and Ya-xiang Yuan},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W3129556906},
 pages = {1--49},
 title = {On the Convergence of Stochastic Gradient Descent with Bandwidth-based Step Size},
 url = {http://jmlr.org/papers/v24/19-1009.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:19-1030,
 abstract = {The Brier score is commonly used for evaluating probability predictions. In survival analysis, with right-censored observations of the event times, this score can be weighted by the inverse probability of censoring (IPCW) to retain its original interpretation. It is common practice to estimate the censoring distribution with the Kaplan-Meier estimator, even though it assumes that the censoring distribution is independent of the covariates. This paper discusses the general impact of the censoring estimates on the Brier score and shows that the estimation of the censoring distribution can be problematic. In particular, when the censoring times can be identified from the covariates, the IPCW score is no longer valid. For administratively censored data, where the potential censoring times are known for all individuals, we propose an alternative version of the Brier score. This administrative Brier score does not require estimation of the censoring distribution and is valid even if the censoring times can be identified from the covariates.},
 author = {Håvard Kvamme and Ørnulf Borgan},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W4287992272},
 pages = {1--26},
 title = {The Brier Score under Administrative Censoring: Problems and Solutions},
 url = {http://jmlr.org/papers/v24/19-1030.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:19-183,
 abstract = {Higher-order motif structures and multi-vertex interactions are becoming increasingly important in studies that aim to improve our understanding of functionalities and evolution patterns of networks. To elucidate the role of higher-order structures in community detection problems over complex networks, we introduce the notion of a Superimposed Stochastic Block Model (SupSBM). The model is based on a random graph framework in which certain higher-order structures or subgraphs are generated through an independent hyperedge generation process, and are then replaced with graphs that are superimposed with directed or undirected edges generated by an inhomogeneous random graph model. Consequently, the model introduces controlled dependencies between edges which allow for capturing more realistic network phenomena, namely strong local clustering in a sparse network, short average path length, and community structure. We proceed to rigorously analyze the performance of a number of recently proposed higher-order spectral clustering methods on the SupSBM. In particular, we prove non-asymptotic upper bounds on the misclustering error of spectral community detection for a SupSBM setting in which triangles or 3-uniform hyperedges are superimposed with undirected edges. As part of our analysis, we also derive new bounds on the misclustering error of higher-order spectral clustering methods for the standard SBM and the 3-uniform hypergraph SBM. Furthermore, for a non-uniform hypergraph SBM model in which one directly observes both edges and 3-uniform hyperedges, we obtain a criterion that describes when to perform spectral clustering based on edges and when on hyperedges, based on a function of hyperedge density and observation quality.},
 author = {Subhadeep Paul and Olgica Milenkovic and Yuguo Chen},
 journal = {Journal of Machine Learning Research},
 number = {320},
 openalex = {W2904927492},
 pages = {1--58},
 title = {Higher-Order Spectral Clustering under Superimposed Stochastic Block Model},
 url = {http://jmlr.org/papers/v24/19-183.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:19-784,
 abstract = {Power iteration has been generalized to solve many interesting problems in machine learning and statistics. Despite its striking success, theoretical understanding of when and how such an algorithm enjoys good convergence property is limited. In this work, we introduce a new class of optimization problems called scale invariant problems and prove that they can be efficiently solved by scale invariant power iteration (SCI-PI) with a generalized convergence guarantee of power iteration. By deriving that a stationary point is an eigenvector of the Hessian evaluated at the point, we show that scale invariant problems indeed resemble the leading eigenvector problem near a local optimum. Also, based on a novel reformulation, we geometrically derive SCI-PI which has a general form of power iteration. The convergence analysis shows that SCI-PI attains local linear convergence with a rate being proportional to the top two eigenvalues of the Hessian at the optimum. Moreover, we discuss some extended settings of scale invariant problems and provide similar convergence results for them. In numerical experiments, we introduce applications to independent component analysis, Gaussian mixtures, and non-negative matrix factorization. Experimental results demonstrate that SCI-PI is competitive to state-of-the-art benchmark algorithms and often yield better solutions.},
 author = {Cheolmin Kim and Youngseok Kim and Diego Klabjan},
 journal = {Journal of Machine Learning Research},
 number = {321},
 openalex = {W2945743233},
 pages = {1--47},
 title = {Scale Invariant Power Iteration},
 url = {http://jmlr.org/papers/v24/19-784.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:19-980,
 abstract = {Finding optimal policies which maximize long term rewards of Markov Decision Processes requires the use of dynamic programming and backward induction to solve the Bellman optimality equation. However, many real-world problems require optimization of an objective that is non-linear in cumulative rewards for which dynamic programming cannot be applied directly. For example, in a resource allocation problem, one of the objectives is to maximize long-term fairness among the users. We notice that when an agent aim to optimize some function of the sum of rewards is considered, the problem loses its Markov nature. This paper addresses and formalizes the problem of optimizing a non-linear function of the long term average of rewards. We propose model-based and model-free algorithms to learn the policy, where the model-based policy is shown to achieve a regret of $\Tilde{O}\left(LKDS\sqrt{\frac{A}{T}}\right)$ for $K$ objectives combined with a concave $L$-Lipschitz function. Further, using the fairness in cellular base-station scheduling, and queueing system scheduling as examples, the proposed algorithm is shown to significantly outperform the conventional RL approaches.},
 author = {Mridul Agarwal and Vaneet Aggarwal},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W4315575041},
 pages = {1--41},
 title = {Reinforcement Learning for Joint Optimization of Multiple Rewards},
 url = {http://jmlr.org/papers/v24/19-980.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-060,
 author = {Linxi Liu and Dangna Li and Wing Hung Wong},
 journal = {Journal of Machine Learning Research},
 number = {50},
 pages = {1--64},
 title = {Convergence Rates of a Class of Multivariate Density Estimation Methods Based on Adaptive Partitioning},
 url = {http://jmlr.org/papers/v24/20-060.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1012,
 abstract = {Statistical modeling and inference problems with sample sizes substantially smaller than the number of available covariates are challenging. Chakraborty et al. (2012) did a full hierarchical Bayesian analysis of nonlinear regression in such situations using relevance vector machines based on reproducing kernel Hilbert space (RKHS). But they did not provide any theoretical properties associated with their procedure. The present paper revisits their problem, introduces a new class of global-local priors different from theirs, and provides results on posterior consistency as well as posterior contraction rates},
 author = {Xiao Fang and Malay Ghosh},
 journal = {Journal of Machine Learning Research},
 number = {174},
 openalex = {W4221158887},
 pages = {1--17},
 title = {Posterior Consistency for Bayesian Relevance Vector Machines},
 url = {http://jmlr.org/papers/v24/20-1012.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1039,
 abstract = {This paper studies convergence of empirical risks in reproducing kernel Hilbert spaces (RKHS). A conventional assumption in the existing research is that empirical training data do not contain any noise but this may not be satisfied in some practical circumstances. Consequently the existing convergence results do not provide a guarantee as to whether empirical risks based on empirical data are reliable or not when the data contain some noise. In this paper, we fill out the gap in a few steps. First, we derive moderate sufficient conditions under which the expected risk changes stably (continuously) against small perturbation of the probability distribution of the underlying random variables and demonstrate how the cost function and kernel affect the stability. Second, we examine the difference between laws of the statistical estimators of the expected optimal loss based on pure data and contaminated data using Prokhorov metric and Kantorovich metric and derive some qualitative and quantitative statistical robustness results. Third, we identify appropriate metrics under which the statistical estimators are uniformly asymptotically consistent. These results provide theoretical grounding for analysing asymptotic convergence and examining reliability of the statistical estimators in a number of well-known machine learning models.},
 author = {Shaoyan Guo and Huifu Xu and Liwei Zhang},
 journal = {Journal of Machine Learning Research},
 number = {125},
 openalex = {W3024686743},
 pages = {1--38},
 title = {Statistical Robustness of Empirical Risks in Machine Learning},
 url = {http://jmlr.org/papers/v24/20-1039.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1101,
 abstract = {In this paper, we develop an online change-point detection procedure in the covariance structure of high-dimensional data. A new stopping rule is proposed to terminate the process as early as possible when a change in covariance structure occurs. The stopping rule allows temporal dependence and can be applied to non-Gaussian data. An explicit expression for the average run length (ARL) is derived, so that the level of threshold in the stopping rule can be easily obtained with no need to run time-consuming Monte Carlo simulations. We also establish an upper bound for the expected detection delay (EDD), the expression of which demonstrates the impact of data dependence and magnitude of change in the covariance structure. Simulation studies are provided to confirm accuracy of the theoretical results. The practical usefulness of the proposed procedure is illustrated by detecting the change of brain's covariance network in a resting-state fMRI dataset.},
 author = {Lingjun Li and Jun Li},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2990197469},
 pages = {1--44},
 title = {Online Change-Point Detection in High-Dimensional Covariance Structure with Application to Dynamic Networks},
 url = {http://jmlr.org/papers/v24/20-1101.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1131,
 abstract = {Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, our goal is to address the fundamental question about its sample complexity. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$ for finding the Nash equilibrium (NE) value up to some $\epsilon$ error, and the $\epsilon$-NE policies with a smooth planning oracle, where $\gamma$ is the discount factor, and $S,A,B$ denote the state space, and the action spaces for the two agents. We further show that such a sample bound is minimax-optimal (up to logarithmic factors) if the algorithm is reward-agnostic, where the algorithm queries state transition samples without reward knowledge, by establishing a matching lower bound. This is in contrast to the usual reward-aware setting, with a $\tilde\Omega(|S|(|A|+|B|)(1-\gamma)^{-3}\epsilon^{-2})$ lower bound, where this model-based approach is near-optimal with only a gap on the $|A|,|B|$ dependence. Our results not only demonstrate the sample-efficiency of this basic model-based approach in MARL, but also elaborate on the fundamental tradeoff between its power (easily handling the more challenging reward-agnostic case) and limitation (less adaptive and suboptimal in $|A|,|B|$), particularly arises in the multi-agent context.},
 author = {Kaiqing Zhang and Sham M. Kakade and Tamer Basar and Lin F. Yang},
 journal = {Journal of Machine Learning Research},
 number = {175},
 openalex = {W3042871037},
 pages = {1--53},
 title = {Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity},
 url = {http://jmlr.org/papers/v24/20-1131.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1202,
 abstract = {This paper presents an inverse reinforcement learning (IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function; then we construct set valued estimates of the cost function. To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. To illustrate our IRL scheme,we consider two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. Finally, for finite datasets, we propose an IRL detection algorithm and give finite sample bounds on its error probabilities. Also we discuss how to identify $\epsilon$-optimal Bayesian decision makers and perform IRL.},
 author = {Kunal Pattanayak and Vikram Krishnamurthy},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W3094216403},
 pages = {1--64},
 title = {Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems},
 url = {http://jmlr.org/papers/v24/20-1202.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1206,
 author = {Leo L Duan and George Michailidis and Mingzhou Ding},
 journal = {Journal of Machine Learning Research},
 number = {3},
 pages = {1--35},
 title = {Bayesian Spiked Laplacian Graphs},
 url = {http://jmlr.org/papers/v24/20-1206.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1226,
 abstract = {We study a multi-round welfare-maximising mechanism design problem in instances where agents do not know their values. On each round, a mechanism first assigns an allocation each to a set of agents and charges them a price; at the end of the round, the agents provide (stochastic) feedback to the mechanism for the allocation they received. This setting is motivated by applications in cloud markets and online advertising where an agent may know her value for an allocation only after experiencing it. Therefore, the mechanism needs to explore different allocations for each agent so that it can learn their values, while simultaneously attempting to find the socially optimal set of allocations. Our focus is on truthful and individually rational mechanisms which imitate the classical VCG mechanism in the long run. To that end, we first define three notions of regret for the welfare, the individual utilities of each agent and that of the mechanism. We show that these three terms are interdependent via an $\Omega(T^{\frac{2}{3}})$ lower bound for the maximum of these three terms after $T$ rounds of allocations, and describe an algorithm which essentially achieves this rate. Our framework also provides flexibility to control the pricing scheme so as to trade-off between the agent and seller regrets. Next, we define asymptotic variants for the truthfulness and individual rationality requirements and provide asymptotic rates to quantify the degree to which both properties are satisfied by the proposed algorithm.},
 author = {Kirthevasan Kandasamy and Joseph E Gonzalez and Michael I Jordan and Ion Stoica},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W4285688132},
 pages = {1--45},
 title = {VCG Mechanism Design with Unknown Agent Values under Stochastic Bandit Feedback},
 url = {http://jmlr.org/papers/v24/20-1226.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1287,
 author = {Patrick F. Burauel},
 journal = {Journal of Machine Learning Research},
 number = {176},
 pages = {1--56},
 title = {Evaluating Instrument Validity using the Principle of Independent Mechanisms},
 url = {http://jmlr.org/papers/v24/20-1287.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1310,
 abstract = {An increasing amount of collected data are high-dimensional multi-way arrays (tensors), and it is crucial for efficient learning algorithms to exploit this tensorial structure as much as possible. The ever-present curse of dimensionality for high dimensional data and the loss of structure when vectorizing the data motivates the use of tailored low-rank tensor classification methods. In the presence of small amounts of training data, kernel methods offer an attractive choice as they provide the possibility for a nonlinear decision boundary. We develop the Tensor Train Multi-way Multi-level Kernel (TT-MMK), which combines the simplicity of the Canonical Polyadic decomposition, the classification power of the Dual Structure-preserving Support Vector Machine, and the reliability of the Tensor Train (TT) approximation. We show by experiments that the TT-MMK method is usually more reliable computationally, less sensitive to tuning parameters, and gives higher prediction accuracy in the SVM classification when benchmarked against other state-of-the-art techniques.},
 author = {Kirandeep Kour and Sergey Dolgov and Martin Stoll and Peter Benner},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W3006120480},
 pages = {1--22},
 title = {Efficient Structure-preserving Support Tensor Train Machine},
 url = {http://jmlr.org/papers/v24/20-1310.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1318,
 abstract = {Item Response Theory (IRT) has been proposed within the field of Educational Psychometrics to assess student ability as well as test question difficulty and discrimination power. More recently, IRT has been applied to evaluate machine learning algorithm performance on a single classification dataset, where the student is now an algorithm, and the test question is an observation to be classified by the algorithm. In this paper we present a modified IRT-based framework for evaluating a portfolio of algorithms across a repository of datasets, while simultaneously eliciting a richer suite of characteristics - such as algorithm consistency and anomalousness - that describe important aspects of algorithm performance. These characteristics arise from a novel inversion and reinterpretation of the traditional IRT model without requiring additional dataset feature computations. We test this framework on algorithm portfolios for a wide range of applications, demonstrating the broad applicability of this method as an insightful algorithm evaluation tool. Furthermore, the explainable nature of IRT parameters yield an increased understanding of algorithm portfolios.},
 author = {Sevvandi Kandanaarachchi and Kate Smith-Miles},
 journal = {Journal of Machine Learning Research},
 number = {177},
 openalex = {W4385473546},
 pages = {1--52},
 title = {Comprehensive Algorithm Portfolio Evaluation using Item Response Theory},
 url = {http://jmlr.org/papers/v24/20-1318.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1321,
 abstract = {A model involving Gaussian processes (GPs) is introduced to simultaneously handle multi-task learning, clustering, and prediction for multiple functional data. This procedure acts as a model-based clustering method for functional data as well as a learning step for subsequent predictions for new tasks. The model is instantiated as a mixture of multi-task GPs with common mean processes. A variational EM algorithm is derived for dealing with the optimisation of the hyper-parameters along with the hyper-posteriors' estimation of latent variables and processes. We establish explicit formulas for integrating the mean processes and the latent clustering variables within a predictive distribution, accounting for uncertainty on both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances the performances when dealing with group-structured data. The model handles irregular grid of observations and offers different hypotheses on the covariance structure for sharing additional information across tasks. The performances on both clustering and prediction tasks are assessed through various simulated scenarios and real datasets. The overall algorithm, called MagmaClust, is publicly available as an R package.},
 author = {Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W3100364760},
 pages = {1--49},
 title = {Cluster-Specific Predictions with Multi-Task Gaussian Processes},
 url = {http://jmlr.org/papers/v24/20-1321.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1355,
 author = {Haifeng Jin and François Chollet and Qingquan Song and Xia Hu},
 journal = {Journal of Machine Learning Research},
 number = {6},
 pages = {1--6},
 title = {AutoKeras: An AutoML Library for Deep Learning},
 url = {http://jmlr.org/papers/v24/20-1355.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1390,
 author = {Siddarth Asokan and Chandra Sekhar Seelamantula},
 journal = {Journal of Machine Learning Research},
 number = {126},
 pages = {1--100},
 title = {Euler-Lagrange Analysis of Generative Adversarial Networks},
 url = {http://jmlr.org/papers/v24/20-1390.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1419,
 abstract = {We study community detection in the contextual stochastic block model arXiv:1807.09596 [cs.SI], arXiv:1607.02675 [stat.ME]. In arXiv:1807.09596 [cs.SI], the second author studied this problem in the setting of sparse graphs with high-dimensional node-covariates. Using the non-rigorous cavity method from statistical physics, they conjectured the sharp limits for community detection in this setting. Further, the information theoretic threshold was verified, assuming that the average degree of the observed graph is large. It is expected that the conjecture holds as soon as the average degree exceeds one, so that the graph has a giant component. We establish this conjecture, and characterize the sharp threshold for detection and weak recovery.},
 author = {Chen Lu and Subhabrata Sen},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W3104549479},
 pages = {1--34},
 title = {Contextual Stochastic Block Model: Sharp Thresholds and Contiguity},
 url = {http://jmlr.org/papers/v24/20-1419.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1437,
 abstract = {Nonparametric varying coefficient (NVC) models are useful for modeling time-varying effects on responses that are measured repeatedly for the same subjects. When the number of covariates is moderate or large, it is desirable to perform variable selection from the varying coefficient functions. However, existing methods for variable selection in NVC models either fail to account for within-subject correlations or require the practitioner to specify a parametric form for the correlation structure. In this paper, we introduce the nonparametric varying coefficient spike-and-slab lasso (NVC-SSL) for Bayesian high-dimensional NVC models. Through the introduction of functional random effects, our method allows for flexible modeling of within-subject correlations without needing to specify a parametric covariance function. We further propose several scalable optimization and Markov chain Monte Carlo (MCMC) algorithms. For variable selection, we propose an Expectation Conditional Maximization (ECM) algorithm to rapidly obtain maximum a posteriori (MAP) estimates. Our ECM algorithm scales linearly in the total number of observations $N$ and the number of covariates $p$. For uncertainty quantification, we introduce an approximate MCMC algorithm that also scales linearly in both $N$ and $p$. We demonstrate the scalability, variable selection performance, and inferential capabilities of our method through simulations and a real data application. These algorithms are implemented in the publicly available R package NVCSSL on the Comprehensive R Archive Network.},
 author = {Ray Bai and Mary R. Boland and Yong Chen},
 journal = {Journal of Machine Learning Research},
 number = {259},
 openalex = {W2961393948},
 pages = {1--49},
 title = {Scalable high-dimensional Bayesian varying coefficient models with unknown within-subject covariance},
 url = {http://jmlr.org/papers/v24/20-1437.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-1461,
 abstract = {This paper considers the partially functional linear model (PFLM) where all predictive features consist of a functional covariate and a high dimensional scalar vector. Over an infinite dimensional reproducing kernel Hilbert space, the proposed estimation for PFLM is a least square approach with two mixed regularizations of a function-norm and an $\ell_1$-norm. Our main task in this paper is to establish the minimax rates for PFLM under high dimensional setting, and the optimal minimax rates of estimation is established by using various techniques in empirical process theory for analyzing kernel classes. In addition, we propose an efficient numerical algorithm based on randomized sketches of the kernel matrix. Several numerical experiments are implemented to support our method and optimization strategy.},
 author = {Shaogao Lv and Xin He and Junhui Wang},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W3206075628},
 pages = {1--38},
 title = {Kernel-based estimation for partially functional linear model: Minimax rates and randomized sketches},
 url = {http://jmlr.org/papers/v24/20-1461.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-238,
 author = {Tianhong Sheng and Bharath K. Sriperumbudur},
 journal = {Journal of Machine Learning Research},
 number = {7},
 pages = {1--16},
 title = {On Distance and Kernel Measures of Conditional Dependence},
 url = {http://jmlr.org/papers/v24/20-238.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-267,
 abstract = {We introduce a relaxed inertial forward-backward-forward (RIFBF) splitting algorithm for approaching the set of zeros of the sum of a maximally monotone operator and a single-valued monotone and Lipschitz continuous operator. This work aims to extend Tseng's forward-backward-forward method by both using inertial effects as well as relaxation parameters. We formulate first a second order dynamical system which approaches the solution set of the monotone inclusion problem to be solved and provide an asymptotic analysis for its trajectories. We provide for RIFBF, which follows by explicit time discretization, a convergence analysis in the general monotone case as well as when applied to the solving of pseudo-monotone variational inequalities. We illustrate the proposed method by applications to a bilinear saddle point problem, in the context of which we also emphasize the interplay between the inertial and the relaxation parameters, and to the training of Generative Adversarial Networks (GANs).},
 author = {Radu I. Bot and Michael Sedlmayer and Phan Tu Vuong},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W3010808375},
 pages = {1--37},
 title = {A Relaxed Inertial Forward-Backward-Forward Algorithm for Solving Monotone Inclusions with Application to GANs},
 url = {http://jmlr.org/papers/v24/20-267.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-364,
 abstract = {The learning rate is perhaps the single most important parameter in the training of neural networks and, more broadly, in stochastic (nonconvex) optimization. Accordingly, there are numerous effective, but poorly understood, techniques for tuning the learning rate, including learning rate decay, which starts with a large initial learning rate that is gradually decreased. In this paper, we present a general theoretical analysis of the effect of the learning rate in stochastic gradient descent (SGD). Our analysis is based on the use of a learning-rate-dependent stochastic differential equation (lr-dependent SDE) that serves as a surrogate for SGD. For a broad class of objective functions, we establish a linear rate of convergence for this continuous-time formulation of SGD, highlighting the fundamental importance of the learning rate in SGD, and contrasting to gradient descent and stochastic gradient Langevin dynamics. Moreover, we obtain an explicit expression for the optimal linear rate by analyzing the spectrum of the Witten-Laplacian, a special case of the Schrödinger operator associated with the lr-dependent SDE. Strikingly, this expression clearly reveals the dependence of the linear convergence rate on the learning rate -- the linear rate decreases rapidly to zero as the learning rate tends to zero for a broad class of nonconvex functions, whereas it stays constant for strongly convex functions. Based on this sharp distinction between nonconvex and convex problems, we provide a mathematical interpretation of the benefits of using learning rate decay for nonconvex optimization.},
 author = {Bin Shi and Weijie Su and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {379},
 openalex = {W3016499390},
 pages = {1--53},
 title = {On Learning Rates and Schrödinger Operators},
 url = {http://jmlr.org/papers/v24/20-364.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-449,
 abstract = {A graph homomorphism is a map between two graphs that preserves adjacency relations. We consider the problem of sampling a random graph homomorphism from a graph $F$ into a large network $\mathcal{G}$. We propose two complementary MCMC algorithms for sampling a random graph homomorphisms and establish bounds on their mixing times and concentration of their time averages. Based on our sampling algorithms, we propose a novel framework for network data analysis that circumvents some of the drawbacks in methods based on independent and neigborhood sampling. Various time averages of the MCMC trajectory give us various computable observables, including well-known ones such as homomorphism density and average clustering coefficient and their generalizations. Furthermore, we show that these network observables are stable with respect to a suitably renormalized cut distance between networks. We provide various examples and simulations demonstrating our framework through synthetic networks. We also apply our framework for network clustering and classification problems using the Facebook100 dataset and Word Adjacency Networks of a set of classic novels.},
 author = {Hanbaek Lyu and Facundo Memoli and David Sivakoff},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2981181201},
 pages = {1--79},
 title = {Sampling random graph homomorphisms and applications to network data analysis},
 url = {http://jmlr.org/papers/v24/20-449.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-536,
 abstract = {Bayesian Networks (BNs) represent conditional probability relations among a set of random variables (nodes) in the form of a directed acyclic graph (DAG), and have found diverse applications in knowledge discovery. We study the problem of learning the sparse DAG structure of a BN from continuous observational data. The central problem can be modeled as a mixed-integer program with an objective function composed of a convex quadratic loss function and a regularization penalty subject to linear constraints. The optimal solution to this mathematical program is known to have desirable statistical properties under certain conditions. However, the state-of-the-art optimization solvers are not able to obtain provably optimal solutions to the existing mathematical formulations for medium-size problems within reasonable computational times. To address this difficulty, we tackle the problem from both computational and statistical perspectives. On the one hand, we propose a concrete early stopping criterion to terminate the branch-and-bound process in order to obtain a near-optimal solution to the mixed-integer program, and establish the consistency of this approximate solution. On the other hand, we improve the existing formulations by replacing the linear "big-$M$" constraints that represent the relationship between the continuous and binary indicator variables with second-order conic constraints. Our numerical results demonstrate the effectiveness of the proposed approaches.},
 author = {Simge Kucukyavuz and Ali Shojaie and Hasan Manzour and Linchuan Wei and Hao-Hsiang Wu},
 journal = {Journal of Machine Learning Research},
 number = {322},
 openalex = {W3032794712},
 pages = {1--38},
 title = {Consistent Second-Order Conic Integer Programming for Learning Bayesian Networks},
 url = {http://jmlr.org/papers/v24/20-536.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-572,
 abstract = {Directed networks are broadly used to represent asymmetric relationships among units. Co-clustering aims to cluster the senders and receivers of directed networks simultaneously. In particular, the well-known spectral clustering algorithm could be modified as the spectral co-clustering to co-cluster directed networks. However, large-scale networks pose great computational challenges to it. In this paper, we leverage sketching techniques and derive two randomized spectral co-clustering algorithms, one \emph{random-projection-based} and the other \emph{random-sampling-based}, to accelerate the co-clustering of large-scale directed networks. We theoretically analyze the resulting algorithms under two generative models -- the stochastic co-block model and the degree-corrected stochastic co-block model, and establish their approximation error rates and misclustering error rates, indicating better bounds than the state-of-the-art results of co-clustering literature. Numerically, we design and conduct simulations to support our theoretical results and test the efficiency of the algorithms on real networks with up to millions of nodes. A publicly available R package \textsf{RandClust} is developed for better usability and reproducibility of the proposed methods.},
 author = {Xiao Guo and Yixuan Qiu and Hai Zhang and Xiangyu Chang},
 journal = {Journal of Machine Learning Research},
 number = {380},
 openalex = {W3018460594},
 pages = {1--68},
 title = {Randomized spectral co-clustering for large-scale directed networks},
 url = {http://jmlr.org/papers/v24/20-572.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-602,
 abstract = {Bayesian inference problems require sampling or approximating high-dimensional probability distributions. The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. We demonstrate significant performance gains of these in various numerical experiments.},
 author = {Andrew Duncan and Nikolas Nüsken and Lukasz Szpruch},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2990020588},
 pages = {1--39},
 title = {On the geometry of Stein variational gradient descent},
 url = {http://jmlr.org/papers/v24/20-602.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-608,
 abstract = {We describe a line-search algorithm which achieves the best-known worst-case complexity results for problems with a certain property that has been observed to hold in low-rank matrix optimization problems. Our algorithm is adaptive, in the sense that it makes use of backtracking line searches and does not require prior knowledge of the parameters that define the strict saddle property.},
 author = {Michael J. O'Neill and Stephen J. Wright},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W3034599196},
 pages = {1--34},
 title = {A Line-Search Descent Algorithm for Strict Saddle Functions with Complexity Guarantees},
 url = {http://jmlr.org/papers/v24/20-608.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-695,
 abstract = {We introduce Tree-AMP, standing for Tree Approximate Message Passing, a python package for compositional inference in high-dimensional tree-structured models. The package provides a unifying framework to study several approximate message passing algorithms previously derived for a variety of machine learning tasks such as generalized linear models, inference in multi-layer networks, matrix factorization, and reconstruction using non-separable penalties. For some models, the asymptotic performance of the algorithm can be theoretically predicted by the state evolution, and the measurements entropy estimated by the free entropy formalism. The implementation is modular by design: each module, which implements a factor, can be composed at will with other modules to solve complex inference tasks. The user only needs to declare the factor graph of the model: the inference algorithm, state evolution and entropy estimation are fully automated.},
 author = {Antoine Baker and Florent Krzakala and Benjamin Aubin and Lenka Zdeborová},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W4206895473},
 pages = {1--89},
 title = {Tree-AMP: Compositional Inference with Tree Approximate Message Passing},
 url = {http://jmlr.org/papers/v24/20-695.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-700,
 abstract = {Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluation are jointly optimized, which can stabilize multi-agent policy learning. Furthermore, our framework can achieve scalability and stability for large-scale environment and reduce information transmission, by the parameter sharing mechanism and a novel modeling-other-agents methods based on theory-of-mind and online supervised learning. Sufficient experiments in cooperative Multi-agent Particle Environment and StarCraft II show that our decentralized MARL instantiation algorithms perform competitively against conventional centralized and decentralized methods.},
 author = {Wenhao Li and Bo Jin and Xiangfeng Wang and Junchi Yan and Hongyuan Zha},
 journal = {Journal of Machine Learning Research},
 number = {178},
 openalex = {W3019081517},
 pages = {1--75},
 title = {F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/20-700.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-902,
 abstract = {Author(s): Tan, Yan Shuo; Vershynin, Roman | Abstract: In recent literature, a general two step procedure has been formulated for solving the problem of phase retrieval. First, a spectral technique is used to obtain a constant-error initial estimate, following which, the estimate is refined to arbitrary precision by first-order optimization of a non-convex loss function. Numerical experiments, however, seem to suggest that simply running the iterative schemes from a random initialization may also lead to convergence, albeit at the cost of slightly higher sample complexity. In this paper, we prove that, in fact, constant step size online stochastic gradient descent (SGD) converges from arbitrary initializations for the non-smooth, non-convex amplitude squared loss objective. In this setting, online SGD is also equivalent to the randomized Kaczmarz algorithm from numerical analysis. Our analysis can easily be generalized to other single index models. It also makes use of new ideas from stochastic process theory, including the notion of a summary state space, which we believe will be of use for the broader field of non-convex optimization.},
 author = {Yan Shuo Tan and Roman Vershynin},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2981804444},
 pages = {1--47},
 title = {Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval.},
 url = {http://jmlr.org/papers/v24/20-902.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-981,
 abstract = {Learning the optimal ordering of content is an important challenge in website design. The learning to rank (LTR) framework models this problem as a sequential problem of selecting lists of content and observing where users decide to click. Most previous work on LTR assumes that the user considers each item in the list in isolation, and makes binary choices to click or not on each. We introduce a multinomial logit (MNL) choice model to the LTR framework, which captures the behaviour of users who consider the ordered list of items as a whole and make a single choice among all the items and a no-click option. Under the MNL model, the user favours items which are either inherently more attractive, or placed in a preferable position within the list. We propose upper confidence bound algorithms to minimise regret in two settings - where the position dependent parameters are known, and unknown. We present theoretical analysis leading to an $\Omega(\sqrt{T})$ lower bound for the problem, an $\tilde{O}(\sqrt{T})$ upper bound on regret for the known parameter version. Our analyses are based on tight new concentration results for Geometric random variables, and novel functional inequalities for maximum likelihood estimators computed on discrete data.},
 author = {James A. Grant and David S. Leslie},
 journal = {Journal of Machine Learning Research},
 number = {260},
 openalex = {W3083712961},
 pages = {1--49},
 title = {Learning to Rank under Multinomial Logit Choice},
 url = {http://jmlr.org/papers/v24/20-981.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:20-998,
 abstract = {Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. Graph clustering has the same overall goal as node pooling in GNNs - does this mean that GNN pooling methods do a good job at clustering graphs? Surprisingly, the answer is no - current GNN pooling methods often fail to recover the cluster structure in cases where simple baselines, such as k-means applied on learned representations, work well. We investigate further by carefully designing a set of experiments to study different signal-to-noise scenarios both in graph structure and attribute data. To address these methods' poor performance in clustering, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results with over 40% improvement over other pooling methods across different metrics.},
 author = {Anton Tsitsulin and John Palowitch and Bryan Perozzi and Emmanuel Müller},
 journal = {Journal of Machine Learning Research},
 number = {127},
 openalex = {W4287749054},
 pages = {1--21},
 title = {Graph Clustering with Graph Neural Networks},
 url = {http://jmlr.org/papers/v24/20-998.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0048,
 abstract = {In classification with a reject option, the classifier is allowed in uncertain cases to abstain from prediction. The classical cost-based model of a reject option classifier requires the cost of rejection to be defined explicitly. An alternative bounded-improvement model, avoiding the notion of the reject cost, seeks for a classifier with a guaranteed selective risk and maximal cover. We coin a symmetric definition, the bounded-coverage model, which seeks for a classifier with minimal selective risk and guaranteed coverage. We prove that despite their different formulations the three rejection models lead to the same prediction strategy: a Bayes classifier endowed with a randomized Bayes selection function. We define a notion of a proper uncertainty score as a scalar summary of prediction uncertainty sufficient to construct the randomized Bayes selection function. We propose two algorithms to learn the proper uncertainty score from examples for an arbitrary black-box classifier. We prove that both algorithms provide Fisher consistent estimates of the proper uncertainty score and we demonstrate their efficiency on different prediction problems including classification, ordinal regression and structured output classification.},
 author = {Vojtech Franc and Daniel Prusa and Vaclav Voracek},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W3127462923},
 pages = {1--49},
 title = {Optimal strategies for reject option classifiers},
 url = {http://jmlr.org/papers/v24/21-0048.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0073,
 author = {Ephy R. Love and Benjamin Filippenko and Vasileios Maroulas and Gunnar Carlsson},
 journal = {Journal of Machine Learning Research},
 number = {59},
 pages = {1--35},
 title = {Topological Convolutional Layers for Deep Learning},
 url = {http://jmlr.org/papers/v24/21-0073.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0096,
 abstract = {The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens' frequencies in a data stream of tokens, i.e. point queries, based on random hashed data. A learning-augmented version of the CMS, referred to as CMS-DP, has been proposed by Cai, Mitzenmacher and Adams (\textit{NeurIPS} 2018), and it relies on Bayesian nonparametric (BNP) modeling of the data stream of tokens via a Dirichlet process (DP) prior, with estimates of a point query being obtained as suitable mean functionals of the posterior distribution of the point query, given the hashed data. While the CMS-DP has proved to improve on some aspects of CMS, it has the major drawback of arising from a ``constructive" proof that builds upon arguments tailored to the DP prior, namely arguments that are not usable for other nonparametric priors. In this paper, we present a ``Bayesian" proof of the CMS-DP that has the main advantage of building upon arguments that are usable, in principle, within a broad class of nonparametric priors arising from normalized completely random measures. This result leads to develop a novel learning-augmented CMS under power-law data streams, referred to as CMS-PYP, which relies on BNP modeling of the data stream of tokens via a Pitman-Yor process (PYP) prior. Under this more general framework, we apply the arguments of the ``Bayesian" proof of the CMS-DP, suitably adapted to the PYP prior, in order to compute the posterior distribution of a point query, given the hashed data. Applications to synthetic data and real textual data show that the CMS-PYP outperforms the CMS and the CMS-DP in estimating low-frequency tokens, which are known to be of critical interest in textual data, and it is competitive with respect to a variation of the CMS designed for low-frequency tokens. An extension of our BNP approach to more general queries is also discussed.},
 author = {Emanuele Dolera and Stefano Favaro and Stefano Peluchetti},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W4287330703},
 pages = {1--60},
 title = {Learning-augmented count-min sketches via Bayesian nonparametrics},
 url = {http://jmlr.org/papers/v24/21-0096.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0116,
 abstract = {There is a rich literature on Bayesian methods for density estimation, which characterize the unknown density as a mixture of kernels. Such methods have advantages in terms of providing uncertainty quantification in estimation, while being adaptive to a rich variety of densities. However, relative to frequentist locally adaptive kernel methods, Bayesian approaches can be slow and unstable to implement in relying on Markov chain Monte Carlo algorithms. To maintain most of the strengths of Bayesian approaches without the computational disadvantages, we propose a class of nearest neighbor-Dirichlet mixtures. The approach starts by grouping the data into neighborhoods based on standard algorithms. Within each neighborhood, the density is characterized via a Bayesian parametric model, such as a Gaussian with unknown parameters. Assigning a Dirichlet prior to the weights on these local kernels, we obtain a pseudo-posterior for the weights and kernel parameters. A simple and embarrassingly parallel Monte Carlo algorithm is proposed to sample from the resulting pseudo-posterior for the unknown density. Desirable asymptotic properties are shown, and the methods are evaluated in simulation studies and applied to a motivating data set in the context of classification.},
 author = {Shounak Chattopadhyay and Antik Chakraborty and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {261},
 openalex = {W4287825943},
 pages = {1--46},
 title = {Nearest Neighbor Dirichlet Mixtures},
 url = {http://jmlr.org/papers/v24/21-0116.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0117,
 author = {Qinbo Bai and Vaneet Aggarwal and Ather Gattami},
 journal = {Journal of Machine Learning Research},
 number = {60},
 pages = {1--25},
 title = {Provably Sample-Efficient Model-Free Algorithm for MDPs with Peak Constraints},
 url = {http://jmlr.org/papers/v24/21-0117.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0148,
 abstract = {We consider stochastic bandit problems with $K$ arms, each associated with a bounded distribution supported on the range $[m,M]$. We do not assume that the range $[m,M]$ is known and show that there is a cost for learning this range. Indeed, a new trade-off between distribution-dependent and distribution-free regret bounds arises, which prevents from simultaneously achieving the typical $\ln T$ and \smash{$\sqrt{T}$} bounds. For instance, a \smash{$\sqrt{T}$} distribution-free regret bound may only be achieved if the distribution-dependent regret bounds are at least of order \smash{$\sqrt{T}$}. We exhibit a strategy achieving the rates for regret indicated by the new trade-off.},
 author = {Hédi Hadiji and Gilles Stoltz},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W3033899073},
 pages = {1--33},
 title = {Adaptation to the Range in $K$-Armed Bandits},
 url = {http://jmlr.org/papers/v24/21-0148.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0169,
 abstract = {In images collected by astronomical surveys, stars and galaxies often overlap visually. Deblending is the task of distinguishing and characterizing individual light sources in survey images. We propose StarNet, a Bayesian method to deblend sources in astronomical images of crowded star fields. StarNet leverages recent advances in variational inference, including amortized variational distributions and an optimization objective targeting an expectation of the forward KL divergence. In our experiments with SDSS images of the M2 globular cluster, StarNet is substantially more accurate than two competing methods: Probabilistic Cataloging (PCAT), a method that uses MCMC for inference, and DAOPHOT, a software pipeline employed by SDSS for deblending. In addition, the amortized approach to inference gives StarNet the scaling characteristics necessary to perform Bayesian inference on modern astronomical surveys.},
 author = {Runjing Liu and Jon D. McAuliffe and Jeffrey Regier and The LSST Dark Energy Science Collaboration},
 journal = {Journal of Machine Learning Research},
 number = {179},
 openalex = {W3127866340},
 pages = {1--36},
 title = {Variational Inference for Deblending Crowded Starfields},
 url = {http://jmlr.org/papers/v24/21-0169.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0187,
 author = {Aaron Sonabend-W and Nilanjana Laha and Ashwin N. Ananthakrishnan and Tianxi Cai and Rajarshi Mukherjee},
 journal = {Journal of Machine Learning Research},
 number = {323},
 pages = {1--86},
 title = {Semi-Supervised Off-Policy Reinforcement Learning and Value Estimation for Dynamic Treatment Regimes},
 url = {http://jmlr.org/papers/v24/21-0187.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0224,
 author = {Shuxiao Chen and Qinqing Zheng and Qi Long and Weijie J. Su},
 journal = {Journal of Machine Learning Research},
 number = {262},
 pages = {1--59},
 title = {Minimax Estimation for Personalized Federated Learning: An Alternative between FedAvg and Local Training?},
 url = {http://jmlr.org/papers/v24/21-0224.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0235,
 abstract = {Normalizing Flows (NFs) are universal density estimators based on Neural Networks. However, this universality is limited: the density's support needs to be diffeomorphic to a Euclidean space. In this paper, we propose a novel method to overcome this limitation without sacrificing universality. The proposed method inflates the data manifold by adding noise in the normal space, trains an NF on this inflated manifold, and, finally, deflates the learned density. Our main result provides sufficient conditions on the manifold and the specific choice of noise under which the corresponding estimator is exact. Our method has the same computational complexity as NFs and does not require computing an inverse flow. We also show that, if the embedding dimension is much larger than the manifold dimension, noise in the normal space can be well approximated by Gaussian noise. This allows using our method for approximating arbitrary densities on unknown manifolds provided that the manifold dimension is known.},
 author = {Christian Horvat and Jean-Pascal Pfister},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W4287169354},
 pages = {1--37},
 title = {Density estimation on low-dimensional manifolds: an inflation-deflation approach},
 url = {http://jmlr.org/papers/v24/21-0235.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0249,
 abstract = {In this paper, we introduce a novel family of iterative algorithms which carry out $\alpha$-divergence minimisation in a Variational Inference context. They do so by ensuring a systematic decrease at each step in the $\alpha$-divergence between the variational and the posterior distributions. In its most general form, the variational distribution is a mixture model and our framework allows us to simultaneously optimise the weights and components parameters of this mixture model. Our approach permits us to build on various methods previously proposed for $\alpha$-divergence minimisation such as Gradient or Power Descent schemes and we also shed a new light on an integrated Expectation Maximization algorithm. Lastly, we provide empirical evidence that our methodology yields improved results on several multimodal target distributions and on a real data example.},
 author = {Kamélia Daudel and Randal Douc and François Roueff},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W4287278360},
 pages = {1--76},
 title = {Monotonic Alpha-divergence Minimisation for Variational Inference},
 url = {http://jmlr.org/papers/v24/21-0249.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0270,
 abstract = {Dynamic multilayer networks frequently represent the structure of multiple co-evolving relations; however, statistical models are not well-developed for this prevalent network type. Here, we propose a new latent space model for dynamic multilayer networks. The key feature of our model is its ability to identify common time-varying structures shared by all layers while also accounting for layer-wise variation and degree heterogeneity. We establish the identifiability of the model's parameters and develop a structured mean-field variational inference approach to estimate the model's posterior, which scales to networks previously intractable to dynamic latent space models. We demonstrate the estimation procedure's accuracy and scalability on simulated networks. We apply the model to two real-world problems: discerning regional conflicts in a data set of international relations and quantifying infectious disease spread throughout a school based on the student's daily contact patterns.},
 author = {Joshua Daniel Loyal and Yuguo Chen},
 journal = {Journal of Machine Learning Research},
 number = {128},
 openalex = {W3136609190},
 pages = {1--69},
 title = {An Eigenmodel for Dynamic Multilayer Networks},
 url = {http://jmlr.org/papers/v24/21-0270.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0321,
 author = {Takashi Ikeuchi and Mayumi Ide and Yan Zeng and Takashi Nicholas Maeda and Shohei Shimizu},
 journal = {Journal of Machine Learning Research},
 number = {14},
 pages = {1--8},
 title = {Python package for causal discovery based on LiNGAM},
 url = {http://jmlr.org/papers/v24/21-0321.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0326,
 abstract = {Despite the remarkable performance and generalization levels of deep learning models in a wide range of artificial intelligence tasks, it has been demonstrated that these models can be easily fooled by the addition of imperceptible but malicious perturbations to natural inputs. These altered inputs are known in the literature as adversarial examples. In this paper we propose a novel probabilistic framework to generalize and extend adversarial attacks in order to produce a desired probability distribution for the classes when we apply the attack method to a large number of inputs. This novel attack strategy provides the attacker with greater control over the target model, and increases the complexity of detecting that the model is being attacked. We introduce three different strategies to efficiently generate such attacks, and illustrate our approach extending DeepFool, a state-of-the-art attack algorithm to generate adversarial examples. We also experimentally validate our approach for the spoken command classification task, an exemplary machine learning problem in the audio domain. Our results demonstrate that we can closely approximate any probability distribution for the classes while maintaining a high fooling rate and by injecting imperceptible perturbations to the inputs.},
 author = {Jon Vadillo and Roberto Santana and Jose A. Lozano},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W3016758115},
 pages = {1--42},
 title = {Extending Adversarial Attacks to Produce Adversarial Class Probability Distributions},
 url = {http://jmlr.org/papers/v24/21-0326.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0377,
 abstract = {This paper shows that dropout training in Generalized Linear Models is the minimax solution of a two-player, zero-sum game where an adversarial nature corrupts a statistician's covariates using a multiplicative nonparametric errors-in-variables model. In this game, nature's least favorable distribution is dropout noise, where nature independently deletes entries of the covariate vector with some fixed probability $δ$. This result implies that dropout training indeed provides out-of-sample expected loss guarantees for distributions that arise from multiplicative perturbations of in-sample data. In addition to the decision-theoretic analysis, the paper makes two more contributions. First, there is a concrete recommendation on how to select the tuning parameter $δ$ to guarantee that, as the sample size grows large, the in-sample loss after dropout training exceeds the true population loss with some pre-specified probability. Second, the paper provides a novel, parallelizable, Unbiased Multi-Level Monte Carlo algorithm to speed-up the implementation of dropout training. Our algorithm has a much smaller computational cost compared to the naive implementation of dropout, provided the number of data points is much smaller than the dimension of the covariate vector.},
 author = {José Blanchet and Yang Kang and José Luis Montiel Olea and Viet Anh Nguyen and Xuhui Zhang},
 journal = {Journal of Machine Learning Research},
 number = {180},
 openalex = {W3084926902},
 pages = {1--60},
 title = {Machine Learning's Dropout Training is Distributionally Robust Optimal},
 url = {http://jmlr.org/papers/v24/21-0377.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0389,
 abstract = {In Machine Learning, the $\mathsf{SHAP}$-score is a version of the Shapley value that is used to explain the result of a learned model on a specific entity by assigning a score to every feature. While in general computing Shapley values is an intractable problem, we prove a strong positive result stating that the $\mathsf{SHAP}$-score can be computed in polynomial time over deterministic and decomposable Boolean circuits. Such circuits are studied in the field of Knowledge Compilation and generalize a wide range of Boolean circuits and binary decision diagrams classes, including binary decision trees and Ordered Binary Decision Diagrams (OBDDs). We also establish the computational limits of the SHAP-score by observing that computing it over a class of Boolean models is always polynomially as hard as the model counting problem for that class. This implies that both determinism and decomposability are essential properties for the circuits that we consider. It also implies that computing $\mathsf{SHAP}$-scores is intractable as well over the class of propositional formulas in DNF. Based on this negative result, we look for the existence of fully-polynomial randomized approximation schemes (FPRAS) for computing $\mathsf{SHAP}$-scores over such class. In contrast to the model counting problem for DNF formulas, which admits an FPRAS, we prove that no such FPRAS exists for the computation of $\mathsf{SHAP}$-scores. Surprisingly, this negative result holds even for the class of monotone formulas in DNF. These techniques can be further extended to prove another strong negative result: Under widely believed complexity assumptions, there is no polynomial-time algorithm that checks, given a monotone DNF formula $\varphi$ and features $x,y$, whether the $\mathsf{SHAP}$-score of $x$ in $\varphi$ is smaller than the $\mathsf{SHAP}$-score of $y$ in $\varphi$.},
 author = {Marcelo Arenas and Pablo Barcelo and Leopoldo Bertossi and Mikael Monet},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W4361864601},
 pages = {1--58},
 title = {On the Complexity of SHAP-Score-Based Explanations: Tractability via Knowledge Compilation and Non-Approximability Results},
 url = {http://jmlr.org/papers/v24/21-0389.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0434,
 abstract = {Most of the successful deep neural network architectures are structured, often consisting of elements like convolutional neural networks and gated recurrent neural networks. Recently, graph neural networks have been successfully applied to graph structured data such as point cloud and molecular data. These networks often only consider pairwise dependencies, as they operate on a graph structure. We generalize the graph neural network into a factor graph neural network (FGNN) in order to capture higher order dependencies. We show that FGNN is able to represent Max-Product Belief Propagation, an approximate inference algorithm on probabilistic graphical models; hence it is able to do well when Max-Product does well. Promising results on both synthetic and real datasets demonstrate the effectiveness of the proposed model.},
 author = {Zhen Zhang and Mohammed Haroon Dupty and Fan Wu and Javen Qinfeng Shi and Wee Sun Lee},
 journal = {Journal of Machine Learning Research},
 number = {181},
 openalex = {W2946956305},
 pages = {1--54},
 title = {Factor Graph Neural Network},
 url = {http://jmlr.org/papers/v24/21-0434.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0438,
 abstract = {In this paper, we consider the estimation of a low Tucker rank tensor from a number of noisy linear measurements. The general problem covers many specific examples arising from applications, including tensor regression, tensor completion, and tensor PCA/SVD. We consider an efficient Riemannian Gauss-Newton (RGN) method for low Tucker rank tensor estimation. Different from the generic (super)linear convergence guarantee of RGN in the literature, we prove the first local quadratic convergence guarantee of RGN for low-rank tensor estimation in the noisy setting under some regularity conditions and provide the corresponding estimation error upper bounds. A deterministic estimation error lower bound, which matches the upper bound, is provided that demonstrates the statistical optimality of RGN. The merit of RGN is illustrated through two machine learning applications: tensor regression and tensor SVD. Finally, we provide the simulation results to corroborate our theoretical findings.},
 author = {Yuetian Luo and Anru R. Zhang},
 journal = {Journal of Machine Learning Research},
 number = {381},
 openalex = {W3157274685},
 pages = {1--48},
 title = {Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence},
 url = {http://jmlr.org/papers/v24/21-0438.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0445,
 abstract = {Despite impressive results, deep learning-based technologies also raise severe privacy and environmental concerns induced by the training procedure often conducted in data centers. In response, alternatives to centralized training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL is starting to be deployed at a global scale by companies that must adhere to new legal demands and policies originating from governments and social groups advocating for privacy protection. \textit{However, the potential environmental impact related to FL remains unclear and unexplored. This paper offers the first-ever systematic study of the carbon footprint of FL.} First, we propose a rigorous model to quantify the carbon footprint, hence facilitating the investigation of the relationship between FL design and carbon emissions. Then, we compare the carbon footprint of FL to traditional centralized learning. Our findings show that, depending on the configuration, FL can emit up to two order of magnitude more carbon than centralized machine learning. However, in certain settings, it can be comparable to centralized learning due to the reduced energy consumption of embedded devices. We performed extensive experiments across different types of datasets, settings and various deep learning models with FL. Finally, we highlight and connect the reported results to the future challenges and trends in FL to reduce its environmental impact, including algorithms efficiency, hardware capabilities, and stronger industry transparency.},
 author = {Xinchi Qiu and Titouan Parcollet and Javier Fernandez-Marques and Pedro P. B. Gusmao and Yan Gao and Daniel J. Beutel and Taner Topal and Akhil Mathur and Nicholas D. Lane},
 journal = {Journal of Machine Learning Research},
 number = {129},
 openalex = {W3093436911},
 pages = {1--23},
 title = {A first look into the carbon footprint of federated learning},
 url = {http://jmlr.org/papers/v24/21-0445.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0449,
 abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have mostly focused on solving problem instances in isolation, ignoring the fact that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks, as a key building block for combinatorial tasks, either directly as solvers or by enhancing the former. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at researchers in both optimization and machine learning.},
 author = {Quentin Cappart and Didier Chételat and Elias B. Khalil and Andrea Lodi and Christopher Morris and Petar Veličković},
 journal = {Journal of Machine Learning Research},
 number = {130},
 openalex = {W3188522200},
 pages = {1--61},
 title = {Combinatorial Optimization and Reasoning with Graph Neural Networks},
 url = {http://jmlr.org/papers/v24/21-0449.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0482,
 abstract = {Selecting a minimal feature set that is maximally informative about a target variable is a central task in machine learning and statistics. Information theory provides a powerful framework for formulating feature selection algorithms -- yet, a rigorous, information-theoretic definition of feature relevancy, which accounts for feature interactions such as redundant and synergistic contributions, is still missing. We argue that this lack is inherent to classical information theory which does not provide measures to decompose the information a set of variables provides about a target into unique, redundant, and synergistic contributions. Such a decomposition has been introduced only recently by the partial information decomposition (PID) framework. Using PID, we clarify why feature selection is a conceptually difficult problem when approached using information theory and provide a novel definition of feature relevancy and redundancy in PID terms. From this definition, we show that the conditional mutual information (CMI) maximizes relevancy while minimizing redundancy and propose an iterative, CMI-based algorithm for practical feature selection. We demonstrate the power of our CMI-based algorithm in comparison to the unconditional mutual information on benchmark examples and provide corresponding PID estimates to highlight how PID allows to quantify information contribution of features and their interactions in feature-selection problems.},
 author = {Patricia Wollstadt and Sebastian Schmitt and Michael Wibral},
 journal = {Journal of Machine Learning Research},
 number = {131},
 openalex = {W4287183407},
 pages = {1--44},
 title = {A Rigorous Information-Theoretic Definition of Redundancy and Relevancy in Feature Selection Based on (Partial) Information Decomposition},
 url = {http://jmlr.org/papers/v24/21-0482.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0488,
 abstract = {Download This Paper Open PDF in Browser Add Paper to My Library Share: Permalink Using these links will ensure access to this page indefinitely Copy URL Copy DOI},
 author = {Cynthia Rudin and Yaron Shaposhnik},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2969964925},
 pages = {1--44},
 title = {Globally-Consistent Rule-Based Summary-Explanations for Machine Learning Models: Application to Credit-Risk Evaluation},
 url = {http://jmlr.org/papers/v24/21-0488.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0505,
 author = {Berkay Anahtarci and Can Deha Kariksiz and Naci Saldi},
 journal = {Journal of Machine Learning Research},
 number = {17},
 pages = {1--59},
 title = {Learning Mean-Field Games with Discounted and Average Costs},
 url = {http://jmlr.org/papers/v24/21-0505.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0515,
 author = {Justin Grimmer and Dean Knox and Brandon Stewart},
 journal = {Journal of Machine Learning Research},
 number = {182},
 pages = {1--70},
 title = {Naive regression requires weaker assumptions than factor models to adjust for multiple cause confounding},
 url = {http://jmlr.org/papers/v24/21-0515.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0523,
 author = {Di Wang and Lijie Hu and Huanyu Zhang and Marco Gaboardi and Jinhui Xu},
 journal = {Journal of Machine Learning Research},
 number = {132},
 pages = {1--57},
 title = {Generalized Linear Models in Non-interactive Local Differential Privacy with Public Data},
 url = {http://jmlr.org/papers/v24/21-0523.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0543,
 abstract = {We establish exact asymptotic expressions for the normalized mutual information and minimum mean-square-error (MMSE) of sparse linear regression in the sub-linear sparsity regime. Our result is achieved by a generalization of the adaptive interpolation method in Bayesian inference for linear regimes to sub-linear ones. A modification of the well-known approximate message passing algorithm to approach the MMSE fundamental limit is also proposed, and its state evolution is rigorously analyzed. Our results show that the traditional linear assumption between the signal dimension and number of observations in the replica and adaptive interpolation methods is not necessary for sparse signals. They also show how to modify the existing well-known AMP algorithms for linear regimes to sub-linear ones.},
 author = {Lan V. Truong},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W4297782657},
 pages = {1--49},
 title = {Fundamental limits and algorithms for sparse linear regression with sublinear sparsity},
 url = {http://jmlr.org/papers/v24/21-0543.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0549,
 abstract = {Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) dataset by just modifying the learning rate schedule of a high performing model.},
 author = {Nikhil Iyer and V. Thejas and Nipun Kwatra and Ramachandran Ramjee and Muthian Sivathanu},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W4287828189},
 pages = {1--37},
 title = {Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule},
 url = {http://jmlr.org/papers/v24/21-0549.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0556,
 abstract = {We study posterior contraction rates for a class of deep Gaussian process priors applied to the nonparametric regression problem under a general composition assumption on the regression function. It is shown that the contraction rates can achieve the minimax convergence rate (up to $\log n$ factors), while being adaptive to the underlying structure and smoothness of the target function. The proposed framework extends the Bayesian nonparametric theory for Gaussian process priors.},
 author = {Gianluca Finocchio and Johannes Schmidt-Hieber},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W4287181175},
 pages = {1--49},
 title = {Posterior contraction for deep Gaussian process priors},
 url = {http://jmlr.org/papers/v24/21-0556.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0571,
 abstract = {In this paper, we introduce TITAN, a novel inerTIal block majorizaTion minimizAtioN framework for non-smooth non-convex optimization problems. To the best of our knowledge, TITAN is the first framework of block-coordinate update method that relies on the majorization-minimization framework while embedding inertial force to each step of the block updates. The inertial force is obtained via an extrapolation operator that subsumes heavy-ball and Nesterov-type accelerations for block proximal gradient methods as special cases. By choosing various surrogate functions, such as proximal, Lipschitz gradient, Bregman, quadratic, and composite surrogate functions, and by varying the extrapolation operator, TITAN produces a rich set of inertial block-coordinate update methods. We study sub-sequential convergence as well as global convergence for the generated sequence of TITAN. We illustrate the effectiveness of TITAN on two important machine learning problems, namely sparse non-negative matrix factorization and matrix completion.},
 author = {Le Thi Khanh Hien and Duy Nhat Phan and Nicolas Gillis},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W3094555922},
 pages = {1--41},
 title = {An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization.},
 url = {http://jmlr.org/papers/v24/21-0571.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0579,
 abstract = {Abstract While classic studies proved that wide networks allow universal approximation, recent research and successes of deep learning demonstrate the power of the network depth. Based on a symmetric consideration, we investigate if the design of artificial neural networks should have a directional preference, and what the mechanism of interaction is between the width and depth of a network. We address this fundamental question by establishing a quasi-equivalence between the width and depth of ReLU networks. Specifically, we formulate a transformation from an arbitrary ReLU network to a wide network and a deep network for either regression or classification so that an essentially same capability of the original network can be implemented. That is, a deep regression/classification ReLU network has a wide equivalent, and vice versa, subject to an arbitrarily small error. Interestingly, the quasi-equivalence between wide and deep classification ReLU networks is a data-driven version of the DeMorgan law.},
 author = {Fenglei Fan and Rongjie Lai and Ge Wang},
 journal = {Journal of Machine Learning Research},
 number = {183},
 openalex = {W3087545942},
 pages = {1--22},
 title = {Quasi-Equivalence of Width and Depth of Neural Networks},
 url = {http://jmlr.org/papers/v24/21-0579.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0599,
 abstract = {This paper characterizes the maximum mean discrepancies (MMD) that metrize the weak convergence of probability measures for a wide class of kernels. More precisely, we prove that, on a locally compact, non-compact, Hausdorff space, the MMD of a bounded continuous Borel measurable kernel k, whose reproducing kernel Hilbert space (RKHS) functions vanish at infinity, metrizes the weak convergence of probability measures if and only if k is continuous and integrally strictly positive definite (i.s.p.d.) over all signed, finite, regular Borel measures. We also correct a prior result of Simon-Gabriel &amp; Schölkopf (JMLR, 2018, Thm.12) by showing that there exist both bounded continuous i.s.p.d. kernels that do not metrize weak convergence and bounded continuous non-i.s.p.d. kernels that do metrize it.},
 author = {Carl-Johann Simon-Gabriel and Alessandro Barp and Bernhard Schölkopf and Lester Mackey},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W3035094823},
 pages = {1--20},
 title = {Metrizing Weak Convergence with Maximum Mean Discrepancies},
 url = {http://jmlr.org/papers/v24/21-0599.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0607,
 abstract = {The recent advent of powerful generative models has triggered the renewed development of quantitative measures to assess the proximity of two probability distributions. As the scalar Frechet inception distance remains popular, several methods have explored computing entire curves, which reveal the trade-off between the fidelity and variability of the first distribution with respect to the second one. Several of such variants have been proposed independently and while intuitively similar, their relationship has not yet been made explicit. In an effort to make the emerging picture of generative evaluation more clear, we propose a unification of four curves known respectively as: the precision-recall (PR) curve, the Lorenz curve, the receiver operating characteristic (ROC) curve and a special case of R\'enyi divergence frontiers. In addition, we discuss possible links between PR / Lorenz curves with the derivation of domain adaptation bounds.},
 author = {Rodrigue Siry and Ryan Webster and Loic Simon and Julien Rabin},
 journal = {Journal of Machine Learning Research},
 number = {185},
 openalex = {W4306294705},
 pages = {1--34},
 title = {On the Theoretical Equivalence of Several Trade-Off Curves Assessing Statistical Proximity},
 url = {http://jmlr.org/papers/v24/21-0607.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0623,
 abstract = {The behavior of many Bayesian models used in machine learning critically depends on the choice of prior distributions, controlled by some hyperparameters that are typically selected by Bayesian optimization or cross-validation. This requires repeated, costly, posterior inference. We provide an alternative for selecting good priors without carrying out posterior inference, building on the prior predictive distribution that marginalizes out the model parameters. We estimate virtual statistics for data generated by the prior predictive distribution and then optimize over the hyperparameters to learn ones for which these virtual statistics match target values provided by the user or estimated from (subset of) the observed data. We apply the principle for probabilistic matrix factorization, for which good solutions for prior selection have been missing. We show that for Poisson factorization models we can analytically determine the hyperparameters, including the number of factors, that best replicate the target statistics, and we study empirically the sensitivity of the approach for model mismatch. We also present a model-independent procedure that determines the hyperparameters for general models by stochastic optimization, and demonstrate this extension in context of hierarchical matrix factorization models.},
 author = {Eliezer de Souza da Silva and Tomasz Kuśmierczyk and Marcelo Hartmann and Arto Klami},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W4288088760},
 pages = {1--51},
 title = {Prior Specification for Bayesian Matrix Factorization via Prior Predictive Matching},
 url = {http://jmlr.org/papers/v24/21-0623.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0670,
 author = {Benjamin Jakubowski and Sriram Somanchi and Edward McFowland III and Daniel B. Neill},
 journal = {Journal of Machine Learning Research},
 number = {133},
 pages = {1--57},
 title = {Exploiting Discovered Regression Discontinuities to Debias Conditioned-on-observable Estimators},
 url = {http://jmlr.org/papers/v24/21-0670.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0673,
 abstract = {Nonparametric regression imputation is commonly used in missing data analysis. However, it suffers from the ``curse of dimension". The problem can be alleviated by the explosive sample size in the era of big data, while the large-scale data size presents some challenges on the storage of data and the calculation of estimators. These challenges make the classical nonparametric regression imputation methods no longer applicable. This motivates us to develop two distributed nonparametric regression imputation methods. One is based on kernel smoothing and the other on the sieve method. The kernel-based distributed imputation method has extremely low communication cost and the sieve-based distributed imputation method can accommodate more local machines. To illustrate the proposed imputation methods, response mean estimation is considered. Two distributed nonparametric regression imputation estimators are proposed for the response mean, which are proved to be asymptotically normal with asymptotic variances achieving the semiparametric efficiency bound. The proposed methods are evaluated through simulation studies and are illustrated by a real data analysis.},
 author = {Ruoyu Wang and Miaomiao Su and Qihua Wang},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W3167959622},
 pages = {1--52},
 title = {Distributed nonparametric regression imputation for missing response problems with large-scale data},
 url = {http://jmlr.org/papers/v24/21-0673.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0697,
 abstract = {Based on the Riemannian manifold model, we study the asymptotic behavior of a widely applied unsupervised learning algorithm, locally linear embedding (LLE), when the point cloud is sampled from a compact, smooth manifold with boundary. We show several peculiar behaviors of LLE near the boundary that are different from those diffusion-based algorithms. Particularly, LLE converges to a mixed-type differential operator with degeneracy. This study leads to an alternative boundary detection algorithm and two potential approaches to recover the Dirichlet Laplace-Beltrami operator.},
 author = {Hau-Tieng Wu and Nan Wu},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2900233741},
 pages = {1--80},
 title = {When Locally Linear Embedding Hits Boundary},
 url = {http://jmlr.org/papers/v24/21-0697.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0699,
 abstract = {Estimation of the precision matrix (or inverse covariance matrix) is of great importance in statistical data analysis and machine learning. However, as the number of parameters scales quadratically with the dimension $p$, computation becomes very challenging when $p$ is large. In this paper, we propose an adaptive sieving reduction algorithm to generate a solution path for the estimation of precision matrices under the $\ell_1$ penalized D-trace loss, with each subproblem being solved by a second-order algorithm. In each iteration of our algorithm, we are able to greatly reduce the number of variables in the {problem} based on the Karush-Kuhn-Tucker (KKT) conditions and the sparse structure of the estimated precision matrix in the previous iteration. As a result, our algorithm is capable of handling datasets with very high dimensions that may go beyond the capacity of the existing methods. Moreover, for the sub-problem in each iteration, other than solving the primal problem directly, we develop a semismooth Newton augmented Lagrangian algorithm with global linear convergence rate on the dual problem to improve the efficiency. Theoretical properties of our proposed algorithm have been established. In particular, we show that the convergence rate of our algorithm is asymptotically superlinear. The high efficiency and promising performance of our algorithm are illustrated via extensive simulation studies and real data applications, with comparison to several state-of-the-art solvers.},
 author = {Qian Li and Binyan Jiang and Defeng Sun},
 journal = {Journal of Machine Learning Research},
 number = {134},
 openalex = {W3173530920},
 pages = {1--44},
 title = {MARS: A second-order reduction algorithm for high-dimensional sparse precision matrices estimation},
 url = {http://jmlr.org/papers/v24/21-0699.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0741,
 author = {Diego Delle Donne and Matthieu Kowalski and Leo Liberti},
 journal = {Journal of Machine Learning Research},
 number = {382},
 pages = {1--28},
 title = {A Novel Integer Linear Programming Approach for Global L0 Minimization},
 url = {http://jmlr.org/papers/v24/21-0741.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0742,
 author = {Jun Shu and Deyu Meng and Zongben Xu},
 journal = {Journal of Machine Learning Research},
 number = {186},
 pages = {1--74},
 title = {Learning an Explicit Hyper-parameter Prediction Function Conditioned on Tasks},
 url = {http://jmlr.org/papers/v24/21-0742.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0745,
 abstract = {Generalized correlation analysis (GCA) is concerned with uncovering linear relationships across multiple datasets. It generalizes canonical correlation analysis that is designed for two datasets. We study sparse GCA when there are potentially multiple generalized correlation tuples in data and the loading matrix has a small number of nonzero rows. It includes sparse CCA and sparse PCA of correlation matrices as special cases. We first formulate sparse GCA as generalized eigenvalue problems at both population and sample levels via a careful choice of normalization constraints. Based on a Lagrangian form of the sample optimization problem, we propose a thresholded gradient descent algorithm for estimating GCA loading vectors and matrices in high dimensions. We derive tight estimation error bounds for estimators generated by the algorithm with proper initialization. We also demonstrate the prowess of the algorithm on a number of synthetic datasets.},
 author = {Sheng Gao and Zongming Ma},
 journal = {Journal of Machine Learning Research},
 number = {135},
 openalex = {W3175424480},
 pages = {1--61},
 title = {Sparse GCA and Thresholded Gradient Descent},
 url = {http://jmlr.org/papers/v24/21-0745.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0751,
 abstract = {Receiver Operating Characteristic (ROC) curves are plots of true positive rate versus false positive rate which are useful for evaluating binary classification models, but difficult to use for learning since the Area Under the Curve (AUC) is non-convex. ROC curves can also be used in other problems that have false positive and true positive rates such as changepoint detection. We show that in this more general context, the ROC curve can have loops, points with highly sub-optimal error rates, and AUC greater than one. This observation motivates a new optimization objective: rather than maximizing the AUC, we would like a monotonic ROC curve with AUC=1 that avoids points with large values for Min(FP,FN). We propose a convex relaxation of this objective that results in a new surrogate loss function called the AUM, short for Area Under Min(FP, FN). Whereas previous loss functions are based on summing over all labeled examples or pairs, the AUM requires a sort and a sum over the sequence of points on the ROC curve. We show that AUM directional derivatives can be efficiently computed and used in a gradient descent learning algorithm. In our empirical study of supervised binary classification and changepoint detection problems, we show that our new AUM minimization learning algorithm results in improved AUC and comparable speed relative to previous baselines.},
 author = {Jonathan Hillman and Toby Dylan Hocking},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W3179518614},
 pages = {1--24},
 title = {Optimizing ROC Curves with a Sort-Based Surrogate Loss Function for Binary Classification and Changepoint Detection},
 url = {http://jmlr.org/papers/v24/21-0751.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0781,
 abstract = {Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. We present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than $5\%$ when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation.},
 author = {Simon Bartels and Wouter Boomsma and Jes Frellsen and Damien Garreau},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W3184888022},
 pages = {1--57},
 title = {Kernel-Matrix Determinant Estimates from stopped Cholesky Decomposition},
 url = {http://jmlr.org/papers/v24/21-0781.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0782,
 abstract = {We present SimultaneousGreedys, a deterministic algorithm for constrained submodular maximization. At a high level, the algorithm maintains $\ell$ solutions and greedily updates them in a simultaneous fashion. SimultaneousGreedys achieves the tightest known approximation guarantees for both $k$-extendible systems and the more general $k$-systems, which are $(k+1)^2/k = k + \mathcal{O}(1)$ and $(1 + \sqrt{k+2})^2 = k + \mathcal{O}(\sqrt{k})$, respectively. This is in contrast to previous algorithms, which are designed to provide tight approximation guarantees in one setting, but not both. We also improve the analysis of RepeatedGreedy, showing that it achieves an approximation ratio of $k + \mathcal{O}(\sqrt{k})$ for $k$-systems when allowed to run for $\mathcal{O}(\sqrt{k})$ iterations, an improvement in both the runtime and approximation over previous analyses. We demonstrate that both algorithms may be modified to run in nearly linear time with an arbitrarily small loss in the approximation. Both SimultaneousGreedys and RepeatedGreedy are flexible enough to incorporate the intersection of $m$ additional knapsack constraints, while retaining similar approximation guarantees: both algorithms yield an approximation guarantee of roughly $k + 2m + \mathcal{O}(\sqrt{k+m})$ for $k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee of $k+2m + \mathcal{O}(\sqrt{m})$ for $k$-extendible systems. To complement our algorithmic contributions, we provide a hardness result which states that no algorithm making polynomially many oracle queries can achieve an approximation better than $k + 1/2 + \varepsilon$. We also present SubmodularGreedy.jl, a Julia package which implements these algorithms and may be downloaded at https://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the effectiveness of these algorithms on real datasets.},
 author = {Moran Feldman and Christopher Harshaw and Amin Karbasi},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W3181027293},
 pages = {1--87},
 title = {How Do You Want Your Greedy: Simultaneous or Repeated?},
 url = {http://jmlr.org/papers/v24/21-0782.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0796,
 abstract = {Regularized regression models are well studied and, under appropriate conditions, offer fast and statistically interpretable results. However, large data in many applications are heterogeneous in the sense of harboring distributional differences between latent groups. Then, the assumption that the conditional distribution of response Y given features X is the same for all samples may not hold. Furthermore, in scientific applications, the covariance structure of the features may contain important signals and its learning is also affected by latent group structure. We propose a class of mixture models for paired data (X, Y) that couples together the distribution of X (using sparse graphical models) and the conditional Y | X (using sparse regression models). The regression and graphical models are specific to the latent groups and model parameters are estimated jointly (hence the name "regularized joint mixtures"). This allows signals in either or both of the feature distribution and regression model to inform learning of latent structure and provides automatic control of confounding by such structure. Estimation is handled via an expectation-maximization algorithm, whose convergence is established theoretically. We illustrate the key ideas via empirical examples. An R package is available at https://github.com/k-perrakis/regjmix.},
 author = {Konstantinos Perrakis and Thomas Lartigue and Frank Dondelinger and Sach Mukherjee},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W4319988762},
 pages = {1--47},
 title = {Regularized joint mixture models},
 url = {http://jmlr.org/papers/v24/21-0796.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0818,
 abstract = {We consider a contextual online learning (multi-armed bandit) problem with high-dimensional covariate $\mathbf{x}$ and decision $\mathbf{y}$. The reward function to learn, $f(\mathbf{x},\mathbf{y})$, does not have a particular parametric form. The literature has shown that the optimal regret is $\tilde{O}(T^{(d_x+d_y+1)/(d_x+d_y+2)})$, where $d_x$ and $d_y$ are the dimensions of $\mathbf x$ and $\mathbf y$, and thus it suffers from the curse of dimensionality. In many applications, only a small subset of variables in the covariate affect the value of $f$, which is referred to as \textit{sparsity} in statistics. To take advantage of the sparsity structure of the covariate, we propose a variable selection algorithm called \textit{BV-LASSO}, which incorporates novel ideas such as binning and voting to apply LASSO to nonparametric settings. Our algorithm achieves the regret $\tilde{O}(T^{(d_x^*+d_y+1)/(d_x^*+d_y+2)})$, where $d_x^*$ is the effective covariate dimension. The regret matches the optimal regret when the covariate is $d^*_x$-dimensional and thus cannot be improved. Our algorithm may serve as a general recipe to achieve dimension reduction via variable selection in nonparametric settings.},
 author = {Wenhao Li and Ningyuan Chen and L. Jeff Hong},
 journal = {Journal of Machine Learning Research},
 number = {136},
 openalex = {W3086530088},
 pages = {1--84},
 title = {Dimension Reduction in Contextual Online Learning via Nonparametric Variable Selection},
 url = {http://jmlr.org/papers/v24/21-0818.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-082,
 abstract = {How might one test the hypothesis that networks were sampled from the same distribution? Here, we compare two statistical tests that use subgraph counts to address this question. The first uses the empirical subgraph densities themselves as estimates of those of the underlying distribution. The second test uses a new approach that converts these subgraph densities into estimates of the \textit{graph cumulants} of the distribution (without any increase in computational complexity). We demonstrate -- via theory, simulation, and application to real data -- the superior statistical power of using graph cumulants. In summary, when analyzing data using subgraph/motif densities, we suggest using the corresponding graph cumulants instead.},
 author = {Gecia Bravo-Hermsdorff and Lee M. Gunderson and Pierre-André Maugis and Carey E. Priebe},
 journal = {Journal of Machine Learning Research},
 number = {187},
 openalex = {W4384919975},
 pages = {1--27},
 title = {Quantifying Network Similarity using Graph Cumulants},
 url = {http://jmlr.org/papers/v24/21-082.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0832,
 abstract = {We investigate gradient descent training of wide neural networks and the corresponding implicit bias in function space. For univariate regression, we show that the solution of training a width-$n$ shallow ReLU network is within $n^{- 1/2}$ of the function which fits the training data and whose difference from the initial function has the smallest 2-norm of the second derivative weighted by a curvature penalty that depends on the probability distribution that is used to initialize the network parameters. We compute the curvature penalty function explicitly for various common initialization procedures. For instance, asymmetric initialization with a uniform distribution yields a constant curvature penalty, and thence the solution function is the natural cubic spline interpolation of the training data. \hj{For stochastic gradient descent we obtain the same implicit bias result.} We obtain a similar result for different activation functions. For multivariate regression we show an analogous result, whereby the second derivative is replaced by the Radon transform of a fractional Laplacian. For initialization schemes that yield a constant penalty function, the solutions are polyharmonic splines. Moreover, we show that the training trajectories are captured by trajectories of smoothing splines with decreasing regularization strength.},
 author = {Hui Jin and Guido Montufar},
 journal = {Journal of Machine Learning Research},
 number = {137},
 openalex = {W4313597259},
 pages = {1--97},
 title = {Implicit Bias of Gradient Descent for Mean Squared Error Regression with Two-Layer Wide Neural Networks},
 url = {http://jmlr.org/papers/v24/21-0832.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0841,
 abstract = {Network data are ubiquitous in modern machine learning, with tasks of interest including node classification, node clustering and link prediction. A frequent approach begins by learning an Euclidean embedding of the network, to which algorithms developed for vector-valued data are applied. For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen. Despite the strong empirical performance of such methods, they are not well understood theoretically. Our work encapsulates representation methods using a subsampling approach, such as node2vec, into a single unifying framework. We prove, under the assumption that the graph is exchangeable, that the distribution of the learned embedding vectors asymptotically decouples. Moreover, we characterize the asymptotic distribution and provided rates of convergence, in terms of the latent parameters, which includes the choice of loss function and the embedding dimension. This provides a theoretical foundation to understand what the embedding vectors represent and how well these methods perform on downstream tasks. Notably, we observe that typically used loss functions may lead to shortcomings, such as a lack of Fisher consistency.},
 author = {Andrew Davison and Morgane Austern},
 journal = {Journal of Machine Learning Research},
 number = {138},
 openalex = {W3181738625},
 pages = {1--120},
 title = {Asymptotics of Network Embeddings Learned via Subsampling},
 url = {http://jmlr.org/papers/v24/21-0841.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0842,
 abstract = {We consider a general-sum N-player linear-quadratic game with stochastic dynamics over a finite horizon and prove the global convergence of the natural policy gradient method to the Nash equilibrium. In order to prove the convergence of the method, we require a certain amount of noise in the system. We give a condition, essentially a lower bound on the covariance of the noise in terms of the model parameters, in order to guarantee convergence. We illustrate our results with numerical experiments to show that even in situations where the policy gradient method may not converge in the deterministic setting, the addition of noise leads to convergence.},
 author = {Ben Hambly and Renyuan Xu and Huining Yang},
 journal = {Journal of Machine Learning Research},
 number = {139},
 openalex = {W3186129804},
 pages = {1--56},
 title = {Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games},
 url = {http://jmlr.org/papers/v24/21-0842.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0843,
 abstract = {An individualized decision rule (IDR) is a decision function that assigns each individual a given treatment based on his/her observed characteristics. Most of the existing works in the literature consider settings with binary or finitely many treatment options. In this paper, we focus on the continuous treatment setting and propose a jump interval-learning to develop an individualized interval-valued decision rule (I2DR) that maximizes the expected outcome. Unlike IDRs that recommend a single treatment, the proposed I2DR yields an interval of treatment options for each individual, making it more flexible to implement in practice. To derive an optimal I2DR, our jump interval-learning method estimates the conditional mean of the outcome given the treatment and the covariates via jump penalized regression, and derives the corresponding optimal I2DR based on the estimated outcome regression function. The regressor is allowed to be either linear for clear interpretation or deep neural network to model complex treatment-covariates interactions. To implement jump interval-learning, we develop a searching algorithm based on dynamic programming that efficiently computes the outcome regression function. Statistical properties of the resulting I2DR are established when the outcome regression function is either a piecewise or continuous function over the treatment space. We further develop a procedure to infer the mean outcome under the (estimated) optimal policy. Extensive simulations and a real data application to a warfarin study are conducted to demonstrate the empirical validity of the proposed I2DR.},
 author = {Hengrui Cai and Chengchun Shi and Rui Song and Wenbin Lu},
 journal = {Journal of Machine Learning Research},
 number = {140},
 openalex = {W3213359025},
 pages = {1--92},
 title = {Jump Interval-Learning for Individualized Decision Making},
 url = {http://jmlr.org/papers/v24/21-0843.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0844,
 abstract = {This paper provides elementary analyses of the regret and generalization of minimum-norm interpolating classifiers (MNIC). The MNIC is the function of smallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a label pattern on a finite data set. We derive a mistake bound for MNIC and a regularized variant that holds for all data sets. This bound follows from elementary properties of matrix inverses. Under the assumption that the data is independently and identically distributed, the mistake bound implies that MNIC generalizes at a rate proportional to the norm of the interpolating solution and inversely proportional to the number of data points. This rate matches similar rates derived for margin classifiers and perceptrons. We derive several plausible generative models where the norm of the interpolating classifier is bounded or grows at a rate sublinear in $n$. We also show that as long as the population class conditional distributions are sufficiently separable in total variation, then MNIC generalizes with a fast rate.},
 author = {Tengyuan Liang and Benjamin Recht},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W3121962434},
 pages = {1--27},
 title = {Interpolating Classifiers Make Few Mistakes},
 url = {http://jmlr.org/papers/v24/21-0844.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0855,
 abstract = {Statistical inference of directed relations given some unspecified interventions (i.e., the intervention targets are unknown) is challenging. In this article, we test hypothesized directed relations with unspecified interventions. First, we derive conditions to yield an identifiable model. Unlike classical inference, testing directed relations requires to identify the ancestors and relevant interventions of hypothesis-specific primary variables. To this end, we propose a peeling algorithm based on nodewise regressions to establish a topological order of primary variables. Moreover, we prove that the peeling algorithm yields a consistent estimator in low-order polynomial time. Second, we propose a likelihood ratio test integrated with a data perturbation scheme to account for the uncertainty of identifying ancestors and interventions. Also, we show that the distribution of a data perturbation test statistic converges to the target distribution. Numerical examples demonstrate the utility and effectiveness of the proposed methods, including an application to infer gene regulatory networks. The R implementation is available at https://github.com/chunlinli/intdag.},
 author = {Chunlin Li and Xiaotong Shen and Wei Pan},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W4295123281},
 pages = {1--48},
 title = {Inference for a Large Directed Acyclic Graph with Unspecified Interventions},
 url = {http://jmlr.org/papers/v24/21-0855.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0870,
 abstract = {Differential privacy (DP) offers strong theoretical privacy guarantees, but implementations of DP mechanisms may be vulnerable to side-channel attacks, such as timing attacks. When sampling methods such as MCMC or rejection sampling are used to implement a mechanism, the runtime can leak private information. We characterize the additional privacy cost due to the runtime of a rejection sampler in terms of both $(\epsilon,\delta)$-DP as well as $f$-DP. We also show that unless the acceptance probability is constant across databases, the runtime of a rejection sampler does not satisfy $\epsilon$-DP for any $\epsilon$. We show that there is a similar breakdown in privacy with adaptive rejection samplers. We propose three modifications to the rejection sampling algorithm, with varying assumptions, to protect against timing attacks by making the runtime independent of the data. The modification with the weakest assumptions is an approximate sampler, introducing a small increase in the privacy cost, whereas the other modifications give perfect samplers. We also use our techniques to develop an adaptive rejection sampler for log-H\"{o}lder densities, which also has data-independent runtime. We give several examples of DP mechanisms that fit the assumptions of our methods and can thus be implemented using our samplers.},
 author = {Jordan Awan and Vinayak Rao},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W3217507660},
 pages = {1--32},
 title = {Privacy-Aware Rejection Sampling},
 url = {http://jmlr.org/papers/v24/21-0870.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0877,
 abstract = {Multi-kernel learning (MKL) has been widely used in function approximation tasks. The key problem of MKL is to combine kernels in a prescribed dictionary. Inclusion of irrelevant kernels in the dictionary can deteriorate accuracy of MKL, and increase the computational complexity. To improve the accuracy of function approximation and reduce the computational complexity, the present paper studies data-driven selection of kernels from the dictionary that provide satisfactory function approximations. Specifically, based on the similarities among kernels, the novel framework constructs and refines a graph to assist choosing a subset of kernels. In addition, random feature approximation is utilized to enable online implementation for sequentially obtained data. Theoretical analysis shows that our proposed algorithms enjoy tighter sub-linear regret bound compared with state-of-art graph-based online MKL alternatives. Experiments on a number of real datasets also showcase the advantages of our novel graph-aided framework.},
 author = {Pouya M. Ghari and Yanning Shen},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W3127959413},
 pages = {1--44},
 title = {Graph-Aided Online Multi-Kernel Learning},
 url = {http://jmlr.org/papers/v24/21-0877.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0890,
 author = {Lanjue Chen and Alan T.K. Wan and Shuyi Zhang and Yong Zhou},
 journal = {Journal of Machine Learning Research},
 number = {263},
 pages = {1--43},
 title = {Distributed Algorithms for U-statistics-based Empirical Risk Minimization},
 url = {http://jmlr.org/papers/v24/21-0890.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0899,
 abstract = {We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-of-the-art black-box models. In addition, after prototype pruning, the resulting ProtoryNet models only need less than or around 20 prototypes for all datasets, which significantly benefits interpretability. Furthermore, we report a survey result indicating that human users find ProtoryNet more intuitive and easier to understand than other prototype-based methods.},
 author = {Dat Hong and Tong Wang and Stephen Baek},
 journal = {Journal of Machine Learning Research},
 number = {264},
 openalex = {W3038979745},
 pages = {1--39},
 title = {ProtoryNet - Interpretable Text Classification Via Prototype Trajectories},
 url = {http://jmlr.org/papers/v24/21-0899.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0949,
 abstract = {Bilevel optimization has recently attracted growing interests due to its wide applications in modern machine learning problems. Although recent studies have characterized the convergence rate for several such popular algorithms, it is still unclear how much further these convergence rates can be improved. In this paper, we address this fundamental question from two perspectives. First, we provide the first-known lower complexity bounds of $\widetilde{\Omega}(\frac{1}{\sqrt{\mu_x}\mu_y})$ and $\widetilde \Omega\big(\frac{1}{\sqrt{\epsilon}}\min\{\frac{1}{\mu_y},\frac{1}{\sqrt{\epsilon^{3}}}\}\big)$ respectively for strongly-convex-strongly-convex and convex-strongly-convex bilevel optimizations. Second, we propose an accelerated bilevel optimizer named AccBiO, for which we provide the first-known complexity bounds without the gradient boundedness assumption (which was made in existing analyses) under the two aforementioned geometries. We also provide significantly tighter upper bounds than the existing complexity when the bounded gradient assumption does hold. We show that AccBiO achieves the optimal results (i.e., the upper and lower bounds match up to logarithmic factors) when the inner-level problem takes a quadratic form with a constant-level condition number. Interestingly, our lower bounds under both geometries are larger than the corresponding optimal complexities of minimax optimization, establishing that bilevel optimization is provably more challenging than minimax optimization.},
 author = {Kaiyi ji and Yingbin Liang},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W3126652544},
 pages = {1--56},
 title = {Lower Bounds and Accelerated Algorithms for Bilevel Optimization},
 url = {http://jmlr.org/papers/v24/21-0949.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0950,
 abstract = {Unobserved confounding is a fundamental obstacle to establishing valid causal conclusions from observational data. Two complementary types of approaches have been developed to address this obstacle: obtaining identification using fortuitous external aids, such as instrumental variables or proxies, or by means of the ID algorithm, using Markov restrictions on the full data distribution encoded in graphical causal models. In this paper we aim to develop a synthesis of the former and latter approaches to identification in causal inference to yield the most general identification algorithm in multivariate systems currently known -- the proximal ID algorithm. In addition to being able to obtain nonparametric identification in all cases where the ID algorithm succeeds, our approach allows us to systematically exploit proxies to adjust for the presence of unobserved confounders that would have otherwise prevented identification. In addition, we outline a class of estimation strategies for causal parameters identified by our method in an important special case. We illustrate our approach by simulation studies and a data application.},
 author = {Ilya Shpitser and Zach Wood-Doughty and Eric J. Tchetgen Tchetgen},
 journal = {Journal of Machine Learning Research},
 number = {188},
 openalex = {W4293436482},
 pages = {1--46},
 title = {The Proximal ID Algorithm},
 url = {http://jmlr.org/papers/v24/21-0950.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0987,
 abstract = {This article investigates the use of random feature neural networks for learning Kolmogorov partial (integro-)differential equations associated to Black-Scholes and more general exponential L\'evy models. Random feature neural networks are single-hidden-layer feedforward neural networks in which only the output weights are trainable. This makes training particularly simple, but (a priori) reduces expressivity. Interestingly, this is not the case for Black-Scholes type PDEs, as we show here. We derive bounds for the prediction error of random neural networks for learning sufficiently non-degenerate Black-Scholes type models. A full error analysis is provided and it is shown that the derived bounds do not suffer from the curse of dimensionality. We also investigate an application of these results to basket options and validate the bounds numerically. These results prove that neural networks are able to \textit{learn} solutions to Black-Scholes type PDEs without the curse of dimensionality. In addition, this provides an example of a relevant learning problem in which random feature neural networks are provably efficient.},
 author = {Lukas Gonon},
 journal = {Journal of Machine Learning Research},
 number = {189},
 openalex = {W4322614761},
 pages = {1--51},
 title = {Random feature neural networks learn Black-Scholes type PDEs without curse of dimensionality},
 url = {http://jmlr.org/papers/v24/21-0987.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1044,
 abstract = {Fil: Borghini, Eugenio. Consejo Nacional de Investigaciones Cientificas y Tecnicas. Oficina de Coordinacion Administrativa Ciudad Universitaria. Instituto de Investigaciones Matematicas Luis A. Santalo. Universidad de Buenos Aires. Facultad de Ciencias Exactas y Naturales. Instituto de Investigaciones Matematicas Luis A. Santalo; Argentina},
 author = {Ximena Fernández and Eugenio Borghini and Gabriel Mindlin and Pablo Groisman},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W3112818286},
 pages = {1--42},
 title = {Intrinsic persistent homology via density-based metric learning},
 url = {http://jmlr.org/papers/v24/21-1044.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1046,
 author = {Di Bo and Hoon Hwangbo and Vinit Sharma and Corey Arndt and Stephanie TerMaath},
 journal = {Journal of Machine Learning Research},
 number = {76},
 pages = {1--31},
 title = {A Randomized Subspace-based Approach for Dimensionality Reduction and Important Variable Selection},
 url = {http://jmlr.org/papers/v24/21-1046.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1049,
 author = {Jian Li and Yong Liu and Weiping Wang},
 journal = {Journal of Machine Learning Research},
 number = {141},
 pages = {1--39},
 title = {Optimal Convergence Rates for Distributed Nystroem Approximation},
 url = {http://jmlr.org/papers/v24/21-1049.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1067,
 author = {Eli N. Weinstein and Jeffrey W. Miller},
 journal = {Journal of Machine Learning Research},
 number = {23},
 pages = {1--72},
 title = {Bayesian Data Selection},
 url = {http://jmlr.org/papers/v24/21-1067.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1075,
 abstract = {Risk modeling with electronic health records (EHR) data is challenging due to no direct observations of the disease outcome and the high-dimensional predictors. In this paper, we develop a surrogate assisted semi-supervised learning approach, leveraging small labeled data with annotated outcomes and extensive unlabeled data of outcome surrogates and high-dimensional predictors. We propose to impute the unobserved outcomes by constructing a sparse imputation model with outcome surrogates and high-dimensional predictors. We further conduct a one-step bias correction to enable interval estimation for the risk prediction. Our inference procedure is valid even if both the imputation and risk prediction models are misspecified. Our novel way of ultilizing unlabelled data enables the high-dimensional statistical inference for the challenging setting with a dense risk prediction model. We present an extensive simulation study to demonstrate the superiority of our approach compared to existing supervised methods. We apply the method to genetic risk prediction of type-2 diabetes mellitus using an EHR biobank cohort.},
 author = {Jue Hou and Zijian Guo and Tianxi Cai},
 journal = {Journal of Machine Learning Research},
 number = {265},
 openalex = {W3157807213},
 pages = {1--58},
 title = {Surrogate Assisted Semi-supervised Inference for High Dimensional Risk Prediction},
 url = {http://jmlr.org/papers/v24/21-1075.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1095,
 abstract = {Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -- which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust optimization (DRO). We develop batch and stochastic first-order optimization methods for solving TERM, provide convergence guarantees for the solvers, and show that the framework can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications in machine learning, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. Despite the straightforward modification TERM makes to traditional ERM objectives, we find that the framework can consistently outperform ERM and deliver competitive performance with state-of-the-art, problem-specific approaches.},
 author = {Tian Li and Ahmad Beirami and Maziar Sanjabi and Virginia Smith},
 journal = {Journal of Machine Learning Research},
 number = {142},
 openalex = {W3199071121},
 pages = {1--79},
 title = {On Tilted Losses in Machine Learning: Theory and Applications},
 url = {http://jmlr.org/papers/v24/21-1095.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1099,
 abstract = {We investigate statistical properties of a likelihood approach to nonparametric estimation of a singular distribution using deep generative models. More specifically, a deep generative model is used to model high-dimensional data that are assumed to concentrate around some low-dimensional structure. Estimating the distribution supported on this low-dimensional structure, such as a low-dimensional manifold, is challenging due to its singularity with respect to the Lebesgue measure in the ambient space. In the considered model, a usual likelihood approach can fail to estimate the target distribution consistently due to the singularity. We prove that a novel and effective solution exists by perturbing the data with an instance noise, which leads to consistent estimation of the underlying distribution with desirable convergence rates. We also characterize the class of distributions that can be efficiently estimated via deep generative models. This class is sufficiently general to contain various structured distributions such as product distributions, classically smooth distributions and distributions supported on a low-dimensional manifold. Our analysis provides some insights on how deep generative models can avoid the curse of dimensionality for nonparametric distribution estimation. We conduct a thorough simulation study and real data analysis to empirically demonstrate that the proposed data perturbation technique improves the estimation performance significantly.},
 author = {Minwoo Chae and Dongha Kim and Yongdai Kim and Lizhen Lin},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W3161841851},
 pages = {1--42},
 title = {A likelihood approach to nonparametric estimation of a singular distribution using deep generative models},
 url = {http://jmlr.org/papers/v24/21-1099.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1110,
 abstract = {We study the problem of estimating the derivatives of a regression function, which has a wide range of applications as a key nonparametric functional of unknown functions. Standard analysis may be tailored to specific derivative orders, and parameter tuning remains a daunting challenge particularly for high-order derivatives. In this article, we propose a simple plug-in kernel ridge regression (KRR) estimator in nonparametric regression with random design that is broadly applicable for multi-dimensional support and arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to study the behavior of the proposed estimator in a unified manner that encompasses the regression function and its derivatives, leading to two error bounds for a general class of kernels under the strong $L_\infty$ norm. In a concrete example specialized to kernels with polynomially decaying eigenvalues, the proposed estimator recovers the minimax optimal rate up to a logarithmic factor for estimating derivatives of functions in H\"older and Sobolev classes. Interestingly, the proposed estimator achieves the optimal rate of convergence with the same choice of tuning parameter for any order of derivatives. Hence, the proposed estimator enjoys a \textit{plug-in property} for derivatives in that it automatically adapts to the order of derivatives to be estimated, enabling easy tuning in practice. Our simulation studies show favorable finite sample performance of the proposed method relative to several existing methods and corroborate the theoretical findings on its minimax optimality.},
 author = {Zejian Liu and Meng Li},
 journal = {Journal of Machine Learning Research},
 number = {266},
 openalex = {W3170574689},
 pages = {1--37},
 title = {On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression Estimators},
 url = {http://jmlr.org/papers/v24/21-1110.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1130,
 abstract = {We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in minutes, our relaxation scales to instances where $n=200$ in hours, and our branch-and-bound algorithm scales to instances where $n=25$ in minutes. Our numerical results demonstrate that our approach outperforms existing state-of-the-art approaches in terms of rank, sparsity, and mean-square error while maintaining a comparable runtime.},
 author = {Dimitris Bertsimas and Ryan Cory-Wright and Nicholas A. G. Johnson},
 journal = {Journal of Machine Learning Research},
 number = {267},
 openalex = {W4366547150},
 pages = {1--51},
 title = {Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach},
 url = {http://jmlr.org/papers/v24/21-1130.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1133,
 abstract = {Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors, MDL-COMP scales linearly with $d$ when $d<n$, but the scaling is exponentially smaller -- $\log d$ for $d>n$. For kernel methods, we show that MDL-COMP informs minimax in-sample error, and can decrease as the dimensionality of the input increases. We also prove that MDL-COMP upper bounds the in-sample mean squared error (MSE). Via an array of simulations and real-data experiments, we show that a data-driven Prac-MDL-COMP informs hyper-parameter tuning for optimizing test MSE with ridge regression in limited data settings, sometimes improving upon cross-validation and (always) saving computational costs. Finally, our findings also suggest that the recently observed double decent phenomenons in overparameterized models might be a consequence of the choice of non-ideal estimators.},
 author = {Raaz Dwivedi and Chandan Singh and Bin Yu and Martin Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {268},
 openalex = {W4387323051},
 pages = {1--59},
 title = {Revisiting minimum description length complexity in overparameterized models},
 url = {http://jmlr.org/papers/v24/21-1133.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1145,
 abstract = {The ability of Variational Autoencoders to learn disentangled representations has made them appealing for practical applications. However, their mean representations, which are generally used for downstream tasks, have recently been shown to be more correlated than their sampled counterpart, on which disentanglement is usually measured. In this paper, we refine this observation through the lens of selective posterior collapse, which states that only a subset of the learned representations, the active variables, is encoding useful information while the rest (the passive variables) is discarded. We first extend the existing definition, originally proposed for sampled representations, to mean representations and show that active variables are equally disentangled in both representations. Based on this new definition and the pre-trained models from disentanglement lib, we then isolate the passive variables and show that they are responsible for the discrepancies between mean and sampled representations. Specifically, passive variables exhibit high correlation scores with other variables in mean representations while being fully uncorrelated in sampled ones. We thus conclude that despite what their higher correlation might suggest, mean representations are still good candidates for downstream tasks applications. However, it may be beneficial to remove their passive variables, especially when used with models sensitive to correlated features.},
 author = {Lisa Bonheme and Marek Grzes},
 journal = {Journal of Machine Learning Research},
 number = {324},
 openalex = {W3202528968},
 pages = {1--30},
 title = {Be More Active! Understanding the Differences between Mean and Sampled Representations of Variational Autoencoders},
 url = {http://jmlr.org/papers/v24/21-1145.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1160,
 abstract = {Originally, tangles were invented as an abstract tool in mathematical graph theory to prove the famous graph minor theorem. In this paper, we showcase the practical potential of tangles in machine learning applications. Given a collection of cuts of any dataset, tangles aggregate these cuts to point in the direction of a dense structure. As a result, a cluster is softly characterized by a set of consistent pointers. This highly flexible approach can solve clustering problems in various setups, ranging from questionnaires over community detection in graphs to clustering points in metric spaces. The output of our proposed framework is hierarchical and induces the notion of a soft dendrogram, which can help explore the cluster structure of a dataset. The computational complexity of aggregating the cuts is linear in the number of data points. Thus the bottleneck of the tangle approach is to generate the cuts, for which simple and fast algorithms form a sufficient basis. In our paper we construct the algorithmic framework for clustering with tangles, prove theoretical guarantees in various settings, and provide extensive simulations and use cases. Python code is available on github.},
 author = {Solveig Klepper and Christian Elbracht and Diego Fioravanti and Jakob Kneip and Luca Rendsburg and Maximilian Teegen and Ulrike von Luxburg},
 journal = {Journal of Machine Learning Research},
 number = {190},
 openalex = {W4287752082},
 pages = {1--56},
 title = {Clustering with Tangles: Algorithmic Framework and Theoretical Guarantees},
 url = {http://jmlr.org/papers/v24/21-1160.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1170,
 abstract = {The objective of ordinal embedding is to find a Euclidean representation of a set of abstract items, using only answers to triplet comparisons of the form "Is item $i$ closer to the item $j$ or item $k$?". In recent years, numerous algorithms have been proposed to solve this problem. However, there does not exist a fair and thorough assessment of these embedding methods and therefore several key questions remain unanswered: Which algorithms perform better when the embedding dimension is constrained or few triplet comparisons are available? Which ones scale better with increasing sample size or dimension? In our paper, we address these questions and provide the first comprehensive and systematic empirical evaluation of existing algorithms as well as a new neural network approach. We find that simple, relatively unknown, non-convex methods consistently outperform all other algorithms, including elaborate approaches based on neural networks or landmark approaches. This finding can be explained by our insight that many of the non-convex optimization approaches do not suffer from local optima. Our comprehensive assessment is enabled by our unified library of popular embedding algorithms that leverages GPU resources and allows for fast and accurate embeddings of millions of data points.},
 author = {Leena Chennuru Vankadara and Michael Lohaus and Siavash Haghiri and Faiz Ul Wahab and Ulrike von Luxburg},
 journal = {Journal of Machine Learning Research},
 number = {191},
 openalex = {W4287999678},
 pages = {1--83},
 title = {Insights into Ordinal Embedding Algorithms: A Systematic Evaluation},
 url = {http://jmlr.org/papers/v24/21-1170.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1174,
 abstract = {Agents that can learn to imitate given video observation -- \emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-art techniques in these environments and can learn to imitate from a single video demonstration.},
 author = {Glen Berseth and Florian Golemo and Christopher Pal},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W4288626207},
 pages = {1--26},
 title = {Towards Learning to Imitate from a Single Video Demonstration},
 url = {http://jmlr.org/papers/v24/21-1174.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1179,
 abstract = {Many applications such as recommendation systems or sports tournaments involve pairwise comparisons within a collection of $n$ items, the goal being to aggregate the binary outcomes of the comparisons in order to recover the latent strength and/or global ranking of the items. In recent years, this problem has received significant interest from a theoretical perspective with a number of methods being proposed, along with associated statistical guarantees under the assumption of a suitable generative model. While these results typically collect the pairwise comparisons as one comparison graph $G$, however in many applications - such as the outcomes of soccer matches during a tournament - the nature of pairwise outcomes can evolve with time. Theoretical results for such a dynamic setting are relatively limited compared to the aforementioned static setting. We study in this paper an extension of the classic BTL (Bradley-Terry-Luce) model for the static setting to our dynamic setup under the assumption that the probabilities of the pairwise outcomes evolve smoothly over the time domain $[0,1]$. Given a sequence of comparison graphs $(G_{t'})_{t' \in \mathcal{T}}$ on a regular grid $\mathcal{T} \subset [0,1]$, we aim at recovering the latent strengths of the items $w_t^* \in \mathbb{R}^n$ at any time $t \in [0,1]$. To this end, we adapt the Rank Centrality method - a popular spectral approach for ranking in the static case - by locally averaging the available data on a suitable neighborhood of $t$. When $(G_{t'})_{t' \in \mathcal{T}}$ is a sequence of Erd\"os-Renyi graphs, we provide non-asymptotic $\ell_2$ and $\ell_{\infty}$ error bounds for estimating $w_t^*$ which in particular establishes the consistency of this method in terms of $n$, and the grid size $\lvert\mathcal{T}\rvert$. We also complement our theoretical analysis with experiments on real and synthetic data.},
 author = {Eglantine Karlé and Hemant Tyagi},
 journal = {Journal of Machine Learning Research},
 number = {269},
 openalex = {W4384274307},
 pages = {1--57},
 title = {Dynamic Ranking with the BTL Model: A Nearest Neighbor based Rank Centrality Method},
 url = {http://jmlr.org/papers/v24/21-1179.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1186,
 abstract = {After selection with the Group LASSO (or generalized variants such as the overlapping, sparse, or standardized Group LASSO), inference for the selected parameters is unreliable in the absence of adjustments for selection bias. In the penalized Gaussian regression setup, existing approaches provide adjustments for selection events that can be expressed as linear inequalities in the data variables. Such a representation, however, fails to hold for selection with the Group LASSO and substantially obstructs the scope of subsequent post-selective inference. Key questions of inferential interest -- for example, inference for the effects of selected variables on the outcome -- remain unanswered. In the present paper, we develop a consistent, post-selective, Bayesian method to address the existing gaps by deriving a likelihood adjustment factor and an approximation thereof that eliminates bias from the selection of groups. Experiments on simulated data and data from the Human Connectome Project demonstrate that our method recovers the effects of parameters within the selected groups while paying only a small price for bias adjustment.},
 author = {Snigdha Panigrahi and Peter W MacDonald and Daniel Kessler},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W4292121532},
 pages = {1--49},
 title = {Approximate Post-Selective Inference for Regression with the Group LASSO},
 url = {http://jmlr.org/papers/v24/21-1186.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1213,
 abstract = {Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation (SR), which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the SR can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent's representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the SR allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for exploration and on the use of the SR to combine them. The results of our experiments shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the SR, such as eigenoptions and the option keyboard.},
 author = {Marlos C. Machado and Andre Barreto and Doina Precup and Michael Bowling},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W4306809390},
 pages = {1--69},
 title = {Temporal Abstraction in Reinforcement Learning with the Successor Representation},
 url = {http://jmlr.org/papers/v24/21-1213.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1219,
 abstract = {The stochastic proximal point (SPP) methods have gained recent attention for stochastic optimization, with strong convergence guarantees and superior robustness to the classic stochastic gradient descent (SGD) methods showcased at little to no cost of computational overhead added. In this article, we study a minibatch variant of SPP, namely M-SPP, for solving convex composite risk minimization problems. The core contribution is a set of novel excess risk bounds of M-SPP derived through the lens of algorithmic stability theory. Particularly under smoothness and quadratic growth conditions, we show that M-SPP with minibatch-size $n$ and iteration count $T$ enjoys an in-expectation fast rate of convergence consisting of an $\mathcal{O}\left(\frac{1}{T^2}\right)$ bias decaying term and an $\mathcal{O}\left(\frac{1}{nT}\right)$ variance decaying term. In the small-$n$-large-$T$ setting, this result substantially improves the best known results of SPP-type approaches by revealing the impact of noise level of model on convergence rate. In the complementary small-$T$-large-$n$ regime, we provide a two-phase extension of M-SPP to achieve comparable convergence rates. Moreover, we derive a near-tight high probability (over the randomness of data) bound on the parameter estimation error of a sampling-without-replacement variant of M-SPP. Numerical evidences are provided to support our theoretical predictions when substantialized to Lasso and logistic regression models.},
 author = {Xiao-Tong Yuan and Ping Li},
 journal = {Journal of Machine Learning Research},
 number = {270},
 openalex = {W4315588781},
 pages = {1--52},
 title = {Sharper Analysis for Minibatch Stochastic Proximal Point Methods: Stability, Smoothness, and Deviation},
 url = {http://jmlr.org/papers/v24/21-1219.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1230,
 abstract = {Many modern applications of online changepoint detection require the ability to process high-frequency observations, sometimes with limited available computational resources. Online algorithms for detecting a change in mean often involve using a moving window, or specifying the expected size of change. Such choices affect which changes the algorithms have most power to detect. We introduce an algorithm, Functional Online CuSUM (FOCuS), which is equivalent to running these earlier methods simultaneously for all sizes of window, or all possible values for the size of change. Our theoretical results give tight bounds on the expected computational cost per iteration of FOCuS, with this being logarithmic in the number of observations. We show how FOCuS can be applied to a number of different change in mean scenarios, and demonstrate its practical utility through its state-of-the art performance at detecting anomalous behaviour in computer server data.},
 author = {Gaetano Romano and Idris A. Eckley and Paul Fearnhead and Guillem Rigaill},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W4286903049},
 pages = {1--36},
 title = {Fast Online Changepoint Detection via Functional Pruning CUSUM statistics},
 url = {http://jmlr.org/papers/v24/21-1230.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1250,
 abstract = {The study of strategic or adversarial manipulation of testing data to fool a classifier has attracted much recent attention. Most previous works have focused on two extreme situations where any testing data point either is completely adversarial or always equally prefers the positive label. In this paper, we generalize both of these through a unified framework for strategic classification, and introduce the notion of strategic VC-dimension (SVC) to capture the PAC-learnability in our general strategic setup. SVC provably generalizes the recent concept of adversarial VC-dimension (AVC) introduced by Cullina et al. arXiv:1806.01471. We instantiate our framework for the fundamental strategic linear classification problem. We fully characterize: (1) the statistical learnability of linear classifiers by pinning down its SVC; (2) its computational tractability by pinning down the complexity of the empirical risk minimization problem. Interestingly, the SVC of linear classifiers is always upper bounded by its standard VC-dimension. This characterization also strictly generalizes the AVC bound for linear classifiers in arXiv:1806.01471.},
 author = {Ravi Sundaram and Anil Vullikanti and Haifeng Xu and Fan Yao},
 journal = {Journal of Machine Learning Research},
 number = {192},
 openalex = {W3111125151},
 pages = {1--38},
 title = {PAC-Learning for Strategic Classification},
 url = {http://jmlr.org/papers/v24/21-1250.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1253,
 abstract = {Parameter learning for high-dimensional, partially observed, and nonlinear stochastic processes is a methodological challenge. Spatiotemporal disease transmission systems provide examples of such processes giving rise to open inference problems. We propose the iterated block particle filter (IBPF) algorithm for learning high-dimensional parameters over graphical state space models with general state spaces, measures, transition densities and graph structure. Theoretical performance guarantees are obtained on beating the curse of dimensionality (COD), algorithm convergence, and likelihood maximization. Experiments on a highly nonlinear and non-Gaussian spatiotemporal model for measles transmission reveal that the iterated ensemble Kalman filter algorithm (Li et al. (2020)) is ineffective and the iterated filtering algorithm (Ionides et al. (2015)) suffers from the COD, while our IBPF algorithm beats COD consistently across various experiments with different metrics.},
 author = {Ning Ning and Edward L. Ionides},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W3206451841},
 pages = {1--76},
 title = {Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality},
 url = {http://jmlr.org/papers/v24/21-1253.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1254,
 abstract = {In this work we study statistical properties of graph-based algorithms for multi-manifold clustering (MMC). In MMC the goal is to retrieve the multi-manifold structure underlying a given Euclidean data set when this one is assumed to be obtained by sampling a distribution on a union of manifolds $\mathcal{M} = \mathcal{M}_1 \cup\dots \cup \mathcal{M}_N$ that may intersect with each other and that may have different dimensions. We investigate sufficient conditions that similarity graphs on data sets must satisfy in order for their corresponding graph Laplacians to capture the right geometric information to solve the MMC problem. Precisely, we provide high probability error bounds for the spectral approximation of a tensorized Laplacian on $\mathcal{M}$ with a suitable graph Laplacian built from the observations; the recovered tensorized Laplacian contains all geometric information of all the individual underlying manifolds. We provide an example of a family of similarity graphs, which we call annular proximity graphs with angle constraints, satisfying these sufficient conditions. We contrast our family of graphs with other constructions in the literature based on the alignment of tangent planes. Extensive numerical experiments expand the insights that our theory provides on the MMC problem.},
 author = {Nicolas Garcia Trillos and Pengfei He and Chenghui Li},
 journal = {Journal of Machine Learning Research},
 number = {143},
 openalex = {W3186513908},
 pages = {1--71},
 title = {Large sample spectral analysis of graph-based multi-manifold clustering},
 url = {http://jmlr.org/papers/v24/21-1254.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1261,
 abstract = {Unexplainable black-box models create scenarios where anomalies cause deleterious responses, thus creating unacceptable risks. These risks have motivated the field of eXplainable Artificial Intelligence (XAI) to improve trust by evaluating local interpretability in black-box neural networks. Unfortunately, the ground truth is unavailable for the model's decision, so evaluation is limited to qualitative assessment. Further, interpretability may lead to inaccurate conclusions about the model or a false sense of trust. We propose to improve XAI from the vantage point of the user's trust by exploring a black-box model's latent feature space. We present an approach, ProtoShotXAI, that uses a Prototypical few-shot network to explore the contrastive manifold between nonlinear features of different classes. A user explores the manifold by perturbing the input features of a query sample and recording the response for a subset of exemplars from any class. Our approach is the first locally interpretable XAI model that can be extended to, and demonstrated on, few-shot networks. We compare ProtoShotXAI to the state-of-the-art XAI approaches on MNIST, Omniglot, and ImageNet to demonstrate, both quantitatively and qualitatively, that ProtoShotXAI provides more flexibility for model exploration. Finally, ProtoShotXAI also demonstrates novel explainabilty and detectabilty on adversarial samples.},
 author = {Samuel Hess and Gregory Ditzler},
 journal = {Journal of Machine Learning Research},
 number = {325},
 openalex = {W3209110901},
 pages = {1--49},
 title = {ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI.},
 url = {http://jmlr.org/papers/v24/21-1261.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1274,
 abstract = {Combining several (sample approximations of) distributions, which we term sub-posteriors, into a single distribution proportional to their product, is a common challenge. For instance, in distributed `big data' problems, or when working under multi-party privacy constraints. Many existing approaches resort to approximating the individual sub-posteriors for practical necessity, then representing the resulting approximate posterior. The quality of the posterior approximation for these approaches is poor when the sub-posteriors fall out-with a narrow range of distributional form. Recently, a Fusion approach has been proposed which finds a direct and exact Monte Carlo approximation of the posterior (as opposed to the sub-posteriors), circumventing the drawbacks of approximate approaches. Unfortunately, existing Fusion approaches have a number of computational limitations, particularly when unifying a large number of sub-posteriors. In this paper, we generalise the theory underpinning existing Fusion approaches, and embed the resulting methodology within a recursive divide-and-conquer sequential Monte Carlo paradigm. This ultimately leads to a competitive Fusion approach, which is robust to increasing numbers of sub-posteriors.},
 author = {Ryan S.Y. Chan and Murray Pollock and Adam M. Johansen and Gareth O. Roberts},
 journal = {Journal of Machine Learning Research},
 number = {193},
 openalex = {W3207682600},
 pages = {1--82},
 title = {Divide-and-Conquer Monte Carlo Fusion},
 url = {http://jmlr.org/papers/v24/21-1274.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1276,
 abstract = {Bayesian mixture models are widely used for clustering of high-dimensional data with appropriate uncertainty quantification. However, as the dimension of the observations increases, posterior inference often tends to favor too many or too few clusters. This article explains this behavior by studying the random partition posterior in a non-standard setting with a fixed sample size and increasing data dimensionality. We provide conditions under which the finite sample posterior tends to either assign every observation to a different cluster or all observations to the same cluster as the dimension grows. Interestingly, the conditions do not depend on the choice of clustering prior, as long as all possible partitions of observations into clusters have positive prior probabilities, and hold irrespective of the true data-generating model. We then propose a class of latent mixtures for Bayesian clustering (Lamb) on a set of low-dimensional latent variables inducing a partition on the observed data. The model is amenable to scalable posterior inference and we show that it can avoid the pitfalls of high-dimensionality under mild assumptions. The proposed approach is shown to have good performance in simulation studies and an application to inferring cell types based on scRNAseq.},
 author = {Noirrit Kiran Chandra and Antonio Canale and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {144},
 openalex = {W4287764453},
 pages = {1--42},
 title = {Escaping the curse of dimensionality in Bayesian model based clustering},
 url = {http://jmlr.org/papers/v24/21-1276.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1280,
 abstract = {We develop a method to generate predictive regions that cover a multivariate response variable with a user-specified probability. Our work is composed of two components. First, we use a deep generative model to learn a representation of the response that has a unimodal distribution. Existing multiple-output quantile regression approaches are effective in such cases, so we apply them on the learned representation, and then transform the solution to the original space of the response. This process results in a flexible and informative region that can have an arbitrary shape, a property that existing methods lack. Second, we propose an extension of conformal prediction to the multivariate response setting that modifies any method to return sets with a pre-specified coverage level. The desired coverage is theoretically guaranteed in the finite-sample case for any distribution. Experiments conducted on both real and synthetic data show that our method constructs regions that are significantly smaller compared to existing techniques.},
 author = {Shai Feldman and Stephen Bates and Yaniv Romano},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W4286912341},
 pages = {1--48},
 title = {Calibrated Multiple-Output Quantile Regression with Representation Learning},
 url = {http://jmlr.org/papers/v24/21-1280.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1289,
 abstract = {We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power), or arbitrary kernel choices such as the median heuristic. We prove that MMDAgg still controls the level non-asymptotically, and achieves the minimax rate over Sobolev balls, up to an iterated logarithmic term. Our guarantees are not restricted to a specific type of kernel, but hold for any product of one-dimensional translation invariant characteristic kernels. We provide a user-friendly parameter-free implementation of MMDAgg using an adaptive collection of bandwidths. We demonstrate that MMDAgg significantly outperforms alternative state-of-the-art MMD-based two-sample tests on synthetic data satisfying the Sobolev smoothness assumption, and that, on real-world image data, MMDAgg closely matches the power of tests leveraging the use of models such as neural networks.},
 author = {Antonin Schrab and Ilmun Kim and Mélisande Albert and Béatrice Laurent and Benjamin Guedj and Arthur Gretton},
 journal = {Journal of Machine Learning Research},
 number = {194},
 openalex = {W4286891023},
 pages = {1--81},
 title = {MMD Aggregated Two-Sample Test},
 url = {http://jmlr.org/papers/v24/21-1289.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1297,
 abstract = {There is an increasing realization that algorithmic inductive biases are central in preventing overfitting; empirically, we often see a benign overfitting phenomenon in overparameterized settings for natural learning algorithms, such as stochastic gradient descent (SGD), where little to no explicit regularization has been employed. This work considers this issue in arguably the most basic setting: constant-stepsize SGD (with iterate averaging) for linear regression in the overparameterized regime. Our main result provides a sharp excess risk bound, stated in terms of the full eigenspectrum of the data covariance matrix, that reveals a bias-variance decomposition characterizing when generalization is possible: (i) the variance bound is characterized in terms of an effective dimension and (ii) the bias bound provides a sharp geometric characterization in terms of the location of the initial iterate (and how it aligns with the data covariance matrix). We reflect on a number of notable differences between the algorithmic regularization afforded by (unregularized) SGD in comparison to ordinary least squares (minimum-norm interpolation) and ridge regression.},
 author = {Difan Zou and Jingfeng Wu and Vladimir Braverman and Quanquan Gu and Sham M. Kakade},
 journal = {Journal of Machine Learning Research},
 number = {326},
 openalex = {W3173402116},
 pages = {1--58},
 title = {Benign Overfitting of Constant-Stepsize SGD for Linear Regression},
 url = {http://jmlr.org/papers/v24/21-1297.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1298,
 abstract = {We formulate natural gradient variational inference (VI), expectation propagation (EP), and posterior linearisation (PL) as extensions of Newton's method for optimising the parameters of a Bayesian posterior distribution. This viewpoint explicitly casts inference algorithms under the framework of numerical optimisation. We show that common approximations to Newton's method from the optimisation literature, namely Gauss-Newton and quasi-Newton methods (e.g., the BFGS algorithm), are still valid under this 'Bayes-Newton' framework. This leads to a suite of novel algorithms which are guaranteed to result in positive semi-definite (PSD) covariance matrices, unlike standard VI and EP. Our unifying viewpoint provides new insights into the connections between various inference schemes. All the presented methods apply to any model with a Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse) Gaussian processes and state space models.},
 author = {William J. Wilkinson and Simo Särkkä and Arno Solin},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W4225883271},
 pages = {1--50},
 title = {Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees},
 url = {http://jmlr.org/papers/v24/21-1298.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1301,
 abstract = {In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federated learning in cross-device (with client sampling) and cross-silo (without client sampling) settings. Our theoretical analysis improves the optimization theory for MAML, and our empirical results corroborate our theoretical findings. Interested readers can access our code at \url{https://github.com/bokun-wang/moml}.},
 author = {Bokun Wang and Zhuoning Yuan and Yiming Ying and Tianbao Yang},
 journal = {Journal of Machine Learning Research},
 number = {145},
 openalex = {W3172133648},
 pages = {1--46},
 title = {Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning},
 url = {http://jmlr.org/papers/v24/21-1301.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1308,
 author = {Xi Wang and Zhipeng Tu and Yiguang Hong and Yingyi Wu and Guodong Shi},
 journal = {Journal of Machine Learning Research},
 number = {84},
 pages = {1--67},
 title = {Online Optimization over Riemannian Manifolds},
 url = {http://jmlr.org/papers/v24/21-1308.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1313,
 abstract = {Standard Monte Carlo computation is widely known to exhibit a canonical square-root convergence speed in terms of sample size. Two recent techniques, one based on control variate and one on importance sampling, both derived from an integration of reproducing kernels and Stein's identity, have been proposed to reduce the error in Monte Carlo computation to supercanonical convergence. This paper presents a more general framework to encompass both techniques that is especially beneficial when the sample generator is biased and noise-corrupted. We show our general estimator, which we call the doubly robust Stein-kernelized estimator, outperforms both existing methods in terms of mean squared error rates across different scenarios. We also demonstrate the superior performance of our method via numerical examples.},
 author = {Henry Lam and Haofeng Zhang},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W3208796896},
 pages = {1--58},
 title = {Doubly Robust Stein-Kernelized Monte Carlo Estimator: Simultaneous Bias-Variance Reduction and Supercanonical Convergence},
 url = {http://jmlr.org/papers/v24/21-1313.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1322,
 abstract = {Graphs are commonly used to represent and visualize causal relations. For a small number of variables, this approach provides a succinct and clear view of the scenario at hand. As the number of variables under study increases, the graphical approach may become impractical, and the clarity of the representation is lost. Clustering of variables is a natural way to reduce the size of the causal diagram, but it may erroneously change the essential properties of the causal relations if implemented arbitrarily. We define a specific type of cluster, called transit cluster, that is guaranteed to preserve the identifiability properties of causal effects under certain conditions. We provide a sound and complete algorithm for finding all transit clusters in a given graph and demonstrate how clustering can simplify the identification of causal effects. We also study the inverse problem, where one starts with a clustered graph and looks for extended graphs where the identifiability properties of causal effects remain unchanged. We show that this kind of structural robustness is closely related to transit clusters.},
 author = {Santtu Tikka and Jouni Helske and Juha Karvanen},
 journal = {Journal of Machine Learning Research},
 number = {195},
 openalex = {W4226225250},
 pages = {1--32},
 title = {Clustering and Structural Robustness in Causal Diagrams},
 url = {http://jmlr.org/papers/v24/21-1322.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1323,
 author = {Cédric M. Campos and Alejandro Mahillo and David Martín de Diego},
 journal = {Journal of Machine Learning Research},
 number = {25},
 pages = {1--33},
 title = {Discrete Variational Calculus for Accelerated Optimization},
 url = {http://jmlr.org/papers/v24/21-1323.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1329,
 abstract = {We consider the problem of recovering causal structure from multivariate observational data. We assume that the data arise from a linear structural equation model (SEM) in which the idiosyncratic errors are allowed to be dependent in order to capture possible latent confounding. Each SEM can be represented by a graph where vertices represent observed variables, directed edges represent direct causal effects, and bidirected edges represent dependence among error terms. Specifically, we assume that the true model corresponds to a bow-free acyclic path diagram, i.e., a graph that has at most one edge between any pair of nodes and is acyclic in the directed part. We show that when the errors are non-Gaussian, the exact causal structure encoded by such a graph, and not merely an equivalence class, can be consistently recovered from observational data. The Bow-free Acylic Non-Gaussian (BANG) method we propose for this purpose uses estimates of suitable moments, but, in contrast to previous results, does not require specifying the number of latent variables a priori. We illustrate the effectiveness of BANG in simulations and an application to an ecology data set.},
 author = {Y. Samuel Wang and Mathias Drton},
 journal = {Journal of Machine Learning Research},
 number = {271},
 openalex = {W3045061576},
 pages = {1--61},
 title = {Causal Discovery with Unobserved Confounding and non-Gaussian Data},
 url = {http://jmlr.org/papers/v24/21-1329.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1333,
 abstract = {We study sparse linear regression over a network of agents, modeled as an undirected graph (with no centralized node). The estimation problem is formulated as the minimization of the sum of the local LASSO loss functions plus a quadratic penalty of the consensus constraint -- the latter being instrumental to obtain distributed solution methods. While penalty-based consensus methods have been extensively studied in the optimization literature, their statistical and computational guarantees in the high dimensional setting remain unclear. This work provides an answer to this open problem. Our contribution is two-fold. First, we establish statistical consistency of the estimator: under a suitable choice of the penalty parameter, the optimal solution of the penalized problem achieves near optimal minimax rate $\mathcal{O}(s \log d/N)$ in $\ell_2$-loss, where $s$ is the sparsity value, $d$ is the ambient dimension, and $N$ is the total sample size in the network -- this matches centralized sample rates. Second, we show that the proximal-gradient algorithm applied to the penalized problem, which naturally leads to distributed implementations, converges linearly up to a tolerance of the order of the centralized statistical error -- the rate scales as $\mathcal{O}(d)$, revealing an unavoidable speed-accuracy dilemma.Numerical results demonstrate the tightness of the derived sample rate and convergence rate scalings.},
 author = {Yao Ji and Gesualdo Scutari and Ying Sun and Harsha Honnappa},
 journal = {Journal of Machine Learning Research},
 number = {272},
 openalex = {W3213019012},
 pages = {1--62},
 title = {Distributed Sparse Regression via Penalization},
 url = {http://jmlr.org/papers/v24/21-1333.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1350,
 abstract = {A variety of theoretically-sound policy gradient algorithms exist for the on-policy setting due to the policy gradient theorem, which provides a simplified form for the gradient. The off-policy setting, however, has been less clear due to the existence of multiple objectives and the lack of an explicit off-policy policy gradient theorem. In this work, we unify these objectives into one off-policy objective, and provide a policy gradient theorem for this unified objective. The derivation involves emphatic weightings and interest functions. We show multiple strategies to approximate the gradients, in an algorithm called Actor Critic with Emphatic weightings (ACE). We prove in a counterexample that previous (semi-gradient) off-policy actor-critic methods--particularly Off-Policy Actor-Critic (OffPAC) and Deterministic Policy Gradient (DPG)--converge to the wrong solution whereas ACE finds the optimal solution. We also highlight why these semi-gradient approaches can still perform well in practice, suggesting strategies for variance reduction in ACE. We empirically study several variants of ACE on two classic control environments and an image-based environment designed to illustrate the tradeoffs made by each gradient approximation. We find that by approximating the emphatic weightings directly, ACE performs as well as or better than OffPAC in all settings tested.},
 author = {Eric Graves and Ehsan Imani and Raksha Kumaraswamy and Martha White},
 journal = {Journal of Machine Learning Research},
 number = {146},
 openalex = {W4226248760},
 pages = {1--63},
 title = {Off-Policy Actor-Critic with Emphatic Weightings},
 url = {http://jmlr.org/papers/v24/21-1350.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1363,
 abstract = {We propose a new data-driven approach for learning the fundamental solutions (Green's functions) of various linear partial differential equations (PDEs) given sample pairs of input-output functions. Building off the theory of functional linear regression (FLR), we estimate the best-fit Green's function and bias term of the fundamental solution in a reproducing kernel Hilbert space (RKHS) which allows us to regularize their smoothness and impose various structural constraints. We derive a general representer theorem for operator RKHSs to approximate the original infinite-dimensional regression problem by a finite-dimensional one, reducing the search space to a parametric class of Green's functions. In order to study the prediction error of our Green's function estimator, we extend prior results on FLR with scalar outputs to the case with functional outputs. Finally, we demonstrate our method on several linear PDEs including the Poisson, Helmholtz, Schr\"{o}dinger, Fokker-Planck, and heat equation. We highlight its robustness to noise as well as its ability to generalize to new data with varying degrees of smoothness and mesh discretization without any additional training.},
 author = {George Stepaniants},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W4287020235},
 pages = {1--72},
 title = {Learning Partial Differential Equations in Reproducing Kernel Hilbert Spaces},
 url = {http://jmlr.org/papers/v24/21-1363.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1373,
 abstract = {Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose method to estimate the parameters of statistical models from incomplete data. We validate VGI on a set of synthetic and real-world estimation tasks, estimating important machine learning models such as variational autoencoders and normalising flows from incomplete data. The proposed method, whilst general-purpose, achieves competitive or better performance than existing model-specific estimation methods.},
 author = {Vaidotas Simkus and Benjamin Rhodes and Michael U. Gutmann},
 journal = {Journal of Machine Learning Research},
 number = {196},
 openalex = {W4307124991},
 pages = {1--72},
 title = {Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data},
 url = {http://jmlr.org/papers/v24/21-1373.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1392,
 abstract = {Semiparametric inference on average causal effects from observational data is based on assumptions yielding identification of the effects. In practice, several distinct identifying assumptions may be plausible; an analyst has to make a delicate choice between these models. In this paper, we study three identifying assumptions based on the potential outcome framework: the back-door assumption, which uses pre-treatment covariates, the front-door assumption, which uses mediators, and the two-door assumption using pre-treatment covariates and mediators simultaneously. We provide the efficient influence functions and the corresponding semiparametric efficiency bounds that hold under these assumptions, and their combinations. We demonstrate that neither of the identification models provides uniformly the most efficient estimation and give conditions under which some bounds are lower than others. We show when semiparametric estimating equation estimators based on influence functions attain the bounds, and study the robustness of the estimators to misspecification of the nuisance models. The theory is complemented with simulation experiments on the finite sample behavior of the estimators. The results obtained are relevant for an analyst facing a choice between several plausible identifying assumptions and corresponding estimators. Our results show that this choice implies a trade-off between efficiency and robustness to misspecification of the nuisance models.},
 author = {Tetiana Gorbach and Xavier de Luna and Juha Karvanen and Ingeborg Waernbaum},
 journal = {Journal of Machine Learning Research},
 number = {197},
 openalex = {W4226148544},
 pages = {1--65},
 title = {Contrasting Identifying Assumptions of Average Causal Effects: Robustness and Semiparametric Efficiency},
 url = {http://jmlr.org/papers/v24/21-1392.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1396,
 abstract = {Machine learning models trained by different optimization algorithms under different data distributions can exhibit distinct generalization behaviors. In this paper, we analyze the generalization of models trained by noisy iterative algorithms. We derive distribution-dependent generalization bounds by connecting noisy iterative algorithms to additive noise channels found in communication and information theory. Our generalization bounds shed light on several applications, including differentially private stochastic gradient descent (DP-SGD), federated learning, and stochastic gradient Langevin dynamics (SGLD). We demonstrate our bounds through numerical experiments, showing that they can help understand recent empirical observations of the generalization phenomena of neural networks.},
 author = {Hao Wang and Rui Gao and Flavio P. Calmon},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W4287332259},
 pages = {1--43},
 title = {Generalization Bounds for Noisy Iterative Algorithms Using Properties of Additive Noise Channels},
 url = {http://jmlr.org/papers/v24/21-1396.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1398,
 abstract = {Compositional data, such as human gut microbiomes, consist of non-negative variables whose only the relative values to other variables are available. Analyzing compositional data such as human gut microbiomes needs a careful treatment of the geometry of the data. A common geometrical understanding of compositional data is via a regular simplex. Majority of existing approaches rely on a log-ratio or power transformations to overcome the innate simplicial geometry. In this work, based on the key observation that a compositional data are projective in nature, and on the intrinsic connection between projective and spherical geometry, we re-interpret the compositional domain as the quotient topology of a sphere modded out by a group action. This re-interpretation allows us to understand the function space on compositional domains in terms of that on spheres and to use spherical harmonics theory along with reflection group actions for constructing a compositional Reproducing Kernel Hilbert Space (RKHS). This construction of RKHS for compositional data will widely open research avenues for future methodology developments. In particular, well-developed kernel embedding methods can be now introduced to compositional data analysis. The polynomial nature of compositional RKHS has both theoretical and computational benefits. The wide applicability of the proposed theoretical framework is exemplified with nonparametric density estimation and kernel exponential family for compositional data.},
 author = {Binglin Li and Changwon Yoon and Jeongyoun Ahn},
 journal = {Journal of Machine Learning Research},
 number = {327},
 openalex = {W4229066365},
 pages = {1--34},
 title = {Reproducing Kernels and New Approaches in Compositional Data Analysis},
 url = {http://jmlr.org/papers/v24/21-1398.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1403,
 abstract = {Many scientific problems require identifying a small set of covariates that are associated with a target response and estimating their effects. Often, these effects are nonlinear and include interactions, so linear and additive methods can lead to poor estimation and variable selection. Unfortunately, methods that simultaneously express sparsity, nonlinearity, and interactions are computationally intractable -- with runtime at least quadratic in the number of covariates, and often worse. In the present work, we solve this computational bottleneck. We show that suitable interaction models have a kernel representation, namely there exists a "kernel trick" to perform variable selection and estimation in $O$(# covariates) time. Our resulting fit corresponds to a sparse orthogonal decomposition of the regression function in a Hilbert space (i.e., a functional ANOVA decomposition), where interaction effects represent all variation that cannot be explained by lower-order effects. On a variety of synthetic and real data sets, our approach outperforms existing methods used for large, high-dimensional data sets while remaining competitive (or being orders of magnitude faster) in runtime.},
 author = {Raj Agrawal and Tamara Broderick},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W4287114344},
 pages = {1--60},
 title = {The SKIM-FA Kernel: High-Dimensional Variable Selection and Nonlinear Interaction Discovery in Linear Time},
 url = {http://jmlr.org/papers/v24/21-1403.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1408,
 abstract = {The fidelity bandits problem is a variant of the $K$-armed bandit problem in which the reward of each arm is augmented by a fidelity reward that provides the player with an additional payoff depending on how 'loyal' the player has been to that arm in the past. We propose two models for fidelity. In the loyalty-points model the amount of extra reward depends on the number of times the arm has previously been played. In the subscription model the additional reward depends on the current number of consecutive draws of the arm. We consider both stochastic and adversarial problems. Since single-arm strategies are not always optimal in stochastic problems, the notion of regret in the adversarial setting needs careful adjustment. We introduce three possible notions of regret and investigate which can be bounded sublinearly. We study in detail the special cases of increasing, decreasing and coupon (where the player gets an additional reward after every $m$ plays of an arm) fidelity rewards. For the models which do not necessarily enjoy sublinear regret, we provide a worst case lower bound. For those models which exhibit sublinear regret, we provide algorithms and bound their regret.},
 author = {Gábor Lugosi and Ciara Pike-Burke and Pierre-André Savalle},
 journal = {Journal of Machine Learning Research},
 number = {328},
 openalex = {W4309435390},
 pages = {1--44},
 title = {Bandit problems with fidelity rewards},
 url = {http://jmlr.org/papers/v24/21-1408.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1410,
 abstract = {We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we show that the tracking efficiency of the proximal stochastic gradient method depends only logarithmically on the initialization quality, when equipped with a step-decay schedule. Numerical experiments illustrate our results.},
 author = {Joshua Cutler and Dmitriy Drusvyatskiy and Zaid Harchaoui},
 journal = {Journal of Machine Learning Research},
 number = {147},
 openalex = {W3216080858},
 pages = {1--56},
 title = {Stochastic optimization under distributional drift.},
 url = {http://jmlr.org/papers/v24/21-1410.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1412,
 author = {Inass Sekkat and Gabriel Stoltz},
 journal = {Journal of Machine Learning Research},
 number = {329},
 pages = {1--58},
 title = {Mini-batching error and adaptive Langevin dynamics},
 url = {http://jmlr.org/papers/v24/21-1412.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1436,
 author = {Adrien Pavao and Isabelle Guyon and Anne-Catherine Letournel and Dinh-Tuan Tran and Xavier Baro and Hugo Jair Escalante and Sergio Escalera and Tyler Thomas and Zhen Xu},
 journal = {Journal of Machine Learning Research},
 number = {198},
 openalex = {W4295682935},
 pages = {1--6},
 title = {CodaLab Competitions: An open source platform to organize scientific challenges},
 url = {http://jmlr.org/papers/v24/21-1436.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1441,
 abstract = {Much research effort has been devoted to explaining the success of deep learning. Random Matrix Theory (RMT) provides an emerging way to this end: spectral analysis of large random matrices involved in a trained deep neural network (DNN) such as weight matrices or Hessian matrices with respect to the stochastic gradient descent algorithm. To have more comprehensive understanding of weight matrices spectra, we conduct extensive experiments on weight matrices in different modules, e.g., layers, networks and data sets. Following the previous work of \cite{martin2018implicit}, we classify the spectra in the terminal stage into three main types: Light Tail (LT), Bulk Transition period (BT) and Heavy Tail(HT). These different types, especially HT, implicitly indicate some regularization in the DNNs. A main contribution from the paper is that we identify the difficulty of the classification problem as a driving factor for the appearance of heavy tail in weight matrices spectra. Higher the classification difficulty, higher the chance for HT to appear. Moreover, the classification difficulty can be affected by the signal-to-noise ratio of the dataset, or by the complexity of the classification problem (complex features, large number of classes) as well. Leveraging on this finding, we further propose a spectral criterion to detect the appearance of heavy tails and use it to early stop the training process without testing data. Such early stopped DNNs have the merit of avoiding overfitting and unnecessary extra training while preserving a much comparable generalization ability. These findings from the paper are validated in several NNs, using Gaussian synthetic data and real data sets (MNIST and CIFAR10).},
 author = {XuranMeng and JeffYao},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W4226320345},
 pages = {1--40},
 title = {Impact of classification difficulty on the weight matrices spectra in Deep Learning and application to early-stopping},
 url = {http://jmlr.org/papers/v24/21-1441.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1444,
 author = {Yu-Hu Yan and Peng Zhao and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {273},
 pages = {1--50},
 title = {Online Non-stochastic Control with Partial Feedback},
 url = {http://jmlr.org/papers/v24/21-1444.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1457,
 abstract = {Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (iii) finally, near-optimality of finite state model approximations for MDPs with weakly continuous kernels which we show to correspond to the fixed point of the constructed POMDP. Thus, our paper presents a very general convergence and approximation result for the applicability of Q-learning for continuous MDPs.},
 author = {Ali Kara and Naci Saldi and Serdar Yüksel},
 journal = {Journal of Machine Learning Research},
 number = {199},
 openalex = {W4286858244},
 pages = {1--34},
 title = {Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity},
 url = {http://jmlr.org/papers/v24/21-1457.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1463,
 abstract = {Optimization problems with continuous data appear in, e.g., robust machine learning, functional data analysis, and variational inference. Here, the target function is given as an integral over a family of (continuously) indexed target functions - integrated with respect to a probability measure. Such problems can often be solved by stochastic optimization methods: performing optimization steps with respect to the indexed target function with randomly switched indices. In this work, we study a continuous-time variant of the stochastic gradient descent algorithm for optimization problems with continuous data. This so-called stochastic gradient process consists in a gradient flow minimizing an indexed target function that is coupled with a continuous-time index process determining the index. Index processes are, e.g., reflected diffusions, pure jump processes, or other L\'evy processes on compact spaces. Thus, we study multiple sampling patterns for the continuous data space and allow for data simulated or streamed at runtime of the algorithm. We analyze the approximation properties of the stochastic gradient process and study its longtime behavior and ergodicity under constant and decreasing learning rates. We end with illustrating the applicability of the stochastic gradient process in a polynomial regression problem with noisy functional data, as well as in a physics-informed neural network.},
 author = {Kexin Jin and Jonas Latz and Chenguang Liu and Carola-Bibiane Schönlieb},
 journal = {Journal of Machine Learning Research},
 number = {274},
 openalex = {W4200630783},
 pages = {1--48},
 title = {A Continuous-time Stochastic Gradient Descent Method for Continuous Data},
 url = {http://jmlr.org/papers/v24/21-1463.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1471,
 abstract = {This paper focuses on stochastic methods for solving smooth non-convex strongly-concave min-max problems, which have received increasing attention due to their potential applications in deep learning (e.g., deep AUC maximization). However, most of the existing algorithms are slow in practice, and their analysis revolves around the convergence to a nearly stationary point. We consider leveraging the Polyak-Łojasiewicz (PL) condition to design faster stochastic algorithms with stronger convergence guarantee. Although PL condition has been utilized for designing many stochastic minimization algorithms, their applications for non-convex min-max optimization remains rare. In this paper, we propose and analyze proximal epoch-based methods, and establish fast convergence in terms of both {\bf the primal objective gap and the duality gap}. Our analysis is interesting in threefold: (i) it is based on a novel Lyapunov function that consists of the primal objective gap and the duality gap of a regularized function; (ii) it only requires a weaker PL condition for establishing the primal objective convergence than that required for the duality gap convergence; (iii) it yields the optimal dependence on the accuracy level $\epsilon$, i.e., $O(1/\epsilon)$. We also make explicit the dependence on the problem parameters and explore regions of weak convexity parameter that lead to improved dependence on condition numbers. Experiments on deep AUC maximization demonstrate the effectiveness of our methods. Our method (MaxAUC) achieved an AUC of 0.922 on private testing set on {\bf CheXpert competition}.},
 author = {Zhishuai Guo and Yan Yan and Zhuoning Yuan and Tianbao Yang},
 journal = {Journal of Machine Learning Research},
 number = {148},
 openalex = {W3035278189},
 pages = {1--63},
 title = {Fast Objective and Duality Gap Convergence for Non-convex Strongly-concave Min-max Problems.},
 url = {http://jmlr.org/papers/v24/21-1471.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1476,
 author = {Junsouk Choi and Yang Ni},
 journal = {Journal of Machine Learning Research},
 number = {200},
 pages = {1--32},
 title = {Model-based Causal Discovery for Zero-Inflated Count Data},
 url = {http://jmlr.org/papers/v24/21-1476.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1480,
 abstract = {Covariate measurement error in nonparametric regression is a common problem in nutritional epidemiology and geostatistics, and other fields. Over the last two decades, this problem has received substantial attention in the frequentist literature. Bayesian approaches for handling measurement error have only been explored recently and are surprisingly successful, although the lack of a proper theoretical justification regarding the asymptotic performance of the estimators. By specifying a Gaussian process prior on the regression function and a Dirichlet process Gaussian mixture prior on the unknown distribution of the unobserved covariates, we show that the posterior distribution of the regression function and the unknown covariates density attain optimal rates of contraction adaptively over a range of H\"{o}lder classes, up to logarithmic terms. This improves upon the existing classical frequentist results which require knowledge of the smoothness of the underlying function to deliver optimal risk bounds. We also develop a novel surrogate prior for approximating the Gaussian process prior that leads to efficient computation and preserves the covariance structure, thereby facilitating easy prior elicitation. We demonstrate the empirical performance of our approach and compare it with competitors in a wide range of simulation experiments and a real data example.},
 author = {Shuang Zhou and Debdeep Pati and Tianying Wang and Yun Yang and Raymond J. Carroll},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2979934467},
 pages = {1--53},
 title = {Gaussian Processes with Errors in Variables: Theory and Computation},
 url = {http://jmlr.org/papers/v24/21-1480.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1501,
 abstract = {Contrastive learning has achieved state-of-the-art performance in various self-supervised learning tasks and even outperforms its supervised counterpart. Despite its empirical success, theoretical understanding of the superiority of contrastive learning is still limited. In this paper, under linear representation settings, (i) we provably show that contrastive learning outperforms the standard autoencoders and generative adversarial networks, two classical generative unsupervised learning methods, for both feature recovery and in-domain downstream tasks; (ii) we also illustrate the impact of labeled data in supervised contrastive learning. This provides theoretical support for recent findings that contrastive learning with labels improves the performance of learned representations in the in-domain downstream task, but it can harm the performance in transfer learning. We verify our theory with numerical experiments.},
 author = {Wenlong Ji and Zhun Deng and Ryumei Nakada and James Zou and Linjun Zhang},
 journal = {Journal of Machine Learning Research},
 number = {330},
 openalex = {W3203468910},
 pages = {1--78},
 title = {The Power of Contrast for Feature Learning: A Theoretical Analysis},
 url = {http://jmlr.org/papers/v24/21-1501.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1513,
 abstract = {Mixed Membership Models (MMMs) are a popular family of latent structure models for complex multivariate data. Instead of forcing each subject to belong to a single cluster, MMMs incorporate a vector of subject-specific weights characterizing partial membership across clusters. With this flexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In this article, we propose a new class of Dimension-Grouped MMMs (Gro-M$^3$s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M$^3$s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identifiability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M$^3$s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically confirm the identifiability results. We illustrate the new methodology through applications to a functional disability survey dataset and a personality test dataset.},
 author = {Yuqi Gu and Elena E. Erosheva and Gongjun Xu and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W3202519246},
 pages = {1--49},
 title = {Dimension-Grouped Mixed Membership Models for Multivariate Categorical Data},
 url = {http://jmlr.org/papers/v24/21-1513.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1516,
 abstract = {Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two classes of distances between probability distributions that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the H\"older Lower Restricted Isometric Property and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distances, we provide guarantees for compressive statistical learning by introducing and studying the concept of Wasserstein regularity of the learning task, that is when some task-specific metric between probability distributions can be bounded by a Wasserstein distance.},
 author = {Titouan Vayer and Rémi Gribonval},
 journal = {Journal of Machine Learning Research},
 number = {149},
 openalex = {W4379032773},
 pages = {1--51},
 title = {Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning},
 url = {http://jmlr.org/papers/v24/21-1516.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1518,
 abstract = {HiClass is an open-source Python library for local hierarchical classification entirely compatible with scikit-learn. It contains implementations of the most common design patterns for hierarchical machine learning models found in the literature, that is, the local classifiers per node, per parent node and per level. Additionally, the package contains implementations of hierarchical metrics, which are more appropriate for evaluating classification performance on hierarchical data. The documentation includes installation and usage instructions, examples within tutorials and interactive notebooks, and a complete description of the API. HiClass is released under the simplified BSD license, encouraging its use in both academic and commercial environments. Source code and documentation are available at https://github.com/scikit-learn-contrib/hiclass.},
 author = {Fábio M. Miranda and Niklas Köhnecke and Bernhard Y. Renard},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W4226083381},
 pages = {1--17},
 title = {HiClass: a Python library for local hierarchical classification compatible with scikit-learn},
 url = {http://jmlr.org/papers/v24/21-1518.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1524,
 abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
 author = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W4287023463},
 pages = {1--97},
 title = {Neural Operator: Learning Maps Between Function Spaces},
 url = {http://jmlr.org/papers/v24/21-1524.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1526,
 abstract = {In recent years, persistent homology (PH) has been successfully applied to real-world data in many different settings. When attempting to study large and noisy data sets, however, many of the currently available algorithms for PH fail due to computational complexity preventing interesting applications. One approach to address computational issues posed by PH is to select a set of landmarks by subsampling from the data. Currently, these landmark points are chosen either at random or using the so called maxmin algorithm. Neither is ideal as random selection tends to favour dense areas of the data while the maxmin algorithm is very sensitive to noise. We propose a novel approach to select landmarks specifically for PH that preserves topological properties of the original data set. Our method is motivated by the Mayer-Vietoris sequence and requires only local PH computation thus enabling efficient computation. We test our landmarks on artificial data sets which contain different levels of noise and compare them to standard landmark selection techniques. We demonstrate that our landmark selection outperforms standard methods as well as a subsampling technique based on an outlier-robust version of the $k$- means algorithm for low sampling densities in noisy data.},
 author = {Bernadette J. Stolz},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W3150921396},
 pages = {1--35},
 title = {Outlier-robust subsampling techniques for persistent homology},
 url = {http://jmlr.org/papers/v24/21-1526.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1532,
 abstract = {We derive and analyze a generic, recursive algorithm for estimating all splits in a finite cluster tree as well as the corresponding clusters. We further investigate statistical properties of this generic clustering algorithm when it receives level set estimates from a kernel density estimator. In particular, we derive finite sample guarantees, consistency, rates of convergence, and an adaptive data-driven strategy for choosing the kernel bandwidth. For these results we do not need continuity assumptions on the density such as Holder continuity, but only require intuitive geometric assumptions of non-parametric nature.},
 author = {Ingo Steinwart and Bharath K. Sriperumbudur and Philipp Thomann},
 journal = {Journal of Machine Learning Research},
 number = {275},
 openalex = {W2750426698},
 pages = {1--56},
 title = {Adaptive Clustering Using Kernel Density Estimators},
 url = {http://jmlr.org/papers/v24/21-1532.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1548,
 abstract = {In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact {\em biased} compressors often show superior performance in practice when compared to the much more studied and understood {\em unbiased} compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. Our {\em distributed} SGD method enjoys the ergodic rate $\mathcal{O}\left(\frac{\delta L \exp(-K) }{\mu} + \frac{(C + D)}{K\mu}\right)$, where $\delta$ is a compression parameter which grows when more compression is applied, $L$ and $\mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C=0$ if full gradients are computed on each node) and $D$ captures the variance of the gradients at the optimum ($D=0$ for over-parameterized models). Further, via a theoretical study of several synthetic and empirical distributions of communicated gradients, we shed light on why and by how much biased compressors outperform their unbiased variants. Finally, we propose a new highly performing biased compressor---combination of Top-$k$ and natural dithering---which in our experiments outperforms all other compression techniques.},
 author = {Aleksandr Beznosikov and Samuel Horváth and Peter Richtárik and Mher Safaryan},
 journal = {Journal of Machine Learning Research},
 number = {276},
 openalex = {W3007528421},
 pages = {1--50},
 title = {On Biased Compression for Distributed Learning.},
 url = {http://jmlr.org/papers/v24/21-1548.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0005,
 abstract = {As machine learning powered decision-making becomes increasingly important in our daily lives, it is imperative to strive for fairness in the underlying data processing. We propose a pre-processing algorithm for fair data representation via which supervised learning results in estimations of the Pareto frontier between prediction error and statistical disparity. Particularly, the present work applies the optimal affine transport to approach the post-processing Wasserstein-2 barycenter characterization of the optimal fair $L^2$-objective supervised learning via a pre-processing data deformation. Furthermore, we show that the Wasserstein-2 geodesics from the conditional (on sensitive information) distributions of the learning outcome to their barycenter characterizes the Pareto frontier between $L^2$-loss and the average pairwise Wasserstein-2 distance among sensitive groups on the learning outcome. Numerical simulations underscore the advantages: (1) the pre-processing step is compositive with arbitrary conditional expectation estimation supervised learning methods and unseen data; (2) the fair representation protects the sensitive information by limiting the inference capability of the remaining data with respect to the sensitive data; (3) the optimal affine maps are computationally efficient even for high-dimensional data.},
 author = {Shizhou Xu and Thomas Strohmer},
 journal = {Journal of Machine Learning Research},
 number = {331},
 openalex = {W4221148696},
 pages = {1--63},
 title = {Fair Data Representation for Machine Learning at the Pareto Frontier},
 url = {http://jmlr.org/papers/v24/22-0005.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0006,
 abstract = {To quantify uncertainties in inverse problems of partial differential equations (PDEs), we formulate them into statistical inference problems using Bayes' formula. Recently, well-justified infinite-dimensional Bayesian analysis methods have been developed to construct dimension-independent algorithms. However, there are three challenges for these infinite-dimensional Bayesian methods: prior measures usually act as regularizers and are not able to incorporate prior information efficiently; complex noises, such as more practical non-i.i.d. distributed noises, are rarely considered; and time-consuming forward PDE solvers are needed to estimate posterior statistical quantities. To address these issues, an infinite-dimensional inference framework has been proposed based on the infinite-dimensional variational inference method and deep generative models. Specifically, by introducing some measure equivalence assumptions, we derive the evidence lower bound in the infinite-dimensional setting and provide possible parametric strategies that yield a general inference framework called the Variational Inverting Network (VINet). This inference framework can encode prior and noise information from learning examples. In addition, relying on the power of deep neural networks, the posterior mean and variance can be efficiently and explicitly generated in the inference stage. In numerical experiments, we design specific network structures that yield a computable VINet from the general inference framework. Numerical examples of linear inverse problems of an elliptic equation and the Helmholtz equation are presented to illustrate the effectiveness of the proposed inference framework.},
 author = {Junxiong Jia and Yanni Wu and Peijun Li and Deyu Meng},
 journal = {Journal of Machine Learning Research},
 number = {201},
 openalex = {W4221167292},
 pages = {1--60},
 title = {Variational Inverting Network for Statistical Inverse Problems of Partial Differential Equations},
 url = {http://jmlr.org/papers/v24/22-0006.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0014,
 author = {Cody Lewis and Vijay Varadharajan and Nasimul Noman},
 journal = {Journal of Machine Learning Research},
 number = {30},
 pages = {1--50},
 title = {Attacks against Federated Learning Defense Systems and their Mitigation},
 url = {http://jmlr.org/papers/v24/22-0014.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0019,
 abstract = {In supervised learning, obtaining a large set of fully-labeled training data is expensive. We show that we do not always need full label information on every single training example to train a competent classifier. Specifically, inspired by the principle of sufficiency in statistics, we present a statistic (a summary) of the fully-labeled training set that captures almost all the relevant information for classification but at the same time is easier to obtain directly. We call this statistic "sufficiently-labeled data" and prove its sufficiency and efficiency for finding the optimal hidden representations, on which competent classifier heads can be trained using as few as a single randomly-chosen fully-labeled example per class. Sufficiently-labeled data can be obtained from annotators directly without collecting the fully-labeled data first. And we prove that it is easier to directly obtain sufficiently-labeled data than obtaining fully-labeled data. Furthermore, sufficiently-labeled data is naturally more secure since it stores relative, instead of absolute, information. Extensive experimental results are provided to support our theory.},
 author = {Shiyu Duan and Spencer Chang and Jose C. Principe},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W4290802627},
 pages = {1--35},
 title = {Labels, Information, and Computation: Efficient Learning Using Sufficient Labels},
 url = {http://jmlr.org/papers/v24/22-0019.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0021,
 author = {Likai Chen and Georg Keilbar and Wei Biao Wu},
 journal = {Journal of Machine Learning Research},
 number = {91},
 pages = {1--25},
 title = {Recursive Quantile Estimation: Non-Asymptotic Confidence Bounds},
 url = {http://jmlr.org/papers/v24/22-0021.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0034,
 abstract = {There has been a surge of interest in developing robust estimators for models with heavy-tailed and bounded variance data in statistics and machine learning, while few works impose unbounded variance. This paper proposes two type of robust estimators, the ridge log-truncated M-estimator and the elastic net log-truncated M-estimator. The first estimator is applied to convex regressions such as quantile regression and generalized linear models, while the other one is applied to high dimensional non-convex learning problems such as regressions via deep neural networks. Simulations and real data analysis demonstrate the {robustness} of log-truncated estimations over standard estimations.},
 author = {Lihu Xu and Fang Yao and Qiuran Yao and Huiming Zhang},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W4304984780},
 pages = {1--46},
 title = {Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite Variance Assumption},
 url = {http://jmlr.org/papers/v24/22-0034.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0044,
 author = {Yucheng Lu and Christopher De Sa},
 journal = {Journal of Machine Learning Research},
 number = {93},
 pages = {1--62},
 title = {Decentralized Learning: Theoretical Optimality and Practical Improvements},
 url = {http://jmlr.org/papers/v24/22-0044.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0088,
 abstract = {We consider the problem of maximizing the variance explained from a data matrix using orthogonal sparse principal components that have a support of fixed cardinality. While most existing methods focus on building principal components (PCs) iteratively through deflation, we propose GeoSPCA, a novel algorithm to build all PCs at once while satisfying the orthogonality constraints which brings substantial benefits over deflation. This novel approach is based on the left eigenvalues of the covariance matrix which helps circumvent the non-convexity of the problem by approximating the optimal solution using a binary linear optimization problem that can find the optimal solution. The resulting approximation can be used to tackle different versions of the sparse PCA problem including the case in which the principal components share the same support or have disjoint supports and the Structured Sparse PCA problem. We also propose optimality bounds and illustrate the benefits of GeoSPCA in selected real world problems both in terms of explained variance, sparsity and tractability. Improvements vs. the greedy algorithm, which is often at par with state-of-the-art techniques, reaches up to 24% in terms of variance while solving real world problems with 10,000s of variables and support cardinality of 100s in minutes. We also apply GeoSPCA in a face recognition problem yielding more than 10% improvement vs. other PCA based technique such as structured sparse PCA.},
 author = {Dimitris Bertsimas and Driss Lahlou Kitane},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W4306295009},
 pages = {1--33},
 title = {Sparse PCA: a Geometric Approach},
 url = {http://jmlr.org/papers/v24/22-0088.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0099,
 abstract = {Learning from multiple related tasks by knowledge sharing and transfer has become increasingly relevant over the last two decades. In order to successfully transfer information from one task to another, it is critical to understand the similarities and differences between the domains. In this paper, we introduce the notion of \emph{performance gap}, an intuitive and novel measure of the distance between learning tasks. Unlike existing measures which are used as tools to bound the difference of expected risks between tasks (e.g., $\mathcal{H}$-divergence or discrepancy distance), we theoretically show that the performance gap can be viewed as a data- and algorithm-dependent regularizer, which controls the model complexity and leads to finer guarantees. More importantly, it also provides new insights and motivates a novel principle for designing strategies for knowledge sharing and transfer: gap minimization. We instantiate this principle with two algorithms: 1. gapBoost, a novel and principled boosting algorithm that explicitly minimizes the performance gap between source and target domains for transfer learning; and 2. gapMTNN, a representation learning algorithm that reformulates gap minimization as semantic conditional matching for multitask learning. Our extensive evaluation on both transfer learning and multitask learning benchmark data sets shows that our methods outperform existing baselines.},
 author = {Boyu Wang and Jorge A. Mendez and Changjian Shui and Fan Zhou and Di Wu and Gezheng Xu and Christian Gagné and Eric Eaton},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W4221149695},
 pages = {1--57},
 title = {Gap Minimization for Knowledge Sharing and Transfer},
 url = {http://jmlr.org/papers/v24/22-0099.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0106,
 author = {Tobias Uelwer and Sebastian Konietzny and Alexander Oberstrass and Stefan Harmeling},
 journal = {Journal of Machine Learning Research},
 number = {332},
 pages = {1--28},
 title = {Learning Conditional Generative Models for Phase Retrieval},
 url = {http://jmlr.org/papers/v24/22-0106.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0119,
 abstract = {The elastic net combines lasso and ridge regression to fuse the sparsity property of lasso with the grouping property of ridge regression. The connections between ridge regression and gradient descent and between lasso and forward stagewise regression have previously been shown. Similar to how the elastic net generalizes lasso and ridge regression, we introduce elastic gradient descent, a generalization of gradient descent and forward stagewise regression. We theoretically analyze elastic gradient descent and compare it to the elastic net and forward stagewise regression. Parts of the analysis are based on elastic gradient flow, a piecewise analytical construction, obtained for elastic gradient descent with infinitesimal step size. We also compare elastic gradient descent to the elastic net on real and simulated data and show that it provides similar solution paths, but is several orders of magnitude faster. Compared to forward stagewise regression, elastic gradient descent selects a model that, although still sparse, provides considerably lower prediction and estimation errors.},
 author = {Oskar Allerbo and Johan Jonasson and Rebecka Jörnsten},
 journal = {Journal of Machine Learning Research},
 number = {277},
 openalex = {W4221147225},
 pages = {1--53},
 title = {Elastic Gradient Descent, an Iterative Optimization Method Approximating the Solution Paths of the Elastic Net},
 url = {http://jmlr.org/papers/v24/22-0119.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0131,
 abstract = {Learning problems commonly exhibit an interesting feedback mechanism wherein the population data reacts to competing decision makers' actions. This paper formulates a new game theoretic framework for this phenomenon, called "multi-player performative prediction". We focus on two distinct solution concepts, namely (i) performatively stable equilibria and (ii) Nash equilibria of the game. The latter equilibria are arguably more informative, but can be found efficiently only when the game is monotone. We show that under mild assumptions, the performatively stable equilibria can be found efficiently by a variety of algorithms, including repeated retraining and the repeated (stochastic) gradient method. We then establish transparent sufficient conditions for strong monotonicity of the game and use them to develop algorithms for finding Nash equilibria. We investigate derivative free methods and adaptive gradient algorithms wherein each player alternates between learning a parametric description of their distribution and gradient steps on the empirical risk. Synthetic and semi-synthetic numerical experiments illustrate the results.},
 author = {Adhyyan Narang and Evan Faulkner and Dmitriy Drusvyatskiy and Maryam Fazel and Lillian J. Ratliff},
 journal = {Journal of Machine Learning Research},
 number = {202},
 openalex = {W4223571424},
 pages = {1--56},
 title = {Multiplayer Performative Prediction: Learning in Decision-Dependent Games},
 url = {http://jmlr.org/papers/v24/22-0131.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0142,
 abstract = {The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus -- a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).},
 author = {Anna Hedström and Leander Weber and Daniel Krakowczyk and Dilyara Bareeva and Franz Motzkus and Wojciech Samek and Sebastian Lapuschkin and Marina M.-C. Höhne},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W4221159886},
 pages = {1--11},
 title = {Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond},
 url = {http://jmlr.org/papers/v24/22-0142.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0151,
 author = {Christoph Käding, and Jakob Runge,},
 journal = {Journal of Machine Learning Research},
 number = {278},
 pages = {1--144},
 title = {Distinguishing Cause and Effect in Bivariate Structural Causal Models: A Systematic Investigation},
 url = {http://jmlr.org/papers/v24/22-0151.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0153,
 author = {Lili Su and Jiaming Xu and Pengkun Yang},
 journal = {Journal of Machine Learning Research},
 number = {203},
 pages = {1--48},
 title = {A Non-parametric View of FedAvg and FedProx:Beyond Stationary Points},
 url = {http://jmlr.org/papers/v24/22-0153.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0169,
 abstract = {Population-based multi-agent reinforcement learning (PB-MARL) refers to the series of methods nested with reinforcement learning (RL) algorithms, which produces a self-generated sequence of tasks arising from the coupled population dynamics. By leveraging auto-curricula to induce a population of distinct emergent strategies, PB-MARL has achieved impressive success in tackling multi-agent tasks. Despite remarkable prior arts of distributed RL frameworks, PB-MARL poses new challenges for parallelizing the training frameworks due to the additional complexity of multiple nested workloads between sampling, training and evaluation involved with heterogeneous policy interactions. To solve these problems, we present MALib, a scalable and efficient computing framework for PB-MARL. Our framework is comprised of three key components: (1) a centralized task dispatching model, which supports the self-generated tasks and scalable training with heterogeneous policy combinations; (2) a programming architecture named Actor-Evaluator-Learner, which achieves high parallelism for both training and sampling, and meets the evaluation requirement of auto-curriculum learning; (3) a higher-level abstraction of MARL training paradigms, which enables efficient code reuse and flexible deployments on different distributed computing paradigms. Experiments on a series of complex tasks such as multi-agent Atari Games show that MALib achieves throughput higher than 40K FPS on a single machine with $32$ CPU cores; 5x speedup than RLlib and at least 3x speedup than OpenSpiel in multi-agent training tasks. MALib is publicly available at https://github.com/sjtu-marl/malib.},
 author = {Ming Zhou and Ziyu Wan and Hanjing Wang and Muning Wen and Runzhe Wu and Ying Wen and Yaodong Yang and Yong Yu and Jun Wang and Weinan Zhang},
 journal = {Journal of Machine Learning Research},
 number = {150},
 openalex = {W3196792970},
 pages = {1--12},
 title = {MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/22-0169.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0184,
 abstract = {Distributed learning has become a hot research topic, due to its wide application in cluster-based large-scale learning, federated learning, edge computing and so on. Most distributed learning methods assume no error and attack on the workers. However, many unexpected cases, such as communication error and even malicious attack, may happen in real applications. Hence, Byzantine learning (BL), which refers to distributed learning with attack or error, has recently attracted much attention. Most existing BL methods are synchronous, which will result in slow convergence when there exist heterogeneous workers. Furthermore, in some applications like federated learning and edge computing, synchronization cannot even be performed most of the time due to the online workers (clients or edge servers). Hence, asynchronous BL (ABL) is more general and practical than synchronous BL (SBL). To the best of our knowledge, there exist only two ABL methods. One of them cannot resist malicious attack. The other needs to store some training instances on the server, which has the privacy leak problem. In this paper, we propose a novel method, called buffered asynchronous stochastic gradient descent (BASGD), for BL. BASGD is an asynchronous method. Furthermore, BASGD has no need to store any training instances on the server, and hence can preserve privacy in ABL. BASGD is theoretically proved to have the ability of resisting against error and malicious attack. Moreover, BASGD has a similar theoretical convergence rate to that of vanilla asynchronous SGD (ASGD), with an extra constant variance. Empirical results show that BASGD can significantly outperform vanilla ASGD and other ABL baselines, when there exists error or attack on workers.},
 author = {Yi-Rui Yang and Wu-Jun Li},
 journal = {Journal of Machine Learning Research},
 number = {204},
 openalex = {W3007741598},
 pages = {1--62},
 title = {BASGD: Buffered Asynchronous SGD for Byzantine Learning},
 url = {http://jmlr.org/papers/v24/22-0184.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0189,
 abstract = {We present L0Learn: an open-source package for sparse linear regression and classification using $\ell_0$ regularization. L0Learn implements scalable, approximate algorithms, based on coordinate descent and local combinatorial optimization. The package is built using C++ and has user-friendly R and Python interfaces. L0Learn can address problems with millions of features, achieving competitive run times and statistical performance with state-of-the-art sparse learning packages. L0Learn is available on both CRAN and GitHub (https://cran.r-project.org/package=L0Learn and https://github.com/hazimehh/L0Learn).},
 author = {Hussein Hazimeh and Rahul Mazumder and Tim Nonet},
 journal = {Journal of Machine Learning Research},
 number = {205},
 openalex = {W4221152715},
 pages = {1--8},
 title = {L0Learn: A Scalable Package for Sparse Learning using L0 Regularization},
 url = {http://jmlr.org/papers/v24/22-0189.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0202,
 abstract = {Shapley values, which were originally designed to assign attributions to individual players in coalition games, have become a commonly used approach in explainable machine learning to provide attributions to input features for black-box machine learning models. A key attraction of Shapley values is that they uniquely satisfy a very natural set of axiomatic properties. However, extending the Shapley value to assigning attributions to interactions rather than individual players, an interaction index, is non-trivial: as the natural set of axioms for the original Shapley values, extended to the context of interactions, no longer specify a unique interaction index. Many proposals thus introduce additional less ''natural'' axioms, while sacrificing the key axiom of efficiency, in order to obtain unique interaction indices. In this work, rather than introduce additional conflicting axioms, we adopt the viewpoint of Shapley values as coefficients of the most faithful linear approximation to the pseudo-Boolean coalition game value function. By extending linear to $\ell$-order polynomial approximations, we can then define the general family of faithful interaction indices. We show that by additionally requiring the faithful interaction indices to satisfy interaction-extensions of the standard individual Shapley axioms (dummy, symmetry, linearity, and efficiency), we obtain a unique Faithful Shapley Interaction index, which we denote Faith-Shap, as a natural generalization of the Shapley value to interactions. We then provide some illustrative contrasts of Faith-Shap with previously proposed interaction indices, and further investigate some of its interesting algebraic properties. We further show the computational efficiency of computing Faith-Shap, together with some additional qualitative insights, via some illustrative experiments.},
 author = {Che-Ping Tsai and Chih-Kuan Yeh and Pradeep Ravikumar},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W4226137384},
 pages = {1--42},
 title = {Faith-Shap: The Faithful Shapley Interaction Index},
 url = {http://jmlr.org/papers/v24/22-0202.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0203,
 author = {Han Zhong and Zhuoran Yang and Zhaoran Wang and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {35},
 pages = {1--52},
 title = {Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational  Followers?},
 url = {http://jmlr.org/papers/v24/22-0203.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0210,
 author = {Chao Xu and Hong Tao and Jing Zhang and Dewen Hu and Chenping Hou},
 journal = {Journal of Machine Learning Research},
 number = {36},
 pages = {1--48},
 title = {Label Distribution Changing Learning with Sample Space Expanding},
 url = {http://jmlr.org/papers/v24/22-0210.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0214,
 abstract = {We consider the statistical inference for noisy incomplete binary (or 1-bit) matrix. Despite the importance of uncertainty quantification to matrix completion, most of the categorical matrix completion literature focuses on point estimation and prediction. This paper moves one step further toward the statistical inference for binary matrix completion. Under a popular nonlinear factor analysis model, we obtain a point estimator and derive its asymptotic normality. Moreover, our analysis adopts a flexible missing-entry design that does not require a random sampling scheme as required by most of the existing asymptotic results for matrix completion. Under reasonable conditions, the proposed estimator is statistically efficient and optimal in the sense that the Cramer-Rao lower bound is achieved asymptotically for the model parameters. Two applications are considered, including (1) linking two forms of an educational test and (2) linking the roll call voting records from multiple years in the United States Senate. The first application enables the comparison between examinees who took different test forms, and the second application allows us to compare the liberal-conservativeness of senators who did not serve in the Senate at the same time.},
 author = {Yunxiao Chen and Chengcheng Li and Jing Ouyang and Gongjun Xu},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W4317664627},
 pages = {1--66},
 title = {Statistical Inference for Noisy Incomplete Binary Matrix},
 url = {http://jmlr.org/papers/v24/22-0214.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0218,
 abstract = {We study the problem of Online Convex Optimization (OCO) with memory, which allows loss functions to depend on past decisions and thus captures temporal effects of learning problems. In this paper, we introduce dynamic policy regret as the performance measure to design algorithms robust to non-stationary environments, which competes algorithms' decisions with a sequence of changing comparators. We propose a novel algorithm for OCO with memory that provably enjoys an optimal dynamic policy regret in terms of time horizon, non-stationarity measure, and memory length. The key technical challenge is how to control the switching cost, the cumulative movements of player's decisions, which is neatly addressed by a novel switching-cost-aware online ensemble approach equipped with a new meta-base decomposition of dynamic policy regret and a careful design of meta-learner and base-learner that explicitly regularizes the switching cost. The results are further applied to tackle non-stationarity in online non-stochastic control (Agarwal et al., 2019), i.e., controlling a linear dynamical system with adversarial disturbance and convex cost functions. We derive a novel gradient-based controller with dynamic policy regret guarantees, which is the first controller provably competitive to a sequence of changing policies for online non-stochastic control.},
 author = {Peng Zhao and Yu-Hu Yan and Yu-Xiang Wang and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {206},
 openalex = {W3128863975},
 pages = {1--70},
 title = {Non-stationary Online Learning with Memory and Non-stochastic Control},
 url = {http://jmlr.org/papers/v24/22-0218.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0227,
 abstract = {A ridge is a function that is characterized by a one-dimensional profile (activation) and a multidimensional direction vector. Ridges appear in the theory of neural networks as functional descriptors of the effect of a neuron, with the direction vector being encoded in the linear weights. In this paper, we investigate properties of the Radon transform in relation to ridges and to the characterization of neural networks. We introduce a broad category of hyper-spherical Banach subspaces (including the relevant subspace of measures) over which the back-projection operator is invertible. We also give conditions under which the back-projection operator is extendable to the full parent space with its null space being identifiable as a Banach complement. Starting from first principles, we then characterize the sampling functionals that are in the range of the filtered Radon transform. Next, we extend the definition of ridges for any distributional profile and determine their (filtered) Radon transform in full generality. Finally, we apply our formalism to clarify and simplify some of the results and proofs on the optimality of ReLU networks that have appeared in the literature.},
 author = {Michael Unser},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W4225343232},
 pages = {1--33},
 title = {Ridges, Neural Networks, and the Radon Transform},
 url = {http://jmlr.org/papers/v24/22-0227.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0233,
 abstract = {In this work, we study the performance of sub-gradient method (SubGM) on a natural nonconvex and nonsmooth formulation of low-rank matrix recovery with $\ell_1$-loss, where the goal is to recover a low-rank matrix from a limited number of measurements, a subset of which may be grossly corrupted with noise. We study a scenario where the rank of the true solution is unknown and over-estimated instead. The over-estimation of the rank gives rise to an over-parameterized model in which there are more degrees of freedom than needed. Such over-parameterization may lead to overfitting, or adversely affect the performance of the algorithm. We prove that a simple SubGM with small initialization is agnostic to both over-parameterization and noise in the measurements. In particular, we show that small initialization nullifies the effect of over-parameterization on the performance of SubGM, leading to an exponential improvement in its convergence rate. Moreover, we provide the first unifying framework for analyzing the behavior of SubGM under both outlier and Gaussian noise models, showing that SubGM converges to the true solution, even under arbitrarily large and arbitrarily dense noise values, and--perhaps surprisingly--even if the globally optimal solutions do not correspond to the ground truth. At the core of our results is a robust variant of restricted isometry property, called Sign-RIP, which controls the deviation of the sub-differential of the $\ell_1$-loss from that of an ideal, expected loss. As a byproduct of our results, we consider a subclass of robust low-rank matrix recovery with Gaussian measurements, and show that the number of required samples to guarantee the global convergence of SubGM is independent of the over-parameterized rank.},
 author = {Jianhao Ma and Salar Fattahi},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W4221150064},
 pages = {1--84},
 title = {Global Convergence of Sub-gradient Method for Robust Matrix Recovery: Small Initialization, Noisy Measurements, and Over-parameterization},
 url = {http://jmlr.org/papers/v24/22-0233.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0240,
 abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler--Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.},
 author = {Christopher Morris and Yaron Lipman and Haggai Maron and Bastian Rieck and Nils M. Kriege and Martin Grohe and Matthias Fey and Karsten Borgwardt},
 journal = {Journal of Machine Learning Research},
 number = {333},
 openalex = {W4226343493},
 pages = {1--59},
 title = {Weisfeiler and Leman go Machine Learning: The Story so far},
 url = {http://jmlr.org/papers/v24/22-0240.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0252,
 abstract = {In multivariate data analysis, it is often important to estimate a graph characterizing dependence among (p) variables. A popular strategy uses the non-zero entries in a (p\times p) covariance or precision matrix, typically requiring restrictive modeling assumptions for accurate graph recovery. To improve model robustness, we instead focus on estimating the {\em backbone} of the dependence graph. We use a spanning tree likelihood, based on a minimalist graphical model that is purposely overly-simplified. Taking a Bayesian approach, we place a prior on the space of trees and quantify uncertainty in the graphical model. In both theory and experiments, we show that this model does not require the population graph to be a spanning tree or the covariance to satisfy assumptions beyond positive-definiteness. The model accurately recovers the backbone of the population graph at a rate competitive with existing approaches but with better robustness. We show combinatorial properties of the spanning tree, which may be of independent interest, and develop an efficient Gibbs sampler for Bayesian inference. Analyzing electroencephalography data using a Hidden Markov Model with each latent state modeled by a spanning tree, we show that results are much more interpretable compared with popular alternatives.},
 author = {Leo L. Duan and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {397},
 openalex = {W3175869155},
 pages = {1--44},
 title = {Bayesian Spanning Tree: Estimating the Backbone of the Dependence Graph},
 url = {http://jmlr.org/papers/v24/22-0252.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0266,
 abstract = {Finite order Markov models are theoretically well-studied models for dependent discrete data. Despite their generality, application in empirical work when the order is large is rare. Practitioners avoid using higher order Markov models because (1) the number of parameters grow exponentially with the order and (2) the interpretation is often difficult. Mixture of transition distribution models (MTD) were introduced to overcome both limitations. MTD represent higher order Markov models as a convex mixture of single step Markov chains, reducing the number of parameters and increasing the interpretability. Nevertheless, in practice, estimation of MTD models with large orders are still limited because of curse of dimensionality and high algorithm complexity. Here, we prove that if only few lags are relevant we can consistently and efficiently recover the lags and estimate the transition probabilities of high-dimensional MTD models. The key innovation is a recursive procedure for the selection of the relevant lags of the model. Our results are based on (1) a new structural result of the MTD and (2) an improved martingale concentration inequality. We illustrate our method using simulations and a weather data.},
 author = {Guilherme Ost and Daniel Y. Takahashi},
 journal = {Journal of Machine Learning Research},
 number = {279},
 openalex = {W4225825951},
 pages = {1--54},
 title = {Sparse Markov Models for High-dimensional Inference},
 url = {http://jmlr.org/papers/v24/22-0266.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0283,
 abstract = {We consider the decentralized stochastic optimization problems, where a network of $n$ nodes, each owning a local cost function, cooperate to find a minimizer of the globally-averaged cost. A widely studied decentralized algorithm for this problem is decentralized SGD (D-SGD), in which each node averages only with its neighbors. D-SGD is efficient in single-iteration communication, but it is very sensitive to the network topology. For smooth objective functions, the transient stage (which measures the number of iterations the algorithm has to experience before achieving the linear speedup stage) of D-SGD is on the order of ${\Omega}(n/(1-\beta)^2)$ and $\Omega(n^3/(1-\beta)^4)$ for strongly and generally convex cost functions, respectively, where $1-\beta \in (0,1)$ is a topology-dependent quantity that approaches $0$ for a large and sparse network. Hence, D-SGD suffers from slow convergence for large and sparse networks. In this work, we study the non-asymptotic convergence property of the D$^2$/Exact-diffusion algorithm. By eliminating the influence of data heterogeneity between nodes, D$^2$/Exact-diffusion is shown to have an enhanced transient stage that is on the order of $\tilde{\Omega}(n/(1-\beta))$ and $\Omega(n^3/(1-\beta)^2)$ for strongly and generally convex cost functions, respectively. Moreover, when D$^2$/Exact-diffusion is implemented with gradient accumulation and multi-round gossip communications, its transient stage can be further improved to $\tilde{\Omega}(1/(1-\beta)^{\frac{1}{2}})$ and $\tilde{\Omega}(n/(1-\beta))$ for strongly and generally convex cost functions, respectively. These established results for D$^2$/Exact-Diffusion have the best (i.e., weakest) dependence on network topology to our knowledge compared to existing decentralized algorithms. We also conduct numerical simulations to validate our theories.},
 author = {Kun Yuan and Sulaiman A. Alghunaim and Xinmeng Huang},
 journal = {Journal of Machine Learning Research},
 number = {280},
 openalex = {W3160274442},
 pages = {1--53},
 title = {Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD},
 url = {http://jmlr.org/papers/v24/22-0283.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0291,
 abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
 author = {Mohammad Emtiyaz Khan and Håvard Rue},
 journal = {Journal of Machine Learning Research},
 number = {281},
 openalex = {W4294825671},
 pages = {1--46},
 title = {The Bayesian Learning Rule},
 url = {http://jmlr.org/papers/v24/22-0291.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0303,
 abstract = {In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable with kernel regression. In order to analyze the resulting regression errors, a novel stability result for kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit the regression function. We apply the general stability result to principal component analysis (PCA). Exploiting known estimates from the literature on both principal component analysis and kernel regression, we deduce convergence rates for the two-step procedure. The latter turns out to be particularly useful in a semi-supervised setting.},
 author = {Stephan Eckstein and Armin Iske and Mathias Trabs},
 journal = {Journal of Machine Learning Research},
 number = {334},
 openalex = {W4226388815},
 pages = {1--35},
 title = {Dimensionality Reduction and Wasserstein Stability for Kernel Regression},
 url = {http://jmlr.org/papers/v24/22-0303.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0305,
 abstract = {In recent years, hypergraph generalizations of many graph cut problems have been introduced and analyzed as a way to better explore and understand complex systems and datasets characterized by multiway relationships. Recent work has made use of a generalized hypergraph cut function which for a hypergraph $\mathcal{H} = (V,E)$ can be defined by associating each hyperedge $e \in E$ with a splitting function ${\bf w}_e$, which assigns a penalty to each way of separating the nodes of $e$. When each ${\bf w}_e$ is a submodular cardinality-based splitting function, meaning that ${\bf w}_e(S) = g(|S|)$ for some concave function $g$, previous work has shown that a generalized hypergraph cut problem can be reduced to a directed graph cut problem on an augmented node set. However, existing reduction procedures often result in a dense graph, even when the hypergraph is sparse, which leads to slow runtimes for algorithms that run on the reduced graph. 
We introduce a new framework of sparsifying hypergraph-to-graph reductions, where a hypergraph cut defined by submodular cardinality-based splitting functions is $(1+\varepsilon)$-approximated by a cut on a directed graph. Our techniques are based on approximating concave functions using piecewise linear curves. For $\varepsilon > 0$ we need at most $O(\varepsilon^{-1}|e| \log |e|)$ edges to reduce any hyperedge $e$, which leads to faster runtimes for approximating generalized hypergraph $s$-$t$ cut problems. For the machine learning heuristic of a clique splitting function, our approach requires only $O(|e| \varepsilon^{-1/2} \log \log \frac{1}{\varepsilon})$ edges. This sparsification leads to faster approximate min $s$-$t$ graph cut algorithms for certain classes of co-occurrence graphs. Finally, we apply our sparsification techniques to develop approximation algorithms for minimizing sums of cardinality-based submodular functions.},
 author = {Nate Veldt and Austin R. Benson and Jon Kleinberg},
 journal = {Journal of Machine Learning Research},
 number = {207},
 openalex = {W3042850937},
 pages = {1--50},
 title = {Augmented Sparsifiers for Generalized Hypergraph Cuts.},
 url = {http://jmlr.org/papers/v24/22-0305.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0310,
 abstract = {We consider the problem of computing an equilibrium in a class of \textit{nonlinear generalized Nash equilibrium problems (NGNEPs)} in which the strategy sets for each player are defined by equality and inequality constraints that may depend on the choices of rival players. While the asymptotic global convergence and local convergence rates of algorithms to solve this problem have been extensively investigated, the analysis of nonasymptotic iteration complexity is still in its infancy. This paper presents two first-order algorithms -- based on the quadratic penalty method (QPM) and augmented Lagrangian method (ALM), respectively -- with an accelerated mirror-prox algorithm as the solver in each inner loop. We establish a global convergence guarantee for solving monotone and strongly monotone NGNEPs and provide nonasymptotic complexity bounds expressed in terms of the number of gradient evaluations. Experimental results demonstrate the efficiency of our algorithms in practice.},
 author = {Michael I. Jordan and Tianyi Lin and Manolis Zampetakis},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W4223443946},
 pages = {1--46},
 title = {First-Order Algorithms for Nonlinear Generalized Nash Equilibrium Problems},
 url = {http://jmlr.org/papers/v24/22-0310.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0313,
 abstract = {Communities are a common and widely studied structure in networks, typically under the assumption that the network is fully and correctly observed. In practice, network data are often collected by querying nodes about their connections. In some settings, all edges of a sampled node will be recorded, and in others, a node may be asked to name its connections. These sampling mechanisms introduce noise and bias which can obscure the community structure and invalidate assumptions underlying standard community detection methods. We propose a general model for a class of network sampling mechanisms based on recording edges via querying nodes, designed to improve community detection for network data collected in this fashion. We model edge sampling probabilities as a function of both individual preferences and community parameters, and show community detection can be performed by spectral clustering under this general class of models. We also propose, as a special case of the general framework, a parametric model for directed networks we call the nomination stochastic block model, which allows for meaningful parameter interpretations and can be fitted by the method of moments. Both spectral clustering and the method of moments in this case are computationally efficient and come with theoretical guarantees of consistency. We evaluate the proposed model in simulation studies on both unweighted and weighted networks and apply it to a faculty hiring dataset, discovering a meaningful hierarchy of communities among US business schools.},
 author = {Tianxi Li and Elizaveta Levina and Ji Zhu},
 journal = {Journal of Machine Learning Research},
 number = {282},
 openalex = {W3139421799},
 pages = {1--36},
 title = {Community models for networks observed through edge nominations},
 url = {http://jmlr.org/papers/v24/22-0313.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0315,
 abstract = {Solving an ill-posed linear inverse problem requires knowledge about the underlying signal model. In many applications, this model is a priori unknown and has to be learned from data. However, it is impossible to learn the model using observations obtained via a single incomplete measurement operator, as there is no information about the signal model in the nullspace of the operator, resulting in a chicken-and-egg problem: to learn the model we need reconstructed signals, but to reconstruct the signals we need to know the model. Two ways to overcome this limitation are using multiple measurement operators or assuming that the signal model is invariant to a certain group action. In this paper, we present necessary and sufficient sensing conditions for learning the signal model from measurement data alone which only depend on the dimension of the model and the number of operators or properties of the group action that the model is invariant to. As our results are agnostic of the learning algorithm, they shed light into the fundamental limitations of learning from incomplete data and have implications in a wide range set of practical algorithms, such as dictionary learning, matrix completion and deep neural networks.},
 author = {Julián Tachella and Dongdong Chen and Mike Davies},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W4221161322},
 pages = {1--45},
 title = {Sensing Theorems for Unsupervised Learning in Linear Inverse Problems},
 url = {http://jmlr.org/papers/v24/22-0315.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0320,
 abstract = {The prediction accuracy of machine learning methods is steadily increasing, but the calibration of their uncertainty predictions poses a significant challenge. Numerous works focus on obtaining well-calibrated predictive models, but less is known about reliably assessing model calibration. This limits our ability to know when algorithms for improving calibration have a real effect, and when their improvements are merely artifacts due to random noise in finite datasets. In this work, we consider detecting mis-calibration of predictive models using a finite validation dataset as a hypothesis testing problem. The null hypothesis is that the predictive model is calibrated, while the alternative hypothesis is that the deviation from calibration is sufficiently large. We find that detecting mis-calibration is only possible when the conditional probabilities of the classes are sufficiently smooth functions of the predictions. When the conditional class probabilities are H\"older continuous, we propose T-Cal, a minimax optimal test for calibration based on a debiased plug-in estimator of the $\ell_2$-Expected Calibration Error (ECE). We further propose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We verify our theoretical findings with a broad range of experiments, including with several popular deep neural net architectures and several standard post-hoc calibration methods. T-Cal is a practical general-purpose tool, which -- combined with classical tests for discrete-valued predictors -- can be used to test the calibration of virtually any probabilistic classification method.},
 author = {Donghwan Lee and Xinmeng Huang and Hamed Hassani and Edgar Dobriban},
 journal = {Journal of Machine Learning Research},
 number = {335},
 openalex = {W4221164009},
 pages = {1--72},
 title = {T-Cal: An optimal test for the calibration of predictive models},
 url = {http://jmlr.org/papers/v24/22-0320.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0330,
 author = {Shaun Fallat and David Kirkpatrick and Hans U. Simon and Abolghasem Soltani and Sandra Zilles},
 journal = {Journal of Machine Learning Research},
 number = {40},
 pages = {1--33},
 title = {On Batch Teaching Without Collusion},
 url = {http://jmlr.org/papers/v24/22-0330.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0331,
 abstract = {Recent work in the matrix completion literature has shown that prior knowledge of a matrix's row and column spaces can be successfully incorporated into reconstruction programs to substantially benefit matrix recovery. This paper proposes a novel methodology that exploits more general forms of known matrix structure in terms of subspaces. The work derives reconstruction error bounds that are informative in practice, providing insight to previous approaches in the literature while introducing novel programs that severely reduce sampling complexity. The main result shows that a family of weighted nuclear norm minimization programs incorporating a $M_1 r$-dimensional subspace of $n\times n$ matrices (where $M_1\geq 1$ conveys structural properties of the subspace) allow accurate approximation of a rank $r$ matrix aligned with the subspace from a near-optimal number of observed entries (within a logarithmic factor of $M_1 r)$. The result is robust, where the error is proportional to measurement noise, applies to full rank matrices, and reflects degraded output when erroneous prior information is utilized. Numerical experiments are presented that validate the theoretical behavior derived for several example weighted programs.},
 author = {Oscar López},
 journal = {Journal of Machine Learning Research},
 number = {283},
 openalex = {W4226197702},
 pages = {1--40},
 title = {Near-Optimal Weighted Matrix Completion},
 url = {http://jmlr.org/papers/v24/22-0331.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0337,
 author = {Xu Han and Xiaohui Chen and Francisco J. R. Ruiz and Li-Ping Liu},
 journal = {Journal of Machine Learning Research},
 number = {97},
 pages = {1--30},
 title = {Fitting Autoregressive Graph Generative Models through Maximum Likelihood Estimation},
 url = {http://jmlr.org/papers/v24/22-0337.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0339,
 abstract = {Supervised classification techniques use training samples to learn a classification rule with small expected 0-1 loss (error probability). Conventional methods enable tractable learning and provide out-of-sample generalization by using surrogate losses instead of the 0-1 loss and considering specific families of rules (hypothesis classes). This paper presents minimax risk classifiers (MRCs) that minize the worst-case 0-1 loss with respect to uncertainty sets of distributions that can include the underlying distribution, with a tunable confidence. We show that MRCs can provide tight performance guarantees at learning and are strongly universally consistent using feature mappings given by characteristic kernels. The paper also proposes efficient optimization techniques for MRC learning and shows that the methods presented can provide accurate classification together with tight performance guarantees in practice.},
 author = {Santiago Mazuelas and Mauricio Romero and Peter Grunwald},
 journal = {Journal of Machine Learning Research},
 number = {208},
 openalex = {W4224910604},
 pages = {1--48},
 title = {Minimax risk classifiers with 0-1 loss},
 url = {http://jmlr.org/papers/v24/22-0339.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0341,
 abstract = {Offline policy evaluation is a fundamental statistical problem in reinforcement learning that involves estimating the value function of some decision-making policy given data collected by a potentially different policy. In order to tackle problems with complex, high-dimensional observations, there has been significant interest from theoreticians and practitioners alike in understanding the possibility of function approximation in reinforcement learning. Despite significant study, a sharp characterization of when we might expect offline policy evaluation to be tractable, even in the simplest setting of linear function approximation, has so far remained elusive, with a surprising number of strong negative results recently appearing in the literature. In this work, we identify simple control-theoretic and linear-algebraic conditions that are necessary and sufficient for classical methods, in particular Fitted Q-iteration (FQI) and least squares temporal difference learning (LSTD), to succeed at offline policy evaluation. Using this characterization, we establish a precise hierarchy of regimes under which these estimators succeed. We prove that LSTD works under strictly weaker conditions than FQI. Furthermore, we establish that if a problem is not solvable via LSTD, then it cannot be solved by a broad class of linear estimators, even in the limit of infinite data. Taken together, our results provide a complete picture of the behavior of linear estimators for offline policy evaluation, unify previously disparate analyses of canonical algorithms, and provide significantly sharper notions of the underlying statistical complexity of offline policy evaluation.},
 author = {Juan C. Perdomo and Akshay Krishnamurthy and Peter Bartlett and Sham Kakade},
 journal = {Journal of Machine Learning Research},
 number = {284},
 openalex = {W4221141176},
 pages = {1--50},
 title = {A Complete Characterization of Linear Estimators for Offline Policy Evaluation},
 url = {http://jmlr.org/papers/v24/22-0341.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0347,
 author = {Baijiong Lin and Yu Zhang},
 journal = {Journal of Machine Learning Research},
 number = {209},
 pages = {1--7},
 title = {LibMTL: A Python Library for Deep Multi-Task Learning},
 url = {http://jmlr.org/papers/v24/22-0347.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0349,
 abstract = {In this paper, a unified batch-online learning approach is introduced to learn a linear representation of nonlinear system dynamics using the Koopman operator. The presented system modeling approach leverages a novel incremental Koopman-based update law that retrieves a mini-batch of samples stored in a memory to not only minimizes the instantaneous Koopman operator's identification errors but also the identification errors for the batch of retrieved samples. Discontinuous modifications of gradient flows are presented for the online update law to assure finite-time convergence under easy-to-verify conditions defined on the batch of data. Therefore, this unified online-batch framework allows performing joint sample- and time-domain analysis for converging the Koopman operator's parameters. More specifically, it is shown that if the collected mini-batch of samples guarantees a rank condition, then finite-time guarantee in the time domain can be certified and the settling time depends on the quality of collected samples being reused in the update law. Moreover, the efficiency of the proposed Koopman-based update law is further analyzed by showing that the identification regret in continuous time grows sub-linearly with time. Furthermore, to avoid learning corrupted dynamics due to the selection of an inappropriate set of Koopman observables, a higher-layer meta learner employs a discrete Bayesian optimization algorithm to obtain the best library of observable functions for the operator. Since finite-time convergence of the Koopman model for each set of observable is guaranteed under a rank condition on stored data, the fitness of each set of observables can be obtained based on the identification error on the stored samples in the proposed framework and even without implementing any controller based on the learned system.},
 author = {Majid Mazouchi and Subramanya Nageshrao and Hamidreza Modares},
 journal = {Journal of Machine Learning Research},
 number = {336},
 openalex = {W3160881809},
 pages = {1--35},
 title = {Finite-time Koopman Identifier: A Unified Batch-online Learning Framework for Joint Learning of Koopman Structure and Parameters.},
 url = {http://jmlr.org/papers/v24/22-0349.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0359,
 abstract = {Latent variable models (LVMs) represent observed variables by parameterized functions of latent variables. Prominent examples of LVMs for unsupervised learning are probabilistic PCA or probabilistic SC which both assume a weighted linear summation of the latents to determine the mean of a Gaussian distribution for the observables. In many cases, however, observables do not follow a Gaussian distribution. For unsupervised learning, LVMs which assume specific non-Gaussian observables have therefore been considered. Already for specific choices of distributions, parameter optimization is challenging and only a few previous contributions considered LVMs with more generally defined observable distributions. Here, we consider LVMs that are defined for a range of different distributions, i.e., observables can follow any (regular) distribution of the exponential family. The novel class of LVMs presented is defined for binary latents, and it uses maximization in place of summation to link the latents to observables. To derive an optimization procedure, we follow an EM approach for maximum likelihood parameter estimation. We show that a set of very concise parameter update equations can be derived which feature the same functional form for all exponential family distributions. The derived generic optimization can consequently be applied to different types of metric data as well as to different types of discrete data. Also, the derived optimization equations can be combined with a recently suggested variational acceleration which is likewise generically applicable to the LVMs considered here. So, the combination maintains generic and direct applicability of the derived optimization procedure, but, crucially, enables efficient scalability. We numerically verify our analytical results and discuss some potential applications such as learning of variance structure, noise type estimation and denoising.},
 author = {Hamid Mousavi and Jakob Drefs and Florian Hirschberger and Jörg Lücke},
 journal = {Journal of Machine Learning Research},
 number = {285},
 openalex = {W4389911153},
 pages = {1--59},
 title = {Generic Unsupervised Optimization for a Latent Variable Model With Exponential Family Observables},
 url = {http://jmlr.org/papers/v24/22-0359.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0360,
 abstract = {Vector autoregression has been widely used for modeling and analysis of multivariate time series data. In high-dimensional settings, model parameter regularization schemes inducing sparsity yield interpretable models and achieved good forecasting performance. However, in many data applications, such as those in neuroscience, the Granger causality graph estimates from existing vector autoregression methods tend to be quite dense and difficult to interpret, unless one compromises on the goodness-of-fit. To address this issue, this paper proposes to incorporate a commonly used structural assumption -- that the ground-truth graph should be largely connected, in the sense that it should only contain at most a few components. We take a Bayesian approach and develop a novel tree-rank prior distribution for the regression coefficients. Specifically, this prior distribution forces the non-zero coefficients to appear only on the union of a few spanning trees. Since each spanning tree connects $p$ nodes with only $(p-1)$ edges, it effectively achieves both high connectivity and high sparsity. We develop a computationally efficient Gibbs sampler that is scalable to large sample size and high dimension. In analyzing test-retest functional magnetic resonance imaging data, our model produces a much more interpretable graph estimate, compared to popular existing approaches. In addition, we show appealing properties of this new method, such as efficient computation, mild stability conditions and posterior consistency.},
 author = {Leo L Duan and Zeyu Yuwen and George Michailidis and Zhengwu Zhang},
 journal = {Journal of Machine Learning Research},
 number = {286},
 openalex = {W4226443106},
 pages = {1--35},
 title = {Low Tree-Rank Bayesian Vector Autoregression Model},
 url = {http://jmlr.org/papers/v24/22-0360.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0364,
 abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
 author = {Yoshua Bengio and Salem Lahlou and Tristan Deleu and Edward J. Hu and Mo Tiwari and Emmanuel Bengio},
 journal = {Journal of Machine Learning Research},
 number = {210},
 openalex = {W4292200101},
 pages = {1--55},
 title = {GFlowNet Foundations},
 url = {http://jmlr.org/papers/v24/22-0364.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0365,
 abstract = {High-dimensional spatio-temporal dynamics can often be encoded in a low-dimensional subspace. Engineering applications for modeling, characterization, design, and control of such large-scale systems often rely on dimensionality reduction to make solutions computationally tractable in real-time. Common existing paradigms for dimensionality reduction include linear methods, such as the singular value decomposition (SVD), and nonlinear methods, such as variants of convolutional autoencoders (CAE). However, these encoding techniques lack the ability to efficiently represent the complexity associated with spatio-temporal data, which often requires variable geometry, non-uniform grid resolution, adaptive meshing, and/or parametric dependencies. To resolve these practical engineering challenges, we propose a general framework called Neural Implicit Flow (NIF) that enables a mesh-agnostic, low-rank representation of large-scale, parametric, spatial-temporal data. NIF consists of two modified multilayer perceptrons (MLPs): (i) ShapeNet, which isolates and represents the spatial complexity, and (ii) ParameterNet, which accounts for any other input complexity, including parametric dependencies, time, and sensor measurements. We demonstrate the utility of NIF for parametric surrogate modeling, enabling the interpretable representation and compression of complex spatio-temporal dynamics, efficient many-spatial-query tasks, and improved generalization performance for sparse reconstruction.},
 author = {Shaowu Pan and Steven L. Brunton and J. Nathan Kutz},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W4223543806},
 pages = {1--60},
 title = {Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data},
 url = {http://jmlr.org/papers/v24/22-0365.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0367,
 abstract = {We consider high-dimensional multiclass classification by sparse multinomial logistic regression. Unlike binary classification, in the multiclass setup one can think about an entire spectrum of possible notions of sparsity associated with different structural assumptions on the regression coefficients matrix. We propose a computationally feasible feature selection procedure based on penalized maximum likelihood with convex penalties capturing a specific type of sparsity at hand. In particular, we consider global sparsity, double row-wise sparsity, and low-rank sparsity, and show that with the properly chosen tuning parameters the derived plug-in classifiers attain the minimax generalization error bounds (in terms of misclassification excess risk) within the corresponding classes of multiclass sparse linear classifiers. The developed approach is general and can be adapted to other types of sparsity as well.},
 author = {Tomer Levy and Felix Abramovich},
 journal = {Journal of Machine Learning Research},
 number = {151},
 openalex = {W4224010979},
 pages = {1--35},
 title = {Generalization Error Bounds for Multiclass Sparse Linear Classifiers},
 url = {http://jmlr.org/papers/v24/22-0367.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0371,
 abstract = {We consider the problem of testing for a difference in means between clusters of observations identified via k-means clustering. In this setting, classical hypothesis tests lead to an inflated Type I error rate. In recent work, Gao et al. (2022) considered a related problem in the context of hierarchical clustering. Unfortunately, their solution is highly-tailored to the context of hierarchical clustering, and thus cannot be applied in the setting of k-means clustering. In this paper, we propose a p-value that conditions on all of the intermediate clustering assignments in the k-means algorithm. We show that the p-value controls the selective Type I error for a test of the difference in means between a pair of clusters obtained using k-means clustering in finite samples, and can be efficiently computed. We apply our proposal on hand-written digits data and on single-cell RNA-sequencing data.},
 author = {Yiqun T. Chen and Daniela M. Witten},
 journal = {Journal of Machine Learning Research},
 number = {152},
 openalex = {W4224313447},
 pages = {1--41},
 title = {Selective inference for k-means clustering},
 url = {http://jmlr.org/papers/v24/22-0371.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0382,
 abstract = {Many asymptotically minimax procedures for function estimation often rely on somewhat arbitrary and restrictive assumptions such as isotropy or spatial homogeneity. This work enhances the theoretical understanding of Bayesian additive regression trees under substantially relaxed smoothness assumptions. We provide a comprehensive study of asymptotic optimality and posterior contraction of Bayesian forests when the regression function has anisotropic smoothness that possibly varies over the function domain. The regression function can also be possibly discontinuous. We introduce a new class of sparse {\em piecewise heterogeneous anisotropic} H\"{o}lder functions and derive their minimax lower bound of estimation in high-dimensional scenarios under the $L_2$-loss. We then find that the Bayesian tree priors, coupled with a Dirichlet subset selection prior for sparse estimation in high-dimensional scenarios, adapt to unknown heterogeneous smoothness, discontinuity, and sparsity. These results show that Bayesian forests are uniquely suited for more general estimation problems that would render other default machine learning tools, such as Gaussian processes, suboptimal. Our numerical study shows that Bayesian forests often outperform other competitors such as random forests and deep neural networks, which are believed to work well for discontinuous or complicated smooth functions. Beyond nonparametric regression, we also examined posterior contraction of Bayesian forests for density estimation and binary classification using the technique developed in this study.},
 author = {Seonghyun Jeong and Veronika Rockova},
 journal = {Journal of Machine Learning Research},
 number = {337},
 openalex = {W4287689322},
 pages = {1--65},
 title = {The art of BART: Minimax optimality over nonhomogeneous smoothness in high dimension},
 url = {http://jmlr.org/papers/v24/22-0382.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0384,
 abstract = {Invertible neural networks (INNs) are neural network architectures with invertibility by design. Thanks to their invertibility and the tractability of Jacobian, INNs have various machine learning applications such as probabilistic modeling, generative modeling, and representation learning. However, their attractive properties often come at the cost of restricting the layer designs, which poses a question on their representation power: can we use these models to approximate sufficiently diverse functions? To answer this question, we have developed a general theoretical framework to investigate the representation power of INNs, building on a structure theorem of differential geometry. The framework simplifies the approximation problem of diffeomorphisms, which enables us to show the universal approximation properties of INNs. We apply the framework to two representative classes of INNs, namely Coupling-Flow-based INNs (CF-INNs) and Neural Ordinary Differential Equations (NODEs), and elucidate their high representation power despite the restrictions on their architectures.},
 author = {Isao Ishikawa and Takeshi Teshima and Koichi Tojo and Kenta Oono and Masahiro Ikeda and Masashi Sugiyama},
 journal = {Journal of Machine Learning Research},
 number = {287},
 openalex = {W4224910142},
 pages = {1--68},
 title = {Universal approximation property of invertible neural networks},
 url = {http://jmlr.org/papers/v24/22-0384.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0387,
 abstract = {We study the non-stationary stochastic multi-armed bandit problem, where the reward statistics of each arm may change several times during the course of learning. The performance of a learning algorithm is evaluated in terms of their dynamic regret, which is defined as the difference between the expected cumulative reward of an agent choosing the optimal arm in every time step and the cumulative reward of the learning algorithm. One way to measure the hardness of such environments is to consider how many times the identity of the optimal arm can change. We propose a method that achieves, in $K$-armed bandit problems, a near-optimal $\widetilde O(\sqrt{K N(S+1)})$ dynamic regret, where $N$ is the time horizon of the problem and $S$ is the number of times the identity of the optimal arm changes, without prior knowledge of $S$. Previous works for this problem obtain regret bounds that scale with the number of changes (or the amount of change) in the reward functions, which can be much larger, or assume prior knowledge of $S$ to achieve similar bounds.},
 author = {Yasin Abbasi-Yadkori and András György and Nevena Lazić},
 journal = {Journal of Machine Learning Research},
 number = {288},
 openalex = {W4221140173},
 pages = {1--37},
 title = {A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits},
 url = {http://jmlr.org/papers/v24/22-0387.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0410,
 abstract = {Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, and also have application to contrastive learning, where we empirically demonstrate the ability of such algorithms to obtain high robust accuracy with low abstention rates. Our model is also motivated by strategic classification, where entities being classified aim to manipulate their observable features to produce a preferred classification, and we provide new insights into that area as well.},
 author = {Maria-Florina Balcan and Avrim Blum and Dravyansh Sharma and Hongyang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W4307533869},
 pages = {1--43},
 title = {An Analysis of Robustness of Non-Lipschitz Networks},
 url = {http://jmlr.org/papers/v24/22-0410.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0411,
 abstract = {We study two-layer neural networks in the mean field limit, where the number of neurons tends to infinity. In this regime, the optimization over the neuron parameters becomes the optimization over the probability measures, and by adding an entropic regularizer, the minimizer of the problem is identified as a fixed point. We propose a novel training algorithm named entropic fictitious play, inspired by the classical fictitious play in game theory for learning Nash equilibriums, to recover this fixed point, and the algorithm exhibits a two-loop iteration structure. Exponential convergence is proved in this paper and we also verify our theoretical results by simple numerical examples.},
 author = {Fan Chen and Zhenjie Ren and Songbo Wang},
 journal = {Journal of Machine Learning Research},
 number = {211},
 openalex = {W4221147757},
 pages = {1--36},
 title = {Entropic Fictitious Play for Mean Field Optimization Problem},
 url = {http://jmlr.org/papers/v24/22-0411.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0415,
 abstract = {Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10-100 compression rates), we find that it plays an increasing role for thinner subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective sparsity to reevaluate several recent pruning algorithms on common benchmark architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their absolute and relative performance changes dramatically in this new and more appropriate framework. To aim for effective, rather than direct, sparsity, we develop a low-cost extension to most pruning algorithms. Further, equipped with effective sparsity as a reference frame, we partially reconfirm that random pruning with appropriate sparsity allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization [Su et al., 2020]. In response to this observation, using a simple analogy of pressure distribution in coupled cylinders from physics, we design novel layerwise sparsity quotas that outperform all existing baselines in the context of random pruning.},
 author = {Artem Vysogorets and Julia Kempe},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W4287100587},
 pages = {1--23},
 title = {Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity},
 url = {http://jmlr.org/papers/v24/22-0415.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0436,
 abstract = {In mixture modeling and clustering applications, the number of components and clusters is often not known. A stick-breaking mixture model, such as the Dirichlet process mixture model, is an appealing construction that assumes infinitely many components, while shrinking the weights of most of the unused components to near zero. However, it is well-known that this shrinkage is inadequate: even when the component distribution is correctly specified, spurious weights appear and give an inconsistent estimate of the number of clusters. In this article, we propose a simple solution: when breaking each mixture weight stick into two pieces, the length of the second piece is multiplied by a quasi-Bernoulli random variable, taking value one or a small constant close to zero. This effectively creates a soft-truncation and further shrinks the unused weights. Asymptotically, we show that as long as this small constant diminishes to zero at a rate faster than $o(1/n^2)$, with $n$ the sample size, the posterior distribution will converge to the true number of clusters. In comparison, we rigorously explore Dirichlet process mixture models using a concentration parameter that is either constant or rapidly diminishes to zero -- both of which lead to inconsistency for the number of clusters. Our proposed model is easy to implement, requiring only a small modification of a standard Gibbs sampler for mixture models. In simulations and a data application of clustering brain networks, our proposed method recovers the ground-truth number of clusters, and leads to a small number of clusters.},
 author = {Cheng Zeng and Jeffrey W Miller and Leo L Duan},
 journal = {Journal of Machine Learning Research},
 number = {153},
 openalex = {W4287686269},
 pages = {1--32},
 title = {Consistent Model-based Clustering: using the Quasi-Bernoulli Stick-Breaking Process},
 url = {http://jmlr.org/papers/v24/22-0436.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0440,
 abstract = {Federated learning (FL) is a machine learning field in which researchers try to facilitate model learning process among multiparty without violating privacy protection regulations. Considerable effort has been invested in FL optimization and communication related researches. In this work, we introduce \texttt{FedLab}, a lightweight open-source framework for FL simulation. The design of \texttt{FedLab} focuses on FL algorithm effectiveness and communication efficiency. Also, \texttt{FedLab} is scalable in different deployment scenario. We hope \texttt{FedLab} could provide flexible API as well as reliable baseline implementations, and relieve the burden of implementing novel approaches for researchers in FL community.},
 author = {Dun Zeng and Siqi Liang and Xiangjing Hu and Hui Wang and Zenglin Xu},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W3184328775},
 pages = {1--7},
 title = {FedLab: A Flexible Federated Learning Framework},
 url = {http://jmlr.org/papers/v24/22-0440.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0449,
 abstract = {Influence estimation analyzes how changes to the training data can lead to different model predictions; this analysis can help us better understand these predictions, the models making those predictions, and the data sets they're trained on. However, most influence-estimation techniques are designed for deep learning models with continuous parameters. Gradient-boosted decision trees (GBDTs) are a powerful and widely-used class of models; however, these models are black boxes with opaque decision-making processes. In the pursuit of better understanding GBDT predictions and generally improving these models, we adapt recent and popular influence-estimation methods designed for deep learning models to GBDTs. Specifically, we adapt representer-point methods and TracIn, denoting our new methods TREX and BoostIn, respectively; source code is available at https://github.com/jjbrophy47/tree_influence. We compare these methods to LeafInfluence and other baselines using 5 different evaluation measures on 22 real-world data sets with 4 popular GBDT implementations. These experiments give us a comprehensive overview of how different approaches to influence estimation work in GBDT models. We find BoostIn is an efficient influence-estimation method for GBDTs that performs equally well or better than existing work while being four orders of magnitude faster. Our evaluation also suggests the gold-standard approach of leave-one-out (LOO) retraining consistently identifies the single-most influential training example but performs poorly at finding the most influential set of training examples for a given target prediction.},
 author = {Jonathan Brophy and Zayd Hammoudeh and Daniel Lowd},
 journal = {Journal of Machine Learning Research},
 number = {154},
 openalex = {W4225424134},
 pages = {1--48},
 title = {Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees},
 url = {http://jmlr.org/papers/v24/22-0449.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0479,
 abstract = {Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks, SNGP outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning. Code is open-sourced at https://github.com/google/uncertainty-baselines},
 author = {Jeremiah Zhe Liu and Shreyas Padhy and Jie Ren and Zi Lin and Yeming Wen and Ghassen Jerfel and Zachary Nado and Jasper Snoek and Dustin Tran and Balaji Lakshminarayanan},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W4225373549},
 pages = {1--63},
 title = {A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness},
 url = {http://jmlr.org/papers/v24/22-0479.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0491,
 abstract = {The leaky ReLU network with a group sparse regularization term has been widely used in the recent years. However, training such a network yields a nonsmooth nonconvex optimization problem and there exists a lack of approaches to compute a stationary point deterministically. In this paper, we first resolve the multi-layer composite term in the original optimization problem by introducing auxiliary variables and additional constraints. We show the new model has a nonempty and bounded solution set and its feasible set satisfies the Mangasarian-Fromovitz constraint qualification. Moreover, we show the relationship between the new model and the original problem. Remarkably, we propose an inexact augmented Lagrangian algorithm for solving the new model and show the convergence of the algorithm to a KKT point. Numerical experiments demonstrate that our algorithm is more efficient for training sparse leaky ReLU neural networks than some well-known algorithms.},
 author = {Wei Liu and Xin Liu and Xiaojun Chen},
 journal = {Journal of Machine Learning Research},
 number = {212},
 openalex = {W4280495644},
 pages = {1--43},
 title = {An Inexact Augmented Lagrangian Algorithm for Training Leaky ReLU Neural Network with Group Sparsity},
 url = {http://jmlr.org/papers/v24/22-0491.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0495,
 abstract = {Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. As we show in experiments, these breakthroughs make thought-to-be-infeasible strategies in active learning of causal structures and causal effect identification with regard to a Markov equivalence class practically applicable.},
 author = {Marcel Wienöbst and Max Bannach and Maciej Liśkiewicz},
 journal = {Journal of Machine Learning Research},
 number = {213},
 openalex = {W4320339879},
 pages = {1--45},
 title = {Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications},
 url = {http://jmlr.org/papers/v24/22-0495.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0496,
 abstract = {The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach outperforms several state-of-the-art task-sequential continual learning algorithms across multiple settings, occasionally even without retaining a memory that scales in size with the number of tasks.},
 author = {Sanket Vaibhav Mehta and Darshan Patil and Sarath Chandar and Emma Strubell},
 journal = {Journal of Machine Learning Research},
 number = {214},
 openalex = {W4225620948},
 pages = {1--50},
 title = {An Empirical Investigation of the Role of Pre-training in Lifelong Learning},
 url = {http://jmlr.org/papers/v24/22-0496.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0497,
 abstract = {Data depth, introduced by Tukey (1975), is an important tool in data science, robust statistics, and computational geometry. One chief barrier to its broader practical utility is that many common measures of depth are computationally intensive, requiring on the order of $n^d$ operations to exactly compute the depth of a single point within a data set of $n$ points in $d$-dimensional space. Often however, we are not directly interested in the absolute depths of the points, but rather in their relative ordering. For example, we may want to find the most central point in a data set (a generalized median), or to identify and remove all outliers (points on the fringe of the data set with low depth). With this observation, we develop a novel and instance-adaptive algorithm for adaptive data depth computation by reducing the problem of exactly computing $n$ depths to an $n$-armed stochastic multi-armed bandit problem which we can efficiently solve. We focus our exposition on simplicial depth, developed by Liu (1990), which has emerged as a promising notion of depth due to its interpretability and asymptotic properties. We provide general instance-dependent theoretical guarantees for our proposed algorithms, which readily extend to many other common measures of data depth including majority depth, Oja depth, and likelihood depth. When specialized to the case where the gaps in the data follow a power law distribution with parameter $\alpha<2$, we show that we can reduce the complexity of identifying the deepest point in the data set (the simplicial median) from $O(n^d)$ to $\tilde{O}(n^{d-(d-1)\alpha/2})$, where $\tilde{O}$ suppresses logarithmic factors. We corroborate our theoretical results with numerical experiments on synthetic data, showing the practical utility of our proposed methods.},
 author = {Tavor Baharav and Tze Leung Lai},
 journal = {Journal of Machine Learning Research},
 number = {155},
 openalex = {W4308755822},
 pages = {1--29},
 title = {Adaptive Data Depth via Multi-Armed Bandits},
 url = {http://jmlr.org/papers/v24/22-0497.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0501,
 abstract = {Modern approaches to supervised learning like deep neural networks (DNNs) typically implicitly assume that observed responses are statistically independent. In contrast, correlated data are prevalent in real-life large-scale applications, with typical sources of correlation including spatial, temporal and clustering structures. These correlations are either ignored by DNNs, or ad-hoc solutions are developed for specific use cases. We propose to use the mixed models framework to handle correlated data in DNNs. By treating the effects underlying the correlation structure as random effects, mixed models are able to avoid overfitted parameter estimates and ultimately yield better predictive performance. The key to combining mixed models and DNNs is using the Gaussian negative log-likelihood (NLL) as a natural loss function that is minimized with DNN machinery including stochastic gradient descent (SGD). Since NLL does not decompose like standard DNN loss functions, the use of SGD with NLL presents some theoretical and implementation challenges, which we address. Our approach which we call LMMNN is demonstrated to improve performance over natural competitors in various correlation scenarios on diverse simulated and real datasets. Our focus is on a regression setting and tabular datasets, but we also show some results for classification. Our code is available at https://github.com/gsimchoni/lmmnn.},
 author = {Giora Simchoni and Saharon Rosset},
 journal = {Journal of Machine Learning Research},
 number = {156},
 openalex = {W4281734508},
 pages = {1--57},
 title = {Integrating Random Effects in Deep Neural Networks},
 url = {http://jmlr.org/papers/v24/22-0501.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0503,
 abstract = {Gaussian processes are widely employed as versatile modeling and predictive tools in spatial statistics, functional data analysis, computer modeling and in diverse applications of machine learning. Such processes have been widely studied over Euclidean spaces, where they are constructed using specified covariance functions or covariograms. These functions specify valid stochastic processes that can be used to model complex dependencies in spatial statistics and other machine learning contexts. Valid (positive definite) covariance functions have been extensively studied for Gaussian processes on Euclidean spaces. Such investigations have focused, among other aspects, on the identifiability and consistency of covariance parameters as well as the problem of spatial interpolation and prediction within the fixed-domain or infill paradigm of asymptotic inference. This manuscript undertakes analogous theoretical developments for Gaussian processes constructed over Riemannian manifolds. We begin by establishing formal notions and conditions for the equivalence of two Gaussian random measures on compact manifolds. We build upon recently introduced Matern covariograms on compact Riemannian manifold, derive the microergodic parameter and formally establish the consistency of maximum likelihood estimators and the asymptotic optimality of the best linear unbiased predictor (BLUP). The circle and sphere are studied as two specific examples of compact Riemannian manifolds with numerical experiments that illustrate the theory.},
 author = {Didong Li and Wenpin Tang and Sudipto Banerjee},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W3149387970},
 pages = {1--26},
 title = {Fixed-Domain Inference for Gausian Processes with Mat\'ern Covariogram on Compact Riemannian Manifolds},
 url = {http://jmlr.org/papers/v24/22-0503.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0511,
 author = {Haili Zhang and Zhaobo Liu and Guohua Zou},
 journal = {Journal of Machine Learning Research},
 number = {215},
 pages = {1--59},
 title = {Least Squares Model Averaging for Distributed Data},
 url = {http://jmlr.org/papers/v24/22-0511.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0512,
 abstract = {We propose a novel multivariate nonparametric multiple change point detection method using classifiers. We construct a classifier log-likelihood ratio that uses class probability predictions to compare different change point configurations. We propose a computationally feasible search method that is particularly well suited for random forests, denoted by changeforest. However, the method can be paired with any classifier that yields class probability predictions, which we illustrate by also using a k-nearest neighbor classifier. We prove that it consistently locates change points in single change point settings when paired with a consistent classifier. Our proposed method changeforest achieves improved empirical performance in an extensive simulation study compared to existing multivariate nonparametric change point detection methods. An efficient implementation of our method is made available for R, Python, and Rust users in the changeforest software package.},
 author = {Malte Londschien and Peter Bühlmann and Solt Kovács},
 journal = {Journal of Machine Learning Research},
 number = {216},
 openalex = {W4280631074},
 pages = {1--45},
 title = {Random Forests for Change Point Detection},
 url = {http://jmlr.org/papers/v24/22-0512.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0520,
 abstract = {Data driven individualized decision making problems have received a lot of attentions in recent years. In particular, decision makers aim to determine the optimal Individualized Treatment Rule (ITR) so that the expected specified outcome averaging over heterogeneous patient-specific characteristics is maximized. Many existing methods deal with binary or a moderate number of treatment arms and may not take potential treatment effect structure into account. However, the effectiveness of these methods may deteriorate when the number of treatment arms becomes large. In this article, we propose GRoup Outcome Weighted Learning (GROWL) to estimate the latent structure in the treatment space and the optimal group-structured ITRs through a single optimization. In particular, for estimating group-structured ITRs, we utilize the Reinforced Angle based Multicategory Support Vector Machines (RAMSVM) to learn group-based decision rules under the weighted angle based multi-class classification framework. Fisher consistency, the excess risk bound, and the convergence rate of the value function are established to provide a theoretical guarantee for GROWL. Extensive empirical results in simulation studies and real data analysis demonstrate that GROWL enjoys better performance than several other existing methods.},
 author = {Haixu Ma and Donglin Zeng and Yufeng Liu},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W4385898254},
 pages = {1--48},
 title = {Learning Optimal Group-structured Individualized Treatment Rules with Many Treatments.},
 url = {http://jmlr.org/papers/v24/22-0520.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0522,
 author = {Huan Li and Zhouchen Lin},
 journal = {Journal of Machine Learning Research},
 number = {157},
 pages = {1--37},
 title = {Restarted Nonconvex Accelerated Gradient Descent:  No More Polylogarithmic Factor in the in the O(epsilon^(-7/4)) Complexity},
 url = {http://jmlr.org/papers/v24/22-0522.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0537,
 abstract = {This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian processes (MoGP) in the large-width limit. The behaviour of the neural network in this regime is very different from the GP regime. One obtains correlated outputs, with non-Gaussian distributions, possibly with heavy tails. Additionally, we show that, in this regime, the weights are compressible, and some nodes have asymptotically non-negligible contributions, therefore representing important hidden features. Many sparsity-promoting neural network models can be recast as special cases of our approach, and we discuss their infinite-width limits; we also present an asymptotic analysis of the pruning error. We illustrate some of the benefits of the MoGP regime over the GP regime in terms of representation learning and compressibility on simulated, MNIST and Fashion MNIST datasets.},
 author = {Hoil Lee and Fadhel Ayed and Paul Jung and Juho Lee and Hongseok Yang and Francois Caron},
 journal = {Journal of Machine Learning Research},
 number = {289},
 openalex = {W4280524072},
 pages = {1--78},
 title = {Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility},
 url = {http://jmlr.org/papers/v24/22-0537.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0555,
 abstract = {We introduce novel convergence results for asynchronous iterations that appear in the analysis of parallel and distributed optimization algorithms. The results are simple to apply and give explicit estimates for how the degree of asynchrony impacts the convergence rates of the iterates. Our results shorten, streamline and strengthen existing convergence proofs for several asynchronous optimization methods and allow us to establish convergence guarantees for popular algorithms that were thus far lacking a complete theoretical understanding. Specifically, we use our results to derive better iteration complexity bounds for proximal incremental aggregated gradient methods, to obtain tighter guarantees depending on the average rather than maximum delay for the asynchronous stochastic gradient descent method, to provide less conservative analyses of the speedup conditions for asynchronous block-coordinate implementations of Krasnoselskii-Mann iterations, and to quantify the convergence rates for totally asynchronous iterations under various assumptions on communication delays and update rates.},
 author = {Hamid Reza Feyzmahdavian and Mikael Johansson},
 journal = {Journal of Machine Learning Research},
 number = {158},
 openalex = {W4286981763},
 pages = {1--75},
 title = {Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees},
 url = {http://jmlr.org/papers/v24/22-0555.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0560,
 abstract = {A basic task in explainable AI (XAI) is to identify the most important features behind a prediction made by a black box function $f$. The insertion and deletion tests of Petsiuk et al. (2018) can be used to judge the quality of algorithms that rank pixels from most to least important for a classification. Motivated by regression problems we establish a formula for their area under the curve (AUC) criteria in terms of certain main effects and interactions in an anchored decomposition of $f$. We find an expression for the expected value of the AUC under a random ordering of inputs to $f$ and propose an alternative area above a straight line for the regression setting. We use this criterion to compare feature importances computed by integrated gradients (IG) to those computed by Kernel SHAP (KS) as well as LIME, DeepLIFT, vanilla gradient and input$\times$gradient methods. KS has the best overall performance in two datasets we consider but it is very expensive to compute. We find that IG is nearly as good as KS while being much faster. Our comparison problems include some binary inputs that pose a challenge to IG because it must use values between the possible variable levels and so we consider ways to handle binary variables in IG. We show that sorting variables by their Shapley value does not necessarily give the optimal ordering for an insertion-deletion test. It will however do that for monotone functions of additive models, such as logistic regression.},
 author = {Naofumi Hama and Masayoshi Mase and Art B. Owen},
 journal = {Journal of Machine Learning Research},
 number = {290},
 openalex = {W4281671063},
 pages = {1--38},
 title = {Deletion and Insertion Tests in Regression Models},
 url = {http://jmlr.org/papers/v24/22-0560.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0567,
 abstract = {In the last few years, graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. This emerging field has witnessed an extensive growth of promising techniques that have been applied with success to computer science, mathematics, biology, physics and chemistry. But for any successful field to become mainstream and reliable, benchmarks must be developed to quantify progress. This led us in March 2020 to release a benchmark framework that i) comprises of a diverse collection of mathematical and real-world graphs, ii) enables fair model comparison with the same parameter budget to identify key architectures, iii) has an open-source, easy-to-use and reproducible code infrastructure, and iv) is flexible for researchers to experiment with new theoretical ideas. As of December 2022, the GitHub repository has reached 2,000 stars and 380 forks, which demonstrates the utility of the proposed open-source framework through the wide usage by the GNN community. In this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics, an additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with a real-world measured chemical target, and discuss how this framework can be leveraged to explore new GNN designs and insights. As a proof of value of our benchmark, we study the case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark and has since spurred interest of exploring more powerful PE for Transformers and GNNs in a robust experimental setting.},
 author = {Vijay Prakash Dwivedi and Chaitanya K. Joshi and Anh Tuan Luu and Thomas Laurent and Yoshua Bengio and Xavier Bresson},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W4287829537},
 pages = {1--48},
 title = {Benchmarking Graph Neural Networks},
 url = {http://jmlr.org/papers/v24/22-0567.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0572,
 abstract = {To capture the inherent geometric features of many community detection problems, we propose to use a new random graph model of communities that we call a Geometric Block Model. The geometric block model builds on the random geometric graphs (Gilbert, 1961), one of the basic models of random graphs for spatial networks, in the same way that the well-studied stochastic block model builds on the Erd\H{o}s-R\'{en}yi random graphs. It is also a natural extension of random community models inspired by the recent theoretical and practical advancements in community detection. To analyze the geometric block model, we first provide new connectivity results for random annulus graphs which are generalizations of random geometric graphs. The connectivity properties of geometric graphs have been studied since their introduction, and analyzing them has been more difficult than their Erd\H{o}s-R\'{en}yi counterparts due to correlated edge formation. We then use the connectivity results of random annulus graphs to provide necessary and sufficient conditions for efficient recovery of communities for the geometric block model. We show that a simple triangle-counting algorithm to detect communities in the geometric block model is near-optimal. For this we consider the following two regimes of graph density. In the regime where the average degree of the graph grows logarithmically with the number of vertices, we show that our algorithm performs extremely well, both theoretically and practically. In contrast, the triangle-counting algorithm is far from being optimum for the stochastic block model in the logarithmic degree regime. We simulate our results on both real and synthetic datasets to show superior performance of both the new model as well as our algorithm.},
 author = {Sainyam Galhotra and Arya Mazumdar and Soumyabrata Pal and Barna Saha},
 journal = {Journal of Machine Learning Research},
 number = {338},
 openalex = {W4283448257},
 pages = {1--53},
 title = {Community Recovery in the Geometric Block Model},
 url = {http://jmlr.org/papers/v24/22-0572.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0581,
 abstract = {Data sets in which measurements of two (or more) types are obtained from a common set of samples arise in many scientific applications. A common problem in the exploratory analysis of such data is to identify groups of features of different data types that are strongly associated. A bimodule is a pair (A, B) of feature sets from two data types such that the aggregate cross-correlation between the features in A and those in B is large. A bimodule (A, B) is stable if A coincides with the set of features that have significant aggregate correlation with the features in B, and vice-versa. In this paper we propose and investigate an iterative testing-based procedure (BSP) to identify stable bimodules in bi-view data. We carry out a thorough simulation study to assess the performance of BSP, and present an extended application to the problem of expression quantitative trait loci (eQTL) analysis using recent data from the GTEx project. In addition, we apply BSP to climatology data to identify regions in North America where annual temperature variation affects precipitation.},
 author = {Miheer Dewaskar and John Palowitch and Mark He and Michael I. Love and Andrew B. Nobel},
 journal = {Journal of Machine Learning Research},
 number = {398},
 openalex = {W4292721683},
 pages = {1--47},
 title = {Finding groups of cross-correlated features in bi-view data},
 url = {http://jmlr.org/papers/v24/22-0581.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0582,
 abstract = {The paper has two major themes. The first part of the paper establishes certain general results for infinite-dimensional optimization problems on Hilbert spaces. These results cover the classical representer theorem and many of its variants as special cases and offer a wider scope of applications. The second part of the paper then develops a systematic approach for learning the drift function of a stochastic differential equation by integrating the results of the first part with Bayesian hierarchical framework. Importantly, our Baysian approach incorporates low-cost sparse learning through proper use of shrinkage priors while allowing proper quantification of uncertainty through posterior distributions. Several examples at the end illustrate the accuracy of our learning scheme.},
 author = {Arnab Ganguly and Riten Mitra and Jinpu Zhou},
 journal = {Journal of Machine Learning Research},
 number = {159},
 openalex = {W4320451343},
 pages = {1--39},
 title = {Infinite-dimensional optimization and Bayesian nonparametric learning of stochastic differential equations},
 url = {http://jmlr.org/papers/v24/22-0582.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0583,
 abstract = {This paper approaches the unsupervised learning problem by gradient descent in the space of probability density functions. A main result shows that along the gradient flow induced by a distribution-dependent ordinary differential equation (ODE), the unknown data distribution emerges as the long-time limit. That is, one can uncover the data distribution by simulating the distribution-dependent ODE. Intriguingly, the simulation of the ODE is shown equivalent to the training of generative adversarial networks (GANs). This equivalence provides a new "cooperative" view of GANs and, more importantly, sheds new light on the divergence of GANs. In particular, it reveals that the GAN algorithm implicitly minimizes the mean squared error (MSE) between two sets of samples, and this MSE fitting alone can cause GANs to diverge. To construct a solution to the distribution-dependent ODE, we first show that the associated nonlinear Fokker-Planck equation has a unique weak solution, by the Crandall-Liggett theorem for differential equations in Banach spaces. Based on this solution to the Fokker-Planck equation, we construct a unique solution to the ODE, using Trevisan's superposition principle. The convergence of the induced gradient flow to the data distribution is obtained by analyzing the Fokker-Planck equation.},
 author = {Yu-Jui Huang and Yuchong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {217},
 openalex = {W4229459982},
 pages = {1--40},
 title = {GANs as Gradient Flows that Converge},
 url = {http://jmlr.org/papers/v24/22-0583.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0587,
 abstract = {This work studies the multi-task functional linear regression models where both the covariates and the unknown regression coefficients (called slope functions) are curves. For slope function estimation, we employ penalized splines to balance bias, variance, and computational complexity. The power of multi-task learning is brought in by imposing additional structures over the slope functions. We propose a general model with double regularization over the spline coefficient matrix: i) a matrix manifold constraint, and ii) a composite penalty as a summation of quadratic terms. Many multi-task learning approaches can be treated as special cases of this proposed model, such as a reduced-rank model and a graph Laplacian regularized model. We show the composite penalty induces a specific norm, which helps to quantify the manifold curvature and determine the corresponding proper subset in the manifold tangent space. The complexity of tangent space subset is then bridged to the complexity of geodesic neighbor via generic chaining. A unified convergence upper bound is obtained and specifically applied to the reduced-rank model and the graph Laplacian regularized model. The phase transition behaviors for the estimators are examined as we vary the configurations of model parameters.},
 author = {Shiyuan He and Hanxuan Ye and Kejun He},
 journal = {Journal of Machine Learning Research},
 number = {291},
 openalex = {W4308758544},
 pages = {1--69},
 title = {A Unified Analysis of Multi-task Functional Linear Regression Models with Manifold Constraint and Composite Quadratic Penalty},
 url = {http://jmlr.org/papers/v24/22-0587.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0605,
 abstract = {A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical "risk" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully agnostic setup, i.e., without requiring any a priori knowledge on the probability distribution of the observations. Not only these results offer a valid support to develop trust in observation-driven methodologies, they also play a fundamental role in learning techniques as a tool for hyper-parameter tuning.},
 author = {Marco C. Campi and Simone Garatti},
 journal = {Journal of Machine Learning Research},
 number = {339},
 openalex = {W4318719143},
 pages = {1--74},
 title = {Compression, Generalization and Learning},
 url = {http://jmlr.org/papers/v24/22-0605.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0606,
 author = {Jian Shen and Hang Lai and Minghuan Liu and Han Zhao and Yong Yu and Weinan Zhang},
 journal = {Journal of Machine Learning Research},
 number = {218},
 pages = {1--35},
 title = {Adaptation Augmented Model-based Policy Optimization},
 url = {http://jmlr.org/papers/v24/22-0606.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0614,
 author = {Hua Liu and Jinhong You and Jiguo Cao},
 journal = {Journal of Machine Learning Research},
 number = {219},
 pages = {1--41},
 title = {Functional L-Optimality Subsampling for Functional Generalized Linear Models with Massive Data},
 url = {http://jmlr.org/papers/v24/22-0614.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0615,
 abstract = {This paper is motivated by structured sparsity for deep neural network training. We study a weighted group L0-norm constraint, and present the projection and normal cone of this set. Using randomized smoothing, we develop zeroth and first-order algorithms for minimizing a Lipschitz continuous function constrained by any closed set which can be projected onto. Non-asymptotic convergence guarantees are proven in expectation for the proposed algorithms for two related convergence criteria which can be considered as approximate stationary points. Two further methods are given using the proposed algorithms: one with non-asymptotic convergence guarantees in high probability, and the other with asymptotic guarantees to a stationary point almost surely. We believe in particular that these are the first such non-asymptotic convergence results for constrained Lipschitz continuous loss functions.},
 author = {Michael R. Metel},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W4221162728},
 pages = {1--44},
 title = {Sparse Training with Lipschitz Continuous Loss Functions and a Weighted Group L0-norm Constraint},
 url = {http://jmlr.org/papers/v24/22-0615.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0616,
 author = {Shoaib Bin Masud and Matthew Werenski and James M. Murphy and Shuchin Aeron},
 journal = {Journal of Machine Learning Research},
 number = {160},
 pages = {1--65},
 title = {Multivariate Soft Rank via Entropy-Regularized Optimal Transport: Sample Efficiency and Generative Modeling},
 url = {http://jmlr.org/papers/v24/22-0616.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0627,
 abstract = {This article presents a novel approach to construct Intrinsic Gaussian Processes for regression on unknown manifolds with probabilistic metrics (GPUM) in point clouds. In many real world applications, one often encounters high dimensional data (e.g. point cloud data) centred around some lower dimensional unknown manifolds. The geometry of manifold is in general different from the usual Euclidean geometry. Naively applying traditional smoothing methods such as Euclidean Gaussian Processes (GPs) to manifold valued data and so ignoring the geometry of the space can potentially lead to highly misleading predictions and inferences. A manifold embedded in a high dimensional Euclidean space can be well described by a probabilistic mapping function and the corresponding latent space. We investigate the geometrical structure of the unknown manifolds using the Bayesian Gaussian Processes latent variable models(BGPLVM) and Riemannian geometry. The distribution of the metric tensor is learned using BGPLVM. The boundary of the resulting manifold is defined based on the uncertainty quantification of the mapping. We use the the probabilistic metric tensor to simulate Brownian Motion paths on the unknown manifold. The heat kernel is estimated as the transition density of Brownian Motion and used as the covariance functions of GPUM. The applications of GPUM are illustrated in the simulation studies on the Swiss roll, high dimensional real datasets of WiFi signals and image data examples. Its performance is compared with the Graph Laplacian GP, Graph Matern GP and Euclidean GP.},
 author = {Mu Niu and Zhenwen Dai and Pokman Cheung and Yizhu Wang},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W4320705946},
 pages = {1--42},
 title = {Intrinsic Gaussian Process on Unknown Manifolds with Probabilistic Metrics},
 url = {http://jmlr.org/papers/v24/22-0627.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0628,
 abstract = {Estimation-of-distribution algorithms (EDAs) are optimization algorithms that learn a distribution on the search space from which good solutions can be sampled easily. A key parameter of most EDAs is the sample size (population size). If the population size is too small, the update of the probabilistic model builds on few samples, leading to the undesired effect of genetic drift. Too large population sizes avoid genetic drift, but slow down the process. Building on a recent quantitative analysis of how the population size leads to genetic drift, we design a smart-restart mechanism for EDAs. By stopping runs when the risk for genetic drift is high, it automatically runs the EDA in good parameter regimes. Via a mathematical runtime analysis, we prove a general performance guarantee for this smart-restart scheme. This in particular shows that in many situations where the optimal (problem-specific) parameter values are known, the restart scheme automatically finds these, leading to the asymptotically optimal performance. We also conduct an extensive experimental analysis. On four classic benchmark problems, we clearly observe the critical influence of the population size on the performance, and we find that the smart-restart scheme leads to a performance close to the one obtainable with optimal parameter values. Our results also show that previous theory-based suggestions for the optimal population size can be far from the optimal ones, leading to a performance clearly inferior to the one obtained via the smart-restart scheme. We also conduct experiments with PBIL (cross-entropy algorithm) on two combinatorial optimization problems from the literature, the max-cut problem and the bipartition problem. Again, we observe that the smart-restart mechanism finds much better values for the population size than those suggested in the literature, leading to a much better performance.},
 author = {Weijie Zheng and Benjamin Doerr},
 journal = {Journal of Machine Learning Research},
 number = {292},
 openalex = {W4283365871},
 pages = {1--40},
 title = {From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms},
 url = {http://jmlr.org/papers/v24/22-0628.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0629,
 abstract = {Previous chapter Next chapter Full AccessProceedings Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)Robust Load Balancing with Machine Learned AdviceSara Ahmadian, Hossein Esfandiari, Vahab Mirrokni, and Binghui PengSara Ahmadian, Hossein Esfandiari, Vahab Mirrokni, and Binghui Pengpp.20 - 34Chapter DOI:https://doi.org/10.1137/1.9781611977073.2PDFBibTexSections ToolsAdd to favoritesExport CitationTrack CitationsEmail SectionsAboutAbstract Motivated by the exploding growth of web-based services and the importance of efficiently managing the computational resources of such systems, we introduce and study a theoretical model for load balancing of very large databases such as commercial search engines. Our model is a more realistic version of the well-received balls-into-bins model with an additional constraint that limits the number of servers that carry each piece of the data. This additional constraint is necessary when, on one hand, the data is so large that we can not copy the whole data on each server. On the other hand, the query response time is so limited that we can not ignore the fact that the number of queries for each piece of the data changes over time, and hence we can not simply split the data over different machines In this paper, we develop an almost optimal load balancing algorithm that works given an estimate of the load of each piece of the data. Our algorithm is almost perfectly robust to wrong estimates, to the extent that even when all of the loads are adversarially chosen the performance of our algorithm is 1–1/e, which is provably optimal. Along the way, we develop various techniques for analyzing the balls-into-bins process under certain correlations and build a novel connection with the multiplicative weights update scheme. Previous chapter Next chapter RelatedDetails Published:2022eISBN:978-1-61197-707-3 https://doi.org/10.1137/1.9781611977073Book Series Name:ProceedingsBook Code:PRDA22Book Pages:xvii + 3771},
 author = {Sara Ahmadian and Hossein Esfandiari and Vahab Mirrokni and Binghui Peng},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W4205857732},
 pages = {1--46},
 title = {Robust Load Balancing with Machine Learned Advice},
 url = {http://jmlr.org/papers/v24/22-0629.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-063,
 abstract = {Embedding-based methods for reasoning in knowledge hypergraphs learn a representation for each entity and relation. Current methods do not capture the procedural rules underlying the relations in the graph. We propose a simple embedding-based model called ReAlE that performs link prediction in knowledge hypergraphs (generalized knowledge graphs) and can represent high-level abstractions in terms of relational algebra operations. We show theoretically that ReAlE is fully expressive and provide proofs and empirical evidence that it can represent a large subset of the primitive relational algebra operations, namely renaming, projection, set union, selection, and set difference. We also verify experimentally that ReAlE outperforms state-of-the-art models in knowledge hypergraph completion, and in representing each of these primitive relational algebra operations. For the latter experiment, we generate a synthetic knowledge hypergraph, for which we design an algorithm based on the Erdos-R'enyi model for generating random graphs.},
 author = {Bahare Fatemi and Perouz Taslakian and David Vazquez and David Poole},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W3131239930},
 pages = {1--34},
 title = {Knowledge Hypergraph Embedding Meets Relational Algebra},
 url = {http://jmlr.org/papers/v24/22-063.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0630,
 abstract = {In fully cooperative multi-agent reinforcement learning (MARL) settings, environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of other agents. To address the above issues, we proposed a unified framework, called DFAC, for integrating distributional RL with value function factorization methods. This framework generalizes expected value function factorization methods to enable the factorization of return distributions. To validate DFAC, we first demonstrate its ability to factorize the value functions of a simple matrix game with stochastic rewards. Then, we perform experiments on all Super Hard maps of the StarCraft Multi-Agent Challenge and six self-designed Ultra Hard maps, showing that DFAC is able to outperform a number of baselines.},
 author = {Wei-Fang Sun and Cheng-Kuang Lee and Simon See and Chun-Yi Lee},
 journal = {Journal of Machine Learning Research},
 number = {220},
 openalex = {W4379539524},
 pages = {1--32},
 title = {A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/22-0630.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0642,
 abstract = {Matrix completion has attracted attention in many fields, including statistics, applied mathematics, and electrical engineering. Most of the works focus on the independent sampling models under which the observed entries are sampled independently. Motivated by applications in the integration of knowledge graphs derived from multi-source biomedical data such as those from Electronic Health Records (EHR) and biomedical text, we propose the {\bf B}lock-wise {\bf O}verlapping {\bf N}oisy {\bf M}atrix {\bf I}ntegration (BONMI) to treat blockwise missingness of symmetric matrices representing relatedness between entity pairs. Our idea is to exploit the orthogonal Procrustes problem to align the eigenspace of the two sub-matrices, then complete the missing blocks by the inner product of the two low-rank components. Besides, we prove the statistical rate for the eigenspace of the underlying matrix, which is comparable to the rate under the independently missing assumption. Simulation studies show that the method performs well under a variety of configurations. In the real data analysis, the method is applied to two tasks: (i) the integrating of several point-wise mutual information matrices built by English EHR and Chinese medical text data, and (ii) the machine translation between English and Chinese medical concepts. Our method shows an advantage over existing methods.},
 author = {Doudou Zhou and Tianxi Cai and Junwei Lu},
 journal = {Journal of Machine Learning Research},
 number = {221},
 openalex = {W3207940143},
 pages = {1--43},
 title = {Multi-source Learning via Completion of Block-wise Overlapping Noisy Matrices},
 url = {http://jmlr.org/papers/v24/22-0642.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0644,
 author = {Mo Zhou and Jianfeng Lu},
 journal = {Journal of Machine Learning Research},
 number = {222},
 pages = {1--34},
 title = {Single Timescale Actor-Critic Method to Solve the Linear Quadratic Regulator with Convergence Guarantees},
 url = {http://jmlr.org/papers/v24/22-0644.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0657,
 abstract = {Most work in neural networks focuses on estimating the conditional mean of a continuous response variable given a set of covariates. In this article, we consider estimating the conditional distribution function using neural networks for both censored and uncensored data. The algorithm is built upon the data structure particularly constructed for the Cox regression with time-dependent covariates. Without imposing any model assumptions, we consider a loss function that is based on the full likelihood where the conditional hazard function is the only unknown nonparametric parameter, for which unconstrained optimization methods can be applied. Through simulation studies, we show that the proposed method possesses desirable performance, whereas the partial likelihood method and the traditional neural networks with L2 loss yields biased estimates when model assumptions are violated. We further illustrate the proposed method with several real-world data sets. The implementation of the proposed methods is made available at https://github.com/bingqing0729/NNCDE.},
 author = {Bingqing Hu and Bin Nan},
 journal = {Journal of Machine Learning Research},
 number = {223},
 openalex = {W4284896965},
 pages = {1--26},
 title = {Conditional Distribution Function Estimation Using Neural Networks for Censored and Uncensored Data},
 url = {http://jmlr.org/papers/v24/22-0657.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0666,
 abstract = {We prove concentration inequalities and associated PAC bounds for continuous- and discrete-time additive functionals for possibly unbounded functions of multivariate, nonreversible diffusion processes. Our analysis relies on an approach via the Poisson equation allowing us to consider a very broad class of subexponentially ergodic processes. These results add to existing concentration inequalities for additive functionals of diffusion processes which have so far been only available for either bounded functions or for unbounded functions of processes from a significantly smaller class. We demonstrate the power of these exponential inequalities by two examples of very different areas. Considering a possibly high-dimensional parametric nonlinear drift model under sparsity constraints, we apply the continuous-time concentration results to validate the restricted eigenvalue condition for Lasso estimation, which is fundamental for the derivation of oracle inequalities. The results for discrete additive functionals are used to investigate the unadjusted Langevin MCMC algorithm for sampling of moderately heavy-tailed densities $\pi$. In particular, we provide PAC bounds for the sample Monte Carlo estimator of integrals $\pi(f)$ for polynomially growing functions $f$ that quantify sufficient sample and step sizes for approximation within a prescribed margin with high probability.},
 author = {Lukas Trottner and Cathrine Aeckerle-Willems and Claudia Strauch},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W4281781655},
 pages = {1--38},
 title = {Concentration analysis of multivariate elliptic diffusion processes},
 url = {http://jmlr.org/papers/v24/22-0666.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-067,
 author = {Olivier Coudray and Christine Keribin and Pascal Massart and Patrick Pamphile},
 journal = {Journal of Machine Learning Research},
 number = {107},
 pages = {1--31},
 title = {Risk Bounds for Positive-Unlabeled Learning Under the Selected At Random Assumption},
 url = {http://jmlr.org/papers/v24/22-067.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0676,
 abstract = {We introduce a computational efficient data-driven framework suitable for quantifying the uncertainty in physical parameters and model formulation of computer models, represented by differential equations. We construct physics-informed priors, which are multi-output GP priors that encode the model's structure in the covariance function. This is extended into a fully Bayesian framework that quantifies the uncertainty of physical parameters and model predictions. Since physical models often are imperfect descriptions of the real process, we allow the model to deviate from the observed data by considering a discrepancy function. For inference, Hamiltonian Monte Carlo is used. Further, approximations for big data are developed that reduce the computational complexity from $\mathcal{O}(N^3)$ to $\mathcal{O}(N\cdot m^2),$ where $m \ll N.$ Our approach is demonstrated in simulation and real data case studies where the physics are described by time-dependent ODEs describe (cardiovascular models) and space-time dependent PDEs (heat equation). In the studies, it is shown that our modelling framework can recover the true parameters of the physical models in cases where 1) the reality is more complex than our modelling choice and 2) the data acquisition process is biased while also producing accurate predictions. Furthermore, it is demonstrated that our approach is computationally faster than traditional Bayesian calibration methods.},
 author = {Michail Spitieris and Ingelin Steinsland},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W4221142064},
 pages = {1--39},
 title = {Bayesian Calibration of Imperfect Computer Models using Physics-Informed Priors},
 url = {http://jmlr.org/papers/v24/22-0676.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0680,
 abstract = {Units equivariance (or units covariance) is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we express this symmetry in terms of a (non-compact) group action, and we employ dimensional analysis and ideas from equivariant machine learning to provide a methodology for exactly units-equivariant machine learning: For any given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis, and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods which are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology.},
 author = {Soledad Villar and Weichi Yao and David W. Hogg and Ben Blum-Smith and Bianca Dumitrascu},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W4224910515},
 pages = {1--32},
 title = {Dimensionless machine learning: Imposing exact units equivariance},
 url = {http://jmlr.org/papers/v24/22-0680.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0685,
 abstract = {The hidden Markov model (HMM) is a classic modeling tool with a wide swath of applications. Its inception considered observations restricted to a finite alphabet, but it was quickly extended to multivariate continuous distributions. In this article, we further extend the HMM from mixtures of normal distributions in $d$-dimensional Euclidean space to general Gaussian measure mixtures in locally convex topological spaces. The main innovation is the use of the Onsager-Machlup functional as a proxy for the probability density function in infinite dimensional spaces. This allows for choice of a Cameron-Martin space suitable for a given application. We demonstrate the versatility of this methodology by applying it to simulated diffusion processes such as Brownian and fractional Brownian sample paths as well as the Ornstein-Uhlenbeck process. Our methodology is applied to the identification of sleep states from overnight polysomnography time series data with the aim of diagnosing Obstructive Sleep Apnea in pediatric patients. It is also applied to a series of annual cumulative snowfall curves from 1940 to 1990 in the city of Edmonton, Alberta.},
 author = {Adam B Kashlak and Prachi Loliencar and Giseon Heo},
 journal = {Journal of Machine Learning Research},
 number = {340},
 openalex = {W4281632532},
 pages = {1--49},
 title = {Topological Hidden Markov Models},
 url = {http://jmlr.org/papers/v24/22-0685.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0689,
 abstract = {We propose a novel framework to study asynchronous federated learning optimization with delays in gradient updates. Our theoretical framework extends the standard FedAvg aggregation scheme by introducing stochastic aggregation weights to represent the variability of the clients update time, due for example to heterogeneous hardware capabilities. Our formalism applies to the general federated setting where clients have heterogeneous datasets and perform at least one step of stochastic gradient descent (SGD). We demonstrate convergence for such a scheme and provide sufficient conditions for the related minimum to be the optimum of the federated problem. We show that our general framework applies to existing optimization schemes including centralized learning, FedAvg, asynchronous FedAvg, and FedBuff. The theory here provided allows drawing meaningful guidelines for designing a federated learning experiment in heterogeneous conditions. In particular, we develop in this work FedFix, a novel extension of FedAvg enabling efficient asynchronous federated training while preserving the convergence stability of synchronous aggregation. We empirically demonstrate our theory on a series of experiments showing that asynchronous FedAvg leads to fast convergence at the expense of stability, and we finally demonstrate the improvements of FedFix over synchronous and asynchronous FedAvg.},
 author = {Yann Fraboni and Richard Vidal and Laetitia Kameni and Marco Lorenzi},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W4283318592},
 pages = {1--43},
 title = {A General Theory for Federated Optimization with Asynchronous and Heterogeneous Clients Updates},
 url = {http://jmlr.org/papers/v24/22-0689.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0698,
 abstract = {We study a family of adversarial multiclass classification problems and provide equivalent reformulations in terms of: 1) a family of generalized barycenter problems introduced in the paper and 2) a family of multimarginal optimal transport problems where the number of marginals is equal to the number of classes in the original classification problem. These new theoretical results reveal a rich geometric structure of adversarial learning problems in multiclass classification and extend recent results restricted to the binary classification setting. A direct computational implication of our results is that by solving either the barycenter problem and its dual, or the MOT problem and its dual, we can recover the optimal robust classification rule and the optimal adversarial strategy for the original adversarial problem. Examples with synthetic and real data illustrate our results.},
 author = {Nicolás García Trillos and Matt Jacobs and Jakwang Kim},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W4225096562},
 pages = {1--56},
 title = {The Multimarginal Optimal Transport Formulation of Adversarial Multiclass Classification},
 url = {http://jmlr.org/papers/v24/22-0698.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0700,
 abstract = {In contemporary statistical learning, covariate shift correction plays an important role in transfer learning when distribution of the testing data is shifted from the training data. Importance weighting, as a natural and principle strategy to adjust for covariate shift, has been commonly used in the field of transfer learning. However, this strategy is not robust to model misspecification or excessive estimation error. In this paper, we propose an augmented transfer regression learning (ATReL) approach that introduces an imputation model for the targeted response, and uses it to augment the importance weighting equation. With novel semi-non-parametric constructions and calibrated moment estimating equations for the two nuisance models, our ATReL method is less prone to (i) the curse of dimensionality compared to nonparametric approaches, and (ii) model mis-specification than parametric approaches. We show that our ATReL estimator is root-n-consistent when at least one nuisance model is correctly specified, estimation for the parametric part of the nuisance models achieves parametric rate, and the nonparametric components are rate doubly robust. Simulation studies demonstrate that our method is more robust and efficient than existing parametric and fully nonparametric (machine learning) estimators under various configurations. We also examine the utility of our method through a real example about transfer learning of phenotyping algorithm for rheumatoid arthritis across different time windows. Finally, we propose ways to enhance the intrinsic efficiency of our estimator and to incorporate modern machine learning methods with our proposed framework.},
 author = {Molei Liu and Yi Zhang and Katherine P. Liao and Tianxi Cai},
 journal = {Journal of Machine Learning Research},
 number = {293},
 openalex = {W4287645642},
 pages = {1--50},
 title = {Augmented Transfer Regression Learning with Semi-non-parametric Nuisance Models},
 url = {http://jmlr.org/papers/v24/22-0700.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0712,
 abstract = {Segmentation has emerged as a fundamental field of computer vision and natural language processing, which assigns a label to every pixel/feature to extract regions of interest from an image/text. To evaluate the performance of segmentation, the Dice and IoU metrics are used to measure the degree of overlap between the ground truth and the predicted segmentation. In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the existing thresholding-based framework with most operating losses are not consistent with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel execution are developed to implement the proposed framework in large-scale and high-dimensional segmentation. We study statistical properties of the proposed framework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and the rate of convergence are also provided. The numerical effectiveness of RankDice/mRankDice is demonstrated in various simulated examples and Fine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with state-of-the-art deep learning architectures.},
 author = {Ben Dai and Chunlin Li},
 journal = {Journal of Machine Learning Research},
 number = {224},
 openalex = {W4283698787},
 pages = {1--50},
 title = {RankSEG: A Consistent Ranking-based Framework for Segmentation},
 url = {http://jmlr.org/papers/v24/22-0712.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0734,
 abstract = {Guaranteeing privacy in released data is an important goal for data-producing agencies. There has been extensive research on developing suitable privacy mechanisms in recent years. Particularly notable is the idea of noise addition with the guarantee of differential privacy. There are, however, concerns about compromising data utility when very stringent privacy mechanisms are applied. Such compromises can be quite stark in correlated data, such as time series data. Adding white noise to a stochastic process may significantly change the correlation structure, a facet of the process that is essential to optimal prediction. We propose the use of all-pass filtering as a privacy mechanism for regularly sampled time series data, showing that this procedure preserves utility while also providing sufficient privacy guarantees to entity-level time series.},
 author = {Tucker McElroy and Anindya Roy and Gaurab Hore},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W4286236364},
 pages = {1--29},
 title = {FLIP: A Utility Preserving Privacy Mechanism for Time Series},
 url = {http://jmlr.org/papers/v24/22-0734.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0744,
 abstract = {We introduce a metric space of clusterings, where clusterings are described by a binary vector indexed by the vertex-pairs. We extend this geometry to a hypersphere and prove that maximizing modularity is equivalent to minimizing the angular distance to some modularity vector over the set of clustering vectors. In that sense, modularity-based community detection methods can be seen as a subclass of a more general class of projection methods, which we define as the community detection methods that adhere to the following two-step procedure: first, mapping the network to a point on the hypersphere; second, projecting this point to the set of clustering vectors. We show that this class of projection methods contains many interesting community detection methods. Many of these new methods cannot be described in terms of null models and resolution parameters, as is customary for modularity-based methods. We provide a new characterization of such methods in terms of meridians and latitudes of the hypersphere. In addition, by relating the modularity resolution parameter to the latitude of the corresponding modularity vector, we obtain a new interpretation of the resolution limit that modularity maximization is known to suffer from.},
 author = {Martijn Gösgens and Remco van der Hofstad and Nelly Litvak},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W4287083922},
 pages = {1--36},
 title = {The Hyperspherical Geometry of Community Detection: Modularity as a Distance},
 url = {http://jmlr.org/papers/v24/22-0744.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0747,
 author = {Louis-Philippe Vignault and Audrey Durand and Pascal Germain},
 journal = {Journal of Machine Learning Research},
 number = {294},
 pages = {1--13},
 title = {Erratum: Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm},
 url = {http://jmlr.org/papers/v24/22-0747.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0755,
 abstract = {Download This Paper Open PDF in Browser Add Paper to My Library Share: Permalink Using these links will ensure access to this page indefinitely Copy URL Copy DOI},
 author = {Yanwei Jia and Xun Yu Zhou},
 journal = {Journal of Machine Learning Research},
 number = {161},
 openalex = {W4283836230},
 pages = {1--61},
 title = {q-Learning in Continuous Time},
 url = {http://jmlr.org/papers/v24/22-0755.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0784,
 abstract = {The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining near-optimal expected loss, has received much attention in recent years, but still remains not fully understood beyond well-specified linear regression setups. In this paper, we provide several new results on when one can or cannot expect benign overfitting to occur, for both regression and classification tasks. We consider a prototypical and rather generic data model for benign overfitting of linear predictors, where an arbitrary input distribution of some fixed dimension $k$ is concatenated with a high-dimensional distribution. For linear regression which is not necessarily well-specified, we show that the minimum-norm interpolating predictor (that standard training methods converge to) is biased towards an inconsistent solution in general, hence benign overfitting will generally not occur. Moreover, we show how this can be extended beyond standard linear regression, by an argument proving how the existence of benign overfitting on some regression problems precludes its existence on other regression problems. We then turn to classification problems, and show that the situation there is much more favorable. Specifically, we prove that the max-margin predictor (to which standard training methods are known to converge in direction) is asymptotically biased towards minimizing a weighted \emph{squared hinge loss}. This allows us to reduce the question of benign overfitting in classification to the simpler question of whether this loss is a good surrogate for the misclassification error, and use it to show benign overfitting in some new settings.},
 author = {Ohad Shamir},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W4226372400},
 pages = {1--40},
 title = {The Implicit Bias of Benign Overfitting},
 url = {http://jmlr.org/papers/v24/22-0784.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0799,
 abstract = {Quantile regression is a fundamental problem in statistical learning motivated by a need to quantify uncertainty in predictions, or to model a diverse population without being overly reductive. For instance, epidemiological forecasts, cost estimates, and revenue predictions all benefit from being able to quantify the range of possible values accurately. As such, many models have been developed for this problem over many years of research in statistics, machine learning, and related fields. Rather than proposing yet another (new) algorithm for quantile regression we adopt a meta viewpoint: we investigate methods for aggregating any number of conditional quantile models, in order to improve accuracy and robustness. We consider weighted ensembles where weights may vary over not only individual models, but also over quantile levels, and feature values. All of the models we consider in this paper can be fit using modern deep learning toolkits, and hence are widely accessible (from an implementation point of view) and scalable. To improve the accuracy of the predicted quantiles (or equivalently, prediction intervals), we develop tools for ensuring that quantiles remain monotonically ordered, and apply conformal calibration methods. These can be used without any modification of the original library of base models. We also review some basic theory surrounding quantile aggregation and related scoring rules, and contribute a few new results to this literature (for example, the fact that post sorting or post isotonic regression can only improve the weighted interval score). Finally, we provide an extensive suite of empirical comparisons across 34 data sets from two different benchmark repositories.},
 author = {Rasool Fakoor and Taesup Kim and Jonas Mueller and Alexander J. Smola and Ryan J. Tibshirani},
 journal = {Journal of Machine Learning Research},
 number = {162},
 openalex = {W4285787731},
 pages = {1--45},
 title = {Flexible Model Aggregation for Quantile Regression},
 url = {http://jmlr.org/papers/v24/22-0799.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0808,
 abstract = {We develop a theory of limits for sequences of dense abstract simplicial complexes, where a sequence is considered convergent if its homomorphism densities converge. The limiting objects are represented by stacks of measurable [0,1]-valued functions on unit cubes of increasing dimension, each corresponding to a dimension of the abstract simplicial complex. We show that convergence in homomorphism density implies convergence in a cut-metric, and vice versa, as well as showing that simplicial complexes sampled from the limit objects closely resemble its structure. Applying this framework, we also partially characterize the convergence of nonuniform hypergraphs.},
 author = {T. Mitchell Roddenberry and Santiago Segarra},
 journal = {Journal of Machine Learning Research},
 number = {225},
 openalex = {W4286233822},
 pages = {1--42},
 title = {Limits of Dense Simplicial Complexes},
 url = {http://jmlr.org/papers/v24/22-0808.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0809,
 author = {Aadyot Bhatnagar and Paul Kassianik and Chenghao Liu and Tian Lan and Wenzhuo Yang and Rowan Cassius and Doyen Sahoo and Devansh Arpit and Sri Subramanian and Gerald Woo and Amrita Saha and Arun Kumar Jagota and Gokulakrishnan Gopalakrishnan and Manpreet Singh and K C Krithika and Sukumar Maddineni and Daeki Cho and Bo Zong and Yingbo Zhou and Caiming Xiong and Silvio Savarese and Steven Hoi and Huan Wang},
 journal = {Journal of Machine Learning Research},
 number = {226},
 pages = {1--6},
 title = {Merlion: End-to-End Machine Learning for Time Series},
 url = {http://jmlr.org/papers/v24/22-0809.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0833,
 abstract = {We propose Bayesian nonparametric Weibull delegate racing (WDR) for survival analysis with competing events and achieve both model interpretability and flexibility. Utilizing a natural mechanism of surviving competing events, we assume a race among a potentially infinite number of sub-events. In doing this, WDR accommodates nonlinear covariate effects with no need of data transformation. Moreover, WDR is able to handle left truncation, time-varying covariates, different types of censoring, and missing event times or types. We develop an efficient MCMC algorithm based on Gibbs sampling for Bayesian inference and provide an \texttt{R} package. Synthetic data analysis and comparison with benchmark approaches demonstrate WDR's outstanding performance and parsimonious nonlinear modeling capacity. In addition, we analyze two real data sets and showcase advantages of WDR. Specifically, we study time to death of three types of lymphoma and show the potential of WDR in modeling nonlinear covariate effects and discovering new diseases. We also use WDR to investigate the age at onset of mild cognitive impairment and interpret the accelerating or decelerating effects of biomarkers on the progression of Alzheimer's disease.},
 author = {Quan Zhang and Yanxun Xu and Mei-Cheng Wang and Mingyuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {295},
 openalex = {W4288031288},
 pages = {1--43},
 title = {Weibull Racing Survival Analysis with Competing Events, Left Truncation, and Time-varying Covariates},
 url = {http://jmlr.org/papers/v24/22-0833.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0834,
 abstract = {Statistical inferences for high-dimensional regression models have been extensively studied for their wide applications ranging from genomics, neuroscience, to economics. However, in practice, there are often potential unmeasured confounders associated with both the response and covariates, which can lead to invalidity of standard debiasing methods. This paper focuses on a generalized linear regression framework with hidden confounding and proposes a debiasing approach to address this high-dimensional problem, by adjusting for the effects induced by the unmeasured confounders. We establish consistency and asymptotic normality for the proposed debiased estimator. The finite sample performance of the proposed method is demonstrated through extensive numerical studies and an application to a genetic data set.},
 author = {Jing Ouyang and Kean Ming Tan and Gongjun Xu},
 journal = {Journal of Machine Learning Research},
 number = {296},
 openalex = {W4295105933},
 pages = {1--61},
 title = {High-Dimensional Inference for Generalized Linear Models with Hidden Confounding},
 url = {http://jmlr.org/papers/v24/22-0834.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0845,
 abstract = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games.},
 author = {Binyan Jiang and Jialiang Li and Qiwei Yao},
 journal = {Journal of Machine Learning Research},
 number = {227},
 openalex = {W2097268041},
 pages = {1--69},
 title = {Deep AutoRegressive Networks},
 url = {http://jmlr.org/papers/v24/22-0845.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0850,
 abstract = {Originally developed for imputing missing entries in low rank, or approximately low rank matrices, matrix completion has proven widely effective in many problems where there is no reason to assume low-dimensional linear structure in the underlying matrix, as would be imposed by rank constraints. In this manuscript, we build some theoretical intuition for this behavior. We consider matrices which are not necessarily low-rank, but lie in a low-dimensional non-linear manifold. We show that nuclear-norm penalization is still effective for recovering these matrices when observations are missing completely at random. In particular, we give upper bounds on the rate of convergence as a function of the number of rows, columns, and observed entries in the matrix, as well as the smoothness and dimension of the non-linear embedding. We additionally give a minimax lower bound: This lower bound agrees with our upper bound (up to a logarithmic factor), which shows that nuclear-norm penalization is (up to log terms) minimax rate optimal for these problems.},
 author = {Yunhua Xiang and Tianyu Zhang and Xu Wang and Ali Shojaie and Noah Simon},
 journal = {Journal of Machine Learning Research},
 number = {228},
 openalex = {W3159252983},
 pages = {1--38},
 title = {On the Optimality of Nuclear-norm-based Matrix Completion for Problems with Smooth Non-linear Structure},
 url = {http://jmlr.org/papers/v24/22-0850.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0865,
 author = {Xingdong Feng and Yuling Jiao and Lican Kang and Baqun Zhang and Fan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {383},
 pages = {1--40},
 title = {Over-parameterized Deep Nonparametric Regression for Dependent Data with Its Applications to Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/22-0865.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0866,
 abstract = {Deep networks are well-known to be fragile to adversarial attacks, and adversarial training is one of the most popular methods used to train a robust model. To take advantage of unlabeled data, recent works have applied adversarial training to contrastive learning (Adversarial Contrastive Learning; ACL for short) and obtain promising robust performance. However, the theory of ACL is not well understood. To fill this gap, we leverage the Rademacher complexity to analyze the generalization performance of ACL, with a particular focus on linear models and multi-layer neural networks under $\ell_p$ attack ($p \ge 1$). Our theory shows that the average adversarial risk of the downstream tasks can be upper bounded by the adversarial unsupervised risk of the upstream task. The experimental results validate our theory.},
 author = {Xin Zou and Weiwei Liu},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W4321593950},
 pages = {1--54},
 title = {Generalization Bounds for Adversarial Contrastive Learning},
 url = {http://jmlr.org/papers/v24/22-0866.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0880,
 abstract = {This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rule mining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate. Compared to other fair and interpretable classifiers, our method is able to find rule sets that meet stricter notions of fairness with a modest trade-off in accuracy.},
 author = {Connor Lawless and Sanjeeb Dash and Oktay Gunluk and Dennis Wei},
 journal = {Journal of Machine Learning Research},
 number = {229},
 openalex = {W3211519965},
 pages = {1--50},
 title = {Interpretable and Fair Boolean Rule Sets via Column Generation},
 url = {http://jmlr.org/papers/v24/22-0880.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0881,
 author = {Zhengyu Zhou and Weiwei Liu},
 journal = {Journal of Machine Learning Research},
 number = {230},
 pages = {1--27},
 title = {Sample Complexity for Distributionally Robust Learning under chi-square divergence},
 url = {http://jmlr.org/papers/v24/22-0881.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0882,
 abstract = {We consider using gradient descent to minimize the nonconvex function $f(X)=\phi(XX^{T})$ over an $n\times r$ factor matrix $X$, in which $\phi$ is an underlying smooth convex cost function defined over $n\times n$ matrices. While only a second-order stationary point $X$ can be provably found in reasonable time, if $X$ is additionally rank deficient, then its rank deficiency certifies it as being globally optimal. This way of certifying global optimality necessarily requires the search rank $r$ of the current iterate $X$ to be overparameterized with respect to the rank $r^{\star}$ of the global minimizer $X^{\star}$. Unfortunately, overparameterization significantly slows down the convergence of gradient descent, from a linear rate with $r=r^{\star}$ to a sublinear rate when $r>r^{\star}$, even when $\phi$ is strongly convex. In this paper, we propose an inexpensive preconditioner that restores the convergence rate of gradient descent back to linear in the overparameterized case, while also making it agnostic to possible ill-conditioning in the global minimizer $X^{\star}$.},
 author = {Gavin Zhang and Salar Fattahi and Richard Y. Zhang},
 journal = {Journal of Machine Learning Research},
 number = {163},
 openalex = {W4281609257},
 pages = {1--55},
 title = {Preconditioned Gradient Descent for Overparameterized Nonconvex Burer--Monteiro Factorization with Global Optimality Certification},
 url = {http://jmlr.org/papers/v24/22-0882.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0902,
 abstract = {Although being a crucial question for the development of machine learning algorithms, there is still no consensus on how to compare classifiers over multiple data sets with respect to several criteria. Every comparison framework is confronted with (at least) three fundamental challenges: the multiplicity of quality criteria, the multiplicity of data sets and the randomness of the selection of data sets. In this paper, we add a fresh view to the vivid debate by adopting recent developments in decision theory. Based on so-called preference systems, our framework ranks classifiers by a generalized concept of stochastic dominance, which powerfully circumvents the cumbersome, and often even self-contradictory, reliance on aggregates. Moreover, we show that generalized stochastic dominance can be operationalized by solving easy-to-handle linear programs and moreover statistically tested employing an adapted two-sample observation-randomization test. This yields indeed a powerful framework for the statistical comparison of classifiers over multiple data sets with respect to multiple quality criteria simultaneously. We illustrate and investigate our framework in a simulation study and with a set of standard benchmark data sets.},
 author = {Christoph Jansen and Malte Nalenz and Georg Schollmeyer and Thomas Augustin},
 journal = {Journal of Machine Learning Research},
 number = {231},
 openalex = {W4297632448},
 pages = {1--37},
 title = {Statistical Comparisons of Classifiers by Generalized Stochastic Dominance},
 url = {http://jmlr.org/papers/v24/22-0902.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0907,
 abstract = {This paper proposes a Bayesian model to compare multiple algorithms on multiple data sets, on any metric. The model is based on the Bradley-Terry model, that counts the number of times one algorithm performs better than another on different data sets. Because of its Bayesian foundations, the Bayesian Bradley Terry model (BBT) has different characteristics than frequentist approaches to comparing multiple algorithms on multiple data sets, such as Demsar (2006) tests on mean rank, and Benavoli et al. (2016) multiple pairwise Wilcoxon tests with p-adjustment procedures. In particular, a Bayesian approach allows for more nuanced statements regarding the algorithms beyond claiming that the difference is or it is not statistically significant. Bayesian approaches also allow to define when two algorithms are equivalent for practical purposes, or the region of practical equivalence (ROPE). Different than a Bayesian signed rank comparison procedure proposed by Benavoli et al. (2017), our approach can define a ROPE for any metric, since it is based on probability statements, and not on differences of that metric. This paper also proposes a local ROPE concept, that evaluates whether a positive difference between a mean measure across some cross validation to the mean of some other algorithms is should be really seen as the first algorithm being better than the second, based on effect sizes. This local ROPE proposal is independent of a Bayesian use, and can be used in frequentist approaches based on ranks. A R package and a Python program that implements the BBT is available.},
 author = {Jacques Wainer},
 journal = {Journal of Machine Learning Research},
 number = {341},
 openalex = {W4290802753},
 pages = {1--34},
 title = {A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets},
 url = {http://jmlr.org/papers/v24/22-0907.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0916,
 abstract = {The d-separation criterion detects the compatibility of a joint probability distribution with a directed acyclic graph through certain conditional independences. In this work, we study this problem in the context of categorical probability theory by introducing a categorical definition of causal models, a categorical notion of d-separation, and proving an abstract version of the d-separation criterion. This approach has two main benefits. First, categorical d-separation is a very intuitive criterion based on topological connectedness. Second, our results apply both to measure-theoretic probability (with standard Borel spaces) and beyond probability theory, including to deterministic and possibilistic networks. It therefore provides a clean proof of the equivalence of local and global Markov properties with causal compatibility for continuous and mixed random variables as well as deterministic and possibilistic variables.},
 author = {Tobias Fritz and Andreas Klingler},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W4285428929},
 pages = {1--49},
 title = {The d-separation criterion in Categorical Probability},
 url = {http://jmlr.org/papers/v24/22-0916.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0917,
 author = {Chengzhuo Ni and Yaqi Duan and Munther Dahleh and Mengdi Wang and Anru R. Zhang},
 journal = {Journal of Machine Learning Research},
 number = {115},
 pages = {1--53},
 title = {Learning Good State and Action Representations for Markov Decision Process via Tensor Decomposition},
 url = {http://jmlr.org/papers/v24/22-0917.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0934,
 abstract = {We introduce a novel mathematical formulation for the training of feed-forward neural networks with (potentially non-smooth) proximal maps as activation functions. This formulation is based on Bregman distances and a key advantage is that its partial derivatives with respect to the network's parameters do not require the computation of derivatives of the network's activation functions. Instead of estimating the parameters with a combination of first-order optimisation method and back-propagation (as is the state-of-the-art), we propose the use of non-smooth first-order optimisation methods that exploit the specific structure of the novel formulation. We present several numerical results that demonstrate that these training approaches can be equally well or even better suited for the training of neural network-based classifiers and (denoising) autoencoders with sparse coding compared to more conventional training frameworks.},
 author = {Xiaoyu Wang and Martin Benning},
 journal = {Journal of Machine Learning Research},
 number = {232},
 openalex = {W4292447944},
 pages = {1--51},
 title = {Lifted Bregman Training of Neural Networks},
 url = {http://jmlr.org/papers/v24/22-0934.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0937,
 abstract = {The acquisition of labels for supervised learning can be expensive. To improve the sample efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations, and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width neural tangent kernels and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or training code. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results.},
 author = {David Holzmüller and Viktor Zaverkin and Johannes Kästner and Ingo Steinwart},
 journal = {Journal of Machine Learning Research},
 number = {164},
 openalex = {W4221139162},
 pages = {1--81},
 title = {A Framework and Benchmark for Deep Batch Active Learning for Regression},
 url = {http://jmlr.org/papers/v24/22-0937.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0964,
 author = {Ibrahim Merad and Stéphane Gaïffas},
 journal = {Journal of Machine Learning Research},
 number = {165},
 pages = {1--44},
 title = {Robust Methods for High-Dimensional Linear Learning},
 url = {http://jmlr.org/papers/v24/22-0964.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0968,
 author = {Max Olan Smith and Thomas Anthony and Michael P. Wellman},
 journal = {Journal of Machine Learning Research},
 number = {233},
 pages = {1--96},
 title = {Strategic Knowledge Transfer},
 url = {http://jmlr.org/papers/v24/22-0968.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0969,
 abstract = {This paper studies the problem of designing an optimal sequence of interventions in a causal graphical model to minimize cumulative regret with respect to the best intervention in hindsight. This is, naturally, posed as a causal bandit problem. The focus is on causal bandits for linear structural equation models (SEMs) and soft interventions. It is assumed that the graph's structure is known and has $N$ nodes. Two linear mechanisms, one soft intervention and one observational, are assumed for each node, giving rise to $2^N$ possible interventions. Majority of the existing causal bandit algorithms assume that at least the interventional distributions of the reward node's parents are fully specified. However, there are $2^N$ such distributions (one corresponding to each intervention), acquiring which becomes prohibitive even in moderate-sized graphs. This paper dispenses with the assumption of knowing these distributions or their marginals. Two algorithms are proposed for the frequentist (UCB-based) and Bayesian (Thompson Sampling-based) settings. The key idea of these algorithms is to avoid directly estimating the $2^N$ reward distributions and instead estimate the parameters that fully specify the SEMs (linear in $N$) and use them to compute the rewards. In both algorithms, under boundedness assumptions on noise and the parameter space, the cumulative regrets scale as $\tilde{\cal O} (d^{L+\frac{1}{2}} \sqrt{NT})$, where $d$ is the graph's maximum degree, and $L$ is the length of its longest causal path. Additionally, a minimax lower of $\Omega(d^{\frac{L}{2}-2}\sqrt{T})$ is presented, which suggests that the achievable and lower bounds conform in their scaling behavior with respect to the horizon $T$ and graph parameters $d$ and $L$.},
 author = {Burak Varici and Karthikeyan Shanmugam and Prasanna Sattigeri and Ali Tajer},
 journal = {Journal of Machine Learning Research},
 number = {297},
 openalex = {W4293566146},
 pages = {1--59},
 title = {Causal Bandits for Linear Structural Equation Models},
 url = {http://jmlr.org/papers/v24/22-0969.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0983,
 abstract = {In this paper we consider a composite optimization problem that minimizes the sum of a weakly smooth function and a convex function with either a bounded domain or a uniformly convex structure. In particular, we first present a parameter-dependent conditional gradient method for this problem, whose step sizes require prior knowledge of the parameters associated with the H\"older continuity of the gradient of the weakly smooth function, and establish its rate of convergence. Given that these parameters could be unknown or known but possibly conservative, such a method may suffer from implementation issue or slow convergence. We therefore propose a parameter-free conditional gradient method whose step size is determined by using a constructive local quadratic upper approximation and an adaptive line search scheme, without using any problem parameter. We show that this method achieves the same rate of convergence as the parameter-dependent conditional gradient method. Preliminary experiments are also conducted and illustrate the superior performance of the parameter-free conditional gradient method over the methods with some other step size rules.},
 author = {Masaru Ito and Zhaosong Lu and Chuan He},
 journal = {Journal of Machine Learning Research},
 number = {166},
 openalex = {W4378771326},
 pages = {1--34},
 title = {A Parameter-Free Conditional Gradient Method for Composite Minimization under Hölder Condition},
 url = {http://jmlr.org/papers/v24/22-0983.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0987,
 abstract = {Statistical decision problems lie at the heart of statistical machine learning. The simplest problems are binary and multiclass classification and class probability estimation. Central to their definition is the choice of loss function, which is the means by which the quality of a solution is evaluated. In this paper we systematically develop the theory of loss functions for such problems from a novel perspective whose basic ingredients are convex sets with a particular structure. The loss function is defined as the subgradient of the support function of the convex set. It is consequently automatically proper (calibrated for probability estimation). This perspective provides three novel opportunities. It enables the development of a fundamental relationship between losses and (anti)-norms that appears to have not been noticed before. Second, it enables the development of a calculus of losses induced by the calculus of convex sets which allows the interpolation between different losses, and thus is a potential useful design tool for tailoring losses to particular problems. In doing this we build upon, and considerably extend existing results on $M$-sums of convex sets. Third, the perspective leads to a natural theory of ``polar'' loss functions, which are derived from the polar dual of the convex set defining the loss, and which form a natural universal substitution function for Vovk's aggregating algorithm.},
 author = {Robert C. Williamson and Zac Cranko},
 journal = {Journal of Machine Learning Research},
 number = {342},
 openalex = {W4294533841},
 pages = {1--72},
 title = {The Geometry and Calculus of Losses},
 url = {http://jmlr.org/papers/v24/22-0987.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-099,
 abstract = {Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully observable case to compute an agent's optimal policy under partial observability in open ad hoc teamwork. Empirical results demonstrate that our solution can learn efficient policies in open ad hoc teamwork in fully and partially observable cases. Further analysis demonstrates that our methods' success is a result of effectively learning the effects of teammates' actions while also inferring the inherent state of the environment under partial observability.},
 author = {Arrasy Rahman and Ignacio Carlucho and Niklas Höpner and Stefano V. Albrecht},
 journal = {Journal of Machine Learning Research},
 number = {298},
 openalex = {W4305031576},
 pages = {1--74},
 title = {A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning},
 url = {http://jmlr.org/papers/v24/22-099.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1001,
 abstract = {In this paper, we study a learning problem in which a forecaster only observes partial information. By properly rescaling the problem, we heuristically derive a limiting PDE on Wasserstein space which characterizes the asymptotic behavior of the regret of the forecaster. Using a verification type argument, we show that the problem of obtaining regret bounds and efficient algorithms can be tackled by finding appropriate smooth sub/supersolutions of this parabolic PDE.},
 author = {Erhan Bayraktar and Ibrahim Ekren and Xin Zhang},
 journal = {Journal of Machine Learning Research},
 number = {299},
 openalex = {W4294956350},
 pages = {1--24},
 title = {A PDE approach for regret bounds under partial monitoring},
 url = {http://jmlr.org/papers/v24/22-1001.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1002,
 author = {Ion Matei and Maksym Zhenirovskyy and Johan de Kleer and John Maxwell},
 journal = {Journal of Machine Learning Research},
 number = {300},
 pages = {1--26},
 title = {Sensitivity-Free Gradient Descent Algorithms},
 url = {http://jmlr.org/papers/v24/22-1002.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1010,
 author = {Karl Kunisch and Donato Vásquez-Varas and Daniel Walter},
 journal = {Journal of Machine Learning Research},
 number = {301},
 pages = {1--38},
 title = {Learning Optimal Feedback Operators and their Sparse Polynomial Approximations},
 url = {http://jmlr.org/papers/v24/22-1010.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1021,
 abstract = {Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of > 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.},
 author = {Paul Pu Liang and Yiwei Lyu and Xiang Fan and Arav Agarwal and Yun Cheng and Louis-Philippe Morency and Ruslan Salakhutdinov},
 journal = {Journal of Machine Learning Research},
 number = {234},
 openalex = {W4382619411},
 pages = {1--7},
 title = {MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep Learning},
 url = {http://jmlr.org/papers/v24/22-1021.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1032,
 abstract = {We consider the linear discriminant analysis problem in the high-dimensional settings. In this work, we propose PANDA(PivotAl liNear Discriminant Analysis), a tuning-insensitive method in the sense that it requires very little effort to tune the parameters. Moreover, we prove that PANDA achieves the optimal convergence rate in terms of both the estimation error and misclassification rate. Our theoretical results are backed up by thorough numerical studies using both simulated and real datasets. In comparison with the existing methods, we observe that our proposed PANDA yields equal or better performance, and requires substantially less effort in parameter tuning.},
 author = {Ethan X. Fang and Yajun Mei and Yuyang Shi and Qunzhi Xu and Tuo Zhao},
 journal = {Journal of Machine Learning Research},
 number = {302},
 openalex = {W4386876123},
 pages = {1--45},
 title = {Pivotal Estimation of Linear Discriminant Analysis in High Dimensions},
 url = {http://jmlr.org/papers/v24/22-1032.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1043,
 abstract = {We analyse a general class of bilevel problems, in which the upper-level problem consists in the minimization of a smooth objective function and the lower-level problem is to find the fixed point of a smooth contraction map. This type of problems include instances of meta-learning, equilibrium models, hyperparameter optimization and data poisoning adversarial attacks. Several recent works have proposed algorithms which warm-start the lower-level problem, i.e.~they use the previous lower-level approximate solution as a staring point for the lower-level solver. This warm-start procedure allows one to improve the sample complexity in both the stochastic and deterministic settings, achieving in some cases the order-wise optimal sample complexity. However, there are situations, e.g., meta learning and equilibrium models, in which the warm-start procedure is not well-suited or ineffective. In this work we show that without warm-start, it is still possible to achieve order-wise (near) optimal sample complexity. In particular, we propose a simple method which uses (stochastic) fixed point iterations at the lower-level and projected inexact gradient descent at the upper-level, that reaches an $\epsilon$-stationary point using $O(\epsilon^{-2})$ and $\tilde{O}(\epsilon^{-1})$ samples for the stochastic and the deterministic setting, respectively. Finally, compared to methods using warm-start, our approach yields a simpler analysis that does not need to study the coupled interactions between the upper-level and lower-level iterates.},
 author = {Riccardo Grazzi and Massimiliano Pontil and Saverio Salzo},
 journal = {Journal of Machine Learning Research},
 number = {167},
 openalex = {W4221142334},
 pages = {1--37},
 title = {Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-start},
 url = {http://jmlr.org/papers/v24/22-1043.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1047,
 author = {Jun Zhou and Ke Zhang and Lin Wang and Hua Wu and Yi Wang and ChaoChao Chen},
 journal = {Journal of Machine Learning Research},
 number = {116},
 pages = {1--9},
 title = {SQLFlow: An Extensible Toolkit Integrating DB and AI},
 url = {http://jmlr.org/papers/v24/22-1047.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1053,
 abstract = {Consider the problem of simultaneous estimation of location and variance matrix under Huber's contaminated Gaussian model. First, we study minimum $f$-divergence estimation at the population level, corresponding to a generative adversarial method with a nonparametric discriminator and establish conditions on $f$-divergences which lead to robust estimation, similarly to robustness of minimum distance estimation. More importantly, we develop tractable adversarial algorithms with simple spline discriminators, which can be implemented via nested optimization such that the discriminator parameters can be fully updated by maximizing a concave objective function given the current generator. The proposed methods are shown to achieve minimax optimal rates or near-optimal rates depending on the $f$-divergence and the penalty used. This is the first time such near-optimal error rates are established for adversarial algorithms with linear discriminators under Huber's contamination model. We present simulation studies to demonstrate advantages of the proposed methods over classic robust estimators, pairwise methods, and a generative adversarial method with neural network discriminators.},
 author = {Ziyue Wang and Zhiqiang Tan},
 journal = {Journal of Machine Learning Research},
 number = {235},
 openalex = {W4226148600},
 pages = {1--112},
 title = {Tractable and Near-Optimal Adversarial Algorithms for Robust Estimation in Contaminated Gaussian Models},
 url = {http://jmlr.org/papers/v24/22-1053.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1065,
 abstract = {We bound the excess risk of interpolating deep linear networks trained using gradient flow. In a setting previously used to establish risk bounds for the minimum $\ell_2$-norm interpolant, we show that randomly initialized deep linear networks can closely approximate or even match known bounds for the minimum $\ell_2$-norm interpolant. Our analysis also reveals that interpolating deep linear models have exactly the same conditional variance as the minimum $\ell_2$-norm solution. Since the noise affects the excess risk only through the conditional variance, this implies that depth does not improve the algorithm's ability to "hide the noise". Our simulations verify that aspects of our bounds reflect typical behavior for simple data distributions. We also find that similar phenomena are seen in simulations with ReLU networks, although the situation there is more nuanced.},
 author = {Niladri S. Chatterji and Philip M. Long},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W4296608195},
 pages = {1--39},
 title = {Deep Linear Networks can Benignly Overfit when Shallow Ones Do},
 url = {http://jmlr.org/papers/v24/22-1065.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1069,
 author = {You Zhao and Xiaofeng Liao and Xing He and Mingliang Zhou and Chaojie Li},
 journal = {Journal of Machine Learning Research},
 number = {343},
 pages = {1--59},
 title = {Accelerated Primal-Dual Mirror Dynamics for Centralized and Distributed Constrained Convex Optimization Problems},
 url = {http://jmlr.org/papers/v24/22-1069.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1075,
 abstract = {Solving high-dimensional partial differential equations (PDEs) is a major challenge in scientific computing. We develop a new numerical method for solving elliptic-type PDEs by adapting the Q-learning algorithm in reinforcement learning. Our "Q-PDE" algorithm is mesh-free and therefore has the potential to overcome the curse of dimensionality. Using a neural tangent kernel (NTK) approach, we prove that the neural network approximator for the PDE solution, trained with the Q-PDE algorithm, converges to the trajectory of an infinite-dimensional ordinary differential equation (ODE) as the number of hidden units $\rightarrow \infty$. For monotone PDE (i.e. those given by monotone operators, which may be nonlinear), despite the lack of a spectral gap in the NTK, we then prove that the limit neural network, which satisfies the infinite-dimensional ODE, converges in $L^2$ to the PDE solution as the training time $\rightarrow \infty$. More generally, we can prove that any fixed point of the wide-network limit for the Q-PDE algorithm is a solution of the PDE (not necessarily under the monotone condition). The numerical performance of the Q-PDE algorithm is studied for several elliptic PDEs.},
 author = {Samuel N. Cohen and Deqing Jiang and Justin Sirignano},
 journal = {Journal of Machine Learning Research},
 number = {236},
 openalex = {W4224330758},
 pages = {1--49},
 title = {Neural Q-learning for solving PDEs},
 url = {http://jmlr.org/papers/v24/22-1075.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1081,
 abstract = {We consider the problem of computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. Existing non-parametric approaches for computing such bounds use linear programming (LP) formulations that quickly become intractable for existing solvers because the size of the LP grows exponentially in the number of edges in the causal graph. We show that this LP can be significantly pruned, allowing us to compute bounds for significantly larger causal inference problems compared to existing techniques. This pruning procedure allows us to compute bounds in closed form for a special class of problems, including a well-studied family of problems where multiple confounded treatments influence an outcome. We extend our pruning methodology to fractional LPs which compute bounds for causal queries which incorporate additional observations about the unit. We show that our methods provide significant runtime improvement compared to benchmarks in experiments and extend our results to the finite data setting. For causal inference without additional observations, we propose an efficient greedy heuristic that produces high quality bounds, and scales to problems that are several orders of magnitude larger than those for which the pruned LP can be solved.},
 author = {Madhumitha Shridharan and Garud Iyengar},
 journal = {Journal of Machine Learning Research},
 number = {237},
 openalex = {W4385681106},
 pages = {1--35},
 title = {Scalable Computation of Causal Bounds},
 url = {http://jmlr.org/papers/v24/22-1081.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1085,
 abstract = {Graph coarsening is a widely used dimensionality reduction technique for approaching large-scale graph machine learning problems. Given a large graph, graph coarsening aims to learn a smaller-tractable graph while preserving the properties of the originally given graph. Graph data consist of node features and graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening methods ignore the node features and rely solely on a graph matrix to simplify graphs. In this paper, we introduce a novel optimization-based framework for graph dimensionality reduction. The proposed framework lies in the unification of graph learning and dimensionality reduction. It takes both the graph matrix and the node features as the input and learns the coarsen graph matrix and the coarsen feature matrix jointly while ensuring desired properties. The proposed optimization formulation is a multi-block non-convex optimization problem, which is solved efficiently by leveraging block majorization-minimization, $\log$ determinant, Dirichlet energy, and regularization frameworks. The proposed algorithms are provably convergent and practically amenable to numerous tasks. It is also established that the learned coarsened graph is $\epsilon\in(0,1)$ similar to the original graph. Extensive experiments elucidate the efficacy of the proposed framework for real-world applications.},
 author = {Manoj Kumar and Anurag Sharma and Sandeep Kumar},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W4302011352},
 pages = {1--50},
 title = {A Unified Framework for Optimization-Based Graph Coarsening},
 url = {http://jmlr.org/papers/v24/22-1085.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1086,
 abstract = {We study the ranking of individuals, teams, or objects, based on pairwise comparisons between them, using the Bradley-Terry model. Estimates of rankings within this model are commonly made using a simple iterative algorithm first introduced by Zermelo almost a century ago. Here we describe an alternative and similarly simple iteration that provably returns identical results but does so much faster -- over a hundred times faster in some cases. We demonstrate this algorithm with applications to a range of example data sets and derive a number of results regarding its convergence.},
 author = {M. E. J. Newman},
 journal = {Journal of Machine Learning Research},
 number = {238},
 openalex = {W4283812891},
 pages = {1--25},
 title = {Efficient computation of rankings from pairwise comparisons},
 url = {http://jmlr.org/papers/v24/22-1086.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1089,
 abstract = {We prove that the dynamics of the MBO scheme for data clustering converge to a viscosity solution to mean curvature flow. The main ingredients are (i) a new abstract convergence result based on quantitative estimates for heat operators and (ii) the derivation of these estimates in the setting of random geometric graphs. To implement the scheme in practice, two important parameters are the number of eigenvalues for computing the heat operator and the step size of the scheme. The results of the current paper give a theoretical justification for the choice of these parameters in relation to sample size and interaction width.},
 author = {Tim Laux and Jona Lelmi},
 journal = {Journal of Machine Learning Research},
 number = {344},
 openalex = {W4297835500},
 pages = {1--49},
 title = {Large data limit of the MBO scheme for data clustering: convergence of the dynamics},
 url = {http://jmlr.org/papers/v24/22-1089.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1104,
 author = {Oh-Ran Kwon and Hui Zou},
 journal = {Journal of Machine Learning Research},
 number = {239},
 pages = {1--40},
 title = {Leaky Hockey Stick Loss: The First Negatively Divergent Margin-based Loss Function for Classification},
 url = {http://jmlr.org/papers/v24/22-1104.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1122,
 author = {Abhishek Kaul and Hongjin Zhang and Konstantinos Tsampourakis and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {168},
 pages = {1--68},
 title = {Inference on the Change Point under a High Dimensional Covariance Shift},
 url = {http://jmlr.org/papers/v24/22-1122.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1131,
 abstract = {In many applications, a large number of features are collected with the goal to identify a few important ones. Sometimes, these features lie in a metric space with a known distance matrix, which partially reflects their co-importance pattern. Proper use of the distance matrix will boost the power of identifying important features. Hence, we develop a new multiple testing framework named the Distance Assisted Recursive Testing (DART). DART has two stages. In stage 1, we transform the distance matrix into an aggregation tree, where each node represents a set of features. In stage 2, based on the aggregation tree, we set up dynamic node hypotheses and perform multiple testing on the tree. All rejections are mapped back to the features. Under mild assumptions, the false discovery proportion of DART converges to the desired level in high probability converging to one. We illustrate by theory and simulations that DART has superior performance under various models compared to the existing methods. We applied DART to a clinical trial in the allogeneic stem cell transplantation study to identify the gut microbiota whose abundance will be impacted by the after-transplant care.},
 author = {Xuechan Li and Anthony D. Sung and Jichun Xie},
 journal = {Journal of Machine Learning Research},
 number = {169},
 openalex = {W3139383728},
 pages = {1--41},
 title = {Distance Assisted Recursive Testing},
 url = {http://jmlr.org/papers/v24/22-1131.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1132,
 abstract = {In this work, we provide a characterization of the feature-learning process in two-layer ReLU networks trained by gradient descent on the logistic loss following random initialization. We consider data with binary labels that are generated by an XOR-like function of the input features. We permit a constant fraction of the training labels to be corrupted by an adversary. We show that, although linear classifiers are no better than random guessing for the distribution we consider, two-layer ReLU networks trained by gradient descent achieve generalization error close to the label noise rate. We develop a novel proof technique that shows that at initialization, the vast majority of neurons function as random features that are only weakly correlated with useful features, and the gradient descent dynamics 'amplify' these weak, random features to strong, useful features.},
 author = {Spencer Frei and Niladri S. Chatterji and Peter L. Bartlett},
 journal = {Journal of Machine Learning Research},
 number = {303},
 openalex = {W4226426379},
 pages = {1--49},
 title = {Random Feature Amplification: Feature Learning and Generalization in Neural Networks},
 url = {http://jmlr.org/papers/v24/22-1132.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1136,
 abstract = {Maximum Mean Discrepancy (MMD) has been widely used in the areas of machine learning and statistics to quantify the distance between two distributions in the $p$-dimensional Euclidean space. The asymptotic property of the sample MMD has been well studied when the dimension $p$ is fixed using the theory of U-statistic. As motivated by the frequent use of MMD test for data of moderate/high dimension, we propose to investigate the behavior of the sample MMD in a high-dimensional environment and develop a new studentized test statistic. Specifically, we obtain the central limit theorems for the studentized sample MMD as both the dimension $p$ and sample sizes $n,m$ diverge to infinity. Our results hold for a wide range of kernels, including popular Gaussian and Laplacian kernels, and also cover energy distance as a special case. We also derive the explicit rate of convergence under mild assumptions and our results suggest that the accuracy of normal approximation can improve with dimensionality. Additionally, we provide a general theory on the power analysis under the alternative hypothesis and show that our proposed test can detect difference between two distributions in the moderately high dimensional regime. Numerical simulations demonstrate the effectiveness of our proposed test statistic and normal approximation.},
 author = {Hanjia Gao and Xiaofeng Shao},
 journal = {Journal of Machine Learning Research},
 number = {304},
 openalex = {W3203519543},
 pages = {1--33},
 title = {Two Sample Testing in High Dimension via Maximum Mean Discrepancy},
 url = {http://jmlr.org/papers/v24/22-1136.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1138,
 abstract = {Directed networks are conveniently represented as graphs in which ordered edges encode interactions between vertices. Despite their wide availability, there is a shortage of statistical models amenable for inference, specially when contextual information and degree heterogeneity are present. This paper presents an annotated graph model with parameters explicitly accounting for these features. To overcome the curse of dimensionality due to modelling degree heterogeneity, we introduce a sparsity assumption and propose a penalized likelihood approach with $\ell_1$-regularization for parameter estimation. We study the estimation and selection consistency of this approach under a sparse network assumption, and show that inference on the covariate parameter is straightforward, thus bypassing the need for the kind of debiasing commonly employed in $\ell_1$-penalized likelihood estimation. Simulation and data analysis corroborate our theoretical findings.},
 author = {Stefan Stein and Chenlei Leng},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W3193782855},
 pages = {1--69},
 title = {An Annotated Graph Model with Differential Degree Heterogeneity for Directed Networks},
 url = {http://jmlr.org/papers/v24/22-1138.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1144,
 abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
 author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
 journal = {Journal of Machine Learning Research},
 number = {240},
 openalex = {W4224308101},
 pages = {1--113},
 title = {PaLM: Scaling Language Modeling with Pathways},
 url = {http://jmlr.org/papers/v24/22-1144.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1149,
 author = {Zhuang Yang},
 journal = {Journal of Machine Learning Research},
 number = {241},
 pages = {1--29},
 title = {Improved Powered Stochastic Optimization Algorithms for Large-Scale Machine Learning},
 url = {http://jmlr.org/papers/v24/22-1149.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1153,
 abstract = {Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed, that is, when the predictions of the regression model are insensitive to small perturbations of the data. This article identifies scenarios where the maximum likelihood estimator fails to be well-posed, in that the predictive distributions are not Lipschitz in the data with respect to the Hellinger distance. These failure cases occur in the noiseless data setting, for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is part of Gaussian process folklore, these rigorous theoretical results appear to be the first of their kind. The implication of these negative results is that well-posedness may need to be assessed post-hoc, on a case-by-case basis, when maximum likelihood estimation is used to train a Gaussian process model.},
 author = {Toni Karvonen and Chris J. Oates},
 journal = {Journal of Machine Learning Research},
 number = {120},
 openalex = {W4221154418},
 pages = {1--47},
 title = {Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed},
 url = {http://jmlr.org/papers/v24/22-1153.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1154,
 abstract = {Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled - yet practical - probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning problem, we are able to achieve state-of-the-art performance while controlling the sparsity of the learned graph and the computational scalability. We empirically assess the effectiveness of the proposed method on synthetic and real-world benchmarks, showing that the proposed solution can be used as a stand-alone graph identification procedure as well as a graph learning component of an end-to-end forecasting architecture.},
 author = {Andrea Cini and Daniele Zambon and Cesare Alippi},
 journal = {Journal of Machine Learning Research},
 number = {242},
 openalex = {W4281727593},
 pages = {1--36},
 title = {Sparse Graph Learning from Spatiotemporal Time Series},
 url = {http://jmlr.org/papers/v24/22-1154.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1158,
 abstract = {We study the Unbalanced Optimal Transport (UOT) between two measures of possibly different masses with at most $n$ components, where the marginal constraints of standard Optimal Transport (OT) are relaxed via Kullback-Leibler divergence with regularization factor $\tau$. Although only Sinkhorn-based UOT solvers have been analyzed in the literature with the iteration complexity of ${O}\big(\tfrac{\tau \log(n)}{\varepsilon} \log\big(\tfrac{\log(n)}{{\varepsilon}}\big)\big)$ and per-iteration cost of $O(n^2)$ for achieving the desired error $\varepsilon$, their positively dense output transportation plans strongly hinder the practicality. On the other hand, while being vastly used as heuristics for computing UOT in modern deep learning applications and having shown success in sparse OT problem, gradient methods applied to UOT have not been formally studied. In this paper, we propose a novel algorithm based on Gradient Extrapolation Method (GEM-UOT) to find an $\varepsilon$-approximate solution to the UOT problem in $O\big( \kappa \log\big(\frac{\tau n}{\varepsilon}\big) \big)$ iterations with $\widetilde{O}(n^2)$ per-iteration cost, where $\kappa$ is the condition number depending on only the two input measures. Our proof technique is based on a novel dual formulation of the squared $\ell_2$-norm UOT objective, which fills the lack of sparse UOT literature and also leads to a new characterization of approximation error between UOT and OT. To this end, we further present a novel approach of OT retrieval from UOT, which is based on GEM-UOT with fine tuned $\tau$ and a post-process projection step. Extensive experiments on synthetic and real datasets validate our theories and demonstrate the favorable performance of our methods in practice.},
 author = {Quang Minh Nguyen and Hoang H. Nguyen and Yi Zhou and Lam M. Nguyen},
 journal = {Journal of Machine Learning Research},
 number = {384},
 openalex = {W4320169968},
 pages = {1--41},
 title = {On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error},
 url = {http://jmlr.org/papers/v24/22-1158.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1160,
 abstract = {Several algorithms involving the Variational R\'enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the Importance Weighted Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the benefits or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.},
 author = {Kamélia Daudel and Joe Benton and Yuyang Shi and Arnaud Doucet},
 journal = {Journal of Machine Learning Research},
 number = {243},
 openalex = {W4306178101},
 pages = {1--83},
 title = {Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics},
 url = {http://jmlr.org/papers/v24/22-1160.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1176,
 abstract = {Decision making or scientific discovery pipelines such as job hiring and drug discovery often involve multiple stages: before any resource-intensive step, there is often an initial screening that uses predictions from a machine learning model to shortlist a few candidates from a large pool. We study screening procedures that aim to select candidates whose unobserved outcomes exceed user-specified values. We develop a method that wraps around any prediction model to produce a subset of candidates while controlling the proportion of falsely selected units. Building upon the conformal inference framework, our method first constructs p-values that quantify the statistical evidence for large outcomes; it then determines the shortlist by comparing the p-values to a threshold introduced in the multiple testing literature. In many cases, the procedure selects candidates whose predictions are above a data-dependent threshold. Our theoretical guarantee holds under mild exchangeability conditions on the samples, generalizing existing results on multiple conformal p-values. We demonstrate the empirical performance of our method via simulations, and apply it to job hiring and drug discovery datasets.},
 author = {Ying Jin and Emmanuel J. Candes},
 journal = {Journal of Machine Learning Research},
 number = {244},
 openalex = {W4302306162},
 pages = {1--41},
 title = {Selection by Prediction with Conformal p-values},
 url = {http://jmlr.org/papers/v24/22-1176.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1181,
 abstract = {This paper revisits the bandit problem in the Bayesian setting. The Bayesian approach formulates the bandit problem as an optimization problem, and the goal is to find the optimal policy which minimizes the Bayesian regret. One of the main challenges facing the Bayesian approach is that computation of the optimal policy is often intractable, especially when the length of the problem horizon or the number of arms is large. In this paper, we first show that under a suitable rescaling, the Bayesian bandit problem converges toward a continuous Hamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJB equation can be explicitly obtained for several common bandit problems, and we give numerical methods to solve the HJB equation when an explicit solution is not available. Based on these results, we propose an approximate Bayes-optimal policy for solving Bayesian bandit problems with large horizons. Our method has the added benefit that its computational cost does not increase as the horizon increases.},
 author = {Yuhua Zhu and Zachary Izzo and Lexing Ying},
 journal = {Journal of Machine Learning Research},
 number = {305},
 openalex = {W4320713465},
 pages = {1--35},
 title = {Continuous-in-time Limit for Bayesian Bandits},
 url = {http://jmlr.org/papers/v24/22-1181.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1190,
 abstract = {We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover, by embedding the visitation measure into a finite-dimensional space, we can handle large state spaces by incorporating function approximation. Two notable examples are (1) Kernelized Nonlinear Regulators and (2) Low-rank MDPs. We prove that with an optimistic planning oracle, our algorithm achieves sublinear regret and constraint violation in both cases and can attain the globally optimal policy of the original constrained problem.},
 author = {Zihao Li and Boyi Liu and Zhuoran Yang and Zhaoran Wang and Mengdi Wang},
 journal = {Journal of Machine Learning Research},
 number = {385},
 openalex = {W4391948931},
 pages = {1--43},
 title = {Double Duality: Variational Primal-Dual Policy Optimization for
  Constrained Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/22-1190.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1191,
 abstract = {A recurrent neural network (RNN) is a widely used deep-learning network for dealing with sequential data. Imitating a dynamical system, an infinite-width RNN can approximate any open dynamical system in a compact domain. In general, deep networks with bounded widths are more effective than wide networks in practice; however, the universal approximation theorem for deep narrow structures has yet to be extensively studied. In this study, we prove the universality of deep narrow RNNs and show that the upper bound of the minimum width for universality can be independent of the length of the data. Specifically, we show that a deep RNN with ReLU activation can approximate any continuous function or $L^p$ function with the widths $d_x+d_y+2$ and $\max\{d_x+1,d_y\}$, respectively, where the target function maps a finite sequence of vectors in $\mathbb{R}^{d_x}$ to a finite sequence of vectors in $\mathbb{R}^{d_y}$. We also compute the additional width required if the activation function is $\tanh$ or more. In addition, we prove the universality of other recurrent networks, such as bidirectional RNNs. Bridging a multi-layer perceptron and an RNN, our theory and proof technique can be an initial step toward further research on deep RNNs.},
 author = {Chang hoon Song and Geonho Hwang and Jun ho Lee and Myungjoo Kang},
 journal = {Journal of Machine Learning Research},
 number = {121},
 openalex = {W4310281035},
 pages = {1--41},
 title = {Minimal Width for Universal Property of Deep RNN},
 url = {http://jmlr.org/papers/v24/22-1191.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1193,
 abstract = {In this paper, we study the Radial Basis Function (RBF) approximation to differential operators on smooth tensor fields defined on closed Riemannian submanifolds of Euclidean space, identified by randomly sampled point cloud data. {The formulation in this paper leverages a fundamental fact that the covariant derivative on a submanifold is the projection of the directional derivative in the ambient Euclidean space onto the tangent space of the submanifold. To differentiate a test function (or vector field) on the submanifold with respect to the Euclidean metric, the RBF interpolation is applied to extend the function (or vector field) in the ambient Euclidean space. When the manifolds are unknown, we develop an improved second-order local SVD technique for estimating local tangent spaces on the manifold. When the classical pointwise non-symmetric RBF formulation is used to solve Laplacian eigenvalue problems, we found that while accurate estimation of the leading spectra can be obtained with large enough data, such an approximation often produces irrelevant complex-valued spectra (or pollution) as the true spectra are real-valued and positive. To avoid such an issue,} we introduce a symmetric RBF discrete approximation of the Laplacians induced by a weak formulation on appropriate Hilbert spaces. Unlike the non-symmetric approximation, this formulation guarantees non-negative real-valued spectra and the orthogonality of the eigenvectors. Theoretically, we establish the convergence of the eigenpairs of both the Laplace-Beltrami operator and Bochner Laplacian {for the symmetric formulation} in the limit of large data with convergence rates. Numerically, we provide supporting examples for approximations of the Laplace-Beltrami operator and various vector Laplacians, including the Bochner, Hodge, and Lichnerowicz Laplacians.},
 author = {John Harlim and Shixiao Willing Jiang and John Wilson Peoples},
 journal = {Journal of Machine Learning Research},
 number = {345},
 openalex = {W4299913734},
 pages = {1--85},
 title = {Radial basis approximation of tensor fields on manifolds: From operator estimation to manifold learning},
 url = {http://jmlr.org/papers/v24/22-1193.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1208,
 abstract = {Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (illustrated via experiments) and offer taxonomic mitigation strategies for them. Finally, we highlight some unsolved research challenges and present promising future directions.},
 author = {Brian R. Bartoldson and Bhavya Kailkhura and Davis Blalock},
 journal = {Journal of Machine Learning Research},
 number = {122},
 openalex = {W4306295041},
 pages = {1--77},
 title = {Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities},
 url = {http://jmlr.org/papers/v24/22-1208.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1210,
 abstract = {This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does \emph{not} require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov's acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.},
 author = {Haishan Ye and Luo Luo and Ziang Zhou and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {306},
 openalex = {W3023928325},
 pages = {1--50},
 title = {Multi-consensus Decentralized Accelerated Gradient Descent},
 url = {http://jmlr.org/papers/v24/22-1210.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1217,
 author = {Yibo Yan and Xiaozhou Wang and Riquan Zhang},
 journal = {Journal of Machine Learning Research},
 number = {245},
 pages = {1--49},
 title = {Confidence Intervals and Hypothesis Testing for High-dimensional Quantile Regression: Convolution Smoothing and Debiasing},
 url = {http://jmlr.org/papers/v24/22-1217.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1246,
 abstract = {We study representations of data from an arbitrary metric space $\mathcal{X}$ in the space of univariate Gaussian mixtures with a transport metric (Delon and Desolneux 2020). We derive embedding guarantees for feature maps implemented by small neural networks called \emph{probabilistic transformers}. Our guarantees are of memorization type: we prove that a probabilistic transformer of depth about $n\log(n)$ and width about $n^2$ can bi-H\"{o}lder embed any $n$-point dataset from $\mathcal{X}$ with low metric distortion, thus avoiding the curse of dimensionality. We further derive probabilistic bi-Lipschitz guarantees, which trade off the amount of distortion and the probability that a randomly chosen pair of points embeds with that distortion. If $\mathcal{X}$'s geometry is sufficiently regular, we obtain stronger, bi-Lipschitz guarantees for all points in the dataset. As applications, we derive neural embedding guarantees for datasets from Riemannian manifolds, metric trees, and certain types of combinatorial graphs. When instead embedding into multivariate Gaussian mixtures, we show that probabilistic transformers can compute bi-H\"{o}lder embeddings with arbitrarily small distortion.},
 author = {Anastasis Kratsios and Valentin Debarnot and Ivan Dokmanić},
 journal = {Journal of Machine Learning Research},
 number = {170},
 openalex = {W4297834325},
 pages = {1--48},
 title = {Small Transformers Compute Universal Metric Embeddings},
 url = {http://jmlr.org/papers/v24/22-1246.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1248,
 abstract = {Partial monitoring is an expressive framework for sequential decision-making with an abundance of applications, including graph-structured and dueling bandits, dynamic pricing and transductive feedback models. We survey and extend recent results on the linear formulation of partial monitoring that naturally generalizes the standard linear bandit setting. The main result is that a single algorithm, information-directed sampling (IDS), is (nearly) worst-case rate optimal in all finite-action games. We present a simple and unified analysis of stochastic partial monitoring, and further extend the model to the contextual and kernelized setting.},
 author = {Johannes Kirschner and Tor Lattimore and Andreas Krause},
 journal = {Journal of Machine Learning Research},
 number = {346},
 openalex = {W4319653952},
 pages = {1--45},
 title = {Linear Partial Monitoring for Sequential Decision-Making: Algorithms, Regret Bounds and Applications},
 url = {http://jmlr.org/papers/v24/22-1248.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-125,
 abstract = {Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular models is graph attention networks. They were introduced to allow a node to aggregate information from features of neighbor nodes in a non-uniform way, in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we theoretically study the behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here, the node features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention is able to distinguish inter-class from intra-class edges. Thus it maintains the weights of important edges and significantly reduces the weights of unimportant edges. Consequently, we show that this implies perfect node classification. In the "hard" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. In addition, we show that graph attention convolution cannot (almost) perfectly classify the nodes even if intra-class edges could be separated from inter-class edges. Beyond perfect node classification, we provide a positive result on graph attention's robustness against structural noise in the graph. In particular, our robustness result implies that graph attention can be strictly better than both the simple graph convolution and the best linear classifier of node features. We evaluate our theoretical results on synthetic and real-world data.},
 author = {Kimon Fountoulakis and Amit Levi and Shenghao Yang and Aseem Baranwal and Aukosh Jagannath},
 journal = {Journal of Machine Learning Research},
 number = {246},
 openalex = {W4226093342},
 pages = {1--52},
 title = {Graph Attention Retrospective},
 url = {http://jmlr.org/papers/v24/22-125.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1254,
 abstract = {Meta-Learning aims to speed up the learning process on new tasks by acquiring useful inductive biases from datasets of related learning tasks. While, in practice, the number of related tasks available is often small, most of the existing approaches assume an abundance of tasks; making them unrealistic and prone to overfitting. A central question in the meta-learning literature is how to regularize to ensure generalization to unseen tasks. In this work, we provide a theoretical analysis using the PAC-Bayesian theory and present a generalization bound for meta-learning, which was first derived by Rothfuss et al. (2021a). Crucially, the bound allows us to derive the closed form of the optimal hyper-posterior, referred to as PACOH, which leads to the best performance guarantees. We provide a theoretical analysis and empirical case study under which conditions and to what extent these guarantees for meta-learning improve upon PAC-Bayesian per-task learning bounds. The closed-form PACOH inspires a practical meta-learning approach that avoids the reliance on bi-level optimization, giving rise to a stochastic optimization problem that is amenable to standard variational methods that scale well. Our experiments show that, when instantiating the PACOH with Gaussian processes and Bayesian Neural Networks models, the resulting methods are more scalable, and yield state-of-the-art performance, both in terms of predictive accuracy and the quality of uncertainty estimates.},
 author = {Jonas Rothfuss and Martin Josifoski and Vincent Fortuin and Andreas Krause},
 journal = {Journal of Machine Learning Research},
 number = {386},
 openalex = {W4309134309},
 pages = {1--62},
 title = {Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice},
 url = {http://jmlr.org/papers/v24/22-1254.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1256,
 abstract = {This paper is concerned with the low Tucker-rank tensor completion problem, which is about reconstructing a tensor $ T \in\mathbb{R}^{n\times n \times n}$ of low multilinear rank from partially observed entries. Riemannian optimization algorithms are a class of efficient methods for this problem, but the theoretical convergence analysis is still lacking. In this manuscript, we establish the entrywise convergence of the vanilla Riemannian gradient method for low Tucker-rank tensor completion under the nearly optimal sampling complexity $O(n^{3/2})$. Meanwhile, the implicit regularization phenomenon of the algorithm has also been revealed. As far as we know, this is the first work that has shown the entrywise convergence and implicit regularization property of a non-convex method for low Tucker-rank tensor completion. The analysis relies on the leave-one-out technique, and some of the technical results developed in the paper might be of broader interest in investigating the properties of other non-convex methods for this problem.},
 author = {Haifeng Wang and Jinchi Chen and Ke Wei},
 journal = {Journal of Machine Learning Research},
 number = {347},
 openalex = {W3211987814},
 pages = {1--84},
 title = {Implicit Regularization and Entrywise Convergence of Riemannian Optimization for Low Tucker-Rank Tensor Completion},
 url = {http://jmlr.org/papers/v24/22-1256.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1274,
 author = {Jia Gu and Song Xi Chen},
 journal = {Journal of Machine Learning Research},
 number = {387},
 pages = {1--57},
 title = {Distributed Statistical Inference under Heterogeneity},
 url = {http://jmlr.org/papers/v24/22-1274.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1278,
 abstract = {This paper develops conformal inference methods to construct a confidence interval for the frequency of a queried object in a very large discrete data set, based on a sketch with a lower memory footprint. This approach requires no knowledge of the data distribution and can be combined with any sketching algorithm, including but not limited to the renowned count-min sketch, the count-sketch, and variations thereof. After explaining how to achieve marginal coverage for exchangeable random queries, we extend our solution to provide stronger inferences that can account for the discreteness of the data and for heterogeneous query frequencies, increasing also robustness to possible distribution shifts. These results are facilitated by a novel conformal calibration technique that guarantees valid coverage for a large fraction of distinct random queries. Finally, we show our methods have improved empirical performance compared to existing frequentist and Bayesian alternatives in simulations as well as in examples of text and SARS-CoV-2 DNA data.},
 author = {Matteo Sesia and Stefano Favaro and Edgar Dobriban},
 journal = {Journal of Machine Learning Research},
 number = {348},
 openalex = {W4308756715},
 pages = {1--80},
 title = {Conformal Frequency Estimation using Discrete Sketched Data with Coverage for Distinct Queries},
 url = {http://jmlr.org/papers/v24/22-1278.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1293,
 abstract = {Existing generalization bounds fail to explain crucial factors that drive the generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the learned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are meaningful and capture the effect of popular regularization methods during training.},
 author = {Songyan Hou and Parnian Kassraie and Anastasis Kratsios and Andreas Krause and Jonas Rothfuss},
 journal = {Journal of Machine Learning Research},
 number = {349},
 openalex = {W4308162092},
 pages = {1--51},
 title = {Instance-Dependent Generalization Bounds via Optimal Transport},
 url = {http://jmlr.org/papers/v24/22-1293.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1302,
 author = {Xiaolong Cui and Lei Shi and Wei Zhong and Changliang Zou},
 journal = {Journal of Machine Learning Research},
 number = {350},
 pages = {1--57},
 title = {Robust High-Dimensional Low-Rank Matrix Estimation: Optimal Rate and Data-Adaptive Tuning},
 url = {http://jmlr.org/papers/v24/22-1302.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1305,
 abstract = {The problems of Lasso regression and optimal design of experiments share a critical property: their optimal solutions are typically \emph{sparse}, i.e., only a small fraction of the optimal variables are non-zero. Therefore, the identification of the support of an optimal solution reduces the dimensionality of the problem and can yield a substantial simplification of the calculations. It has recently been shown that linear regression with a \emph{squared} $\ell_1$-norm sparsity-inducing penalty is equivalent to an optimal experimental design problem. In this work, we use this equivalence to derive safe screening rules that can be used to discard inessential samples. Compared to previously existing rules, the new tests are much faster to compute, especially for problems involving a parameter space of high dimension, and can be used dynamically within any iterative solver, with negligible computational overhead. Moreover, we show how an existing homotopy algorithm to compute the regularization path of the lasso method can be reparametrized with respect to the squared $\ell_1$-penalty. This allows the computation of a Bayes $c$-optimal design in a finite number of steps and can be several orders of magnitude faster than standard first-order algorithms. The efficiency of the new screening rules and of the homotopy algorithm are demonstrated on different examples based on real data.},
 author = {Guillaume Sagnol and Luc Pronzato},
 journal = {Journal of Machine Learning Research},
 number = {307},
 openalex = {W4387687281},
 pages = {1--32},
 title = {Fast Screening Rules for Optimal Design via Quadratic Lasso Reformulation},
 url = {http://jmlr.org/papers/v24/22-1305.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1311,
 abstract = {Sinkhorn algorithm has been used pervasively to approximate the solution to optimal transport (OT) and unbalanced optimal transport (UOT) problems. However, its practical application is limited due to the high computational complexity. To alleviate the computational burden, we propose a novel importance sparsification method, called Spar-Sink, to efficiently approximate entropy-regularized OT and UOT solutions. Specifically, our method employs natural upper bounds for unknown optimal transport plans to establish effective sampling probabilities, and constructs a sparse kernel matrix to accelerate Sinkhorn iterations, reducing the computational cost of each iteration from $O(n^2)$ to $\widetilde{O}(n)$ for a sample of size $n$. Theoretically, we show the proposed estimators for the regularized OT and UOT problems are consistent under mild regularity conditions. Experiments on various synthetic data demonstrate Spar-Sink outperforms mainstream competitors in terms of both estimation error and speed. A real-world echocardiogram data analysis shows Spar-Sink can effectively estimate and visualize cardiac cycles, from which one can identify heart failure and arrhythmia. To evaluate the numerical accuracy of cardiac cycle prediction, we consider the task of predicting the end-systole time point using the end-diastole one. Results show Spar-Sink performs as well as the classical Sinkhorn algorithm, requiring significantly less computational time.},
 author = {Mengyu Li and Jun Yu and Tao Li and Cheng Meng},
 journal = {Journal of Machine Learning Research},
 number = {247},
 openalex = {W4380551942},
 pages = {1--44},
 title = {Importance Sparsification for Sinkhorn Algorithm},
 url = {http://jmlr.org/papers/v24/22-1311.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1318,
 abstract = {This paper develops a new framework, called modular regression, to utilize auxiliary information -- such as variables other than the original features or additional data sets -- in the training process of linear models. At a high level, our method follows the routine: (i) decomposing the regression task into several sub-tasks, (ii) fitting the sub-task models, and (iii) using the sub-task models to provide an improved estimate for the original regression problem. This routine applies to widely-used low-dimensional (generalized) linear models and high-dimensional regularized linear regression. It also naturally extends to missing-data settings where only partial observations are available. By incorporating auxiliary information, our approach improves the estimation efficiency and prediction accuracy upon linear regression or the Lasso under a conditional independence assumption for predicting the outcome. For high-dimensional settings, we develop an extension of our procedure that is robust to violations of the conditional independence assumption, in the sense that it improves efficiency if this assumption holds and coincides with the Lasso otherwise. We demonstrate the efficacy of our methods with simulated and real data sets.},
 author = {Ying Jin and Dominik Rothenhäusler},
 journal = {Journal of Machine Learning Research},
 number = {351},
 openalex = {W4309584565},
 pages = {1--52},
 title = {Modular Regression: Improving Linear Models by Incorporating Auxiliary Data},
 url = {http://jmlr.org/papers/v24/22-1318.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1327,
 abstract = {This article aims to seek a selection and estimation procedure for a class of tensor regression problems with multivariate covariates and matrix responses, which can provide theoretical guarantees for model selection in finite samples. Considering the frontal slice sparsity and low-rankness inherited in the coefficient tensor, we formulate the regression procedure as a group SLOPE penalized low-rank tensor optimization problem based on an orthogonal decomposition, namely TgSLOPE. This procedure provably controls the newly introduced tensor group false discovery rate (TgFDR), provided that the predictor matrix is column-orthogonal. Moreover, we establish the asymptotically minimax convergence with respect to the TgSLOPE estimate risk. For efficient problem resolution, we equivalently transform the TgSLOPE problem into a difference-of-convex (DC) program with the level-coercive objective function. This allows us to solve the reformulation problem of TgSLOPE by an efficient proximal DC algorithm (DCA) with global convergence. Numerical studies conducted on synthetic data and a real human brain connection data illustrate the efficacy of the proposed TgSLOPE estimation procedure.},
 author = {Yang Chen and Ziyan Luo},
 journal = {Journal of Machine Learning Research},
 number = {352},
 openalex = {W4293168036},
 pages = {1--30},
 title = {Group SLOPE Penalized Low-Rank Tensor Regression},
 url = {http://jmlr.org/papers/v24/22-1327.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1345,
 abstract = {A shared goal of several machine learning communities like continual learning, meta-learning and transfer learning, is to design algorithms and models that efficiently and robustly adapt to unseen tasks. An even more ambitious goal is to build models that never stop adapting, and that become increasingly more efficient through time by suitably transferring the accrued knowledge. Beyond the study of the actual learning algorithm and model architecture, there are several hurdles towards our quest to build such models, such as the choice of learning protocol, metric of success and data needed to validate research hypotheses. In this work, we introduce the Never-Ending VIsual-classification Stream (NEVIS'22), a benchmark consisting of a stream of over 100 visual classification tasks, sorted chronologically and extracted from papers sampled uniformly from computer vision proceedings spanning the last three decades. The resulting stream reflects what the research community thought was meaningful at any point in time, and it serves as an ideal test bed to assess how well models can adapt to new tasks, and do so better and more efficiently as time goes by. Despite being limited to classification, the resulting stream has a rich diversity of tasks from OCR, to texture analysis, scene recognition, and so forth. The diversity is also reflected in the wide range of dataset sizes, spanning over four orders of magnitude. Overall, NEVIS'22 poses an unprecedented challenge for current sequential learning approaches due to the scale and diversity of tasks, yet with a low entry barrier as it is limited to a single modality and well understood supervised learning problems. Moreover, we provide a reference implementation including strong baselines and an evaluation protocol to compare methods in terms of their trade-off between accuracy and compute.},
 author = {Jorg Bornschein and Alexandre Galashov and Ross Hemsley and Amal Rannen-Triki and Yutian Chen and Arslan Chaudhry and Xu Owen He and Arthur Douillard and Massimo Caccia and Qixuan Feng and Jiajun Shen and Sylvestre-Alvise Rebuffi and Kitty Stacpoole and Diego de las Casas and Will Hawkins and Angeliki Lazaridou and Yee Whye Teh and Andrei A. Rusu and Razvan Pascanu and Marc’Aurelio Ranzato},
 journal = {Journal of Machine Learning Research},
 number = {308},
 openalex = {W4309865872},
 pages = {1--77},
 title = {NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research},
 url = {http://jmlr.org/papers/v24/22-1345.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1351,
 abstract = {Multiple-try Metropolis (MTM) is a popular Markov chain Monte Carlo method with the appealing feature of being amenable to parallel computing. At each iteration, it samples several candidates for the next state of the Markov chain and randomly selects one of them based on a weight function. The canonical weight function is proportional to the target density. We show both theoretically and empirically that this weight function induces pathological behaviours in high dimensions, especially during the convergence phase. We propose to instead use weight functions akin to the locally-balanced proposal distributions of Zanella (2020), thus yielding MTM algorithms that do not exhibit those pathological behaviours. To theoretically analyse these algorithms, we study the high-dimensional performance of ideal schemes that can be thought of as MTM algorithms which sample an infinite number of candidates at each iteration, as well as the discrepancy between such schemes and the MTM algorithms which sample a finite number of candidates. Our analysis unveils a strong distinction between the convergence and stationary phases: in the former, local balancing is crucial and effective to achieve fast convergence, while in the latter, the canonical and novel weight functions yield similar performance. Numerical experiments include an application in precision medicine involving a computationally-expensive forward model, which makes the use of parallel computing within MTM iterations beneficial.},
 author = {Philippe Gagnon and Florian Maire and Giacomo Zanella},
 journal = {Journal of Machine Learning Research},
 number = {248},
 openalex = {W4309804929},
 pages = {1--59},
 title = {Improving multiple-try Metropolis with local balancing},
 url = {http://jmlr.org/papers/v24/22-1351.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1381,
 abstract = {We prove Carl's type inequalities for the error of approximation of compact sets K by deep and shallow neural networks. This in turn gives lower bounds on how well we can approximate the functions in K when requiring the approximants to come from outputs of such networks. Our results are obtained as a byproduct of the study of the recently introduced Lipschitz widths.},
 author = {Guergana Petrova and Przemyslaw Wojtaszczyk},
 journal = {Journal of Machine Learning Research},
 number = {353},
 openalex = {W4310829263},
 pages = {1--38},
 title = {Limitations on approximation by deep and shallow neural networks},
 url = {http://jmlr.org/papers/v24/22-1381.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1395,
 abstract = {Diagonal linear networks (DLNs) are a toy simplification of artificial neural networks; they consist in a quadratic reparametrization of linear regression inducing a sparse implicit regularization. In this paper, we describe the trajectory of the gradient flow of DLNs in the limit of small initialization. We show that incremental learning is effectively performed in the limit: coordinates are successively activated, while the iterate is the minimizer of the loss constrained to have support on the active coordinates only. This shows that the sparse implicit regularization of DLNs decreases with time. This work is restricted to the underparametrized regime with anti-correlated features for technical reasons.},
 author = {Raphaël Berthier},
 journal = {Journal of Machine Learning Research},
 number = {171},
 openalex = {W4294015512},
 pages = {1--26},
 title = {Incremental Learning in Diagonal Linear Networks},
 url = {http://jmlr.org/papers/v24/22-1395.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1398,
 abstract = {In many modern applications of deep learning the neural network has many more parameters than the data points used for its training. Motivated by those practices, a large body of recent theoretical research has been devoted to studying overparameterized models. One of the central phenomena in this regime is the ability of the model to interpolate noisy data, but still have test error lower than the amount of noise in that data. arXiv:1906.11300 characterized for which covariance structure of the data such a phenomenon can happen in linear regression if one considers the interpolating solution with minimum $\ell_2$-norm and the data has independent components: they gave a sharp bound on the variance term and showed that it can be small if and only if the data covariance has high effective rank in a subspace of small co-dimension. We strengthen and complete their results by eliminating the independence assumption and providing sharp bounds for the bias term. Thus, our results apply in a much more general setting than those of arXiv:1906.11300, e.g., kernel regression, and not only characterize how the noise is damped but also which part of the true signal is learned. Moreover, we extend the result to the setting of ridge regression, which allows us to explain another interesting phenomenon: we give general sufficient conditions under which the optimal regularization is negative.},
 author = {Alexander Tsigler and Peter L. Bartlett},
 journal = {Journal of Machine Learning Research},
 number = {123},
 openalex = {W3090091013},
 pages = {1--76},
 title = {Benign overfitting in ridge regression},
 url = {http://jmlr.org/papers/v24/22-1398.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1422,
 abstract = {The multivariate adaptive regression spline (MARS) is one of the popular estimation methods for nonparametric multivariate regressions. However, as MARS is based on marginal splines, to incorporate interactions of covariates, products of the marginal splines must be used, which leads to an unmanageable number of basis functions when the order of interaction is high and results in low estimation efficiency. In this paper, we improve the performance of MARS by using linear combinations of the covariates which achieve sufficient dimension reduction. The special basis functions of MARS facilitate calculation of gradients of the regression function, and estimation of the linear combinations is obtained via eigen-analysis of the outer-product of the gradients. Under some technical conditions, the asymptotic theory is established for the proposed estimation method. Numerical studies including both simulation and empirical applications show its effectiveness in dimension reduction and improvement over MARS and other commonly-used nonparametric methods in regression estimation and prediction.},
 author = {Yu Liu LIU and Degui Li and Yingcun Xia},
 journal = {Journal of Machine Learning Research},
 number = {309},
 openalex = {W4320843431},
 pages = {1--30},
 title = {Dimension Reduction and MARS},
 url = {http://jmlr.org/papers/v24/22-1422.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1425,
 abstract = {We study experiment design for unique identification of the causal graph of a simple SCM, where the graph may contain cycles. The presence of cycles in the structure introduces major challenges for experiment design as, unlike acyclic graphs, learning the skeleton of causal graphs with cycles may not be possible from merely the observational distribution. Furthermore, intervening on a variable in such graphs does not necessarily lead to orienting all the edges incident to it. In this paper, we propose an experiment design approach that can learn both cyclic and acyclic graphs and hence, unifies the task of experiment design for both types of graphs. We provide a lower bound on the number of experiments required to guarantee the unique identification of the causal graph in the worst case, showing that the proposed approach is order-optimal in terms of the number of experiments up to an additive logarithmic term. Moreover, we extend our result to the setting where the size of each experiment is bounded by a constant. For this case, we show that our approach is optimal in terms of the size of the largest experiment required for uniquely identifying the causal graph in the worst case.},
 author = {Ehsan Mokhtarian and Saber Salehkaleybar and AmirEmad Ghassami and Negar Kiyavash},
 journal = {Journal of Machine Learning Research},
 number = {354},
 openalex = {W4281400371},
 pages = {1--31},
 title = {A Unified Experiment Design Approach for Cyclic and Acyclic Causal Models},
 url = {http://jmlr.org/papers/v24/22-1425.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1446,
 author = {Lukas Graf and Tobias Harks and Kostas Kollias and Michael Markl},
 journal = {Journal of Machine Learning Research},
 number = {310},
 pages = {1--33},
 title = {Prediction Equilibrium for Dynamic Network Flows},
 url = {http://jmlr.org/papers/v24/22-1446.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1450,
 abstract = {We develop Microcanonical Hamiltonian Monte Carlo (MCHMC), a class of models which follow a fixed energy Hamiltonian dynamics, in contrast to Hamiltonian Monte Carlo (HMC), which follows canonical distribution with different energy levels. MCHMC tunes the Hamiltonian function such that the marginal of the uniform distribution on the constant-energy-surface over the momentum variables gives the desired target distribution. We show that MCHMC requires occasional energy conserving billiard-like momentum bounces for ergodicity, analogous to momentum resampling in HMC. We generalize the concept of bounces to a continuous version with partial direction preserving bounces at every step, which gives an energy conserving underdamped Langevin-like dynamics with non-Gaussian noise (MCLMC). MCHMC and MCLMC exhibit favorable scalings with condition number and dimensionality. We develop an efficient hyperparameter tuning scheme that achieves high performance and consistently outperforms NUTS HMC on several standard benchmark problems, in some cases by more than an order of magnitude.},
 author = {Jakob Robnik and G. Bruno De Luca and Eva Silverstein and Uroš Seljak},
 journal = {Journal of Machine Learning Research},
 number = {311},
 openalex = {W4311994930},
 pages = {1--34},
 title = {Microcanonical Hamiltonian Monte Carlo},
 url = {http://jmlr.org/papers/v24/22-1450.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1468,
 abstract = {Constructing unbiased estimators from Markov chain Monte Carlo (MCMC) outputs is a difficult problem that has recently received a lot of attention in the statistics and machine learning communities. However, the current unbiased MCMC framework only works when the quantity of interest is an expectation, which excludes many practical applications. In this paper, we propose a general method for constructing unbiased estimators for functions of expectations and extend it to construct unbiased estimators for nested expectations. Our approach combines and generalizes the unbiased MCMC and Multilevel Monte Carlo (MLMC) methods. In contrast to traditional sequential methods, our estimator can be implemented on parallel processors. We show that our estimator has a finite variance and computational complexity and can achieve $\varepsilon$-accuracy within the optimal $O(1/\varepsilon^2)$ computational cost under mild conditions. Our numerical experiments confirm our theoretical findings and demonstrate the benefits of unbiased estimators in the massively parallel regime.},
 author = {Tianze Wang and Guanyang Wang},
 journal = {Journal of Machine Learning Research},
 number = {249},
 openalex = {W4224903801},
 pages = {1--40},
 title = {Unbiased Multilevel Monte Carlo methods for intractable distributions: MLMC meets MCMC},
 url = {http://jmlr.org/papers/v24/22-1468.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1471,
 abstract = {In data-parallel optimization of machine learning models, workers collaborate to improve their estimates of the model: more accurate gradients allow them to use larger learning rates and optimize faster. We consider the setting in which all workers sample from the same dataset, and communicate over a sparse graph (decentralized). In this setting, current theory fails to capture important aspects of real-world behavior. First, the 'spectral gap' of the communication graph is not predictive of its empirical performance in (deep) learning. Second, current theory does not explain that collaboration enables larger learning rates than training alone. In fact, it prescribes smaller learning rates, which further decrease as graphs become larger, failing to explain convergence in infinite graphs. This paper aims to paint an accurate picture of sparsely-connected distributed optimization when workers share the same data distribution. We quantify how the graph topology influences convergence in a quadratic toy problem and provide theoretical results for general smooth and (strongly) convex objectives. Our theory matches empirical observations in deep learning, and accurately describes the relative merits of different graph topologies.},
 author = {Thijs Vogels and Hadrien Hendrikx and Martin Jaggi},
 journal = {Journal of Machine Learning Research},
 number = {355},
 openalex = {W4281656148},
 pages = {1--31},
 title = {Beyond spectral gap: The role of the topology in decentralized learning},
 url = {http://jmlr.org/papers/v24/22-1471.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1488,
 abstract = {We improve the understanding of the $\textit{golden ratio algorithm}$, which solves monotone variational inequalities (VI) and convex-concave min-max problems via the distinctive feature of adapting the step sizes to the local Lipschitz constants. Adaptive step sizes not only eliminate the need to pick hyperparameters, but they also remove the necessity of global Lipschitz continuity and can increase from one iteration to the next. We first establish the equivalence of this algorithm with popular VI methods such as reflected gradient, Popov or optimistic gradient descent-ascent in the unconstrained case with constant step sizes. We then move on to the constrained setting and introduce a new analysis that allows to use larger step sizes, to complete the bridge between the golden ratio algorithm and the existing algorithms in the literature. Doing so, we actually eliminate the link between the golden ratio $\frac{1+\sqrt{5}}{2}$ and the algorithm. Moreover, we improve the adaptive version of the algorithm, first by removing the maximum step size hyperparameter (an artifact from the analysis) to improve the complexity bound, and second by adjusting it to nonmonotone problems with weak Minty solutions, with superior empirical performance.},
 author = {Ahmet Alacaoglu and Axel Böhm and Yura Malitsky},
 journal = {Journal of Machine Learning Research},
 number = {172},
 openalex = {W4313303889},
 pages = {1--33},
 title = {Beyond the Golden Ratio for Variational Inequality Algorithms},
 url = {http://jmlr.org/papers/v24/22-1488.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1511,
 abstract = {The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.},
 author = {Sam Corbett-Davies and Johann D. Gaebler and Hamed Nilforoshan and Ravi Shroff and Sharad Goel},
 journal = {Journal of Machine Learning Research},
 number = {312},
 openalex = {W2885659818},
 pages = {1--117},
 title = {The Measure and Mismeasure of Fairness},
 url = {http://jmlr.org/papers/v24/22-1511.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1514,
 author = {Mirco Mutti and Riccardo De Santi and Piersilvio De Bartolomeis and Marcello Restelli},
 journal = {Journal of Machine Learning Research},
 number = {250},
 pages = {1--42},
 title = {Convex Reinforcement Learning in Finite Trials},
 url = {http://jmlr.org/papers/v24/22-1514.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1518,
 abstract = {In this paper, we consider a class of nonconvex-nonconcave minimax problems, i.e., NC-PL minimax problems, whose objective functions satisfy the Polyak-\L ojasiewicz (PL) condition with respect to the inner variable. We propose a zeroth-order alternating gradient descent ascent (ZO-AGDA) algorithm and a zeroth-order variance reduced alternating gradient descent ascent (ZO-VRAGDA) algorithm for solving NC-PL minimax problem under the deterministic and the stochastic setting, respectively. The total number of function value queries to obtain an $\epsilon$-stationary point of ZO-AGDA and ZO-VRAGDA algorithm for solving NC-PL minimax problem is upper bounded by $\mathcal{O}(\varepsilon^{-2})$ and $\mathcal{O}(\varepsilon^{-3})$, respectively. To the best of our knowledge, they are the first two zeroth-order algorithms with the iteration complexity gurantee for solving NC-PL minimax problems.},
 author = {Zi Xu and Zi-Qi Wang and Jun-Lin Wang and Yu-Hong Dai},
 journal = {Journal of Machine Learning Research},
 number = {313},
 openalex = {W4320858140},
 pages = {1--25},
 title = {Zeroth-Order Alternating Gradient Descent Ascent Algorithms for a Class of Nonconvex-Nonconcave Minimax Problems},
 url = {http://jmlr.org/papers/v24/22-1518.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0023,
 abstract = {Generative artificial intelligence has made significant strides, producing text indistinguishable from human prose and remarkably photorealistic images. Automatically measuring how close the generated data distribution is to the target distribution is central to diagnosing existing models and developing better ones. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore three approaches to statistically estimate these scores: vector quantization, non-parametric estimation, and classifier-based estimation. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of $f$-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We demonstrate in the vision domain that MAUVE can identify known properties of generated images on par with or better than existing metrics. In conclusion, we present practical recommendations for using MAUVE effectively with language and image modalities.},
 author = {Krishna Pillutla and Lang Liu and John Thickstun and Sean Welleck and Swabha Swayamdipta and Rowan Zellers and Sewoong Oh and Yejin Choi and Zaid Harchaoui},
 journal = {Journal of Machine Learning Research},
 number = {356},
 openalex = {W4313447172},
 pages = {1--92},
 title = {MAUVE Scores for Generative Models: Theory and Practice},
 url = {http://jmlr.org/papers/v24/23-0023.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0025,
 abstract = {Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s > 0$ for which the corresponding Sobolev or Besov space compactly embeds into $L_p$. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p > q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon VC-dimension when $p < \infty$. Our results show that very deep ReLU networks significantly outperform classical methods of approximation in terms of the number of parameters, but that this comes at the cost of parameters which are not encodable.},
 author = {Jonathan W. Siegel},
 journal = {Journal of Machine Learning Research},
 number = {357},
 openalex = {W4310413607},
 pages = {1--52},
 title = {Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces},
 url = {http://jmlr.org/papers/v24/23-0025.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0030,
 author = {Xiaonan Hu and Xinyu Zhang},
 journal = {Journal of Machine Learning Research},
 number = {358},
 pages = {1--53},
 title = {Optimal Parameter-Transfer Learning by Semiparametric Model Averaging},
 url = {http://jmlr.org/papers/v24/23-0030.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0037,
 abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.},
 author = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
 journal = {Journal of Machine Learning Research},
 number = {251},
 openalex = {W4301243929},
 pages = {1--43},
 title = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
 url = {http://jmlr.org/papers/v24/23-0037.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0039,
 abstract = {Differentially private multiple testing procedures can protect the information of individuals used in hypothesis tests while guaranteeing a small fraction of false discoveries. In this paper, we propose a differentially private adaptive FDR control method that can control the classic FDR metric exactly at a user-specified level $\alpha$ with privacy guarantee, which is a non-trivial improvement compared to the differentially private Benjamini-Hochberg method proposed in Dwork et al. (2021). Our analysis is based on two key insights: 1) a novel p-value transformation that preserves both privacy and the mirror conservative property, and 2) a mirror peeling algorithm that allows the construction of the filtration and application of the optimal stopping technique. Numerical studies demonstrate that the proposed DP-AdaPT performs better compared to the existing differentially private FDR control methods. Compared to the non-private AdaPT, it incurs a small accuracy loss but significantly reduces the computation cost.},
 author = {Xintao Xia and Zhanrui Cai},
 journal = {Journal of Machine Learning Research},
 number = {252},
 openalex = {W4379087224},
 pages = {1--35},
 title = {Adaptive False Discovery Rate Control with Privacy Guarantee},
 url = {http://jmlr.org/papers/v24/23-0039.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0041,
 abstract = {We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity as so many works aim to do -- instead, we have a bias/variance/diversity trade-off to manage.},
 author = {Danny Wood and Tingting Mu and Andrew M. Webb and Henry W. J. Reeve and Mikel Luj{{\'a}}n and Gavin Brown},
 journal = {Journal of Machine Learning Research},
 number = {359},
 openalex = {W4315706600},
 pages = {1--49},
 title = {A Unified Theory of Diversity in Ensemble Learning},
 url = {http://jmlr.org/papers/v24/23-0041.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0042,
 abstract = {Different users of machine learning methods require different explanations, depending on their goals. To make machine learning accountable to society, one important goal is to get actionable options for recourse, which allow an affected user to change the decision $f(x)$ of a machine learning system by making limited changes to its input $x$. We formalize this by providing a general definition of recourse sensitivity, which needs to be instantiated with a utility function that describes which changes to the decisions are relevant to the user. This definition applies to local attribution methods, which attribute an importance weight to each input feature. It is often argued that such local attributions should be robust, in the sense that a small change in the input $x$ that is being explained, should not cause a large change in the feature weights. However, we prove formally that it is in general impossible for any single attribution method to be both recourse sensitive and robust at the same time. It follows that there must always exist counterexamples to at least one of these properties. We provide such counterexamples for several popular attribution methods, including LIME, SHAP, Integrated Gradients and SmoothGrad. Our results also cover counterfactual explanations, which may be viewed as attributions that describe a perturbation of $x$. We further discuss possible ways to work around our impossibility result, for instance by allowing the output to consist of sets with multiple attributions, and we provide sufficient conditions for specific classes of continuous functions to be recourse sensitive. Finally, we strengthen our impossibility result for the restricted case where users are only able to change a single attribute of $x$, by providing an exact characterization of the functions $f$ to which impossibility applies.},
 author = {Hidde Fokkema and Rianne de Heide and Tim van Erven},
 journal = {Journal of Machine Learning Research},
 number = {360},
 openalex = {W4281685029},
 pages = {1--37},
 title = {Attribution-based Explanations that Provide Recourse Cannot be Robust},
 url = {http://jmlr.org/papers/v24/23-0042.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0045,
 author = {Daniel G. Alabi and Salil P. Vadhan},
 journal = {Journal of Machine Learning Research},
 number = {361},
 pages = {1--50},
 title = {Differentially Private Hypothesis Testing for Linear Regression},
 url = {http://jmlr.org/papers/v24/23-0045.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0064,
 abstract = {Deep learning surrogate models have shown promise in solving partial differential equations (PDEs). Among them, the Fourier neural operator (FNO) achieves good accuracy, and is significantly faster compared to numerical solvers, on a variety of PDEs, such as fluid flows. However, the FNO uses the Fast Fourier transform (FFT), which is limited to rectangular domains with uniform grids. In this work, we propose a new framework, viz., geo-FNO, to solve PDEs on arbitrary geometries. Geo-FNO learns to deform the input (physical) domain, which may be irregular, into a latent space with a uniform grid. The FNO model with the FFT is applied in the latent space. The resulting geo-FNO model has both the computation efficiency of FFT and the flexibility of handling arbitrary geometries. Our geo-FNO is also flexible in terms of its input formats, viz., point clouds, meshes, and design parameters are all valid inputs. We consider a variety of PDEs such as the Elasticity, Plasticity, Euler's, and Navier-Stokes equations, and both forward modeling and inverse design problems. Geo-FNO is $10^5$ times faster than the standard numerical solvers and twice more accurate compared to direct interpolation on existing ML-based PDE solvers such as the standard FNO.},
 author = {Zongyi Li and Daniel Zhengyu Huang and Burigede Liu and Anima Anandkumar},
 journal = {Journal of Machine Learning Research},
 number = {388},
 openalex = {W4285429067},
 pages = {1--26},
 title = {Fourier Neural Operator with Learned Deformations for PDEs on General Geometries},
 url = {http://jmlr.org/papers/v24/23-0064.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0069,
 abstract = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.},
 author = {Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
 journal = {Journal of Machine Learning Research},
 number = {253},
 openalex = {W4308245305},
 pages = {1--15},
 title = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
 url = {http://jmlr.org/papers/v24/23-0069.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0074,
 abstract = {While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, little attention has been paid towards individual neurons. We present a technique called as Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property - with the goal of understanding how such a knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that capture specific linguistic properties? (ii) how localized or distributed neurons are across the network? iii) how redundantly is the information preserved? iv) how fine-tuning pre-trained models towards downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do architectures vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neurons that can predict different linguistic tasks, ii) with neurons capturing basic lexical information (such as suffixation) localized in lower most layers, iii) while those learning complex concepts (such as syntactic role) predominantly in middle and higher layers, iii) that salient linguistic neurons are relocated from higher to lower layers during transfer learning, as the network preserve the higher layers for task specific information, iv) we found interesting differences across pre-trained models, with respect to how linguistic information is preserved within, and v) we found that concept exhibit similar neuron distribution across different languages in the multilingual transformer models. Our code is publicly available as part of the NeuroX toolkit.},
 author = {Nadir Durrani and Fahim Dalvi and Hassan Sajjad},
 journal = {Journal of Machine Learning Research},
 number = {362},
 openalex = {W4283708521},
 pages = {1--40},
 title = {Discovering Salient Neurons in Deep NLP Models},
 url = {http://jmlr.org/papers/v24/23-0074.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0089,
 abstract = {We establish a general Bernstein--von Mises theorem for approximately linear semiparametric functionals of fractional posterior distributions based on nonparametric priors. This is illustrated in a number of nonparametric settings and for different classes of prior distributions, including Gaussian process priors. We show that fractional posterior credible sets can provide reliable semiparametric uncertainty quantification, but have inflated size. To remedy this, we further propose a \textit{shifted-and-rescaled} fractional posterior set that is an efficient confidence set having optimal size under regularity conditions. As part of our proofs, we also refine existing contraction rate results for fractional posteriors by sharpening the dependence of the rate on the fractional exponent.},
 author = {Alice L'Huillier and Luke Travis and Ismaël Castillo and Kolyan Ray},
 journal = {Journal of Machine Learning Research},
 number = {389},
 openalex = {W4317672120},
 pages = {1--61},
 title = {Semiparametric inference using fractional posteriors},
 url = {http://jmlr.org/papers/v24/23-0089.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0104,
 author = {Jackson Zhou and John T. Ormerod and Clara Grazian},
 journal = {Journal of Machine Learning Research},
 number = {314},
 pages = {1--39},
 title = {Fast Expectation Propagation for Heteroscedastic, Lasso-Penalized, and Quantile Regression},
 url = {http://jmlr.org/papers/v24/23-0104.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0106,
 abstract = {In the face of uncertainty, the need for probabilistic assessments has long been recognized in the literature on forecasting. In classification, however, comparative evaluation of classifiers often focuses on predictions specifying a single class through the use of simple accuracy measures, which disregard any probabilistic uncertainty quantification. I propose probabilistic top lists as a novel type of prediction in classification, which bridges the gap between single-class predictions and predictive distributions. The probabilistic top list functional is elicitable through the use of strictly consistent evaluation metrics. The proposed evaluation metrics are based on symmetric proper scoring rules and admit comparison of various types of predictions ranging from single-class point predictions to fully specified predictive distributions. The Brier score yields a metric that is particularly well suited for this kind of comparison.},
 author = {Johannes Resin},
 journal = {Journal of Machine Learning Research},
 number = {173},
 openalex = {W4318621097},
 pages = {1--21},
 title = {From Classification Accuracy to Proper Scoring Rules: Elicitability of Probabilistic Top List Predictions},
 url = {http://jmlr.org/papers/v24/23-0106.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0112,
 abstract = {skrl is an open-source modular library for reinforcement learning written in Python and designed with a focus on readability, simplicity, and transparency of algorithm implementations. In addition to supporting environments that use the traditional interfaces from OpenAI Gym and DeepMind, it provides the facility to load, configure, and operate NVIDIA Isaac Gym and NVIDIA Omniverse Isaac Gym environments. Furthermore, it enables the simultaneous training of several agents with customizable scopes (subsets of environments among all available ones), which may or may not share resources, in the same run. The library's documentation can be found at https://skrl.readthedocs.io and its source code is available on GitHub at https://github.com/Toni-SM/skrl.},
 author = {Antonio Serrano-Muñoz and Dimitrios Chrysostomou and Simon Bøgh and Nestor Arana-Arexolaleiba},
 journal = {Journal of Machine Learning Research},
 number = {254},
 openalex = {W4226449706},
 pages = {1--9},
 title = {skrl: Modular and Flexible Library for Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/23-0112.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0130,
 abstract = {Continual learning is the problem of learning from a nonstationary stream of data, a fundamental issue for sustainable and efficient training of deep neural networks over time. Unfortunately, deep learning libraries only provide primitives for offline training, assuming that model's architecture and data are fixed. Avalanche is an open source library maintained by the ContinualAI non-profit organization that extends PyTorch by providing first-class support for dynamic architectures, streams of datasets, and incremental training and evaluation methods. Avalanche provides a large set of predefined benchmarks and training algorithms and it is easy to extend and modular while supporting a wide range of continual learning scenarios. Documentation is available at \url{https://avalanche.continualai.org}.},
 author = {Antonio Carta and Lorenzo Pellegrini and Andrea Cossu and Hamed Hemati and Vincenzo Lomonaco},
 journal = {Journal of Machine Learning Research},
 number = {363},
 openalex = {W4319323699},
 pages = {1--6},
 title = {Avalanche: A PyTorch Library for Deep Continual Learning},
 url = {http://jmlr.org/papers/v24/23-0130.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0135,
 abstract = {Differential replication through copying refers to the process of replicating the decision behavior of a machine learning model using another model that possesses enhanced features and attributes. This process is relevant when external constraints limit the performance of an industrial predictive system. Under such circumstances, copying enables the retention of original prediction capabilities while adapting to new demands. Previous research has focused on the single-pass implementation for copying. This paper introduces a novel sequential approach that significantly reduces the amount of computational resources needed to train or maintain a copy, leading to reduced maintenance costs for companies using machine learning models in production. The effectiveness of the sequential approach is demonstrated through experiments with synthetic and real-world datasets, showing significant reductions in time and resources, while maintaining or improving accuracy.},
 author = {Nahuel Statuto and Irene Unceta and Jordi Nin and Oriol Pujol},
 journal = {Journal of Machine Learning Research},
 number = {390},
 openalex = {W4320712039},
 pages = {1--34},
 title = {A Scalable and Efficient Iterative Method for Copying Machine Learning Classifiers},
 url = {http://jmlr.org/papers/v24/23-0135.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0149,
 abstract = {Post-hoc global/local feature attribution methods are progressively being employed to understand the decisions of complex machine learning models. Yet, because of limited amounts of data, it is possible to obtain a diversity of models with good empirical performance but that provide very different explanations for the same prediction, making it hard to derive insight from them. In this work, instead of aiming at reducing the under-specification of model explanations, we fully embrace it and extract logical statements about feature attributions that are consistent across all models with good empirical performance (i.e. all models in the Rashomon Set). We show that partial orders of local/global feature importance arise from this methodology enabling more nuanced interpretations by allowing pairs of features to be incomparable when there is no consensus on their relative importance. We prove that every relation among features present in these partial orders also holds in the rankings provided by existing approaches. Finally, we present three use cases employing hypothesis spaces with tractable Rashomon Sets (Additive models, Kernel Ridge, and Random Forests) and show that partial orders allow one to extract consistent local and global interpretations of models despite their under-specification.},
 author = {Gabriel Laberge and Yann Pequignot and Alexandre Mathieu and Foutse Khomh and Mario Marchand},
 journal = {Journal of Machine Learning Research},
 number = {364},
 openalex = {W4300772104},
 pages = {1--50},
 title = {Partial Order in Chaos: Consensus on Feature Attributions in the Rashomon Set},
 url = {http://jmlr.org/papers/v24/23-0149.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0158,
 author = {Paula Harder and Alex Hernandez-Garcia and Venkatesh Ramesh and Qidong Yang and Prasanna Sattegeri and Daniela Szwarcman and Campbell Watson and David Rolnick},
 journal = {Journal of Machine Learning Research},
 number = {365},
 pages = {1--40},
 title = {Hard-Constrained Deep Learning for Climate Downscaling},
 url = {http://jmlr.org/papers/v24/23-0158.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0185,
 abstract = {The Distributional Random Forest (DRF) is a recently introduced Random Forest algorithm to estimate multivariate conditional distributions. Due to its general estimation procedure, it can be employed to estimate a wide range of targets such as conditional average treatment effects, conditional quantiles, and conditional correlations. However, only results about the consistency and convergence rate of the DRF prediction are available so far. We characterize the asymptotic distribution of DRF and develop a bootstrap approximation of it. This allows us to derive inferential tools for quantifying standard errors and the construction of confidence regions that have asymptotic coverage guarantees. In simulation studies, we empirically validate the developed theory for inference of low-dimensional targets and for testing distributional differences between two populations.},
 author = {Jeffrey Näf and Corinne Emmenegger and Peter Bühlmann and Nicolai Meinshausen},
 journal = {Journal of Machine Learning Research},
 number = {366},
 openalex = {W4320843241},
 pages = {1--77},
 title = {Confidence and Uncertainty Assessment for Distributional Random Forests},
 url = {http://jmlr.org/papers/v24/23-0185.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0191,
 abstract = {Recent years have witnessed the booming of various differentiable optimization algorithms. These algorithms exhibit different execution patterns, and their execution needs massive computational resources that go beyond a single CPU and GPU. Existing differentiable optimization libraries, however, cannot support efficient algorithm development and multi-CPU/GPU execution, making the development of differentiable optimization algorithms often cumbersome and expensive. This paper introduces TorchOpt, a PyTorch-based efficient library for differentiable optimization. TorchOpt provides a unified and expressive differentiable optimization programming abstraction. This abstraction allows users to efficiently declare and analyze various differentiable optimization programs with explicit gradients, implicit gradients, and zero-order gradients. TorchOpt further provides a high-performance distributed execution runtime. This runtime can fully parallelize computation-intensive differentiation operations (e.g. tensor tree flattening) on CPUs / GPUs and automatically distribute computation to distributed devices. Experimental results show that TorchOpt achieves $5.2\times$ training time speedup on an 8-GPU server. TorchOpt is available at: https://github.com/metaopt/torchopt/.},
 author = {Jie Ren* and Xidong Feng* and Bo Liu* and Xuehai Pan* and Yao Fu and Luo Mai and Yaodong Yang},
 journal = {Journal of Machine Learning Research},
 number = {367},
 openalex = {W4309131813},
 pages = {1--14},
 title = {TorchOpt: An Efficient Library for Differentiable Optimization},
 url = {http://jmlr.org/papers/v24/23-0191.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0207,
 abstract = {Recent advances in reinforcement learning (RL) have increased the promise of introducing cognitive assistance and automation to robot-assisted laparoscopic surgery (RALS). However, progress in algorithms and methods depends on the availability of standardized learning environments that represent skills relevant to RALS. We present LapGym, a framework for building RL environments for RALS that models the challenges posed by surgical tasks, and sofa_env, a diverse suite of 12 environments. Motivated by surgical training, these environments are organized into 4 tracks: Spatial Reasoning, Deformable Object Manipulation & Grasping, Dissection, and Thread Manipulation. Each environment is highly parametrizable for increasing difficulty, resulting in a high performance ceiling for new algorithms. We use Proximal Policy Optimization (PPO) to establish a baseline for model-free RL algorithms, investigating the effect of several environment parameters on task difficulty. Finally, we show that many environments and parameter configurations reflect well-known, open problems in RL research, allowing researchers to continue exploring these fundamental problems in a surgical context. We aim to provide a challenging, standard environment suite for further development of RL for RALS, ultimately helping to realize the full potential of cognitive surgical robotics. LapGym is publicly accessible through GitHub (https://github.com/ScheiklP/lap_gym).},
 author = {Paul Maria Scheikl and Balázs Gyenes and Rayan Younis and Christoph Haas and Gerhard Neumann and Martin Wagner and Franziska Mathis-Ullrich},
 journal = {Journal of Machine Learning Research},
 number = {368},
 openalex = {W4321473287},
 pages = {1--42},
 title = {LapGym -- An Open Source Framework for Reinforcement Learning in Robot-Assisted Laparoscopic Surgery},
 url = {http://jmlr.org/papers/v24/23-0207.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0248,
 abstract = {In nonparametric independence testing, we observe i.i.d.\ data $\{(X_i,Y_i)\}_{i=1}^n$, where $X \in \mathcal{X}, Y \in \mathcal{Y}$ lie in any general spaces, and we wish to test the null that $X$ is independent of $Y$. Modern test statistics such as the kernel Hilbert-Schmidt Independence Criterion (HSIC) and Distance Covariance (dCov) have intractable null distributions due to the degeneracy of the underlying U-statistics. Thus, in practice, one often resorts to using permutation testing, which provides a nonasymptotic guarantee at the expense of recalculating the quadratic-time statistics (say) a few hundred times. This paper provides a simple but nontrivial modification of HSIC and dCov (called xHSIC and xdCov, pronounced ``cross'' HSIC/dCov) so that they have a limiting Gaussian distribution under the null, and thus do not require permutations. This requires building on the newly developed theory of cross U-statistics by Kim and Ramdas (2020), and in particular developing several nontrivial extensions of the theory in Shekhar et al. (2022), which developed an analogous permutation-free kernel two-sample test. We show that our new tests, like the originals, are consistent against fixed alternatives, and minimax rate optimal against smooth local alternatives. Numerical simulations demonstrate that compared to the full dCov or HSIC, our variants have the same power up to a $\sqrt 2$ factor, giving practitioners a new option for large problems or data-analysis pipelines where computation, not sample size, could be the bottleneck.},
 author = {Shubhanshu Shekhar and Ilmun Kim and Aaditya Ramdas},
 journal = {Journal of Machine Learning Research},
 number = {369},
 openalex = {W4312048562},
 pages = {1--68},
 title = {A Permutation-Free Kernel Independence Test},
 url = {http://jmlr.org/papers/v24/23-0248.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0294,
 abstract = {We introduce and investigate, for finite groups $G$, $G$-invariant deep neural network ($G$-DNN) architectures with ReLU activation that are densely connected-- i.e., include all possible skip connections. In contrast to other $G$-invariant architectures in the literature, the preactivations of the$G$-DNNs presented here are able to transform by \emph{signed} permutation representations (signed perm-reps) of $G$. Moreover, the individual layers of the $G$-DNNs are not required to be $G$-equivariant; instead, the preactivations are constrained to be $G$-equivariant functions of the network input in a way that couples weights across all layers. The result is a richer family of $G$-invariant architectures never seen previously. We derive an efficient implementation of $G$-DNNs after a reparameterization of weights, as well as necessary and sufficient conditions for an architecture to be ``admissible''-- i.e., nondegenerate and inequivalent to smaller architectures. We include code that allows a user to build a $G$-DNN interactively layer-by-layer, with the final architecture guaranteed to be admissible. We show that there are far more admissible $G$-DNN architectures than those accessible with the ``concatenated ReLU'' activation function from the literature. Finally, we apply $G$-DNNs to two example problems -- (1) multiplication in $\{-1, 1\}$ (with theoretical guarantees) and (2) 3D object classification -- % finding that the inclusion of signed perm-reps significantly boosts predictive performance compared to baselines with only ordinary (i.e., unsigned) perm-reps.},
 author = {Devanshu Agrawal and James Ostrowski},
 journal = {Journal of Machine Learning Research},
 number = {370},
 openalex = {W4323697206},
 pages = {1--40},
 title = {Densely Connected $G$-invariant Deep Neural Networks with Signed Permutation Representations},
 url = {http://jmlr.org/papers/v24/23-0294.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0300,
 abstract = {Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is a framework for computing with distributed representations by exploiting properties of random high-dimensional vector spaces. The commitment of the scientific community to aggregate and disseminate research in this particularly multidisciplinary area has been fundamental for its advancement. Joining these efforts, we present Torchhd, a high-performance open source Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and serves as an efficient foundation for further research and application development. The easy-to-use library builds on top of PyTorch and features state-of-the-art HD/VSA functionality, clear documentation, and implementation examples from well-known publications. Comparing publicly available code with their corresponding Torchhd implementation shows that experiments can run up to 100x faster. Torchhd is available at: https://github.com/hyperdimensional-computing/torchhd.},
 author = {Mike Heddes and Igor Nunes and Pere Vergés and Denis Kleyko and Danny Abraham and Tony Givargis and Alexandru Nicolau and Alexander Veidenbaum},
 journal = {Journal of Machine Learning Research},
 number = {255},
 openalex = {W4281260224},
 pages = {1--10},
 title = {Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures},
 url = {http://jmlr.org/papers/v24/23-0300.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0310,
 author = {Shaocong Ma and Ziyi Chen and Shaofeng Zou and Yi Zhou},
 journal = {Journal of Machine Learning Research},
 number = {371},
 pages = {1--40},
 title = {Decentralized Robust V-learning for Solving Markov Games with Model Uncertainty},
 url = {http://jmlr.org/papers/v24/23-0310.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0367,
 abstract = {Constructing states from sequences of observations is an important component of reinforcement learning agents. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires complete trajectories of observations before it can compute the gradients and is unsuitable for online updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules or learning the network in stages, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade off the functional capacity of the network for computationally efficient learning. We demonstrate the effectiveness of our approach over Truncated-BPTT on a prediction benchmark inspired by animal learning and by doing policy evaluation of pre-trained policies for Atari 2600 games.},
 author = {Khurram Javed and Haseeb Shah and Richard S. Sutton and Martha White},
 journal = {Journal of Machine Learning Research},
 number = {256},
 openalex = {W4320560264},
 pages = {1--34},
 title = {Scalable Real-Time Recurrent Learning Using Columnar-Constructive Networks},
 url = {http://jmlr.org/papers/v24/23-0367.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0378,
 abstract = {A significant challenge facing researchers in the area of multi-agent reinforcement learning (MARL) pertains to the identification of a library that can offer fast and compatible development for multi-agent tasks and algorithm combinations, while obviating the need to consider compatibility issues. In this paper, we present MARLlib, a library designed to address the aforementioned challenge by leveraging three key mechanisms: 1) a standardized multi-agent environment wrapper, 2) an agent-level algorithm implementation, and 3) a flexible policy mapping strategy. By utilizing these mechanisms, MARLlib can effectively disentangle the intertwined nature of the multi-agent task and the learning process of the algorithm, with the ability to automatically alter the training strategy based on the current task's attributes. The MARLlib library's source code is publicly accessible on GitHub: \url{https://github.com/Replicable-MARL/MARLlib}.},
 author = {Siyi Hu and Yifan Zhong and Minquan Gao and Weixun Wang and Hao Dong and Xiaodan Liang and Zhihui Li and Xiaojun Chang and Yaodong Yang},
 journal = {Journal of Machine Learning Research},
 number = {315},
 openalex = {W4307416208},
 pages = {1--23},
 title = {MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library},
 url = {http://jmlr.org/papers/v24/23-0378.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0389,
 abstract = {Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.},
 author = {Hilde Weerts and Miroslav Dudík and Richard Edgar and Adrin Jalali and Roman Lutz and Michael Madaio},
 journal = {Journal of Machine Learning Research},
 number = {257},
 openalex = {W4361806824},
 pages = {1--8},
 title = {Fairlearn: Assessing and Improving Fairness of AI Systems},
 url = {http://jmlr.org/papers/v24/23-0389.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0401,
 abstract = {We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivation of past bounds and ease the search for future bounds: one may simply check if our supermartingale or submartingale conditions are met and, if so, be guaranteed a (time-uniform) PAC-Bayes bound.},
 author = {Ben Chugg and Hongjian Wang and Aaditya Ramdas},
 journal = {Journal of Machine Learning Research},
 number = {372},
 openalex = {W4319653703},
 pages = {1--61},
 title = {A unified recipe for deriving (time-uniform) PAC-Bayes bounds},
 url = {http://jmlr.org/papers/v24/23-0401.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0421,
 abstract = {We combine concepts from multilevel solvers for partial differential equations (PDEs) with neural network based deep learning and propose a new methodology for the efficient numerical solution of high-dimensional parametric PDEs. An in-depth theoretical analysis shows that the proposed architecture is able to approximate multigrid V-cycles to arbitrary precision with the number of weights only depending logarithmically on the resolution of the finest mesh. As a consequence, approximation bounds for the solution of parametric PDEs by neural networks that are independent on the (stochastic) parameter dimension can be derived. The performance of the proposed method is illustrated on high-dimensional parametric linear elliptic PDEs that are common benchmark problems in uncertainty quantification. We find substantial improvements over state-of-the-art deep learning-based solvers. As particularly challenging examples, random conductivity with high-dimensional non-affine Gaussian fields in 100 parameter dimensions and a random cookie problem are examined. Due to the multilevel structure of our method, the amount of training samples can be reduced on finer levels, hence significantly lowering the generation time for training data and the training time of our method.},
 author = {Cosmas Heiß and Ingo Gühring and Martin Eigel},
 journal = {Journal of Machine Learning Research},
 number = {373},
 openalex = {W4362643572},
 pages = {1--42},
 title = {Multilevel CNNs for Parametric PDEs},
 url = {http://jmlr.org/papers/v24/23-0421.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-043,
 abstract = {We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence. In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM's update may be regarded as a third derivative -- the derivative of the Hessian in the leading eigenvector direction -- that encourages drift toward wider minima.},
 author = {Peter L. Bartlett and Philip M. Long and Olivier Bousquet},
 journal = {Journal of Machine Learning Research},
 number = {316},
 openalex = {W4302306313},
 pages = {1--36},
 title = {The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima},
 url = {http://jmlr.org/papers/v24/23-043.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0473,
 abstract = {We study the problem of regression in a generalized linear model (GLM) with multiple signals and latent variables. This model, which we call a matrix GLM, covers many widely studied problems in statistical learning, including mixed linear regression, max-affine regression, and mixture-of-experts. In mixed linear regression, each observation comes from one of $L$ signal vectors (regressors), but we do not know which one; in max-affine regression, each observation comes from the maximum of $L$ affine functions, each defined via a different signal vector. The goal in all these problems is to estimate the signals, and possibly some of the latent variables, from the observations. We propose a novel approximate message passing (AMP) algorithm for estimation in a matrix GLM and rigorously characterize its performance in the high-dimensional limit. This characterization is in terms of a state evolution recursion, which allows us to precisely compute performance measures such as the asymptotic mean-squared error. The state evolution characterization can be used to tailor the AMP algorithm to take advantage of any structural information known about the signals. Using state evolution, we derive an optimal choice of AMP `denoising' functions that minimizes the estimation error in each iteration. The theoretical results are validated by numerical simulations for mixed linear regression, max-affine regression, and mixture-of-experts. For max-affine regression, we propose an algorithm that combines AMP with expectation-maximization to estimate intercepts of the model along with the signals. The numerical results show that AMP significantly outperforms other estimators for mixed linear regression and max-affine regression in most parameter regimes.},
 author = {Nelvin Tan and Ramji Venkataramanan},
 journal = {Journal of Machine Learning Research},
 number = {317},
 openalex = {W4362679260},
 pages = {1--44},
 title = {Mixed Regression via Approximate Message Passing},
 url = {http://jmlr.org/papers/v24/23-0473.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0478,
 abstract = {PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate operators between infinite-dimensional function spaces. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction: First, a novel universal approximation result is derived, under minimal assumptions on the underlying operator and the data-generating distribution. Then, two potential obstacles to efficient operator learning with PCA-Net are identified, and made precise through lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates to the inherent complexity of the space of operators between infinite-dimensional input and output spaces, resulting in a rigorous and quantifiable statement of a "curse of parametric complexity", an infinite-dimensional analogue of the well-known curse of dimensionality encountered in high-dimensional approximation problems. In addition to these lower bounds, upper complexity bounds are finally derived. A suitable smoothness criterion is shown to ensure an algebraic decay of the PCA eigenvalues. Furthermore, it is shown that PCA-Net can overcome the general curse for specific operators of interest, arising from the Darcy flow and the Navier-Stokes equations.},
 author = {Samuel Lanthaler},
 journal = {Journal of Machine Learning Research},
 number = {318},
 openalex = {W4361865118},
 pages = {1--67},
 title = {Operator learning with PCA-Net: upper and lower complexity bounds},
 url = {http://jmlr.org/papers/v24/23-0478.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0513,
 author = {Yue Deng and Zirui Wang and Xi Chen and Yin Zhang},
 journal = {Journal of Machine Learning Research},
 number = {399},
 pages = {1--34},
 title = {Boosting Multi-agent Reinforcement Learning via Contextual Prompting},
 url = {http://jmlr.org/papers/v24/23-0513.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0527,
 abstract = {The dynamic Schr\"odinger bridge problem seeks a stochastic process that defines a transport between two target probability measures, while optimally satisfying the criteria of being closest, in terms of Kullback-Leibler divergence, to a reference process. We propose a novel sampling-based iterative algorithm, the iterated diffusion bridge mixture (IDBM) procedure, aimed at solving the dynamic Schr\"odinger bridge problem. The IDBM procedure exhibits the attractive property of realizing a valid transport between the target probability measures at each iteration. We perform an initial theoretical investigation of the IDBM procedure, establishing its convergence properties. The theoretical findings are complemented by numerical experiments illustrating the competitive performance of the IDBM procedure. Recent advancements in generative modeling employ the time-reversal of a diffusion process to define a generative process that approximately transports a simple distribution to the data distribution. As an alternative, we propose utilizing the first iteration of the IDBM procedure as an approximation-free method for realizing this transport. This approach offers greater flexibility in selecting the generative process dynamics and exhibits accelerated training and superior sample quality over larger discretization intervals. In terms of implementation, the necessary modifications are minimally intrusive, being limited to the training loss definition.},
 author = {Stefano Peluchetti},
 journal = {Journal of Machine Learning Research},
 number = {374},
 openalex = {W4362598759},
 pages = {1--51},
 title = {Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and Generative Modeling},
 url = {http://jmlr.org/papers/v24/23-0527.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0538,
 author = {Wentao Huang and Houbao Lu and Haizhang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {391},
 pages = {1--30},
 title = {Hierarchical Kernels in Deep Kernel Learning},
 url = {http://jmlr.org/papers/v24/23-0538.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0646,
 abstract = {Various algorithms for reinforcement learning (RL) exhibit dramatic variation in their convergence rates as a function of problem structure. Such problem-dependent behavior is not captured by worst-case analyses and has accordingly inspired a growing effort in obtaining instance-dependent guarantees and deriving instance-optimal algorithms for RL problems. This research has been carried out, however, primarily within the confines of theory, providing guarantees that explain \textit{ex post} the performance differences observed. A natural next step is to convert these theoretical guarantees into guidelines that are useful in practice. We address the problem of obtaining sharp instance-dependent confidence regions for the policy evaluation problem and the optimal value estimation problem of an MDP, given access to an instance-optimal algorithm. As a consequence, we propose a data-dependent stopping rule for instance-optimal algorithms. The proposed stopping rule adapts to the instance-specific difficulty of the problem and allows for early termination for problems with favorable structure.},
 author = {Eric Xia and Koulik Khamaru and Martin J. Wainwright and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {392},
 openalex = {W4221151967},
 pages = {1--43},
 title = {Instance-Dependent Confidence and Early Stopping for Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/23-0646.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0668,
 author = {Zekai Wang and Weiwei Liu},
 journal = {Journal of Machine Learning Research},
 number = {396},
 pages = {1--43},
 title = {RVCL: Evaluating the Robustness of Contrastive Learning via Verification},
 url = {http://jmlr.org/papers/v24/23-0668.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0712,
 author = {Zhou Wang and Xingye Qiao},
 journal = {Journal of Machine Learning Research},
 number = {375},
 pages = {1--39},
 title = {Set-valued Classification with Out-of-distribution Detection for Many Classes},
 url = {http://jmlr.org/papers/v24/23-0712.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0771,
 abstract = {Recent works have studied implicit biases in deep learning, especially the behavior of last-layer features and classifier weights. However, they usually need to simplify the intermediate dynamics under gradient flow or gradient descent due to the intractability of loss functions and model architectures. In this paper, we introduce the unhinged loss, a concise loss function, that offers more mathematical opportunities to analyze the closed-form dynamics while requiring as few simplifications or assumptions as possible. The unhinged loss allows for considering more practical techniques, such as time-vary learning rates and feature normalization. Based on the layer-peeled model that views last-layer features as free optimization variables, we conduct a thorough analysis in the unconstrained, regularized, and spherical constrained cases, as well as the case where the neural tangent kernel remains invariant. To bridge the performance of the unhinged loss to that of Cross-Entropy (CE), we investigate the scenario of fixing classifier weights with a specific structure, (e.g., a simplex equiangular tight frame). Our analysis shows that these dynamics converge exponentially fast to a solution depending on the initialization of features and classifier weights. These theoretical results not only offer valuable insights, including explicit feature regularization and rescaled learning rates for enhancing practical training with the unhinged loss, but also extend their applicability to other loss functions. Finally, we empirically demonstrate these theoretical results and insights through extensive experiments.},
 author = {Xiong Zhou and Xianming Liu and Hanzhang Wang and Deming Zhai and Jiangjunjun and Xiangyang Ji},
 journal = {Journal of Machine Learning Research},
 number = {376},
 openalex = {W4389768025},
 pages = {1--62},
 title = {On the Dynamics Under the Unhinged Loss and Beyond},
 url = {http://jmlr.org/papers/v24/23-0771.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0795,
 abstract = {Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. $\texttt{t5x}$ and $\texttt{seqio}$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively.},
 author = {Adam Roberts and Hyung Won Chung and Gaurav Mishra and Anselm Levskaya and James Bradbury and Daniel Andor and Sharan Narang and Brian Lester and Colin Gaffney and Afroz Mohiuddin and Curtis Hawthorne and Aitor Lewkowycz and Alex Salcianu and Marc van Zee and Jacob Austin and Sebastian Goodman and Livio Baldini Soares and Haitang Hu and Sasha Tsvyashchenko and Aakanksha Chowdhery and Jasmijn Bastings and Jannis Bulian and Xavier Garcia and Jianmo Ni and Andrew Chen and Kathleen Kenealy and Kehang Han and Michelle Casbon and Jonathan H. Clark and Stephan Lee and Dan Garrette and James Lee-Thorp and Colin Raffel and Noam Shazeer and Marvin Ritter and Maarten Bosma and Alexandre Passos and Jeremy Maitin-Shepard and Noah Fiedel and Mark Omernick and Brennan Saeta and Ryan Sepassi and Alexander Spiridonov and Joshua Newlan and Andrea Gesmundo},
 journal = {Journal of Machine Learning Research},
 number = {377},
 openalex = {W4224442590},
 pages = {1--8},
 title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
 url = {http://jmlr.org/papers/v24/23-0795.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0836,
 abstract = {Inspired by the remarkable success of large neural networks, there has been significant interest in understanding the generalization performance of over-parameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their "preferred" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the general class of homogeneous potential functions converges in direction to a generalized maximum-margin solution for linear classification problems, thereby answering a long-standing question in the classification setting. Further, we show that MD can be implemented efficiently and enjoys fast convergence under suitable conditions. Through comprehensive experiments, we demonstrate that MD is a versatile method to produce learned models with different regularizers, which in turn have different generalization performances.},
 author = {Haoyuan Sun and Khashayar Gatmiry and Kwangjun Ahn and Navid Azizan},
 journal = {Journal of Machine Learning Research},
 number = {393},
 openalex = {W4382319594},
 pages = {1--58},
 title = {A Unified Approach to Controlling Implicit Regularization via Mirror Descent},
 url = {http://jmlr.org/papers/v24/23-0836.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0838,
 author = {Akshayaa Magesh and Venugopal V. Veeravalli and Anirban Roy and Susmit Jha},
 journal = {Journal of Machine Learning Research},
 number = {378},
 pages = {1--35},
 title = {Principled Out-of-Distribution Detection via Multiple Testing},
 url = {http://jmlr.org/papers/v24/23-0838.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0887,
 abstract = {Bagging is a commonly used ensemble technique in statistics and machine learning to improve the performance of prediction procedures. In this paper, we study the prediction risk of variants of bagged predictors under the proportional asymptotics regime, in which the ratio of the number of features to the number of observations converges to a constant. Specifically, we propose a general strategy to analyze the prediction risk under squared error loss of bagged predictors using classical results on simple random sampling. Specializing the strategy, we derive the exact asymptotic risk of the bagged ridge and ridgeless predictors with an arbitrary number of bags under a well-specified linear model with arbitrary feature covariance matrices and signal vectors. Furthermore, we prescribe a generic cross-validation procedure to select the optimal subsample size for bagging and discuss its utility to eliminate the non-monotonic behavior of the limiting risk in the sample size (i.e., double or multiple descents). In demonstrating the proposed procedure for bagged ridge and ridgeless predictors, we thoroughly investigate the oracle properties of the optimal subsample size and provide an in-depth comparison between different bagging variants.},
 author = {Pratik Patil and Jin-Hong Du and Arun Kumar Kuchibhotla},
 journal = {Journal of Machine Learning Research},
 number = {319},
 openalex = {W4307079372},
 pages = {1--113},
 title = {Bagging in overparameterized learning: Risk characterization and risk monotonization},
 url = {http://jmlr.org/papers/v24/23-0887.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0896,
 abstract = {Recent work has focused on the very common practice of prediction-based inference: that is, (i) using a pre-trained machine learning model to predict an unobserved response variable, and then (ii) conducting inference on the association between that predicted response and some covariates. As pointed out by Wang et al. (2020), applying a standard inferential approach in (ii) does not accurately quantify the association between the unobserved (as opposed to the predicted) response and the covariates. In recent work, Wang et al. (2020) and Angelopoulos et al. (2023) propose corrections to step (ii) in order to enable valid inference on the association between the unobserved response and the covariates. Here, we show that the method proposed by Angelopoulos et al. (2023) successfully controls the type 1 error rate and provides confidence intervals with correct nominal coverage, regardless of the quality of the pre-trained machine learning model used to predict the unobserved response. However, the method proposed by Wang et al. (2020) provides valid inference only under very strong conditions that rarely hold in practice: for instance, if the machine learning model perfectly estimates the true regression function in the study population of interest.},
 author = {Keshav Motwani and Daniela Witten},
 journal = {Journal of Machine Learning Research},
 number = {394},
 openalex = {W4382319320},
 pages = {1--18},
 title = {Revisiting inference after prediction},
 url = {http://jmlr.org/papers/v24/23-0896.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-1004,
 abstract = {Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.},
 author = {Werner Zellinger and Stefan Kindermann and Sergei V. Pereverzyev},
 journal = {Journal of Machine Learning Research},
 number = {395},
 openalex = {W4385473876},
 pages = {1--28},
 title = {Adaptive learning of density ratios in RKHS},
 url = {http://jmlr.org/papers/v24/23-1004.html},
 volume = {24},
 year = {2023}
}
