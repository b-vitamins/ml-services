@article{JMLR:v14:ahlgren13a,
 abstract = {We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph's default) and Progol's A* search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A*, NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A* substantially sacrificed accuracy to induce faster, and one in which Progol A* was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for.},
 author = {John Ahlgren and Shiu Yin Yuen},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W2122947307},
 pages = {3649--3681},
 title = {Efficient program synthesis using constraint satisfaction in inductive logic programming},
 url = {http://jmlr.org/papers/v14/ahlgren13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:alquier13a,
 abstract = {Let $(\bX, Y)$ be a random pair taking values in $\mathbb R^p \times \mathbb R$. In the so-called single-index model, one has $Y=f^{\star}(θ^{\star T}\bX)+\bW$, where $f^{\star}$ is an unknown univariate measurable function, $θ^{\star}$ is an unknown vector in $\mathbb R^d$, and $W$ denotes a random noise satisfying $\mathbb E[\bW|\bX]=0$. The single-index model is known to offer a flexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations ("$p$ larger than $n$" paradigm). To circumvent this difficulty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inequality, which is more powerful than the best known oracle inequalities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technique and its performance is compared with that of standard procedures.},
 author = {Pierre Alquier and Gérard Biau},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2152020329},
 pages = {243--280},
 title = {Sparse single-index model},
 url = {http://jmlr.org/papers/v14/alquier13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:angluin13a,
 abstract = {Although PAC learning unrestricted regular languages is long known to be a very difficult problem, one might suppose the existence (and even an abundance) of natural efficiently learnable sub-families. When our literature search for a natural efficiently learnable regular family came up empty, we proposed the shuffle ideals as a prime candidate. A shuffle ideal generated by a string u is simply the collection of all strings containing u as a (discontiguous) subsequence. This fundamental language family is of theoretical interest in its own right and also provides the building blocks for other important language families. Somewhat surprisingly, we discovered that even a class as simple as the shuffle ideals is not properly PAC learnable, unless RP=NP. In the positive direction, we give an efficient algorithm for properly learning shuffle ideals in the statistical query (and therefore also PAC) model under the uniform distribution.},
 author = {Dana Angluin and James Aspnes and Sarah Eisenstat and Aryeh Kontorovich},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2123405454},
 pages = {1513--1531},
 title = {On the Learnability of Shuffle Ideals},
 url = {http://jmlr.org/papers/v14/angluin13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:aravkin13a,
 abstract = {We introduce a class of quadratic support (QS) functions, many of which play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and Kalman smoothing. Well known examples include the l2, Huber, l1 and Vapnik losses. We build on a dual representation for QS functions using convex analysis, revealing the structure necessary for a QS function to be interpreted as the negative log of a probability density, and providing the foundation for statistical interpretation and analysis of QS loss functions. For a subclass of QS functions called piecewise linear quadratic (PLQ) penalties, we also develop efficient numerical estimation schemes. These components form a flexible statistical modeling framework for a variety of learning applications, together with a toolbox of efficient numerical methods for inference. In particular, for PLQ densities, interior point (IP) methods can be used. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. The efficiency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaussian errors are assumed in the process and measurement models. The extended framework allows arbitrary PLQ densities to be used, and the proposed IP approach solves the generalized Kalman smoothing problem while maintaining the linear complexity in the size of the time series, just as in the Gaussian case. This extends the computational efficiency of classic algorithms to a much broader nonsmooth setting, and includes many recently proposed robust and sparse smoothers as special cases.},
 author = {Aleks and r Y. Aravkin and James V. Burke and Gianluigi Pillonetto},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2148069367},
 pages = {2689--2728},
 title = {Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory},
 url = {http://jmlr.org/papers/v14/aravkin13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:arias-castro13a,
 abstract = {Maximum Variance Unfolding is one of the main methods for (nonlinear) dimensionality reduction. We study its large sample limit, providing specific rates of convergence under standard assumptions. We find that it is consistent when the underlying submanifold is isometric to a convex subset, and we provide some simple examples where it fails to be consistent.},
 author = {Ery Arias-Castro and Bruno Pelletier},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2952477160},
 pages = {1747--1770},
 title = {On the convergence of maximum variance unfolding},
 url = {http://jmlr.org/papers/v14/arias-castro13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:arora13a,
 abstract = {For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efficient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efficient hierarchical variant performs surprisingly well.},
 author = {Raman Arora and Maya R. Gupta and Amol Kapila and Maryam Fazel},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2139504623},
 pages = {1715--1746},
 title = {Similarity-based clustering by left-stochastic matrix factorization},
 url = {http://jmlr.org/papers/v14/arora13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:bahmani13a,
 abstract = {Sparsity-constrained optimization has wide applicability in machine learning, statistics, and signal processing problems such as feature selection and Compressed Sensing. A vast body of work has studied the sparsity-constrained optimization from theoretical, algorithmic, and application aspects in the context of sparse estimation in linear models where the fidelity of the estimate is measured by the squared error. In contrast, relatively less effort has been made in the study of sparsity-constrained optimization in cases where nonlinear models are involved or the cost function is not quadratic. In this paper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), to approximate sparse minima of cost functions of arbitrary form. Should a cost function have a Stable Restricted Hessian (SRH) or a Stable Restricted Linearization (SRL), both of which are introduced in this paper, our algorithm is guaranteed to produce a sparse vector within a bounded distance from the true sparse optimum. Our approach generalizes known results for quadratic cost functions that arise in sparse linear regression and Compressed Sensing. We also evaluate the performance of GraSP through numerical simulations on synthetic and real data, where the algorithm is employed for sparse logistic regression with and without l2-regularization.},
 author = {Sohail Bahmani and Bhiksha Raj and Petros T. Boufounos},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2550629656},
 pages = {807--841},
 title = {Greedy sparsity-constrained optimization},
 url = {http://jmlr.org/papers/v14/bahmani13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:boehmer13a,
 abstract = {Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modification to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modified SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance.},
 author = {Wendelin B{{\"o}}hmer and Steffen Gr{{\"u}}new{{\"a}}lder and Yun Shen and Marek Musial and Klaus Obermayer},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2135362819},
 pages = {2067--2118},
 title = {Construction of approximation spaces for reinforcement learning},
 url = {http://jmlr.org/papers/v14/boehmer13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:bosagh-zadeh13a,
 abstract = {We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are p...},
 author = {Reza Bosagh Zadeh and Ashish Goel},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W3141299647},
 pages = {1605--1626},
 title = {Dimension independent similarity computation},
 url = {http://jmlr.org/papers/v14/bosagh-zadeh13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:bottou13a,
 abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
 author = {L{{\'e}}on Bottou and Jonas Peters and Joaquin Qui{{\~n}}onero-Candela and Denis X. Charles and D. Max Chickering and Elon Portugaly and Dipankar Ray and Patrice Simard and Ed Snelson},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2122124659},
 pages = {3207--3260},
 title = {Counterfactual reasoning and learning systems: the example of computational advertising},
 url = {http://jmlr.org/papers/v14/bottou13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:brakel13a,
 abstract = {Imputing missing values in high dimensional time-series is a difficult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difficulties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-field iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artificial and two real-world data sets.},
 author = {Phil{{\'e}}mon Brakel and Dirk Stroobandt and Benjamin Schrauwen},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W2148862211},
 pages = {2771--2797},
 title = {Training energy-based models for time-series imputation},
 url = {http://jmlr.org/papers/v14/brakel13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:bubeck13a,
 abstract = {We consider an original problem that arises from the issue of security analysis of a power system and that we name optimal discovery with probabilistic expert advice. We address it with an algorithm based on the optimistic paradigm and on the Good-Turing missing mass estimator. We prove two different regret bounds on the performance of this algorithm under weak assumptions on the probabilistic experts. Under more restrictive hypotheses, we also prove a macroscopic optimality result, comparing the algorithm both with an oracle strategy and with uniform sampling. Finally, we provide numerical experiments illustrating these theoretical findings.},
 author = {Sébastien Bubeck and Damien Ernst and Aurélien Garivier},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2950000058},
 pages = {601--623},
 title = {Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality},
 url = {http://jmlr.org/papers/v14/bubeck13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:cai13a,
 abstract = {This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in [Formula: see text] as the number of points n → ∞, while the dimension p is either fixed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that "all high-dimensional random vectors are almost always nearly orthogonal to each other". Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed.},
 author = {Tony Cai and Jianqing Fan and Tiefeng Jiang},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2145615204},
 pages = {1837--1864},
 title = {Distributions of Angles in Random Packing on Spheres.},
 url = {http://jmlr.org/papers/v14/cai13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:cai13b,
 abstract = {We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed.},
 author = {Tony Cai and Wen-Xin Zhou},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W2963381277},
 pages = {3619--3647},
 title = {A max-norm constrained minimization approach to 1-bit matrix completion},
 url = {http://jmlr.org/papers/v14/cai13b.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:cesa-bianchi13a,
 abstract = {We show that the mistake bound for predicting the nodes of an arbitrary weighted graph is characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving the optimal mistake bound on any weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label-propagation) methods, while being much faster.},
 author = {Nicol{{\`o}} Cesa-Bianchi and Claudio Gentile and Fabio Vitale and Giovanni Zappella},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2126973498},
 pages = {1251--1284},
 title = {Random spanning trees and the prediction of weighted graphs},
 url = {http://jmlr.org/papers/v14/cesa-bianchi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:challis13a,
 abstract = {We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.},
 author = {Edward Challis and David Barber},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2144518280},
 pages = {2239--2286},
 title = {Gaussian Kullback-Leibler approximate inference},
 url = {http://jmlr.org/papers/v14/challis13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:chalupka13a,
 abstract = {Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for a dataset of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons.},
 author = {Krzysztof Chalupka and Christopher K. I. Williams and Iain Murray},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2952795334},
 pages = {303--331},
 title = {A Framework for Evaluating Approximation Methods for Gaussian Process Regression},
 url = {http://jmlr.org/papers/v14/chalupka13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:chaudhuri13a,
 abstract = {The principal components analysis (PCA) algorithm is a standard tool for identifying good low-dimensional approximations to high-dimensional data. Many data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We show that the sample complexity of the proposed method differs from the existing procedure in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. We furthermore illustrate our results, showing that on real data there is a large performance gap between the existing method and our method.},
 author = {Kamalika Chaudhuri and Anand D. Sarwate and Kaushik Sinha},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2144190046},
 pages = {2905--2943},
 title = {A near-optimal algorithm for differentially-private principal components},
 url = {http://jmlr.org/papers/v14/chaudhuri13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:chen13a,
 abstract = {Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called functions. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following benefits and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs.},
 author = {Lisha Chen and Andreas Buja},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2170540108},
 pages = {1145--1173},
 title = {Stress functions for nonlinear dimension reduction, proximity analysis, and graph drawing},
 url = {http://jmlr.org/papers/v14/chen13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:chertkov13a,
 author = {Michael Chertkov and Adam B. Yedidia},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W2215819374},
 pages = {2029--2066},
 title = {Approximating the Permanent with Fractional Belief Propagation},
 url = {http://jmlr.org/papers/v14/chertkov13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:clark13a,
 abstract = {Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially infinite, set of strings generated by some target grammar. Here we define the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modification of Gold's identification in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars.},
 author = {Alexander Clark},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W2110447197},
 pages = {3537--3559},
 title = {Learning trees from strings: a strong learning algorithm for some context-free grammars},
 url = {http://jmlr.org/papers/v14/clark13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:clemencon13a,
 abstract = {Abstract The problem of Label Ranking is receiving increasing attention from several research communities. The algorithms that have been developed/adapted to treat rankings of a fixed set of labels as the target object, including several different types of decision trees (DT). One DT‐based algorithm, which has been very successful in other tasks but which has not been adapted for label ranking is the Random Forests (RF) algorithm. RFs are an ensemble learning method that combines different trees obtained using different randomization techniques. In this work, we propose an ensemble of decision trees for Label Ranking, based on Random Forests, which we refer to as Label Ranking Forests (LRF). Two different algorithms that learn DT for label ranking are used to obtain the trees. We then compare and discuss the results of LRF with standalone decision tree approaches. The results indicate that the method is highly competitive.},
 author = {Stéphan Clémençon and Marine Depecker and Nicolas Vayatis},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2605228598},
 pages = {39--73},
 title = {Label Ranking Forests},
 url = {http://jmlr.org/papers/v14/clemencon13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:culp13a,
 abstract = {The cluster assumption had a significant impact on the reasoning behind semi-supervised classification methods in graph-based learning. The literature includes numerous applications where harmonic functions provided estimates that conformed to data satisfying this well-known assumption, but the relationship between this assumption and harmonic functions is not as well-understood theoretically. We investigate these matters from the perspective of supervised kernel classification and provide concrete answers to two fundamental questions. (i) Under what conditions do semi-supervised harmonic approaches satisfy this assumption? (ii) If such an assumption is satisfied then why precisely would an observation sacrifice its own supervised estimate in favor of the cluster? First, a harmonic function is guaranteed to assign labels to data in harmony with the cluster assumption if a specific condition on the boundary of the harmonic function is satisfied. Second, it is shown that any harmonic function estimate within the interior is a probability weighted average of supervised estimates, where the weight is focused on supervised kernel estimates near labeled cases. We demonstrate that the uniqueness criterion for harmonic estimators is sensitive when the graph is sparse or the size of the boundary is relatively small. This sets the stage for a third contribution, a new regularized joint harmonic function for semi-supervised learning based on a joint optimization criterion. Mathematical properties of this estimator, such as its uniqueness even when the graph is sparse or the size of the boundary is relatively small, are proven. A main selling point is its ability to operate in circumstances where the cluster assumption may not be fully satisfied on real data by compromising between the purely harmonic and purely supervised estimators. The competitive stature of the new regularized joint harmonic approach is established.},
 author = {Mark Vere Culp and Kenneth Joseph Ryan},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W2105004029},
 pages = {3721--3752},
 title = {Joint harmonic functions and their supervised connections},
 url = {http://jmlr.org/papers/v14/culp13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:curtin13a,
 abstract = {MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.},
 author = {Ryan R. Curtin and James R. Cline and N. P. Slagle and William B. March and Parikshit Ram and Nishant A. Mehta and Alexander G. Gray},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W4301584875},
 pages = {801--805},
 title = {MLPACK: A Scalable C++ Machine Learning Library},
 url = {http://jmlr.org/papers/v14/curtin13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:demsar13a,
 abstract = {Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.},
 author = {Janez Dem{\v{s}}ar and Toma{\v{z}} Curk and Ale{\v{s}} Erjavec and {\v{C}}rt Gorup and Toma{\v{z}} Ho{\v{c}}evar and Mitar Milutinovi{\v{c}} and Martin Mo{\v{z}}ina and Matija Polajnar and Marko Toplak and An{\v{z}}e Stari{\v{c}} and Miha {\v{S}}tajdohar and Lan Umek and Lan {\v{Z}}agar and Jure {\v{Z}}bontar and Marinka {\v{Z}}itnik and Bla{\v{z}} Zupan},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2131850886},
 pages = {2349--2353},
 title = {Orange: data mining toolbox in python},
 url = {http://jmlr.org/papers/v14/demsar13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:dhillon13a,
 abstract = {We compare the risk of ridge regression to a simple variant of ordinary least squares, in which one simply projects the data onto a finite dimensional subspace (as specified by a principal component analysis) and then performs an ordinary (un-regularized) least squares regression in this subspace. This note shows that the risk of this ordinary least squares method (PCA-OLS) is within a constant factor (namely 4) of the risk of ridge regression (RR).},
 author = {Paramveer S. Dhillon and Dean P.  Foster and Sham M.  Kakade and Lyle H. Ungar},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W1501882007},
 pages = {1505--1511},
 title = {A risk comparison of ordinary least squares vs ridge regression},
 url = {http://jmlr.org/papers/v14/dhillon13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:djuric13a,
 abstract = {We present BudgetedSVM, an open-source C++ toolbox comprising highly-optimized implementations of recently proposed algorithms for scalable training of Support Vector Machine (SVM) approximators: Adaptive Multi-hyperplane Machines, Low-rank Linearization SVM, and Budgeted Stochastic Gradient Descent. BudgetedSVM trains models with accuracy comparable to LibSVM in time comparable to LibLinear, solving non-linear problems with millions of high-dimensional examples within minutes on a regular computer. We provide command-line and Matlab interfaces to BudgetedSVM, an efficient API for handling large-scale, high-dimensional data sets, as well as detailed documentation to help developers use and further extend the toolbox.},
 author = {Nemanja Djuric and Liang Lan and Slobodan Vucetic and Zhuang Wang},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2141642784},
 pages = {3813--3817},
 title = {BudgetedSVM: a toolbox for scalable SVM approximations},
 url = {http://jmlr.org/papers/v14/djuric13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:dyer13a,
 abstract = {Unions of subspaces provide a powerful generalization to linear subspace models for collections of high-dimensional data. To learn a union of subspaces from a collection of data, sets of signals in the collection that belong to the same subspace must be identified in order to obtain accurate estimates of the subspace structures present in the data. Recently, sparse recovery methods have been shown to provide a provable and robust strategy for exact feature selection (EFS)--recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with L1-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and the gap between the two approaches is particularly pronounced when the sampling of subspaces in the dataset is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble.},
 author = {Eva L. Dyer and Aswin C. Sankaranarayanan and Richard G. Baraniuk},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W4293563904},
 pages = {2487--2517},
 title = {Greedy Feature Selection for Subspace Clustering},
 url = {http://jmlr.org/papers/v14/dyer13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:escalante13a,
 abstract = {Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the final label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classification, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufficient as well as when feature selection is difficult.},
 author = {Alberto N. Escalante-B. and Laurenz Wiskott},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W2114194474},
 pages = {3683--3719},
 title = {How to solve classification and regression problems on high-dimensional data with a supervised extension of slow feature analysis},
 url = {http://jmlr.org/papers/v14/escalante13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:fanello13a,
 abstract = {Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efficient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective real-time system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different datasets: a benchmark dataset for one-shot action learning (the ChaLearn Gesture Dataset), an in-house dataset acquired by a Kinect sensor including complex actions and gestures differing by small details, and a dataset created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, “All Gestures You Can”, to be played against a humanoid robot.},
 author = {Sean Ryan Fanello and Ilaria Gori and Giorgio Metta and Francesca Odone},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2107443860},
 pages = {2617--2640},
 title = {Keep It Simple and Sparse: Real-Time Action Recognition},
 url = {http://jmlr.org/papers/v14/fanello13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:forghani13a,
 abstract = {This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, Vol. 10, pp. 1485-1510, 2009) by H. Xu et al. ...},
 author = {Yahya Forghani and Hadi Sadoghi},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W3041824598},
 pages = {3493--3494},
 title = {Comment on "Robustness and regularization of support vector machines" by H. Xu et al. (Journal of machine learning research, vol. 10, pp. 1485-1510, 2009)},
 url = {http://jmlr.org/papers/v14/forghani13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:frezza-buet13a,
 abstract = {This paper introduces the rllib as an original C++ template-based library oriented toward value function estimation. Generic programming is promoted here as a way of having a good fit between the mathematics of reinforcement learning and their implementation in a library. The main concepts of rllib are presented, as well as a short example.},
 author = {Hervé Frezza-Buet and Matthieu Geist},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W153509885},
 pages = {625--628},
 title = {A C++ Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics},
 url = {http://jmlr.org/papers/v14/frezza-buet13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:fukumizu13a,
 abstract = {A kernel method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes' rule are presented, including Bayesian computation without likelihood and filtering with a nonparametric state-space model.},
 author = {Kenji Fukumizu and Le Song and Arthur Gretton},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W2146906344},
 pages = {3753--3783},
 title = {Kernel Bayes' rule: Bayesian inference with positive definite kernels},
 url = {http://jmlr.org/papers/v14/fukumizu13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:gerber13a,
 abstract = {Principal curves and manifolds provide a framework to formulate manifold learning within a statistical context. Principal curves define the notion of a curve passing through the middle of a distribution. While the intuition is clear, the formal definition leads to some technical and practical difficulties. In particular, principal curves are saddle points of the mean-squared projection distance, which poses severe challenges for estimation and model selection. This paper demonstrates that the difficulties in model selection associated with the saddle point property of principal curves are intrinsically tied to the minimization of the mean-squared projection distance. We introduce a new objective function, facilitated through a modification of the principal curve estimation approach, for which all critical points are principal curves and minima. Thus, the new formulation removes the fundamental issue for model selection in principal curve estimation. A gradient-descent-based estimator demonstrates the effectiveness of the new formulation for controlling model complexity on numerical experiments with synthetic and real data.},
 author = {Samuel Gerber and Ross Whitaker},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2126413878},
 pages = {1285--1302},
 title = {Regularization-free principal curve estimation},
 url = {http://jmlr.org/papers/v14/gerber13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:gerchinovitz13a,
 abstract = {We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T. We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same flavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with fixed design.},
 author = {Sébastien Gerchinovitz},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2963252079},
 pages = {729--769},
 title = {Sparsity regret bounds for individual sequences in online linear regression},
 url = {http://jmlr.org/papers/v14/gerchinovitz13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:gilad-bachrach13a,
 abstract = {Typically, one approaches a supervised machine learning problem by writing down an objective function and finding a hypothesis that minimizes it. This is equivalent to finding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by defining a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median.

One contribution of this work is an efficient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we define: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work.},
 author = {Ran Gilad-Bachrach and Christopher J.C. Burges},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W2154282606},
 pages = {3591--3618},
 title = {Classifier selection using the predicate depth},
 url = {http://jmlr.org/papers/v14/gilad-bachrach13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:gonen13a,
 abstract = {We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.},
 author = {Alon Gonen and Sivan Sabato and Shai Shalev-Shwartz},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2131093109},
 pages = {2583--2615},
 title = {Efficient Active Learning of Halfspaces: an Aggressive Approach},
 url = {http://jmlr.org/papers/v14/gonen13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:gong13a,
 abstract = {Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.},
 author = {Pinghua Gong and Jieping Ye and Changshui Zhang},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2401764675},
 pages = {2979--3010},
 title = {Multi-Stage Multi-Task Feature Learning},
 url = {http://jmlr.org/papers/v14/gong13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:guyader13a,
 abstract = {Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function m(x) = E[Y|X = x] as follows: first identify the k nearest neighbors of x in the sample Dn, then keep only those for which x is itself one of the k nearest neighbors, and finally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data-splitting.},
 author = {Arnaud Guyader and Nick Hengartner},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2117661330},
 pages = {2361--2376},
 title = {On the mutual nearest neighbors estimate in regression},
 url = {http://jmlr.org/papers/v14/guyader13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hable13a,
 abstract = {In supervised learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, the prediction of a local learning algorithm in a testing point is only based on training data which are close to the testing point. Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every testing point, the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the testing point. In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized in order to get a universally consistent learning algorithm.},
 author = {Robert Hable},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2152961405},
 pages = {153--186},
 title = {Universal consistency of localized versions of regularized kernel methods},
 url = {http://jmlr.org/papers/v14/hable13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hall13a,
 abstract = {Differential privacy is a rigorous cryptographically-motivated characterization of data privacy which may be applied when releasing summaries of a database. Previous work has focused mainly on methods for which the output is a finite dimensional vector, or an element of some discrete set. We develop methods for releasing functions while preserving differential privacy. Specifically, we show that adding an appropriate Gaussian process to the function of interest yields differential privacy. When the functions lie in the reproducing kernel Hilbert space (RKHS) generated by the covariance kernel of the Gaussian process, then the correct noise level is established by measuring the sensitivity of the function in the RKHS norm. As examples we consider kernel density estimation, kernel support vector machines, and functions in RKHSs.},
 author = {Rob Hall and Alessandro Rinaldo and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2160495867},
 pages = {703--727},
 title = {Differential privacy for functions and functional data},
 url = {http://jmlr.org/papers/v14/hall13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:han13a,
 abstract = {We propose a high dimensional classification method, named the Copula Discriminant Analysis (CODA). The CODA generalizes the normal-based linear discriminant analysis to the larger Gaussian Copula models (or the nonparanormal) as proposed by Liu et al. (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman's rho and Kendall's tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be an alternative choice besides the normal-based high dimensional linear discriminant analysis.},
 author = {Fang Han and Tuo Zhao and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2133107474},
 pages = {629--671},
 title = {CODA: high dimensional copula discriminant analysis},
 url = {http://jmlr.org/papers/v14/han13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hannah13a,
 abstract = {We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is a computationally efficient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options.},
 author = {Lauren A. Hannah and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W2103183297},
 pages = {3261--3294},
 title = {Multivariate convex regression with adaptive partitioning},
 url = {http://jmlr.org/papers/v14/hannah13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:harris13a,
 abstract = {The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the 'Rank PC' algorithm works as well as the 'Pearson PC' algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations.},
 author = {Naftali Harris and Mathias Drton},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W2168652321},
 pages = {3365--3383},
 title = {PC algorithm for nonparanormal graphical models},
 url = {http://jmlr.org/papers/v14/harris13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:he13a,
 abstract = {Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identification. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efficient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efficiency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems.},
 author = {Yuejia He and Yiyuan She and Dapeng Wu},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W2121763547},
 pages = {3073--3104},
 title = {Stationary-sparse causality network learning},
 url = {http://jmlr.org/papers/v14/he13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hennig13a,
 abstract = {Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.},
 author = {Philipp Hennig and Martin Kiefel},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2615477230},
 pages = {843--865},
 title = {Quasi-Newton methods: a new direction},
 url = {http://jmlr.org/papers/v14/hennig13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hernandez-lobato13a,
 abstract = {We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy.},
 author = {Daniel Hern{{\'a}}ndez-Lobato and Jos{{\'e}} Miguel Hern{{\'a}}ndez-Lobato and Pierre Dupont},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2097408416},
 pages = {1891--1945},
 title = {Generalized spike-and-slab priors for Bayesian group feature selection using expectation propagation},
 url = {http://jmlr.org/papers/v14/hernandez-lobato13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hoffman13a,
 abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
 author = {Matthew D. Hoffman and David M. Blei and Chong Wang and John Paisley},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W2166851633},
 pages = {1303--1347},
 title = {Stochastic variational inference},
 url = {http://jmlr.org/papers/v14/hoffman13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hyttinen13a,
 abstract = {Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufficient) set of variables. Recent results in the causal discovery literature have explored various identifiability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection.},
 author = {Antti Hyttinen and Frederick Eberhardt and Patrik O. Hoyer},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W2117519995},
 pages = {3041--3071},
 title = {Experiment selection for causal discovery},
 url = {http://jmlr.org/papers/v14/hyttinen13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:hyvarinen13a,
 abstract = {We present new measures of the causal direction, or direction of effect, between two non-Gaussian random variables. They are based on the likelihood ratio under the linear non-Gaussian acyclic model (LiNGAM). We also develop simple first-order approximations of the likelihood ratio and analyze them based on related cumulant-based measures, which can be shown to find the correct causal directions. We show how to apply these measures to estimate LiNGAM for more than two variables, and even in the case of more variables than observations. We further extend the method to cyclic and nonlinear models. The proposed framework is statistically at least as good as existing ones in the cases of few data points or noisy data, and it is computationally and conceptually very simple. Results on simulated fMRI data indicate that the method may be useful in neuroimaging where the number of time points is typically quite small.},
 author = {Aapo Hyvärinen and Stephen M. Smith},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W3021064838},
 pages = {111--152},
 title = {Pairwise likelihood ratios for estimation of non-Gaussian structural equation models},
 url = {http://jmlr.org/papers/v14/hyvarinen13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:jethava13a,
 author = {Vinay Jethava and Anders Martinsson and Chiranjib Bhattacharyya and Devdatt Dubhashi},
 journal = {Journal of Machine Learning Research},
 number = {110},
 pages = {3495--3536},
 title = {Lovasz theta function, SVMs and Finding Dense Subgraphs},
 url = {http://jmlr.org/papers/v14/jethava13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:johnson13a,
 abstract = {There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markov modeling, which has been developed mainly in the parametric non-Bayesian setting, to allow construction of highly interpretable models that admit natural prior information on state durations.

In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference methods on both synthetic and real experiments.},
 author = {Matthew J. Johnson and Alan S. Willsky},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W2132991226},
 pages = {673--701},
 title = {Bayesian nonparametric hidden semi-Markov models},
 url = {http://jmlr.org/papers/v14/johnson13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:joseph13a,
 abstract = {The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefficient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefficient vector, whereby one has only control on the l1 norm of the smaller coefficients, is also analyzed. As consequence of these results, we also show that the coefficient estimate satisfies strong oracle type inequalities.},
 author = {Antony Joseph},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2135393227},
 pages = {1771--1800},
 title = {Variable selection in high-dimension with random designs and orthogonal matching pursuit},
 url = {http://jmlr.org/papers/v14/joseph13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:kanamori13a,
 abstract = {There are two main approaches to binary classification problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is defined for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufficiently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy.},
 author = {Takafumi Kanamori and Akiko Takeda and Taiji Suzuki},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2113782937},
 pages = {1461--1504},
 title = {Conjugate relation between loss functions and uncertainty sets in classification problems},
 url = {http://jmlr.org/papers/v14/kanamori13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:klami13a,
 abstract = {Canonical correlation analysis (CCA) is a classical method for seeking correlations between two multivariate data sets. During the last ten years, it has received more and more attention in the machine learning community in the form of novel computational formulations and a plethora of applications. We review recent developments in Bayesian models and inference methods for CCA which are attractive for their potential in hierarchical extensions and for coping with the combination of large dimensionalities and small sample sizes. The existing methods have not been particularly successful in fulfilling the promise yet; we introduce a novel efficient solution that imposes group-wise sparsity to estimate the posterior of an extended model which not only extracts the statistical dependencies (correlations) between data sets but also decomposes the data into shared and data set-specific components. In statistics literature the model is known as inter-battery factor analysis (IBFA), for which we now provide a Bayesian treatment.},
 author = {Arto Klami and Seppo Virtanen and Samuel Kaski},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W2144903813},
 pages = {965--1003},
 title = {Bayesian Canonical correlation analysis},
 url = {http://jmlr.org/papers/v14/klami13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:kohlsdorf13a,
 abstract = {Gestures for interfaces should be short, pleasing, intuitive, and easily recognized by a computer. However, it is a challenge for interface designers to create gestures easily distinguishable from users’ normal movements. Our tool MAGIC Summoning addresses this problem. Given a specific platform and task, we gather a large database of unlabeled sensor data captured in the environments in which the system will be used (an “Everyday Gesture Library” or EGL). The EGL is quantized and indexed via multi-dimensional Symbolic Aggregate approXimation (SAX) to enable quick searching. MAGIC exploits the SAX representation of the EGL to suggest gestures with a low likelihood of false triggering. Suggested gestures are ordered according to brevity and simplicity, freeing the interface designer to focus on the user experience. Once a gesture is selected, MAGIC can output synthetic examples of the gesture to train a chosen classifier (for example, with a hidden Markov model). If the interface designer suggests his own gesture and provides several examples, MAGIC estimates how accurately that gesture can be recognized and estimates its false positive rate by comparing it against the natural movements in the EGL. We demonstrate MAGIC’s effectiveness in gesture selection and helpfulness in creating accurate gesture recognizers.},
 author = {Daniel Kyu Hwa Kohlsdorf and Thad E. Starner},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W24544081},
 pages = {209--242},
 title = {MAGIC Summoning: Towards Automatic Suggesting and Testing of Gestures with Low Probability of False Positives During Use},
 url = {http://jmlr.org/papers/v14/kohlsdorf13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:lewis13a,
 abstract = {Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps themachine learning field have a greater impact on research practices elsewhere.},
 author = {Joshua M. Lewis and Virginia R. de Sa and Laurens van der Maaten},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W2164959980},
 pages = {3159--3163},
 title = {Divvy: fast and intuitive exploratory data analysis},
 url = {http://jmlr.org/papers/v14/lewis13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:li13a,
 abstract = {In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi-supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WellSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WellSVM is also readily applicable on large data sets.},
 author = {Yu-Feng Li and Ivor W. Tsang and James T. Kwok and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2952002457},
 pages = {2151--2188},
 title = {Convex and Scalable Weakly Labeled SVMs},
 url = {http://jmlr.org/papers/v14/li13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:lin13a,
 abstract = {We propose a novel local isometry based dimensionality reduction method from the perspective of vector fields, which is called parallel vector field embedding (PFE). We first give a discussion on l...},
 author = {Binbin Lin and Xiaofei He and Chiyuan Zhang and Ming Ji},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W3082098116},
 pages = {2945--2977},
 title = {Parallel vector field embedding},
 url = {http://jmlr.org/papers/v14/lin13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:lisitsyn13a,
 abstract = {We present Tapkee, a C++ template library that provides efficient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced Barnes-Hut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and flexibility. For performance, we combine efficient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve flexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me.},
 author = {Sergey Lisitsyn and Christian Widmer and Fernando J. Iglesias Garcia},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2131142674},
 pages = {2355--2359},
 title = {Tapkee: an efficient dimension reduction library},
 url = {http://jmlr.org/papers/v14/lisitsyn13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:liu13a,
 abstract = {Hard and soft classifiers are two important groups of techniques for classification problems. Logistic regression and Support Vector Machines are typical examples of soft and hard classifiers respectively. The essential difference between these two groups is whether one needs to estimate the class conditional probability for the classification task or not. In particular, soft classifiers predict the label based on the obtained class conditional probabilities, while hard classifiers bypass the estimation of probabilities and focus on the decision boundary. In practice, for the goal of accurate classification, it is unclear which one to use in a given situation. To tackle this problem, the Large-margin Unified Machine (LUM) was recently proposed as a unified family to embrace both groups. The LUM family enables one to study the behavior change from soft to hard binary classifiers. For multicategory cases, however, the concept of soft and hard classification becomes less clear. In that case, class probability estimation becomes more involved as it requires estimation of a probability vector. In this paper, we propose a new Multicategory LUM (MLUM) framework to investigate the behavior of soft versus hard classification under multicategory settings. Our theoretical and numerical results help to shed some light on the nature of multicategory classification and its transition behavior from soft to hard classifiers. The numerical results suggest that the proposed tuned MLUM yields very competitive performance.},
 author = {Chong Zhang and Yufeng Liu},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2128409240},
 pages = {1349--1386},
 title = {Multicategory Large-Margin Unified Machines.},
 url = {http://jmlr.org/papers/v14/liu13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:liu13b,
 abstract = {Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparison-exchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The final solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difficult combinatorial problems.},
 author = {Qiang Liu and Alexander Ihler},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2152278102},
 pages = {303--331},
 title = {Using symmetry and evolutionary search to minimize sorting networks},
 url = {http://jmlr.org/papers/v14/liu13b.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:long13a,
 abstract = {We study the fundamental problem of learning an unknown large-margin half-space in the context of parallel computation.

Our main positive result is a parallel algorithm for learning a large-margin half-space that is based on interior point methods from convex optimization and fast parallel algorithms for matrix computations. We show that this algorithm learns an unknown γ-margin halfspace over n dimensions using poly(n, 1/γ) processors and runs in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have Ω(1/γ2) runtime dependence on γ.

Our main negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We give an information-theoretic proof that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized: the ability to call the weak learner multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required.},
 author = {Philip M. Long and Rocco A. Servedio},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2169065562},
 pages = {3105--3128},
 title = {Algorithms and hardness results for parallel large margin learning},
 url = {http://jmlr.org/papers/v14/long13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:mahdi13a,
 abstract = {Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conflicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conflicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to significantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG.},
 author = {Rami Mahdi and Jason Mezey},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2098250639},
 pages = {1563--1603},
 title = {Sub-local constraint-based learning of Bayesian networks using a joint dependence criterion},
 url = {http://jmlr.org/papers/v14/mahdi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:mairal13a,
 abstract = {We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called "path coding" penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efficiently solve by leveraging network flow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs.},
 author = {Julien Mairal and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2120470041},
 pages = {2449--2485},
 title = {Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows},
 url = {http://jmlr.org/papers/v14/mairal13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:malgireddy13a,
 abstract = {We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-specified activities (or gestures) in a video sequence, analogous to the use of filler models for keyword detection in speech processing. We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach.},
 author = {Manavender R. Malgireddy and Ifeoma Nwogu and Venu Govindaraju},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2098376331},
 pages = {2189--2212},
 title = {Language-Motivated Approaches to Action Recognition},
 url = {http://jmlr.org/papers/v14/malgireddy13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:mcfowland13a,
 abstract = {We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets.},
 author = {Edward McFowl and III and Skyler Speakman and Daniel B. Neill},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W2146022760},
 pages = {1533--1561},
 title = {Fast generalized subset scan for anomalous pattern detection},
 url = {http://jmlr.org/papers/v14/mcfowland13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:mukherjee13a,
 abstract = {Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the correct requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.},
 author = {Indraneel Mukherjee and Robert E. Schapire},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2570929519},
 pages = {437--497},
 title = {A theory of multiclass boosting},
 url = {http://jmlr.org/papers/v14/mukherjee13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:mukherjee13b,
 abstract = {The AdaBoost algorithm was designed to combine many weak hypotheses that perform slightly better than random guessing into a strong hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are finite. Our first result shows that the exponential loss of AdaBoost's computed parameter vector will be at most e more than that of any parameter vector of l1-norm bounded by B in a number of rounds that is at most a polynomial in B and 1/e. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/e iterations, AdaBoost achieves a value of the exponential loss that is at most e more than the best possible value, where C depends on the data set. We show that this dependence of the rate on e is optimal up to constant factors, that is, at least Ω(1/e) rounds are necessary to achieve within e of the optimal exponential loss.},
 author = {Indraneel Mukherjee and Cynthia Rudin and Robert E. Schapire},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2164468218},
 pages = {2315--2347},
 title = {The rate of convergence of AdaBoost},
 url = {http://jmlr.org/papers/v14/mukherjee13b.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:nakajima13a,
 abstract = {The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, finding the VB solution is a nonconvex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF--the global solution can be analytically computed. More specifically, the global solution is a reweighted SVD of the observed matrix, and each weight can be obtained by solving a quartic equation with its coefficients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis.},
 author = {Shinichi Nakajima and Masashi Sugiyama and S. Derin Babacan and Ryota Tomioka},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W69540849},
 pages = {1--37},
 title = {Global analytic solution of fully-observed variational Bayesian matrix factorization},
 url = {http://jmlr.org/papers/v14/nakajima13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:neuvial13a,
 abstract = {The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is defined as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the Benjamini-Hochberg procedure achieves FDR control at any pre-specified level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the $p$-values, as the number $m$ of tested hypotheses grows to infinity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form $m^{-k/(2k+1)}$, where $k$ is determined by the regularity of the density of the $p$-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one- and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model.},
 author = {Pierre Neuvial},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2105429912},
 pages = {1423--1459},
 title = {Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators},
 url = {http://jmlr.org/papers/v14/neuvial13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:niehren13a,
 abstract = {Inference algorithms for tree automata that define node selecting queries in unranked trees rely on tree pruning strategies. These impose additional assumptions on node selection that are needed to compensate for small numbers of annotated examples. Pruning-based heuristics in query learning algorithms for Web information extraction often boost the learning quality and speed up the learning process. We will distinguish the class of regular queries that are stable under a given schemaguided pruning strategy, and show that this class is learnable with polynomial time and data. Our learning algorithm is obtained by adding pruning heuristics to the traditional learning algorithm for tree automata from positive and negative examples. While justified by a formal learning model, our learning algorithm for stable queries also performs very well in practice of XML information extraction.},
 author = {Joachim Niehren and Jérôme Champavère and Aurélien Lemay and Rémi Gilleron},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W2129714350},
 pages = {927--964},
 title = {Query induction with schema-guided pruning strategies},
 url = {http://jmlr.org/papers/v14/niehren13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:niu13a,
 abstract = {The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using semi-definite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hard-label MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method possesses a clustering error bound. Thirdly, MVC includes the optimization problems of a spectral clustering, two relaxed k-means clustering and an information-maximization clustering as special limit cases when its regularization parameter goes to infinity. Experiments on several artificial and benchmark data sets demonstrate that the proposed MVC compares favorably with state-of-the-art clustering methods.},
 author = {Gang Niu and Bo Dai and Lin Shang and Masashi Sugiyama},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2125310487},
 pages = {2641--2687},
 title = {Maximum volume clustering: a new discriminative clustering approach},
 url = {http://jmlr.org/papers/v14/niu13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:niyogi13a,
 abstract = {Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on finite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning.},
 author = {Partha Niyogi},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2134284153},
 pages = {1229--1250},
 title = {Manifold regularization and semi-supervised learning: some theoretical analyses},
 url = {http://jmlr.org/papers/v14/niyogi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:noorshams13a,
 abstract = {The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP fixed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefficients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP fixed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefficients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical flow estimation.},
 author = {Nima Noorshams and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2103156296},
 pages = {2799--2835},
 title = {Belief propagation for continuous state spaces: stochastic message-passing with quantitative guarantees},
 url = {http://jmlr.org/papers/v14/noorshams13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:opper13a,
 abstract = {Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian field, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model's partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP's local matching of moments. Through the expansion, we see that EP is correct to first order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution.},
 author = {Manfred Opper and Ulrich Paquet and Ole Winther},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2100754418},
 pages = {2857--2898},
 title = {Perturbative corrections for approximate inference in Gaussian latent variable models},
 url = {http://jmlr.org/papers/v14/opper13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:pan13a,
 abstract = {Clustering analysis is widely used in many fields. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classification and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classification and regression, such as model selection criteria to select the number of clusters, a difficult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method's promising performance.},
 author = {Wei Pan and Xiaotong Shen and Binghui Liu},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2141766442},
 pages = {1865--1889},
 title = {Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty.},
 url = {http://jmlr.org/papers/v14/pan13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:parrish13a,
 abstract = {We consider the problem of classifying a test sample given incomplete information. This problem arises naturally when data about a test sample is collected over time, or when costs must be incurred to compute the classification features. For example, in a distributed sensor network only a fraction of the sensors may have reported measurements at a certain time, and additional time, power, and bandwidth is needed to collect the complete data to classify. A practical goal is to assign a class label as soon as enough data is available to make a good decision. We formalize this goal through the notion of reliability--the probability that a label assigned given incomplete data would be the same as the label assigned given the complete data, and we propose a method to classify incomplete data only if some reliability threshold is met. Our approach models the complete data as a random variable whose distribution is dependent on the current incomplete data and the (complete) training data. The method differs from standard imputation strategies in that our focus is on determining the reliability of the classification decision, rather than just the class label. We show that the method provides useful reliability estimates of the correctness of the imputed class labels on a set of experiments on time-series data sets, where the goal is to classify the time-series as early as possible while still guaranteeing that the reliability threshold is met.},
 author = {Nathan Parrish and Hyrum S. Anderson and Maya R. Gupta and Dun Yu Hsiao},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W2098899567},
 pages = {3561--3589},
 title = {Classifying with confidence from incomplete information},
 url = {http://jmlr.org/papers/v14/parrish13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:parviainen13a,
 abstract = {We consider the problem of finding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n, to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: first, the user may trade space against time; second, the proposed algorithms easily and efficiently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P, which can be significantly less than 2n. Considering sufficiently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders.},
 author = {Pekka Parviainen and Mikko Koivisto},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2152623729},
 pages = {1387--1415},
 title = {Finding optimal Bayesian networks using precedence constraints},
 url = {http://jmlr.org/papers/v14/parviainen13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:picard13a,
 abstract = {JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classification performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning).},
 author = {David Picard and Nicolas Thome and Matthieu Cord},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2133046540},
 pages = {1417--1421},
 title = {JKernelMachines: a simple framework for kernel machine},
 url = {http://jmlr.org/papers/v14/picard13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:rao13a,
 abstract = {Markov jump processes (or continuous-time Markov chains) are a simple and important class of continuous-time dynamical systems. In this paper, we tackle the problem of simulating from the posterior distribution over paths in these models, given partial and noisy observations. Our approach is an auxiliary variable Gibbs sampler, and is based on the idea of uniformization. This sets up a Markov chain over paths by alternately sampling a finite set of virtual jump times given the current path, and then sampling a new path given the set of extant and virtual jump times. The first step involves simulating a piecewise-constant inhomogeneous Poisson process, while for the second, we use a standard hidden Markov model forward filtering-backward sampling algorithm. Our method is exact and does not involve approximations like time-discretization. We demonstrate how our sampler extends naturally to MJP-based models like Markov-modulated Poisson processes and continuous-time Bayesian networks, and show significant computational benefits over state-of-the-art MCMC samplers for these models.},
 author = {Vinayak Rao and Yee Whye Teh},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W2114134972},
 pages = {3295--3320},
 title = {Fast MCMC sampling for Markov jump processes and extensions},
 url = {http://jmlr.org/papers/v14/rao13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:riihimaki13a,
 abstract = {We consider probabilistic multinomial probit classification using Gaussian process (GP) priors. The challenges with the multiclass GP classification are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classification rely on numerical quadratures or independence assumptions between the latent values from different classes to facilitate the computations. In this paper, we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all between-class posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method with respect to MCMC sampling, but the differences between the compared methods were small if only the classification accuracy is concerned.},
 author = {Jaakko Riihimäki and Pasi Jylänki and Aki Vehtari},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2992769304},
 pages = {75--109},
 title = {Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood},
 url = {http://jmlr.org/papers/v14/riihimaki13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:rosasco13a,
 abstract = {In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods.},
 author = {Lorenzo Rosasco and Silvia Villa and Sofia Mosci and Matteo Santoro and Aless and ro Verri},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2107194942},
 pages = {1665--1714},
 title = {Nonparametric sparsity and regularization},
 url = {http://jmlr.org/papers/v14/rosasco13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:roussos13a,
 abstract = {We propose the novel approach of dynamic affine-invariant shape-appearance model (Aff-SAM) and employ it for handshape classification and sign recognition in sign language (SL) videos. Aff-SAM offers a compact and descriptive representation of hand configurations as well as regularized model-fitting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by affine transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an affine signer adaptation component at the visual level, without requiring training from scratch a new singer-specific model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classification when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results.},
 author = {Anastasios Roussos and Stavros Theodorakis and Vassilis Pitsikalis and Petros Maragos},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2113512957},
 pages = {1627--1663},
 title = {Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos},
 url = {http://jmlr.org/papers/v14/roussos13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:rudin13a,
 abstract = {We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called sequential...},
 author = {Cynthia Rudin and Benjamin Letham and David Madigan},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W3004336054},
 pages = {3441--3492},
 title = {Learning theory analysis for association rules and sequential event prediction},
 url = {http://jmlr.org/papers/v14/rudin13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:ruozzi13a,
 abstract = {Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean (and variances) of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive definite quadratic function. Sufficient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive definite covariance matrix. As was observed by Malioutov et al. (2006), the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive definite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive definite whenever the input matrix is positive definite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges.},
 author = {Nicholas Ruozzi and Sekhar Tatikonda},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2155852402},
 pages = {2287--2314},
 title = {Message-passing algorithms for quadratic minimization},
 url = {http://jmlr.org/papers/v14/ruozzi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:ryabko13a,
 abstract = {A metric between time-series distributions is proposed that can be evaluated using binary classification methods, which were originally developed to work on i.i.d. data. It is shown how this metric can be used for solving statistical problems that are seemingly unrelated to classification and concern highly dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.},
 author = {Daniil Ryabko and J{{\'e}}r{{\'e}}mie Mary},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2116311558},
 pages = {2837--2856},
 title = {A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems},
 url = {http://jmlr.org/papers/v14/ryabko13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:sabato13a,
 abstract = {We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classifiers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning.},
 author = {Sivan Sabato and Nathan Srebro and Naftali Tishby},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2963203412},
 pages = {2119--2149},
 title = {Distribution-Dependent Sample Complexity of Large Margin Learning},
 url = {http://jmlr.org/papers/v14/sabato13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:salleb-aouissi13a,
 abstract = {In this paper, we propose QUANTMINER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers good intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QUANTMINER as an interactive, exploratory data mining tool.},
 author = {Ansaf Salleb-Aouissi and Christel Vrain and Cyril Nortet and Xiangrong Kong and Vivek Rathod and Daniel Cassard},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W2138358572},
 pages = {3153--3157},
 title = {QuantMiner for Mining Quantitative Association Rules},
 url = {http://jmlr.org/papers/v14/salleb-aouissi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:salomon13a,
 abstract = {This paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit. A well-known result of Lai and Robbins, which has then been extended by Burnetas and Katehakis, has established the presence of a logarithmic bound for all consistent policies. We relax the notion of consistency, and exhibit a generalisation of the bound. We also study the existence of logarithmic bounds in general and in the case of Hannan consistency. Moreover, we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment. To get these results, we study variants of popular Upper Confidence Bounds (UCB) policies.},
 author = {Antoine Salomon and Jean-Yves Audibert and Issam El Alaoui},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2149188027},
 pages = {187--207},
 title = {Lower bounds and selectivity of weak-consistent policies in stochastic multi-armed bandit problem},
 url = {http://jmlr.org/papers/v14/salomon13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:scherrer13a,
 abstract = {We consider the discrete-time infinite-horizon optimal control problem formalized by Markov decision processes (Puterman, 1994; Bertsekas and Tsitsiklis, 1996). We revisit the work of Bertsekas and Ioffe (1996), that introduced λ policy iteration--a family of algorithms parametrized by a parameter λ--that generalizes the standard algorithms value and policy iteration, and has some deep connections with the temporal-difference algorithms described by Sutton and Barto (1998). We deepen the original theory developed by the authors by providing convergence rate bounds which generalize standard bounds for value iteration described for instance by Puterman (1994). Then, the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form. We extend and unify the separate analyzes developed by Munos for approximate value iteration (Munos, 2007) and approximate policy iteration (Munos, 2003), and provide performance bounds in the discounted and the undiscounted situations. Finally, we revisit the use of this algorithm in the training of a Tetris playing controller as originally done by Bertsekas and Ioffe (1996). Our empirical results are different from those of Bertsekas and Ioffe (which were originally qualified as paradoxical and intriguing). We track down the reason to be a minor implementation error of the algorithm, which suggests that, in practice, l policy iteration may be more stable than previously thought.},
 author = {Bruno Scherrer},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W1526673156},
 pages = {1181--1227},
 title = {Performance bounds for λ policy iteration and application to the game of Tetris},
 url = {http://jmlr.org/papers/v14/scherrer13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:shalev-shwartz13a,
 abstract = {Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.},
 author = {Shai Shalev-Shwartz and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2952594493},
 pages = {567--599},
 title = {Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization},
 url = {http://jmlr.org/papers/v14/shalev-shwartz13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:shankar13a,
 abstract = {Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.},
 author = {Karthik H. Shankar and Marc W. Howard},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W4295188500},
 pages = {3785--3812},
 title = {Optimally fuzzy temporal memory},
 url = {http://jmlr.org/papers/v14/shankar13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:slivkins13a,
 abstract = {Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisfied users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justifications for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches.},
 author = {Aleksandrs Slivkins and Filip Radlinski and Sreenivas Gollapudi},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2117600734},
 pages = {399--436},
 title = {Ranked bandits in metric spaces: learning diverse rankings over large document collections},
 url = {http://jmlr.org/papers/v14/slivkins13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:statnikov13a,
 abstract = {Algorithms for Markov boundary discovery from data constitute an important recent development in machine learning, primarily because they offer a principled solution to the variable/feature selection problem and give insight on local causal structure. Over the last decade many sound algorithms have been proposed to identify a single Markov boundary of the response variable. Even though faithful distributions and, more broadly, distributions that satisfy the intersection property always have a single Markov boundary, other distributions/data sets may have multiple Markov boundaries of the response variable. The latter distributions/data sets are common in practical data-analytic applications, and there are several reasons why it is important to induce multiple Markov boundaries from such data. However, there are currently no sound and efficient algorithms that can accomplish this task. This paper describes a family of algorithms TIE* that can discover all Markov boundaries in a distribution. The broad applicability as well as efficiency of the new algorithmic family is demonstrated in an extensive benchmarking study that involved comparison with 26 state-of-the-art algorithms/variants in 15 data sets from a diversity of application domains.},
 author = {Alexander Statnikov and Nikita I. Lytkin and Jan Lemeire and Constantin F. Aliferis},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2171589180},
 pages = {499--566},
 title = {Algorithms for Discovery of Multiple Markov Boundaries.},
 url = {http://jmlr.org/papers/v14/statnikov13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:stowell13a,
 abstract = {We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via a synthetic experiment as well as an experiment to track a mixture of singing birds.},
 author = {Dan Stowell and Mark D. Plumbley},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2149684108},
 pages = {2213--2238},
 title = {Segregating event streams and noise with a Markov renewal process model},
 url = {http://jmlr.org/papers/v14/stowell13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:sun13a,
 abstract = {We propose a new method of learning a sparse nonnegative-definite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm first estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation.

We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other l1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the l1 and spectrum norms of the target inverse matrix diverges to infinity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method.

Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso.},
 author = {Tingni Sun and Cun-Hui Zhang},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W2963934928},
 pages = {3385--3418},
 title = {Sparse matrix inversion with scaled Lasso},
 url = {http://jmlr.org/papers/v14/sun13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:sun13b,
 abstract = {Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross-validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on a novel concept of variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. The effectiveness of the proposed criterion is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data.},
 author = {Wei Sun and Junhui Wang and Yixin Fang},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W2952704202},
 pages = {3419--3440},
 title = {Consistent selection of tuning parameters via variable selection stability},
 url = {http://jmlr.org/papers/v14/sun13b.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:tacchetti13a,
 abstract = {We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD licence and is available for download at https://github.com/CBCL/GURLS.},
 author = {Andrea Tacchetti and Pavan K. Mallapragada and Matteo Santoro and Lorenzo Rosasco},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2137592396},
 pages = {3201--3205},
 title = {GURLS: a Least Squares Library for Supervised Learning},
 url = {http://jmlr.org/papers/v14/tacchetti13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:talwalkar13a,
 abstract = {This paper examines the efficacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition tech...},
 author = {Ameet Talwalkar and Sanjiv Kumar and Mehryar Mohri and Henry Rowley},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W3006211001},
 pages = {3129--3152},
 title = {Large-scale SVD and manifold learning},
 url = {http://jmlr.org/papers/v14/talwalkar13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:thom13a,
 abstract = {Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activit...},
 author = {Markus Thom and Günther Palm},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W3016342592},
 pages = {1091--1143},
 title = {Sparse activity and sparse connectivity in supervised learning},
 url = {http://jmlr.org/papers/v14/thom13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:tong13a,
 abstract = {The Neyman-Pearson (NP) paradigm in binary classification treats type I and type II errors with different priorities. It seeks classifiers that minimize type II error, subject to a type I error constraint under a user specified level α. In this paper, plug-in classifiers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classifiers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classifiers handle different sampling schemes. This work focuses on theoretical properties of the proposed classifiers; in particular, we derive oracle inequalities that can be viewed as finite sample versions of risk bounds. NP classification can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a specific form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed.},
 author = {Xin Tong},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W2172143581},
 pages = {3011--3040},
 title = {A plug-in approach to neyman-pearson classification},
 url = {http://jmlr.org/papers/v14/tong13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:tulabandhula13a,
 abstract = {This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization.},
 author = {Theja Tulab and hula and Cynthia Rudin},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2964245082},
 pages = {1989--2028},
 title = {Machine learning with operational costs},
 url = {http://jmlr.org/papers/v14/tulabandhula13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:urry13a,
 abstract = {We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike, graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance matrices of e.g.\ Gaussian processes (GPs). In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from the probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation make it clear that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs.},
 author = {Matthew J. Urry and Peter Sollich},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W4293683928},
 pages = {1801--1835},
 title = {Random walk kernels and learning curves for Gaussian process regression on random graphs},
 url = {http://jmlr.org/papers/v14/urry13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:valsalam13a,
 abstract = {Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparison-exchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The final solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difficult combinatorial problems.},
 author = {Vinod K. Valsalam and Risto Miikkulainen},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2152278102},
 pages = {303--331},
 title = {Using symmetry and evolutionary search to minimize sorting networks},
 url = {http://jmlr.org/papers/v14/valsalam13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:vanhatalo13a,
 abstract = {The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods.},
 author = {Jarno Vanhatalo and Jaakko Riihimäki and Jouni Hartikainen and Pasi Jylänki and Ville Tolvanen and Aki Vehtari},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W136174036},
 pages = {1175--1179},
 title = {GPstuff: Bayesian modeling with Gaussian processes},
 url = {http://jmlr.org/papers/v14/vanhatalo13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:verma13a,
 abstract = {Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic finite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that embed a general n-dimensionalmanifold into Rd (where d only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) that guarantee to approximately preserve all interpoint geodesic distances.},
 author = {Nakul Verma},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2153072115},
 pages = {2415--2448},
 title = {Distance preserving embeddings for general n-dimensional manifolds},
 url = {http://jmlr.org/papers/v14/verma13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wan13a,
 abstract = {For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class....},
 author = {Jun Wan and Qiuqi Ruan and Wei Li and Shuang Deng},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W3149113639},
 pages = {2549--2582},
 title = {One-shot learning gesture recognition from RGB-D data using bag of features},
 url = {http://jmlr.org/papers/v14/wan13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wang13a,
 abstract = {Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classification function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classification function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efficient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artificial and standard benchmark data sets where it obtains superior classification accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes.},
 author = {Jun Wang and Tony Jebara and Shih-Fu Chang},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2152034960},
 pages = {771--800},
 title = {Semi-supervised learning using greedy max-cut},
 url = {http://jmlr.org/papers/v14/wang13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wang13b,
 abstract = {Canonical correlation analysis (CCA) is a classical method for seeking correlations between two multivariate data sets. During the last ten years, it has received more and more attention in the machine learning community in the form of novel computational formulations and a plethora of applications. We review recent developments in Bayesian models and inference methods for CCA which are attractive for their potential in hierarchical extensions and for coping with the combination of large dimensionalities and small sample sizes. The existing methods have not been particularly successful in fulfilling the promise yet; we introduce a novel efficient solution that imposes group-wise sparsity to estimate the posterior of an extended model which not only extracts the statistical dependencies (correlations) between data sets but also decomposes the data into shared and data set-specific components. In statistics literature the model is known as inter-battery factor analysis (IBFA), for which we now provide a Bayesian treatment.},
 author = {Chong Wang and David M. Blei},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2144903813},
 pages = {965--1003},
 title = {Bayesian Canonical correlation analysis},
 url = {http://jmlr.org/papers/v14/wang13b.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wang13c,
 author = {Shusen Wang and Zhihua Zhang},
 journal = {Journal of Machine Learning Research},
 number = {47},
 pages = {2729--2769},
 title = {Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling},
 url = {http://jmlr.org/papers/v14/wang13c.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wang13d,
 abstract = {We describe a R-Java CAM (convex analysis of mixtures) package that provides comprehensive analytic functions and a graphic user interface (GUI) for blindly separating mixed nonnegative sources. This open-source multiplatform software implements recent and classic algorithms in the literature including Chan et al. (2008), Wang et al. (2010), Chen et al. (2011a) and Chen et al. (2011b). The CAM package offers several attractive features: (1) instead of using proprietary MATLAB, its analytic functions are written in R, which makes the codes more portable and easier to modify; (2) besides producing and plotting results in R, it also provides a Java GUI for automatic progress update and convenient visual monitoring; (3) multi-thread interactions between the R and Java modules are driven and integrated by a Java GUI, assuring that the whole CAM software runs responsively; (4) the package offers a simple mechanism to allow others to plug-in additional R-functions.},
 author = {Niya Wang and Fan Meng and Li Chen and Subha Madhavan and Robert Clarke and Eric P. Hoffman and Jianhua Xuan and Yue Wang},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2130705250},
 pages = {2899--2903},
 title = {The CAM software for nonnegative blind source separation in R-Java},
 url = {http://jmlr.org/papers/v14/wang13d.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:watanabe13a,
 abstract = {A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature $1/\log n$, where $n$ is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for and unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models.},
 author = {Sumio Watanabe},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2950482235},
 pages = {867--897},
 title = {A Widely Applicable Bayesian Information Criterion},
 url = {http://jmlr.org/papers/v14/watanabe13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wu13a,
 abstract = {The task of matching data from two heterogeneous domains naturally arises in various areas such as web search, collaborative filtering, and drug design. In web search, existing work has designed relevance models to match queries and documents by exploiting either user clicks or content of queries and documents. To the best of our knowledge, however, there has been little work on principled approaches to leveraging both clicks and content to learn a matching model for search. In this paper, we propose a framework for learning to match heterogeneous objects. The framework learns two linear mappings for two objects respectively, and matches them via the dot product of their images after mapping. Moreover, when different regularizations are enforced, the framework renders a rich family of matching models. With orthonormal constraints on mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with a l1+l2 regularization, we obtain a new model called Regularized Mapping to Latent Structures (RMLS). RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization. To further understand the matching framework, we conduct generalization analysis and apply the result to both PLS and RMLS. We apply the framework to web search and implement both PLS and RMLS using a click-through bipartite with metadata representing features of queries and documents. We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods, while RMLS substantially speeds up the learning process.},
 author = {Wei Wu and Zhengdong  Lu and Hang Li},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2127564752},
 pages = {2519--2548},
 title = {Learning bilinear model for matching queries and documents},
 url = {http://jmlr.org/papers/v14/wu13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:yuan13a,
 abstract = {This paper considers the sparse eigenvalue problem, which is to extract dominant (largest) sparse eigenvectors with at most $k$ non-zero components. We propose a simple yet effective solution called truncated power method that can approximately solve the underlying nonconvex optimization problem. A strong sparse recovery result is proved for the truncated power method, and this theory is our key motivation for developing the new algorithm. The proposed method is tested on applications such as sparse principal component analysis and the densest $k$-subgraph problem. Extensive experiments on several synthetic and real-world large scale datasets demonstrate the competitive empirical performance of our method.},
 author = {Xiao-Tong Yuan and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2137404238},
 pages = {899--925},
 title = {Truncated Power Method for Sparse Eigenvalue Problems},
 url = {http://jmlr.org/papers/v14/yuan13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:zaidi13a,
 abstract = {Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE.},
 author = {Nayyar A. Zaidi and Jes{{\'u}}s Cerquides and Mark J. Carman and Geoffrey I. Webb},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2108626773},
 pages = {1947--1988},
 title = {Alleviating naive Bayes attribute independence assumption by attribute weighting},
 url = {http://jmlr.org/papers/v14/zaidi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:zhang13b,
 abstract = {We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as O(N <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">-1</sup> + (N/m) <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">-2</sup> ). Whenever m ≤ √N, this guarantee matches the best possible rate achievable by a centralized algorithm with access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap. Requiring only a single round of communication, it has mean-squared error that decays as O(N <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">-1</sup> + (N/m) <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">-3</sup> ), and so is more robust to the amount of parallelization.},
 author = {Yuchen Zhang and John C. Duchi and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2962696932},
 pages = {3321--3363},
 title = {Communication-efficient algorithms for statistical optimization},
 url = {http://jmlr.org/papers/v14/zhang13b.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:zhao13a,
 abstract = {Fano's inequality lower bounds the probability of transmission error through a communication channel. Applied to classification problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general definition of conditional entropy (including Shannon's as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano's result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classifier. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate--we derive similar optimal thresholds for F-score and BER.},
 author = {Ming-Jie Zhao and Narayanan Edakunni and Adam Pocock and Gavin Brown},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W152506389},
 pages = {1033--1090},
 title = {Beyond Fano's inequality: bounds on the optimal F-score, BER, and cost-sensitive risk and their implications},
 url = {http://jmlr.org/papers/v14/zhao13a.html},
 volume = {14},
 year = {2013}
}
