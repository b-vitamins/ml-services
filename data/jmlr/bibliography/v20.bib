@article{JMLR:v20:13-124,
 author = {Lei Shi and Xiaolin Huang and Yunlong Feng and Johan A.K. Suykens},
 journal = {Journal of Machine Learning Research},
 number = {161},
 openalex = {W2998555618},
 pages = {1--44},
 title = {Sparse Kernel Regression with Coefficient-based $\ell_q-$regularization},
 url = {http://jmlr.org/papers/v20/13-124.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:13-580,
 author = {Joey Tianyi Zhou and Ivor W. Tsang and Sinno Jialin Pan and Mingkui Tan},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2946170651},
 pages = {1--31},
 title = {Multi-class heterogeneous domain adaptation},
 url = {http://jmlr.org/papers/v20/13-580.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:15-192,
 abstract = {We present a new algorithm for domain adaptation improving upon a discrepancy minimization algorithm, (DM), previously shown to outperform a number of algorithms for this problem. Unlike many previ...},
 author = {Corinna Cortes and Mehryar Mohri and Andr{{\'e}}s Mu{{\~n}}oz Medina},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2593597837},
 pages = {1--30},
 title = {Adaptation Based on Generalized Discrepancy},
 url = {http://jmlr.org/papers/v20/15-192.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:15-504,
 author = {Lijun Zhang and Tianbao Yang and Rong Jin and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W2960983841},
 pages = {1--22},
 title = {Relative Error Bound Analysis for Nuclear Norm Regularized Matrix Completion},
 url = {http://jmlr.org/papers/v20/15-504.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-128,
 abstract = {As a model problem for clustering, we consider the densest k-disjoint-clique problem of partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the densities of these ...},
 author = {Aleksis Pirinen and Brendan Ames},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W3208087360},
 pages = {1--34},
 title = {Exact clustering of weighted graphs via semidefinite programming},
 url = {http://jmlr.org/papers/v20/16-128.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-155,
 abstract = {We tackle the change-point problem with data belonging to a general set. We build a penalty for choosing the number of change-points in the kernel-based method of Harchaoui and Capp{é} (2007). This penalty generalizes the one proposed by Lebarbier (2005) for one-dimensional signals. We prove a non-asymptotic oracle inequality for the proposed method, thanks to a new concentration result for some function of Hilbert-space valued random variables. Experiments on synthetic data illustrate the accuracy of our method, showing that it can detect changes in the whole distribution of data, even when the mean and variance are constant.},
 author = {Sylvain Arlot and Alain Celisse and Zaid Harchaoui},
 journal = {Journal of Machine Learning Research},
 number = {162},
 openalex = {W2308827608},
 pages = {1--56},
 title = {A Kernel Multiple Change-point Algorithm via Model Selection},
 url = {http://jmlr.org/papers/v20/16-155.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-243,
 abstract = {We investigated the feature map inside deep neural networks (DNNs) by tracking the transport map. We are interested in the role of depth--why do DNNs perform better than shallow models?--and the interpretation of DNNs--what do intermediate layers do? Despite the rapid development in their application, DNNs remain analytically unexplained because the hidden layers are nested and the parameters are not faithful. Inspired by the integral representation of shallow NNs, which is the continuum limit of the width, or the hidden unit number, we developed the flow representation and transport analysis of DNNs. The flow representation is the continuum limit of the depth, or the hidden layer number, and it is specified by an ordinary differential equation (ODE) with a vector field. We interpret an ordinary DNN as a transport map or an Euler broken line approximation of the flow. Technically speaking, a dynamical system is a natural model for the nested feature maps. In addition, it opens a new way to the coordinate-free treatment of DNNs by avoiding the redundant parametrization of DNNs. Following Wasserstein geometry, we analyze a flow in three aspects: dynamical system, continuity equation, and Wasserstein gradient flow. A key finding is that we specified a series of transport maps of the denoising autoencoder (DAE), which is a cornerstone for the development of deep learning. Starting from the shallow DAE, this paper develops three topics: the transport map of the deep DAE, the equivalence between the stacked DAE and the composition of DAEs, and the development of the double continuum limit or the integral representation of the flow representation. As partial answers to the research questions, we found that deeper DAEs converge faster and the extracted features are better; in addition, a deep Gaussian DAE transports mass to decrease the Shannon entropy of the data distribution. We expect that further investigations on these questions lead to the development of an interpretable and principled alternatives to DNNs.},
 author = {Sho Sonoda and Noboru Murata},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2899297079},
 pages = {1--52},
 title = {Transport analysis of infinitely deep neural network},
 url = {http://jmlr.org/papers/v20/16-243.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-309,
 author = {Po-Wei Wang and Ching-pei Lee and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2945677310},
 pages = {1--49},
 title = {The Common-directions Method for Regularized Empirical Risk Minimization},
 url = {http://jmlr.org/papers/v20/16-309.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-314,
 abstract = {We present a novel framework for kernel learning with sequential data of any kind, such as time series, sequences of graphs, or strings. Our approach is based on signature features which can be seen as an ordered variant of sample (cross-)moments; it allows to obtain a sequentialized version of any static kernel. The sequential kernels are efficiently computable for discrete sequences and are shown to approximate a continuous moment form in a sampling sense. A number of known kernels for sequences arise as sequentializations of suitable static kernels: string kernels may be obtained as a special case, and alignment kernels are closely related up to a modification that resolves their open non-definiteness issue. Our experiments indicate that our signature-based sequential kernel framework may be a promising approach to learning with sequential data, such as time series, that allows to avoid extensive manual pre-processing.},
 author = {Franz J. Kiraly and Harald Oberhauser},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2963611658},
 pages = {1--45},
 title = {Kernels for sequentially ordered data},
 url = {http://jmlr.org/papers/v20/16-314.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-383,
 abstract = {Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel Two-Layer Feature REduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method---called DPC (DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.},
 author = {Jie Wang and Zhanqiu Zhang and Jieping Ye},
 journal = {Journal of Machine Learning Research},
 number = {163},
 openalex = {W2949385534},
 pages = {1--42},
 title = {Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets},
 url = {http://jmlr.org/papers/v20/16-383.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-437,
 abstract = {We develop an automated variational method for inference in models with Gaussian process (GP) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using samples from univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse GP approximations, along with parallel computation and stochastic optimization. We evaluate our approach quantitatively and qualitatively with experiments on small datasets, medium-scale datasets and large datasets, showing its competitiveness under different likelihood models and sparsity levels. On the large-scale experiments involving prediction of airline delays and classification of handwritten digits, we show that our method is on par with the state-of-the-art hard-coded approaches for scalable GP regression and classification.},
 author = {Edwin V. Bonilla and Karl Krauth and Amir Dezfouli},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W2512115115},
 pages = {1--63},
 title = {Generic Inference in Latent Gaussian Process Models},
 url = {http://jmlr.org/papers/v20/16-437.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-585,
 abstract = {We consider stochastic nonparametric regression problems in a reproducing kernel Hilbert space (RKHS), an extension of expected risk minimization to nonlinear function estimation. Popular perception is that kernel methods are inapplicable to online settings, since the generalization of stochastic methods to kernelized function spaces require memory storage that is cubic in the iteration index (“the curse of kernelization”). We alleviate this intractability in two ways: (1) we consider the use of functional stochastic gradient method (FSGD) which operates on a subset of training examples at each step; and (2), we extract parsimonious approximations of the resulting stochastic sequence via a greedy sparse subspace projection scheme based on kernel orthogonal matching pursuit (KOMP). We establish that this method converges almost surely in both diminishing and constant algorithm step-size regimes for a specific selection of sparse approximation budget. The method is evaluated on a kernel multi-class support vector machine problem, where data samples are generated from class-dependent Gaussian mixture models.},
 author = {Alec Koppel and Garrett Warnell and Ethan Stump and Alejandro Ribeiro},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2962755279},
 pages = {1--44},
 title = {Parsimonious Online Learning with Kernels via sparse projections in function space},
 url = {http://jmlr.org/papers/v20/16-585.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-588,
 abstract = {In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.},
 author = {Cl{{\'e}}ment Bouttier and Ioana Gavra},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2593946989},
 pages = {1--45},
 title = {Convergence rate of a simulated annealing algorithm with noisy observations},
 url = {http://jmlr.org/papers/v20/16-588.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-607,
 abstract = {In this paper, we consider the problem of learning high-dimensional tensor regression problems with low-rank structure. One of the core challenges associated with learning high-dimensional models is computation since the underlying optimization problems are often non-convex. While convex relaxations could lead to polynomial-time algorithms they are often slow in practice. On the other hand, limited theoretical guarantees exist for non-convex methods. In this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set $\Theta$ in terms of its \emph{localized Gaussian width}. We juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches. The two main differences between the convex and non-convex approach are: (i) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and (ii) from a statistical upper bound perspective, the non-convex approach has a superior rate for a number of examples. We provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches. We supplement our theoretical results with simulations which show that, under several common settings of generalized low rank tensor regression, the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen.},
 author = {Han Chen and Garvesh Raskutti and Ming Yuan},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2963559762},
 pages = {1--37},
 title = {Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression},
 url = {http://jmlr.org/papers/v20/16-607.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-615,
 author = {Leonardo V. Teixeira and Renato M. Assun{\c{c}}{{\~a}}o and Rosangela H. Loschi},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2949511096},
 pages = {1--35},
 title = {Bayesian Space-Time Partitioning by Sampling and Pruning Spanning Trees},
 url = {http://jmlr.org/papers/v20/16-615.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:16-627,
 abstract = {Linear subspace models are pervasive in computational sciences and particularly used for large datasets which are often incomplete due to privacy issues or sampling constraints. Therefore, a critical problem is developing an efficient algorithm for detecting low-dimensional linear structure from incomplete data efficiently, in terms of both computational complexity and storage. 
In this paper we propose a streaming subspace estimation algorithm called Subspace Navigation via Interpolation from Partial Entries (SNIPE) that efficiently processes blocks of incomplete data to estimate the underlying subspace model. In every iteration, SNIPE finds the subspace that best fits the new data block but remains close to the previous estimate. We show that SNIPE is a streaming solver for the underlying nonconvex matrix completion problem, that it converges globally to a stationary point of this program regardless of initialization, and that the convergence is locally linear with high probability. We also find that SNIPE shows state-of-the-art performance in our numerical simulations.},
 author = {Armin Eftekhari and Gregory Ongie and Laura Balzano and Michael B. Wakin},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2949971160},
 pages = {1--62},
 title = {Streaming Principal Component Analysis From Incomplete Data},
 url = {http://jmlr.org/papers/v20/16-627.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-026,
 abstract = {We study large-scale kernel methods for acoustic modeling in speech recognition and compare their performance to deep neural networks (DNNs). We perform experiments on four speech recognition datasets, including the TIMIT and Broadcast News benchmark tasks, and compare these two types of models on frame-level performance metrics (accuracy, cross-entropy), as well as on recognition metrics (word/character error rate). In order to scale kernel methods to these large datasets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, in order to reduce the number of random features required by kernel models, we propose a simple but effective method for feature selection. The method is able to explore a large number of non-linear features while maintaining a compact model more efficiently than existing approaches. Second, we present a number of frame-level metrics which correlate very strongly with recognition performance when computed on the heldout set; we take advantage of these correlations by monitoring these metrics during training in order to decide when to stop learning. This technique can noticeably improve the recognition performance of both DNN and kernel models, while narrowing the gap between them. Additionally, we show that the linear bottleneck method of Sainath et al. (2013) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Together, these three methods dramatically improve the performance of kernel acoustic models, making their performance comparable to DNNs on the tasks we explored.},
 author = {Avner May and Alireza Bagheri Garakani and Zhiyun Lu and Dong Guo and Kuan Liu and Aurélien Bellet and Linxi Fan and Michael Collins and Daniel Hsu and Brian Kingsbury and Michael Picheny and Fei Sha},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2579029485},
 pages = {1--36},
 title = {Kernel Approximation Methods for Speech Recognition},
 url = {http://jmlr.org/papers/v20/17-026.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-066,
 author = {Jan Kralj and Marko Robnik-Sikonja and Nada Lavrac},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2933483417},
 pages = {1--50},
 title = {NetSDM: Semantic Data Mining with Network Analysis},
 url = {http://jmlr.org/papers/v20/17-066.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-100,
 author = {Piotr Szyma{{\'n}}ski and Tomasz Kajdanowicz},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2912218607},
 pages = {1--22},
 title = {scikit-multilearn: A Python library for Multi-Label Classification},
 url = {http://jmlr.org/papers/v20/17-100.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-137,
 author = {Mohammad Saberian and Nuno Vasconcelos},
 journal = {Journal of Machine Learning Research},
 number = {137},
 openalex = {W2982416285},
 pages = {1--68},
 title = {Multiclass Boosting: Margins, Codewords, Losses, and Algorithms},
 url = {http://jmlr.org/papers/v20/17-137.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-147,
 abstract = {A selective classifier (f,g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A rejection rate is achieved if the rejection mass is bounded from above by O(1/m) where m is the number of labeled examples used to train the classifier (and O hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. 
We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke's disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke's disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke's disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called ActiveiLESS.},
 author = {Roei Gelbhart and Ran El-Yaniv},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2603161330},
 pages = {1--38},
 title = {The Relationship Between Agnostic Selective Classification, Active Learning and the Disagreement Coefficient},
 url = {http://jmlr.org/papers/v20/17-147.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-153,
 abstract = {Author(s): Razaee, ZS; Amini, AA; Li, JJ | Abstract: Community detection or clustering is a fundamental task in the analysis of network data. Many real networks have a bipartite structure which makes community detection challenging. In this paper, we consider a model which allows for matched communities in the bipartite setting, in addition to node covariates with information about the matching. We derive a simple fast algorithm for fitting the model based on variational inference ideas and show its effectiveness on both simulated and real data. A variation of the model to allow for degree-correction is also considered, in addition to a novel approach to fitting such degree-corrected models.},
 author = {Zahra S. Razaee and Arash A. Amini and Jingyi Jessica Li},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2962912239},
 pages = {1--44},
 title = {Matched Bipartite Block Model with Covariates},
 url = {http://jmlr.org/papers/v20/17-153.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-170,
 abstract = {This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called \emph{binarsity}. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint. This induces two interesting properties on the model weights of the one-hot encoded features: they are piecewise constant, and are eventually block sparse. Non-asymptotic oracle inequalities for generalized linear models are proposed. Moreover, under a sparse additive model assumption, we prove that our procedure matches the state-of-the-art in this setting. Numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard $\ell_1$ penalization.},
 author = {Mokhtar Z. Alaya and Simon Bussy and St{{\'e}}phane Ga{{\"i}}ffas and Agathe Guilloux},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W2976282046},
 pages = {1--34},
 title = {Binarsity: a penalization for one-hot encoded features in linear supervised learning},
 url = {http://jmlr.org/papers/v20/17-170.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-185,
 abstract = {The trade-off between the cost of acquiring and processing data, and uncertainty due to a lack of data is fundamental in machine learning. A basic instance of this trade-off is the problem of deciding when to make noisy and costly observations of a discrete-time Gaussian random walk, so as to minimise the posterior variance plus observation costs. We present the first proof that a simple policy, which observes when the posterior variance exceeds a threshold, is optimal for this problem. The proof generalises to a wide range of cost functions other than the posterior variance. 
This result implies that optimal policies for linear-quadratic-Gaussian control with costly observations have a threshold structure. It also implies that the restless bandit problem of observing multiple such time series, has a well-defined Whittle index. We discuss computation of that index, give closed-form formulae for it, and compare the performance of the associated index policy with heuristic policies. 
The proof is based on a new verification theorem that demonstrates threshold structure for Markov decision processes, and on the relation between binary sequences known as mechanical words and the dynamics of discontinuous nonlinear maps, which frequently arise in physics, control and biology.},
 author = {Christopher R. Dance and Tomi Silander},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2610378560},
 pages = {1--93},
 title = {Optimal Policies for Observing Time Series and Related Restless Bandit Problems},
 url = {http://jmlr.org/papers/v20/17-185.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-252,
 abstract = {We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution $ρ$ in $\mathbb{R}^D$, that is nearly supported on a $d$-dimensional set $\mathcal{M}$ - for example supported on a $d$-dimensional Riemannian manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of $\mathcal{M}$ at varying resolutions. We introduce a thresholding algorithm on the geometric wavelet coefficients, leading to what we call adaptive GMRA approximations. We show that these data-driven, empirical approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples $n$, on a wide variety of measures $ρ$, that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These approximations yield a data-driven dictionary, together with a fast transform mapping data to coefficients, and an inverse of such a map. The algorithms for both the dictionary construction and the transforms have complexity $C n \log n$ with the constant linear in $D$ and exponential in $d$. Our work therefore establishes adaptive GMRA as a fast dictionary learning algorithm with approximation guarantees. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of adaptive GMRA.},
 author = {Wenjing Liao and Mauro Maggioni},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W2549650688},
 pages = {1--63},
 title = {Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data},
 url = {http://jmlr.org/papers/v20/17-252.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-279,
 abstract = {In stochastic optimization, the population risk is generally approximated by the empirical risk which is in turn minimized by an iterative algorithm. However, in the large-scale setting, empirical ...},
 author = {Murat Erdogdu and Mohsen Bayati and Lee H. Dicker},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2963962909},
 pages = {1--45},
 title = {Scalable approximations for generalized linear problems},
 url = {http://jmlr.org/papers/v20/17-279.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-286,
 abstract = {This paper investigates the behavior of the Min-Sum message passing scheme to solve systems of linear equations in the Laplacian matrices of graphs and to compute electric flows. Voltage and flow problems involve the minimization of quadratic functions and are fundamental primitives that arise in several domains. Algorithms that have been proposed are typically centralized and involve multiple graph-theoretic constructions or sampling mechanisms that make them difficult to implement and analyze. On the other hand, message passing routines are distributed, simple, and easy to implement. In this paper we establish a framework to analyze Min-Sum to solve voltage and flow problems. We characterize the error committed by the algorithm on general weighted graphs in terms of hitting times of random walks defined on the computation trees that support the operations of the algorithms with time. For $d$-regular graphs with equal weights, we show that the convergence of the algorithms is controlled by the total variation distance between the distributions of non-backtracking random walks defined on the original graph that start from neighboring nodes. The framework that we introduce extends the analysis of Min-Sum to settings where the contraction arguments previously considered in the literature (based on the assumption of walk summability or scaled diagonal dominance) can not be used, possibly in the presence of constraints.},
 author = {Patrick Rebeschini and Sekhar Tatikonda},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W2554239292},
 pages = {1--37},
 title = {A New Approach to Laplacian Solvers and Flow Problems},
 url = {http://jmlr.org/papers/v20/17-286.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-324,
 abstract = {We present a mathematical analysis of a non-convex energy landscape for robust subspace recovery. We prove that an underlying subspace is the only stationary point and local minimizer in a specified neighborhood under a deterministic condition on a dataset. If the deterministic condition is satisfied, we further show that a geodesic gradient descent method over the Grassmannian manifold can exactly recover the underlying subspace when the method is properly initialized. Proper initialization by principal component analysis is guaranteed with a simple deterministic condition. Under slightly stronger assumptions, the gradient descent method with a piecewise constant step-size scheme achieves linear convergence. The practicality of the deterministic condition is demonstrated on some statistical models of data, and the method achieves almost state-of-the-art recovery guarantees on the Haystack Model for different regimes of sample size and ambient dimension. In particular, when the ambient dimension is fixed and the sample size is large enough, we show that our gradient method can exactly recover the underlying subspace for any fixed fraction of outliers (less than 1).},
 author = {Tyler Maunu and Teng Zhang and Gilad Lerman},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2962685343},
 pages = {1--59},
 title = {A Well-Tempered Landscape for Non-convex Robust Subspace Recovery},
 url = {http://jmlr.org/papers/v20/17-324.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-334,
 abstract = {Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this...},
 author = {Giorgos Borboudakis and Ioannis Tsamardinos},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W3132999519},
 pages = {1--39},
 title = {Forward-backward selection with early dropping},
 url = {http://jmlr.org/papers/v20/17-334.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-340,
 author = {WenWu Wang and Ping Yu and Lu Lin and Tiejun Tong},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2946302343},
 pages = {1--49},
 title = {Robust Estimation of Derivatives Using Locally Weighted Least Absolute Deviation Regression},
 url = {http://jmlr.org/papers/v20/17-340.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-352,
 abstract = {Identifying changes in model parameters is fundamental in machine learning and statistics. However, standard changepoint models are limited in expressiveness, often addressing unidimensional problems and assuming instantaneous changes. We introduce change surfaces as a multidimensional and highly expressive generalization of changepoints. We provide a model-agnostic formalization of change surfaces, illustrating how they can provide variable, heterogeneous, and non-monotonic rates of change across multiple dimensions. Additionally, we show how change surfaces can be used for counterfactual prediction. As a concrete instantiation of the change surface framework, we develop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual prediction with Bayesian posterior mean and credible sets, as well as massive scalability by introducing novel methods for additive non-separable kernels. Using two large spatio-temporal datasets we employ GPCS to discover and characterize complex changes that can provide scientific and policy relevant insights. Specifically, we analyze twentieth century measles incidence across the United States and discover previously unknown heterogeneous changes after the introduction of the measles vaccine. Additionally, we apply the model to requests for lead testing kits in New York City, discovering distinct spatial and demographic patterns.},
 author = {William Herlands and Daniel B. Neill and Hannes Nickisch and Andrew Gordon Wilson},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2898470510},
 pages = {1--51},
 title = {Change Surfaces for Expressive Multidimensional Changepoints and Counterfactual Prediction},
 url = {http://jmlr.org/papers/v20/17-352.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-357,
 abstract = {We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. Customers independently make purchasi...},
 author = {Adel Javanmard and Hamid Nazerzadeh},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W3012535064},
 pages = {1--49},
 title = {Dynamic pricing in high-dimensions},
 url = {http://jmlr.org/papers/v20/17-357.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-373,
 author = {Yichen Chen and Yinyu Ye and Mengdi Wang},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2930120848},
 pages = {1--27},
 title = {Approximation Hardness for A Class of Sparse Optimization Problems},
 url = {http://jmlr.org/papers/v20/17-373.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-397,
 abstract = {The higher order singular value decomposition (HOSVD) of tensors is a generalization of matrix SVD. The perturbation analysis of HOSVD under random noise is more delicate than its matrix counterpart. Recently, polynomial time algorithms have been proposed where statistically optimal estimates of the singular subspaces and the low rank tensors are attainable in the Euclidean norm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and introduce estimators of the singular subspaces with sharp deviation bounds in the sup-norm. We also investigate a low rank tensor denoising estimator and demonstrate its fast convergence rate with respect to the entry-wise errors. The sup-norm perturbation bounds reveal unconventional phase transitions for statistical learning applications such as the exact clustering in high dimensional Gaussian mixture model and the exact support recovery in sub-tensor localizations. In addition, the bounds established for HOSVD also elaborate the one-sided sup-norm perturbation bounds for the singular subspaces of unbalanced (or fat) matrices.},
 author = {Dong Xia and Fan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2907863856},
 pages = {1--42},
 title = {The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising},
 url = {http://jmlr.org/papers/v20/17-397.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-451,
 abstract = {In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. In this way, the dimension reduction step encodes a tradeoff between cost and accuracy. However, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know (1) how accurate a given solution is, or (2) how much computation is needed to achieve a given level of accuracy. In the current paper, we study randomized matrix multiplication (sketching) as a prototype setting for addressing these general problems. As a solution, we develop a bootstrap method for \emph{directly estimating} the accuracy as a function of the reduced dimension (as opposed to deriving worst-case bounds on the accuracy in terms of the reduced dimension). From a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an extrapolation technique. In addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method.},
 author = {Miles E. Lopes and Shusen Wang and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2963990444},
 pages = {1--40},
 title = {A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication},
 url = {http://jmlr.org/papers/v20/17-451.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-452,
 abstract = {Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large datasets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that utilize gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients.},
 author = {Khue-Dung Dang and Matias Quiroz and Robert Kohn and Minh-Ngoc Tran and Mattias Villani},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2963841486},
 pages = {1--31},
 title = {Hamiltonian Monte Carlo with Energy Conserving Subsampling},
 url = {http://jmlr.org/papers/v20/17-452.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-486,
 abstract = {In this article, we describe the user-written gmentropylinear command, which implements the generalized maximum entropy estimation method for linear models. This is an information-theoretic procedure preferable to its maximum likelihood counterparts in many applications; it avoids making distributional assumptions, works well when the sample is small or covariates are highly correlated, and is more efficient than its maximum likelihood equivalent. We give a brief introduction to the generalized maximum entropy procedure, present the gmentropylinear command, and give an example using the command.},
 author = {Tobias Sutter and David Sutter and Peyman Mohajerin Esfahani and John Lygeros},
 journal = {Journal of Machine Learning Research},
 number = {138},
 openalex = {W2610950626},
 pages = {1--29},
 title = {Generalized Maximum Entropy Estimation of Linear Models},
 url = {http://jmlr.org/papers/v20/17-486.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-498,
 author = {S{{\'e}}bastien Bubeck and Nikhil R. Devanur and Zhiyi Huang and Rad Niazadeh},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W2945557615},
 pages = {1--37},
 title = {Multi-scale Online Learning: Theory and Applications to Online Auctions and Pricing},
 url = {http://jmlr.org/papers/v20/17-498.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-501,
 abstract = {Graphical Lasso (GL) is a popular method for learning the structure of an undirected graphical model, which is based on an $l_1$ regularization technique. The objective of this paper is to compare the computationally-heavy GL technique with a numerically-cheap heuristic method that is based on simply thresholding the sample covariance matrix. To this end, two notions of sign-consistent and inverse-consistent matrices are developed, and then it is shown that the thresholding and GL methods are equivalent if: (i) the thresholded sample covariance matrix is both sign-consistent and inverse-consistent, and (ii) the gap between the largest thresholded and the smallest un-thresholded entries of the sample covariance matrix is not too small. By building upon this result, it is proved that the GL method---as a conic optimization problem---has an explicit closed-form solution if the thresholded sample covariance matrix has an acyclic structure. This result is then generalized to arbitrary sparse support graphs, where a formula is found to obtain an approximate solution of GL. Furthermore, it is shown that the approximation error of the derived explicit formula decreases exponentially fast with respect to the length of the minimum-length cycle of the sparsity graph. The developed results are demonstrated on synthetic data, functional MRI data, traffic flows for transportation networks, and massive randomly generated data sets. We show that the proposed method can obtain an accurate approximation of the GL for instances with the sizes as large as $80,000\times 80,000$ (more than 3.2 billion variables) in less than 30 minutes on a standard laptop computer running MATLAB, while other state-of-the-art methods do not converge within 4 hours.},
 author = {Salar Fattahi and Somayeh Sojoudi},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2964307711},
 pages = {1--44},
 title = {Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions},
 url = {http://jmlr.org/papers/v20/17-501.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-504,
 abstract = {In this paper, the problem of one-bit compressed sensing (OBCS) is formulated as a problem in probably approximately correct (PAC) learning. It is shown that the Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in ℝn generated by k-sparse vectors is bounded below by k(⌊lg(n=k)⌋+1) and above by ⌊2k lg(en)⌋. By coupling this estimate with well-established results in PAC learning theory, we show that a consistent algorithm can recover a k-sparse vector with O(k lg n) measurements, given only the signs of the measurement vector. This result holds for all probability measures on ℝn. The theory is also applicable to the case of noisy labels, where the signs of the measurements are flipped with some unknown probability.},
 author = {Mehmet Eren Ahsen and Mathukumalli Vidyasagar},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2911710630},
 pages = {1--23},
 title = {An approach to one-bit compressed sensing based on probably approximately correct learning theory},
 url = {http://jmlr.org/papers/v20/17-504.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-517,
 abstract = {Kernel $k$-means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear $k$-means clustering algorithm. However, kernel $k$-means clustering is computationally expensive when the non-linear feature map is high-dimensional and there are many input points. Kernel approximation, e.g., the Nyström method, has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present. This work analyzes the application of this paradigm to kernel $k$-means clustering, and shows that applying the linear $k$-means clustering algorithm to $\frac{k}ε (1 + o(1))$ features constructed using a so-called rank-restricted Nyström approximation results in cluster assignments that satisfy a $1 + ε$ approximation ratio in terms of the kernel $k$-means cost function, relative to the guarantee provided by the same algorithm without the use of the Nyström method. As part of the analysis, this work establishes a novel $1 + ε$ relative-error trace norm guarantee for low-rank approximation using the rank-restricted Nyström approximation. Empirical evaluations on the $8.1$ million instance MNIST8M dataset demonstrate the scalability and usefulness of kernel $k$-means clustering with Nyström approximation. This work argues that spectral clustering using Nyström approximation---a popular and computationally efficient, but theoretically unsound approach to non-linear clustering---should be replaced with the efficient and theoretically sound combination of kernel $k$-means clustering with Nyström approximation. The superior performance of the latter approach is empirically verified.},
 author = {Shusen Wang and Alex Gittens and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W2625793078},
 pages = {1--49},
 title = {Scalable Kernel K-Means Clustering with Nystrom Approximation: Relative-Error Bounds},
 url = {http://jmlr.org/papers/v20/17-517.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-525,
 author = {Kean Ming Tan and Junwei Lu and Tong Zhang and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W2974029743},
 pages = {1--38},
 title = {Layer-Wise Learning Strategy for Nonparametric Tensor Product Smoothing Spline Regression and Graphical Models},
 url = {http://jmlr.org/papers/v20/17-525.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-526,
 abstract = {We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.},
 author = {Qianxiao Li and Cheng Tai and Weinan E},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W2962798807},
 pages = {1--47},
 title = {Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations},
 url = {http://jmlr.org/papers/v20/17-526.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-534,
 abstract = {This paper studies Dictionary Learning problems wherein the learning task is distributed over a multi-agent network, modeled as a time-varying directed graph. This formulation is relevant, for instance, in Big Data scenarios where massive amounts of data are collected/stored in different locations (e.g., sensors, clouds) and aggregating and/or processing all data in a fusion center might be inefficient or unfeasible, due to resource limitations, communication overheads or privacy issues. We develop a unified decentralized algorithmic framework for this class of nonconvex problems, which is proved to converge to stationary solutions at a sublinear rate. The new method hinges on Successive Convex Approximation techniques, coupled with a decentralized tracking mechanism aiming at locally estimating the gradient of the smooth part of the sum-utility. To the best of our knowledge, this is the first provably convergent decentralized algorithm for Dictionary Learning and, more generally, bi-convex problems over (time-varying) (di)graphs.},
 author = {Amir Daneshmand and Ying Sun and Gesualdo Scutari and Francisco Facchinei and Brian M. Sadler},
 journal = {Journal of Machine Learning Research},
 number = {139},
 openalex = {W2988479891},
 pages = {1--62},
 title = {Decentralized Dictionary Learning Over Time-Varying Digraphs},
 url = {http://jmlr.org/papers/v20/17-534.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-535,
 abstract = {Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.},
 author = {Ofer Meshi and Ben London and Adrian Weller and David Sontag},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2962805661},
 pages = {1--34},
 title = {Train and test tightness of LP relaxations in structured prediction},
 url = {http://jmlr.org/papers/v20/17-535.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-545,
 abstract = {We consider the problem of noisy matrix completion, in which the goal is to reconstruct a structured matrix whose entries are partially observed in noise. Standard approaches to this underdetermined inverse problem are based on assuming that the underlying matrix has low rank, or is well-approximated by a low rank matrix. In this paper, we first identify how the classical non-negative rank model enforces restrictions that may be undesirable in practice. We propose a richer model based on what we term the “permutation-rank” of a matrix and show how the restrictions due to classical low rank assumptions can be avoided by using the richer permutation-rank model. We establish information-theoretic lower bounds on the rates of estimation, and design an estimator which we prove is simultaneously optimal (up to logarithmic factors) for both the permutation-rank and the low-rank models. Our results thus show that the proposed permutation-rank model and estimator enjoy a surprising win-win in terms of the statistical bias-variance tradeoff as compared to the classical low-rank models. An extended version of this paper is available on arXiv [1].},
 author = {Nihar B. Shah and Sivaraman Balakrishnan and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2963138620},
 pages = {1--43},
 title = {Low Permutation-Rank Matrices: Structural Properties and Noisy Completion},
 url = {http://jmlr.org/papers/v20/17-545.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-547,
 abstract = {The restless bandit problem is one of the most well-studied generalizations of the celebrated stochastic multi-armed bandit (MAB) problem in decision theory. In its ultimate generality, the restless bandit problem is known to be PSPACE-Hard to approximate to any nontrivial factor, and little progress has been made on this problem despite its significance in modeling activity allocation under uncertainty. In this article, we consider the Feedback MAB problem, where the reward obtained by playing each of n independent arms varies according to an underlying on/off Markov process whose exact state is only revealed when the arm is played. The goal is to design a policy for playing the arms in order to maximize the infinite horizon time average expected reward. This problem is also an instance of a Partially Observable Markov Decision Process (POMDP), and is widely studied in wireless scheduling and unmanned aerial vehicle (UAV) routing. Unlike the stochastic MAB problem, the Feedback MAB problem does not admit to greedy index-based optimal policies. We develop a novel duality-based algorithmic technique that yields a surprisingly simple and intuitive (2+ϵ)-approximate greedy policy to this problem. We show that both in terms of approximation factor and computational efficiency, our policy is closely related to the Whittle index , which is widely used for its simplicity and efficiency of computation. Subsequently we define a multi-state generalization, that we term Monotone bandits, which remains subclass of the restless bandit problem. We show that our policy remains a 2-approximation in this setting, and further, our technique is robust enough to incorporate various side-constraints such as blocking plays, switching costs, and even models where determining the state of an arm is a separate operation from playing it. Our technique is also of independent interest for other restless bandit problems, and we provide an example in nonpreemptive machine replenishment. Interestingly, in this case, our policy provides a constant factor guarantee, whereas the Whittle index is provably polynomially worse. By presenting the first O(1) approximations for nontrivial instances of restless bandits as well as of POMDPs, our work initiates the study of approximation algorithms in both these contexts.},
 author = {Steffen Gr{{\"u}}new{{\"a}}lder and Azadeh Khaleghi},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2040774021},
 pages = {1--37},
 title = {Approximation algorithms for restless bandit problems},
 url = {http://jmlr.org/papers/v20/17-547.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-576,
 abstract = {Many machine learning problems can be characterized by mutual contamination models. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions and the goal is to infer these base distributions. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine three popular machine learning problems that arise in this general setting: multiclass classification with label noise, demixing of mixed membership models, and classification with partial labels. In each case, we give sufficient conditions for identifiability and present algorithms for the infinite and finite sample settings, with associated performance guarantees.},
 author = {Julian Katz-Samuels and Gilles Blanchard and Clayton Scott},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2762264318},
 pages = {1--57},
 title = {Decontamination of Mutual Contamination Models},
 url = {http://jmlr.org/papers/v20/17-576.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-594,
 author = {Jialei Wang and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2935410731},
 pages = {1--56},
 title = {Utilizing Second Order Information in Minibatch Stochastic Variance Reduced Proximal Iterations},
 url = {http://jmlr.org/papers/v20/17-594.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-601,
 abstract = {We consider the task of estimating a high-dimensional directed acyclic graph, given observations from a linear structural equation model with arbitrary noise distribution. By exploiting properties of common random graphs, we develop a new algorithm that requires conditioning only on small sets of variables. The proposed algorithm, which is essentially a modified version of the PC-Algorithm, offers significant gains in both computational complexity and estimation accuracy. In particular, it results in more efficient and accurate estimation in large networks containing hub nodes, which are common in biological systems. We prove the consistency of the proposed algorithm, and show that it also requires a less stringent faithfulness assumption than the PC-Algorithm. Simulations in low and high-dimensional settings are used to illustrate these findings. An application to gene expression data suggests that the proposed algorithm can identify a greater number of clinically relevant genes than current methods.},
 author = {Arjun Sondhi and Ali Shojaie},
 journal = {Journal of Machine Learning Research},
 number = {164},
 openalex = {W2997642512},
 pages = {1--31},
 title = {The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks},
 url = {http://jmlr.org/papers/v20/17-601.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-608,
 abstract = {Machine learning with big data often involves large optimization models. For distributed optimization over a cluster of machines, frequent communication and synchronization of all model parameters (optimization variables) can be very costly. A promising solution is to use parameter servers to store different subsets of the model parameters, and update them asynchronously at different machines using local datasets. In this paper, we focus on distributed optimization of large linear models with convex loss functions, and propose a family of randomized primal-dual block coordinate algorithms that are especially suitable for asynchronous distributed implementation with parameter servers. In particular, we work with the saddle-point formulation of such problems which allows simultaneous data and model partitioning, and exploit its structure by doubly stochastic coordinate optimization with variance reduction (DSCOVR). Compared with other first-order distributed algorithms, we show that DSCOVR may require less amount of overall computation and communication, and less or no synchronization. We discuss the implementation details of the DSCOVR algorithms, and present numerical experiments on an industrial distributed computing system.},
 author = {Lin Xiao and Adams Wei Yu and Qihang Lin and Weizhu Chen},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2964089856},
 pages = {1--58},
 title = {DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization},
 url = {http://jmlr.org/papers/v20/17-608.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-611,
 author = {Maria-Florina Balcan and Yingyu Liang and Zhao Song and David P. Woodruff and Hongyang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W2958980107},
 pages = {1--56},
 title = {Non-Convex Matrix Completion and Related Problems via Strong Duality},
 url = {http://jmlr.org/papers/v20/17-611.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-612,
 abstract = {We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \log(W))$, and provide examples with VC-dimension $\Omega( W L \log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\Theta(W U)$ on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. 
Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial.},
 author = {Peter L. Bartlett and Nick Harvey and Christopher Liaw and Abbas Mehrabian},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2800415562},
 pages = {1--17},
 title = {Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks},
 url = {http://jmlr.org/papers/v20/17-612.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-613,
 abstract = {The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.},
 author = {Trevor Campbell and Tamara Broderick},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2964010828},
 pages = {1--38},
 title = {Automated Scalable Bayesian Inference via Hilbert Coresets},
 url = {http://jmlr.org/papers/v20/17-613.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-621,
 abstract = {In this paper we provide a finite-sample and an infinite-sample representer theorem for the concatenation of (linear combinations of) kernel functions of reproducing kernel Hilbert spaces. These results serve as mathematical foundation for the analysis of machine learning algorithms based on compositions of functions. As a direct consequence in the finite-sample case, the corresponding infinite-dimensional minimization problems can be recast into (nonlinear) finite-dimensional minimization problems, which can be tackled with nonlinear optimization algorithms. Moreover, we show how concatenated machine learning problems can be reformulated as neural networks and how our representer theorem applies to a broad class of state-of-the-art deep learning methods.},
 author = {Bastian Bohn and Michael Griebel and Christian Rieger},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2963539275},
 pages = {1--32},
 title = {A Representer Theorem for Deep Kernel Learning},
 url = {http://jmlr.org/papers/v20/17-621.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-629,
 abstract = {Recommender systems predict users’ preferences over a large number of items by pooling similar information from other users and/or items in the presence of sparse observations. One major challenge is how to utilize user-item speciﬁc covariates and networks describing user-item interactions in a high-dimensional situation, for accurate personalized prediction. In this article, we propose a smooth neighborhood recommender in the framework of the latent factor models. A similarity kernel is utilized to borrow neighborhood information from continuous covariates over a user-item speciﬁc network, such as a user’s social network, where the grouping information deﬁned by discrete covariates is also integrated through the network. Consequently, user-item speciﬁc information is built into the recommender to battle the ‘cold-start” issue in the absence of observations in collaborative and contentbased ﬁltering. Moreover, we utilize a “divide-and-conquer” version of the alternating least squares algorithm to achieve scalable computation, and establish asymptotic results for the proposed method, demonstrating that it achieves superior prediction accuracy. Finally, we illustrate that the proposed method improves substantially over its competitors in simulated examples and real benchmark data–Last.fm music data.},
 author = {Ben Dai and Junhui Wang and Xiaotong Shen and Annie Qu},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2913464709},
 pages = {1--24},
 title = {Smooth neighborhood recommender systems},
 url = {http://jmlr.org/papers/v20/17-629.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-631,
 abstract = {We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, whered is a delay parameter. We introduce EXP3-COOP, a cooperative version of the EXP3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order q d + 1 + K d (T lnK), where d is the independence number of the d-th power of the communication graphG. We then show that for any connected graph, ford = p K the regret bound isK 1=4 p T , strictly better than the minimax regret p KT for noncooperating agents. More informed choices ofd lead to bounds which are arbitrarily close to the full information minimax regret p T lnK when G is dense. When G has sparse components, we show that a variant of EXP3-COOP, allowing agents to choose their parameters according to their centrality inG, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.},
 author = {Nicol{{\`o}} Cesa-Bianchi and Claudio Gentile and Yishay Mansour},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2280442445},
 pages = {1--38},
 title = {Delay and Cooperation in Nonstochastic Bandits},
 url = {http://jmlr.org/papers/v20/17-631.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-633,
 abstract = {The goal of regression and classification methods in supervised learning is to minimize the empirical risk, that is, the expectation of some loss function quantifying the prediction error under the empirical distribution. When facing scarce training data, overfitting is typically mitigated by adding regularization terms to the objective that penalize hypothesis complexity. In this paper we introduce new regularization techniques using ideas from distributionally robust optimization, and we give new probabilistic interpretations to existing techniques. Specifically, we propose to minimize the worst-case expected loss, where the worst case is taken over the ball of all (continuous or discrete) distributions that have a bounded transportation distance from the (discrete) empirical distribution. By choosing the radius of this ball judiciously, we can guarantee that the worst-case expected loss provides an upper confidence bound on the loss on test data, thus offering new generalization bounds. We prove that the resulting regularized learning problems are tractable and can be tractably kernelized for many popular loss functions. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.},
 author = {Soroosh Shafieezadeh-Abadeh and Daniel Kuhn and Peyman Mohajerin Esfahani},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W2964110122},
 pages = {1--68},
 title = {Regularization via Mass Transportation},
 url = {http://jmlr.org/papers/v20/17-633.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-641,
 abstract = {We develop a set of scalable Bayesian inference procedures for a general class of nonparametric regression models. Specifically, nonparametric Bayesian inferences are separately performed on each subset randomly split from a massive dataset, and then the obtained local results are aggregated into global counterparts. This aggregation step is explicit without involving any additional computation cost. By a careful partition, we show that our aggregated inference results obtain an oracle rule in the sense that they are equivalent to those obtained directly from the entire data (which are computationally prohibitive). For example, an aggregated credible ball achieves desirable credibility level and also frequentist coverage while possessing the same radius as the oracle ball.},
 author = {Zuofeng Shang and Botao Hao and Guang Cheng},
 journal = {Journal of Machine Learning Research},
 number = {140},
 openalex = {W2982350142},
 pages = {1--81},
 title = {Nonparametric Bayesian Aggregation for Massive Data},
 url = {http://jmlr.org/papers/v20/17-641.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-663,
 abstract = {This paper develops detailed mathematical statistical theory of a new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. The new class of cross-validation combines principles of local information and recent advances in indirect cross-validation. A few applications of cross-validating multiplicative kernel hazard estimation do exist in the literature. However, detailed mathematical statistical theory and small sample performance are introduced via this paper and further upgraded to our new class of best one-sided cross-validation. Best one-sided cross-validation turns out to have excellent performance in its practical illustrations, in its small sample performance and in its mathematical statistical theoretical performance.},
 author = {Maria Luz G{{\'a}}miz and Mar{{\'i}}a Dolores  Mart{{\'i}}nez-Miranda and Jens Perch Nielsen},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2766458921},
 pages = {1--29},
 title = {Multiplicative local linear hazard estimation and best one-sided cross-validation},
 url = {http://jmlr.org/papers/v20/17-663.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-666,
 abstract = {We investigate and compare the fundamental performance of several distributed learning methods that have been proposed recently. We do this in the context of a distributed version of the classical signal-in-Gaussian-white-noise model, which serves as a benchmark model for studying performance in this setting. The results show how the design and tuning of a distributed method can have great impact on convergence rates and validity of uncertainty quantification. Moreover, we highlight the difficulty of designing nonparametric distributed procedures that automatically adapt to smoothness.},
 author = {Botond Szab{{\'o}} and Harry van Zanten},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2963013807},
 pages = {1--30},
 title = {An asymptotic analysis of distributed nonparametric methods},
 url = {http://jmlr.org/papers/v20/17-666.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-669,
 abstract = {We derive an objective function that can be optimized to give an estimator of the Vapnik- Chervonenkis dimension for model selection in regression problems. We verify our estimator is consistent. Then, we verify it performs well compared to seven other model selection techniques. We do this for a variety of types of data sets.},
 author = {Merlin Mpoudeu and Bertrand Clarke},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2886503715},
 pages = {1--26},
 title = {Model Selection via the VC-Dimension},
 url = {http://jmlr.org/papers/v20/17-669.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-672,
 abstract = {We study kernel least-squares estimation under a norm constraint. This form of regularisation is known as Ivanov regularisation and it provides better control of the norm of the estimator than the well-established Tikhonov regularisation. Ivanov regularisation can be studied under minimal assumptions. In particular, we assume only that the RKHS is separable with a bounded and measurable kernel. We provide rates of convergence for the expected squared L2 error of our estimator under the weak assumption that the variance of the response variables is bounded and the unknown regression function lies in an interpolation space between L2 and the RKHS. We then obtain faster rates of convergence when the regression function is bounded by clipping the estimator. In fact, we attain the optimal rate of convergence. Furthermore, we provide a high-probability bound under the stronger assumption that the response variables have subgaussian errors and that the regression function lies in an interpolation space between L∞ and the RKHS. Finally, we derive adaptive results for the settings in which the regression function is bounded.},
 author = {Stephen Page and Steffen Gr{{\"u}}new{{\"a}}lder},
 journal = {Journal of Machine Learning Research},
 number = {120},
 openalex = {W2974909662},
 pages = {1--49},
 title = {Ivanov-Regularised Least-Squares Estimators over Large RKHSs and Their Interpolation Spaces},
 url = {http://jmlr.org/papers/v20/17-672.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-681,
 abstract = {We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label's cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets.},
 author = {Akshay Krishnamurthy and Alekh Agarwal and Tzu-Kuo Huang and Hal Daum{{\'e}} III and John Langford},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2604204103},
 pages = {1--50},
 title = {Active Learning for Cost-Sensitive Classification},
 url = {http://jmlr.org/papers/v20/17-681.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-687,
 abstract = {Proximal distance algorithms combine the classical penalty method of constrained minimization with distance majorization. If f(x) is the loss function, and C is the constraint set in a constrained minimization problem, then the proximal distance principle mandates minimizing the penalized loss f(x)+ρ2dist(x,C)2 and following the solution xρ to its limit as ρ tends to ∞. At each iteration the squared Euclidean distance dist(x,C)2 is majorized by the spherical quadratic ‖x- PC (xk )‖2, where PC (xk ) denotes the projection of the current iterate xk onto C. The minimum of the surrogate function f(x)+ρ2‖x-PC(xk)‖2 is given by the proximal map prox ρ -1f [PC (xk )]. The next iterate xk+1 automatically decreases the original penalized loss for fixed ρ. Since many explicit projections and proximal maps are known, it is straightforward to derive and implement novel optimization algorithms in this setting. These algorithms can take hundreds if not thousands of iterations to converge, but the simple nature of each iteration makes proximal distance algorithms competitive with traditional algorithms. For convex problems, proximal distance algorithms reduce to proximal gradient algorithms and therefore enjoy well understood convergence properties. For nonconvex problems, one can attack convergence by invoking Zangwill's theorem. Our numerical examples demonstrate the utility of proximal distance algorithms in various high-dimensional settings, including a) linear programming, b) constrained least squares, c) projection to the closest kinship matrix, d) projection onto a second-order cone constraint, e) calculation of Horn's copositive matrix index, f) linear complementarity programming, and g) sparse principal components analysis. The proximal distance algorithm in each case is competitive or superior in speed to traditional methods such as the interior point method and the alternating direction method of multipliers (ADMM). Source code for the numerical examples can be found at https://github.com/klkeys/proxdist.},
 author = {Kevin L. Keys and Hua Zhou and Kenneth Lange},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2963842279},
 pages = {1--38},
 title = {Proximal Distance Algorithms: Theory and Practice.},
 url = {http://jmlr.org/papers/v20/17-687.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-722,
 abstract = {We describe a new library named picasso, which implements a unified framework of pathwise coordinate optimization for a variety of sparse learning problems (e.g., sparse linear regression, sparse logistic regression, sparse Poisson regression and scaled sparse linear regression) combined with efficient active set selection strategies. Besides, the library allows users to choose different sparsity-inducing regularizers, including the convex $\ell_1$, nonconvex MCP and SCAD regularizers. The library is coded in C++ and has user-friendly R and Python wrappers. Numerical experiments demonstrate that picasso can scale up to large problems efficiently.},
 author = {Jason Ge and Xingguo Li and Haoming Jiang and Han Liu and Tong Zhang and Mengdi Wang and Tuo Zhao},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2935670023},
 pages = {1--5},
 title = {Picasso: A sparse learning library for high dimensional data analysis in R and python},
 url = {http://jmlr.org/papers/v20/17-722.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-723,
 abstract = {Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and ultra-high dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the inactive features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in the computational cost without sacrificing the accuracy. Moreover, we show that our method can be extended to multi-class sparse support vector machines. To the best of our knowledge, the proposed method is the \emph{first} \emph{static} feature and sample reduction method for sparse SVMs and multi-class sparse SVMs. Experiments on both synthetic and real data sets demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.},
 author = {Bin Hong and Weizhong Zhang and Wei Liu and Jieping Ye and Deng Cai and Xiaofei He and Jie Wang},
 journal = {Journal of Machine Learning Research},
 number = {121},
 openalex = {W2476385526},
 pages = {1--39},
 title = {Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction},
 url = {http://jmlr.org/papers/v20/17-723.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-728,
 abstract = {Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning, and other machine learning applications. The central goal is to learn an overcomplete dictionary that can sparsely represent a given input dataset. However, a key challenge is that storage, transmission, and processing of the learned dictionary can be untenably high if the data dimension is high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b) where the dictionary itself is the product of a fixed, known basis and a data-adaptive sparse component. First, we introduce a simple algorithm for double-sparse coding that can be amenable to efficient implementation via neural architectures. Second, we theoretically analyze its performance and demonstrate asymptotic sample complexity and running time benefits over existing (provable) approaches for sparse coding. To our knowledge, our work introduces the first computationally efficient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally, we support our analysis via several numerical experiments on simulated data, confirming that our method can indeed be useful in problem sizes encountered in practical applications.},
 author = {Thanh V. Nguyen and Raymond K. W. Wong and Chinmay Hegde},
 journal = {Journal of Machine Learning Research},
 number = {141},
 openalex = {W2982640936},
 pages = {1--43},
 title = {Provably Accurate Double-Sparse Coding},
 url = {http://jmlr.org/papers/v20/17-728.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-734,
 abstract = {The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classification program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In order to obtain our negative result, we make use of universal-algebraic concepts, and our result is phrased in terms of the varietal property of non-congruence modularity.},
 author = {Hubie Chen and Matthew Valeriote},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W1956860503},
 pages = {1--28},
 title = {Learnability of Solutions to Conjunctive Queries: The Full Dichotomy},
 url = {http://jmlr.org/papers/v20/17-734.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-743,
 author = {Enrique G. Rodrigo and Juan A. Aledo and Jos{{\'e}} A. G{{\'a}}mez},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2913859482},
 pages = {1--5},
 title = {spark-crowd: A Spark Package for Learning from Crowdsourced Big Data},
 url = {http://jmlr.org/papers/v20/17-743.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-750,
 abstract = {We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.},
 author = {John Duchi and Hongseok Namkoong},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2944407464},
 pages = {1--55},
 title = {Variance-based Regularization with Convex Objectives},
 url = {http://jmlr.org/papers/v20/17-750.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-757,
 abstract = {In many problem settings, parameter vectors are not merely sparse but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as region sparsity. Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging.},
 author = {Anqi Wu and Oluwasanmi Koyejo and Jonathan Pillow},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2963038403},
 pages = {1--43},
 title = {Dependent relevance determination for smooth and structured sparse regression},
 url = {http://jmlr.org/papers/v20/17-757.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-773,
 abstract = {The frequent directions (FD) technique is a deterministic approach for online sketching that has many applications in machine learning. The conventional FD is a heuristic procedure that often outputs rank deficient matrices. To overcome the rank deficiency problem, we propose a new sketching strategy called robust frequent directions (RFD) by introducing a regularization term. RFD can be derived from an optimization problem. It updates the sketch matrix and the regularization term adaptively and jointly. RFD reduces the approximation error of FD without increasing the computational cost. We also apply RFD to online learning and propose an effective hyperparameter-free online Newton algorithm. We derive a regret bound for our online Newton algorithm based on RFD, which guarantees the robustness of the algorithm. The experimental studies demonstrate that the proposed method outperforms state-of-the-art second order online learning algorithms.},
 author = {Luo Luo and Cheng Chen and Zhihua Zhang and Wu-Jun Li and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2784223480},
 pages = {1--41},
 title = {Robust Frequent Directions with Application in Online Learning},
 url = {http://jmlr.org/papers/v20/17-773.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-776,
 abstract = {This work studies low-rank approximation of a positive semidefinite matrix from partial entries via nonconvex optimization. We characterized how well local-minimum based low-rank factorization approximates a fixed positive semidefinite matrix without any assumptions on the rank-matching, the condition number or eigenspace incoherence parameter. Furthermore, under certain assumptions on rank-matching and well-boundedness of condition numbers and eigenspace incoherence parameters, a corollary of our main theorem improves the state-of-the-art sampling rate results for nonconvex matrix completion with no spurious local minima in Ge et al. [2016, 2017]. In addition, we investigated when the proposed nonconvex optimization results in accurate low-rank approximations even in presence of large condition numbers, large incoherence parameters, or rank mismatching. We also propose to apply the nonconvex optimization to memory-efficient Kernel PCA. Compared to the well-known Nystrom methods, numerical experiments indicate that the proposed nonconvex optimization approach yields more stable results in both low-rank approximation and clustering.},
 author = {Ji Chen and Xiaodong Li},
 journal = {Journal of Machine Learning Research},
 number = {142},
 openalex = {W2988501358},
 pages = {1--39},
 title = {Model-free Nonconvex Matrix Completion: Local Minima Analysis and Applications in Memory-efficient Kernel PCA},
 url = {http://jmlr.org/papers/v20/17-776.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-022,
 abstract = {We study robust PCA for the fully observed setting, which is about separating a low rank matrix L and a sparse matrix S from their sum D = L + S. In this paper, a new algorithm, dubbed accelerated ...},
 author = {HanQin Cai and Jian-Feng Cai and Ke Wei},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W3114287677},
 pages = {1--33},
 title = {Accelerated alternating projections for robust principal component analysis},
 url = {http://jmlr.org/papers/v20/18-022.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-027,
 abstract = {Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.},
 author = {Ashish Khetan and Sewoong Oh},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2600006013},
 pages = {1--55},
 title = {Spectrum Estimation from a Few Entries},
 url = {http://jmlr.org/papers/v20/18-027.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-030,
 abstract = {Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior informatio...},
 author = {Yanning Shen and Tianyi Chen and Georgios B. Giannakis},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2963976431},
 pages = {1--36},
 title = {Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics},
 url = {http://jmlr.org/papers/v20/18-030.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-035,
 author = {Salvatore Ruggieri},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2958816001},
 pages = {1--34},
 title = {Complete Search for Feature Selection in Decision Trees},
 url = {http://jmlr.org/papers/v20/18-035.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-037,
 abstract = {Statistical relational learning is primarily concerned with learning and inferring relationships between entities in large-scale knowledge graphs. Nickel et al. (2011) proposed a RESCAL tensor factorization model for statistical relational learning, which achieves better or at least comparable results on common benchmark data sets when compared to other state-of-the-art methods. Given a positive integer s, RESCAL computes an s-dimensional latent vector for each entity. The latent factors can be further used for solving relational learning tasks, such as collective classification, collective entity resolution and link-based clustering. The focus of this paper is to determine the number of latent factors in the RESCAL model. Due to the structure of the RESCAL model, its log-likelihood function is not concave. As a result, the corresponding maximum likelihood estimators (MLEs) may not be consistent. Nonetheless, we design a specific pseudometric, prove the consistency of the MLEs under this pseudometric and establish its rate of convergence. Based on these results, we propose a general class of information criteria and prove their model selection consistencies when the number of relations is either bounded or diverges at a proper rate of the number of entities. Simulations and real data examples show that our proposed information criteria have good finite sample properties.},
 author = {Chengchun Shi and Wenbin Lu and Rui Song},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2913187981},
 pages = {1--38},
 title = {Determining the Number of Latent Factors in Statistical Multi-Relational Learning.},
 url = {http://jmlr.org/papers/v20/18-037.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-040,
 abstract = {CITATION: Kamper, F., Steel, S. J. & Du Preez, J. A. 2019. On the convergence of Gaussian belief propagation with nodes of arbitrary size. 
Journal of Machine Learning Research, 20(165):1-37.},
 author = {Francois Kamper and Sarel J. Steel and Johan A. du Preez},
 journal = {Journal of Machine Learning Research},
 number = {165},
 openalex = {W2998111886},
 pages = {1--37},
 title = {On the Convergence of Gaussian Belief Propagation with Nodes of Arbitrary Size},
 url = {http://jmlr.org/papers/v20/18-040.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-048,
 abstract = {Given a vertex of interest in a network $G_1$, the vertex nomination problem seeks to find the corresponding vertex of interest (if it exists) in a second network $G_2$. A vertex nomination scheme produces a list of the vertices in $G_2$, ranked according to how likely they are judged to be the corresponding vertex of interest in $G_2$. The vertex nomination problem and related information retrieval tasks have attracted much attention in the machine learning literature, with numerous applications to social and biological networks. However, the current framework has often been confined to a comparatively small class of network models, and the concept of statistically consistent vertex nomination schemes has been only shallowly explored. In this paper, we extend the vertex nomination problem to a very general statistical model of graphs. Further, drawing inspiration from the long-established classification framework in the pattern recognition literature, we provide definitions for the key notions of Bayes optimality and consistency in our extended vertex nomination framework, including a derivation of the Bayes optimal vertex nomination scheme. In addition, we prove that no universally consistent vertex nomination schemes exist. Illustrative examples are provided throughout.},
 author = {Vince Lyzinski and Keith Levin and Carey E. Priebe},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2963630149},
 pages = {1--39},
 title = {On Consistent Vertex Nomination Schemes},
 url = {http://jmlr.org/papers/v20/18-048.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-063,
 author = {Shao-Bo Lin and Yunwen Lei and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W2932684714},
 pages = {1--36},
 title = {Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping},
 url = {http://jmlr.org/papers/v20/18-063.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-075,
 abstract = {We propose an efficient algorithm for approximate computation of the profile maximum likelihood (PML), a variant of maximum likelihood maximizing the probability of observing a sufficient statistic rather than the empirical sample. The PML has appealing theoretical properties, but is difficult to compute exactly. Inspired by observations gleaned from exactly solvable cases, we look for an approximate PML solution, which, intuitively, clumps comparably frequent symbols into one symbol. This amounts to lower-bounding a certain matrix permanent by summing over a subgroup of the symmetric group rather than the whole group during the computation. We extensively experiment with the approximate solution, and find the empirical performance of our approach is competitive and sometimes significantly better than state-of-the-art performance for various estimation problems.},
 author = {Dmitri S. Pavlichin and Jiantao Jiao and Tsachy Weissman},
 journal = {Journal of Machine Learning Research},
 number = {122},
 openalex = {W2976691103},
 pages = {1--55},
 title = {Approximate Profile Maximum Likelihood},
 url = {http://jmlr.org/papers/v20/18-075.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-079,
 abstract = {We propose a simple subsampling scheme for fast randomized approximate computation of optimal transport distances. This scheme operates on a random subset of the full data and can use any exact algorithm as a black-box back-end, including state-of-the-art solvers and entropically penalized versions. It is based on averaging the exact distances between empirical measures generated from independent samples from the original measures and can easily be tuned towards higher accuracy or shorter computation times. To this end, we give non-asymptotic deviation bounds for its accuracy in the case of discrete optimal transport problems. In particular, we show that in many important instances, including images (2D-histograms), the approximation error is independent of the size of the full problem. We present numerical experiments that demonstrate that a very good approximation in typical applications can be obtained in a computation time that is several orders of magnitude smaller than what is required for exact computation of the full problem.},
 author = {Max Sommerfeld and J{{\"o}}rn Schrieber and Yoav Zemel and Axel Munk},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W2963226285},
 pages = {1--23},
 title = {Optimal Transport: Fast Probabilistic Approximation with Exact Solvers},
 url = {http://jmlr.org/papers/v20/18-079.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-084,
 abstract = {Subspace segmentation or subspace learning is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. Existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via self-expressiveness of samples. A closed form of representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is then addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, taking into account of solving both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data, and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method on retrieving minimal sample subspaces that are heavily intersected.},
 author = {Zhenyue Zhang and Yuqing Xia},
 journal = {Journal of Machine Learning Research},
 number = {143},
 openalex = {W2956802405},
 pages = {1--57},
 title = {Minimal Sample Subspace Learning: Theory and Algorithms},
 url = {http://jmlr.org/papers/v20/18-084.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-094,
 author = {Mehmet Eren Ahsen and Robert M Vogel and Gustavo A Stolovitzky},
 journal = {Journal of Machine Learning Research},
 number = {166},
 openalex = {W2998693186},
 pages = {1--40},
 title = {Unsupervised Evaluation and Weighted Aggregation of Ranked Classification Predictions},
 url = {http://jmlr.org/papers/v20/18-094.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-095,
 abstract = {We study the sample complexity of canonical correlation analysis (CCA), \ie, the number of samples needed to estimate the population canonical correlation and directions up to arbitrarily small error. With mild assumptions on the data distribution, we show that in order to achieve $\epsilon$-suboptimality in a properly defined measure of alignment between the estimated canonical directions and the population solution, we can solve the empirical objective exactly with $N(\epsilon, \Delta, \gamma)$ samples, where $\Delta$ is the singular value gap of the whitened cross-covariance matrix and $1/\gamma$ is an upper bound of the condition number of auto-covariance matrices. Moreover, we can achieve the same learning accuracy by drawing the same level of samples and solving the empirical objective approximately with a stochastic optimization algorithm; this algorithm is based on the shift-and-invert power iterations and only needs to process the dataset for $\mathcal{O}\left(\log \frac{1}{\epsilon} \right)$ passes. Finally, we show that, given an estimate of the canonical correlation, the streaming version of the shift-and-invert power iterations achieves the same learning accuracy with the same level of sample complexity, by processing the data only once.},
 author = {Chao Gao and Dan Garber and Nathan Srebro and Jialei Wang and Weiran Wang},
 journal = {Journal of Machine Learning Research},
 number = {167},
 openalex = {W2998409652},
 pages = {1--46},
 title = {Stochastic Canonical Correlation Analysis},
 url = {http://jmlr.org/papers/v20/18-095.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-109,
 abstract = {An approximate method for conducting resampling in Lasso, the $\ell_1$ penalized linear regression, in a semi-analytic manner is developed, whereby the average over the resampled datasets is directly computed without repeated numerical sampling, thus enabling an inference free of the statistical fluctuations due to sampling finiteness, as well as a significant reduction of computational time. The proposed method is based on a message passing type algorithm, and its fast convergence is guaranteed by the state evolution analysis, when covariates are provided as zero-mean independently and identically distributed Gaussian random variables. It is employed to implement bootstrapped Lasso (Bolasso) and stability selection, both of which are variable selection methods using resampling in conjunction with Lasso, and resolves their disadvantage regarding computational cost. To examine approximation accuracy and efficiency, numerical experiments were carried out using simulated datasets. Moreover, an application to a real-world dataset, the wine quality dataset, is presented. To process such real-world datasets, an objective criterion for determining the relevance of selected variables is also introduced by the addition of noise variables and resampling.},
 author = {Tomoyuki Obuchi and Yoshiyuki Kabashima},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2962986545},
 pages = {1--33},
 title = {Semi-Analytic Resampling in Lasso},
 url = {http://jmlr.org/papers/v20/18-109.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-114,
 abstract = {Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.},
 author = {G{{\'a}}bor Braun and Sebastian Pokutta and Daniel Zink},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2963072698},
 pages = {1--42},
 title = {Lazifying Conditional Gradient Algorithms},
 url = {http://jmlr.org/papers/v20/18-114.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-133,
 author = {Bin Li and Yik-Chung Wu},
 journal = {Journal of Machine Learning Research},
 number = {144},
 openalex = {W2983204857},
 pages = {1--30},
 title = {Convergence of Gaussian Belief Propagation Under General Pairwise Factorization: Connecting Gaussian MRF with Pairwise Linear Gaussian Model},
 url = {http://jmlr.org/papers/v20/18-133.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-134,
 abstract = {Probabilistic linear discriminant analysis (PLDA) is a method used for biometric problems like speaker or face recognition that models the variability of the samples using two latent variables, one that depends on the class of the sample and another one that is assumed independent across samples and models the within-class variability. In this work, we propose a generalization of PLDA that enables joint modeling of two sample-dependent factors: the class of interest and a nuisance condition. The approach does not change the basic form of PLDA but rather modifies the training procedure to consider the dependency across samples of the latent variable that models within-class variability. While the identity of the nuisance condition is needed during training, it is not needed during testing since we propose a scoring procedure that marginalizes over the corresponding latent variable. We show results on a multilingual speaker-verification task, where the language spoken is considered a nuisance condition. We show that the proposed joint PLDA approach leads to significant performance gains in this task for two different datasets, in particular when the training data contains mostly or only monolingual speakers.},
 author = {Luciana Ferrer and Mitchell McLaren},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W4300485899},
 pages = {1--29},
 title = {Joint PLDA for Simultaneous Modeling of Two Factors},
 url = {http://jmlr.org/papers/v20/18-134.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-148,
 abstract = {Performance of distributed optimization and learning systems is bottlenecked by "straggler" nodes and slow communication links, which significantly delay computation. We propose a distributed optimization framework where the dataset is "encoded" to have an over-complete representation with built-in redundancy, and the straggling nodes in the system are dynamically left out of the computation at every iteration, whose loss is compensated by the embedded redundancy. We show that oblivious application of several popular optimization algorithms on encoded data, including gradient descent, L-BFGS, proximal gradient under data parallelism, and coordinate descent under model parallelism, converge to either approximate or exact solutions of the original problem when stragglers are treated as erasures. These convergence results are deterministic, i.e., they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes, and are independent of the tail behavior of the delay distribution. We demonstrate that equiangular tight frames have desirable properties as encoding matrices, and propose efficient mechanisms for encoding large-scale data. We implement the proposed technique on Amazon EC2 clusters, and demonstrate its performance over several learning problems, including matrix factorization, LASSO, ridge regression and logistic regression, and compare the proposed method with uncoded, asynchronous, and data replication strategies.},
 author = {Can Karakus and Yifan Sun and Suhas Diggavi and Wotao Yin},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2792603880},
 pages = {1--47},
 title = {Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning},
 url = {http://jmlr.org/papers/v20/18-148.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-153,
 abstract = {Bayesian Non-negative Matrix Factorization (NMF) is a promising approach for understanding uncertainty and structure in matrix data. However, a large volume of applied work optimizes traditional non-Bayesian NMF objectives that fail to provide a principled understanding of the non-identifiability inherent in NMF-- an issue ideally addressed by a Bayesian approach. Despite their suitability, current Bayesian NMF approaches have failed to gain popularity in an applied setting; they sacrifice flexibility in modeling for tractable computation, tend to get stuck in local modes, and require many thousands of samples for meaningful uncertainty estimates. We address these issues through a particle-based variational approach to Bayesian NMF that only requires the joint likelihood to be differentiable for tractability, uses a novel initialization technique to identify multiple modes in the posterior, and allows domain experts to inspect a `small' set of factorizations that faithfully represent the posterior. We introduce and employ a class of likelihood and prior distributions for NMF that formulate a Bayesian model using popular non-Bayesian NMF objectives. On several real datasets, we obtain better particle approximations to the Bayesian NMF posterior in less time than baselines and demonstrate the significant role that multimodality plays in NMF-related tasks.},
 author = {Muhammad A Masood and Finale Doshi-Velez},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W4297814395},
 pages = {1--56},
 title = {A particle-based variational approach to Bayesian Non-negative Matrix Factorization},
 url = {http://jmlr.org/papers/v20/18-153.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-167,
 abstract = {When faced with a data set too large to be processed all at once, an obvious solution is to retain only part of it. In practice this takes a wide variety of different forms, and among them "coresets" are especially appealing. A coreset is a (small) weighted sample of the original data that comes with the following guarantee: a cost function can be evaluated on the smaller set instead of the larger one, with low relative error. For some classes of problems, and via a careful choice of sampling distribution (based on the so-called "sensitivity" metric), iid random sampling has turned to be one of the most successful methods for building coresets efficiently. However, independent samples are sometimes overly redundant, and one could hope that enforcing diversity would lead to better performance. The difficulty lies in proving coreset properties in non-iid samples. We show that the coreset property holds for samples formed with determinantal point processes (DPP). DPPs are interesting because they are a rare example of repulsive point processes with tractable theoretical properties, enabling us to prove general coreset theorems. We apply our results to both the k-means and the linear regression problems, and give extensive empirical evidence that the small additional computational cost of DPP sampling comes with superior performance over its iid counterpart. Of independent interest, we also provide analytical formulas for the sensitivity in the linear regression and 1-means cases.},
 author = {Nicolas Tremblay and Simon Barthelm{{\'e}} and Pierre-Olivier Amblard},
 journal = {Journal of Machine Learning Research},
 number = {168},
 openalex = {W2997098139},
 pages = {1--70},
 title = {Determinantal Point Processes for Coresets},
 url = {http://jmlr.org/papers/v20/18-167.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-170,
 abstract = {We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.},
 author = {Zhixin Zhou and Arash A.Amini},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W2962947748},
 pages = {1--47},
 title = {Analysis of spectral clustering algorithms for community detection: the general bipartite setting},
 url = {http://jmlr.org/papers/v20/18-170.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-172,
 abstract = {The octagonal shrinkage and clustering algorithm for regression (OSCAR), equipped with the $\ell_1$-norm and a pair-wise $\ell_{\infty}$-norm regularizer, is a useful tool for feature selection and grouping in high-dimensional data analysis. The computational challenge posed by OSCAR, for high dimensional and/or large sample size data, has not yet been well resolved due to the non-smoothness and inseparability of the regularizer involved. In this paper, we successfully resolve this numerical challenge by proposing a sparse semismooth Newton-based augmented Lagrangian method to solve the more general SLOPE (the sorted L-one penalized estimation) model. By appropriately exploiting the inherent sparse and low-rank property of the generalized Jacobian of the semismooth Newton system in the augmented Lagrangian subproblem, we show how the computational complexity can be substantially reduced. Our algorithm presents a notable advantage in the high-dimensional statistical regression settings. Numerical experiments are conducted on real data sets, and the results demonstrate that our algorithm is far superior, in both speed and robustness, than the existing state-of-the-art algorithms based on first-order iterative schemes, including the widely used accelerated proximal gradient (APG) method and the alternating direction method of multipliers (ADMM).},
 author = {Ziyan Luo and Defeng Sun and Kim-Chuan Toh and Naihua Xiu},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W2963865258},
 pages = {1--25},
 title = {Solving the OSCAR and SLOPE Models Using a Semismooth Newton-Based Augmented Lagrangian Method},
 url = {http://jmlr.org/papers/v20/18-172.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-173,
 abstract = {In this paper, we provide new insights on the Unadjusted Langevin Algorithm. We show that this method can be formulated as a first order optimization algorithm of an objective functional defined on the Wasserstein space of order $2$. Using this interpretation and techniques borrowed from convex optimization, we give a non-asymptotic analysis of this method to sample from logconcave smooth target distribution on $\mathbb{R}^d$. Based on this interpretation, we propose two new methods for sampling from a non-smooth target distribution, which we analyze as well. Besides, these new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm. Similar to SGLD, they only rely on approximations of the gradient of the target log density and can be used for large-scale Bayesian inference.},
 author = {Alain Durmus and Szymon Majewski and B{{\l}}a{\.{z}}ej Miasojedow},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2962817219},
 pages = {1--46},
 title = {Analysis of Langevin Monte Carlo via Convex Optimization},
 url = {http://jmlr.org/papers/v20/18-173.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-190,
 abstract = {The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.},
 author = {Alberto Bietti and Julien Mairal},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2768793812},
 pages = {1--49},
 title = {Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations},
 url = {http://jmlr.org/papers/v20/18-190.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-191,
 abstract = {Individualized treatment rules aim to identify if, when, which, and to whom treatment should be applied. A globally aging population, rising healthcare costs, and increased access to patient-level data have created an urgent need for high-quality estimators of individualized treatment rules that can be applied to observational data. A recent and promising line of research for estimating individualized treatment rules recasts the problem of estimating an optimal treatment rule as a weighted classification problem. We consider a class of estimators for optimal treatment rules that are analogous to convex large-margin classifiers. The proposed class applies to observational data and is doubly-robust in the sense that correct specification of either a propensity or outcome model leads to consistent estimation of the optimal individualized treatment rule. Using techniques from semiparametric efficiency theory, we derive rates of convergence for the proposed estimators and use these rates to characterize the bias-variance trade-off for estimating individualized treatment rules with classification-based methods. Simulation experiments informed by these results demonstrate that it is possible to construct new estimators within the proposed framework that significantly outperform existing ones. We illustrate the proposed methods using data from a labor training program and a study of inflammatory bowel syndrome.},
 author = {Ying-Qi Zhao and Eric B. Laber and Yang Ning and Sumona Saha and Bruce E. Sands},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W2907213040},
 pages = {1--23},
 title = {Efficient augmentation and relaxation learning for individualized treatment rules using observational data},
 url = {http://jmlr.org/papers/v20/18-191.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-196,
 abstract = {Learning for control can acquire controllers for novel robotic tasks, paving the path for autonomous agents. Such controllers can be expert-designed policies, which typically require tuning of parameters for each task scenario. In this context, Bayesian optimization (BO) has emerged as a promising approach for automatically tuning controllers. However, when performing BO on hardware for high-dimensional policies, sample-efficiency can be an issue. Here, we develop an approach that utilizes simulation to map the original parameter space into a domain-informed space. During BO, similarity between controllers is now calculated in this transformed space. Experiments on the ATRIAS robot hardware and another bipedal robot simulation show that our approach succeeds at sample-efficiently learning controllers for multiple robots. Another question arises: What if the simulation significantly differs from hardware? To answer this, we create increasingly approximate simulators and study the effect of increasing simulation-hardware mismatch on the performance of Bayesian optimization. We also compare our approach to other approaches from literature, and find it to be more reliable, especially in cases of high mismatch. Our experiments show that our approach succeeds across different controller types, bipedal robot models and simulator fidelity levels, making it applicable to a wide range of bipedal locomotion problems.},
 author = {Akshara Rai and Rika Antonova and Franziska Meier and Christopher G. Atkeson},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2799517192},
 pages = {1--24},
 title = {Using Simulation to Improve Sample-Efficiency of Bayesian Optimization for Bipedal Robots},
 url = {http://jmlr.org/papers/v20/18-196.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-200,
 abstract = {Sparse reduced-rank regression is an important tool to uncover meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-$r$ singular value decomposition to find the optimal low-rank and sparse matrix by solving a sparse generalized eigenvalue problem. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.},
 author = {Zemin Zheng and M. Taha Bahadori and Yan Liu and Jinchi Lv},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W2963767514},
 pages = {1--34},
 title = {Scalable Interpretable Multi-Response Regression via SEED},
 url = {http://jmlr.org/papers/v20/18-200.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-213,
 abstract = {Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.},
 author = {Felix Berkenkamp and Angela P. Schoellig and Andreas Krause},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W4288639403},
 pages = {1--24},
 title = {No-Regret Bayesian Optimization with Unknown Hyperparameters},
 url = {http://jmlr.org/papers/v20/18-213.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-225,
 abstract = {Online field experiments are the gold-standard way of evaluating changes to real-world interactive machine learning systems. Yet our ability to explore complex, multi-dimensional policy spaces - such as those found in recommendation and ranking problems - is often constrained by the limited number of experiments that can be run simultaneously. To alleviate these constraints, we augment online experiments with an offline simulator and apply multi-task Bayesian optimization to tune live machine learning systems. We describe practical issues that arise in these types of applications, including biases that arise from using a simulator and assumptions for the multi-task kernel. We measure empirical learning curves which show substantial gains from including data from biased offline experiments, and show how these learning curves are consistent with theoretical results for multi-task Gaussian process generalization. We find that improved kernel inference is a significant driver of multi-task generalization. Finally, we show several examples of Bayesian optimization efficiently tuning a live machine learning system by combining offline and online experiments.},
 author = {Benjamin Letham and Eytan Bakshy},
 journal = {Journal of Machine Learning Research},
 number = {145},
 openalex = {W2982659598},
 pages = {1--30},
 title = {Bayesian Optimization for Policy Search via Online-Offline Experimentation},
 url = {http://jmlr.org/papers/v20/18-225.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-227,
 abstract = {There exist many problems in science and engineering that involve optimization of an unknown or partially unknown objective function. Recently, Bayesian Optimization (BO) has emerged as a powerful tool for solving optimization problems whose objective functions are only available as a black box and are expensive to evaluate. Many practical problems, however, involve optimization of an unknown objective function subject to unknown constraints. This is an important yet challenging problem for which, unlike optimizing an unknown function, existing methods face several limitations. In this paper, we present a novel constrained Bayesian optimization framework to optimize an unknown objective function subject to unknown constraints. We introduce an equivalent optimization by augmenting the objective function with constraints, introducing auxiliary variables for each constraint, and forcing the new variables to be equal to the main variable. Building on the Alternating Direction Method of Multipliers (ADMM) algorithm, we propose ADMM-Bayesian Optimization (ADMMBO) to solve the problem in an iterative fashion. Our framework leads to multiple unconstrained subproblems with unknown objective functions, which we then solve via BO. Our method resolves several challenges of state-of-the-art techniques: it can start from infeasible points, is insensitive to initialization, can efficiently handle 'decoupled problems' and has a concrete stopping criterion. Extensive experiments on a number of challenging BO benchmark problems show that our proposed approach outperforms the state-of-the-art methods in terms of the speed of obtaining a feasible solution and convergence to the global optimum as well as minimizing the number of total evaluations of unknown objective and constraints functions.},
 author = {Setareh Ariafar and Jaume Coll-Font and Dana Brooks and Jennifer Dy},
 journal = {Journal of Machine Learning Research},
 number = {123},
 openalex = {W2974398825},
 pages = {1--26},
 title = {ADMMBO: Bayesian Optimization with Unknown Constraints using ADMM.},
 url = {http://jmlr.org/papers/v20/18-227.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-232,
 abstract = {In this paper we develop a deep learning method for optimal stopping problems which directly learns the optimal stopping rule from Monte Carlo samples. As such, it is broadly applicable in situations where the underlying randomness can efficiently be simulated. We test the approach on three problems: the pricing of a Bermudan max-call option, the pricing of a callable multi barrier reverse convertible and the problem of optimally stopping a fractional Brownian motion. In all three cases it produces very accurate results in high-dimensional situations with short computing times.},
 author = {Sebastian Becker and Patrick Cheridito and Arnulf Jentzen},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2963615834},
 pages = {1--25},
 title = {Deep Optimal Stopping},
 url = {http://jmlr.org/papers/v20/18-232.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-241,
 author = {Gregor Pir{\v{s}} and Erik {\v{S}}trumbelj},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2934142340},
 pages = {1--18},
 title = {Bayesian Combination of Probabilistic Classifiers using Multivariate Normal Mixtures},
 url = {http://jmlr.org/papers/v20/18-241.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-262,
 author = {Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez-Rodriguez and Krishna P. Gummadi},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2943927551},
 pages = {1--42},
 title = {Fairness Constraints: A Flexible Approach for Fair Classification},
 url = {http://jmlr.org/papers/v20/18-262.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-263,
 abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of on-line stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler that repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the {\it Stochastic Point Location (SPL) Problem}. The gambler is here only told whether the optimal arm (point) lies to the left or to the right of the arm pulled, with the feedback being erroneous with probability $1-\pi$. This formulation thus captures optimization in continuous action spaces with both {\it informative} and {\it deceptive} feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning $\pi$, TS-SPL also supports {\it deceptive} environments that are lying about the direction of the optimal arm. This, in turn, allows us to solve the fundamental Stochastic Root Finding (SRF) Problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
 author = {Sondre Glimsdal and Ole-Christoffer Granmo},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2742723920},
 pages = {1--24},
 title = {Thompson Sampling Guided Stochastic Searching on the Line for Deceptive Environments with Applications to Root-Finding Problems},
 url = {http://jmlr.org/papers/v20/18-263.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-269,
 author = {Amos Beimel and Kobbi Nissim and Uri Stemmer},
 journal = {Journal of Machine Learning Research},
 number = {146},
 openalex = {W2982531080},
 pages = {1--33},
 title = {Characterizing the Sample Complexity of Pure Private Learners},
 url = {http://jmlr.org/papers/v20/18-269.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-277,
 abstract = {Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly},
 author = {Jean Kossaifi and Yannis Panagakis and Anima Anandkumar and Maja Pantic},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2544822491},
 pages = {1--6},
 title = {TensorLy: tensor learning in python},
 url = {http://jmlr.org/papers/v20/18-277.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-278,
 abstract = {A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. The score matching method of Hyvärinen (2005) avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over Rm . Hyvärinen (2007) extended the approach to distributions supported on the non-negative orthant, R+m . In this paper, we give a generalized form of score matching for non-negative data that improves estimation efficiency. As an example, we consider a general class of pairwise interaction models. Addressing an overlooked inexistence problem, we generalize the regularized score matching method of Lin et al. (2016) and improve its theoretical guarantees for non-negative Gaussian graphical models.},
 author = {Shiqing Yu and Mathias Drton and Ali Shojaie},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2908411392},
 pages = {1--70},
 title = {Generalized Score Matching for Non-Negative Data},
 url = {http://jmlr.org/papers/v20/18-278.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-281,
 abstract = {We introduce a new neural network model, together with a tractable and monotone online learning algorithm. Our model describes feed-forward networks for classification, with one output node for each class. The only nonlinear operation is rectification using a ReLU function with a bias. However, there is a rectifier on every edge rather than at the nodes of the network. There are also weights, but these are positive, static, and associated with the nodes. Our rectified wire networks are able to represent arbitrary Boolean functions. Only the bias parameters, on the edges of the network, are learned. Another departure in our approach, from standard neural networks, is that the loss function is replaced by a constraint. This constraint is simply that the value of the output node associated with the correct class should be zero. Our model has the property that the exact norm-minimizing parameter update, required to correctly classify a training item, is the solution to a quadratic program that can be computed with a few passes through the network. We demonstrate a training algorithm using this update, called sequential deactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a natural choice for the nodal weights, SDA has no hyperparameters other than those describing the network structure. Our experiments explore behavior with respect to network size and depth in a family of sparse expander networks.},
 author = {Veit Elser and Dan Schmidt and Jonathan Yedidia},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2809393407},
 pages = {1--42},
 title = {Monotone Learning with Rectified Wire Networks},
 url = {http://jmlr.org/papers/v20/18-281.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-298,
 abstract = {Modern data sets in various domains often include units that were sampled non-randomly from the population and have a latent correlation structure. Here we investigate a common form of this setting, where every unit is associated with a latent variable, all latent variables are correlated, and the probability of sampling a unit depends on its response. Such settings often arise in case-control studies, where the sampled units are correlated due to spatial proximity, family relations, or other sources of relatedness. Maximum likelihood estimation in such settings is challenging from both a computational and statistical perspective, necessitating approximations that take the sampling scheme into account. We propose a family of approximate likelihood approaches which combine composite likelihood and expectation propagation. We demonstrate the efficacy of our solutions via extensive simulations. We utilize them to investigate the genetic architecture of several complex disorders collected in case-control genetic association studies, where hundreds of thousands of genetic variants are measured for every individual, and the underlying disease liabilities of individuals are correlated due to genetic similarity. Our work is the first to provide a tractable likelihood-based solution for case-control data with complex dependency structures.},
 author = {Omer Weissbrod and Shachar Kaufman and David Golan and Saharon Rosset},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W2804119583},
 pages = {1--30},
 title = {Maximum Likelihood for Gaussian Process Classification and Generalized Linear Mixed Models under Case-Control Sampling},
 url = {http://jmlr.org/papers/v20/18-298.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-314,
 abstract = {Evaluating the joint significance of covariates is of fundamental importance in a wide range of applications. To this end, p-values are frequently employed and produced by algorithms that are powered by classical large-sample asymptotic theory. It is well known that the conventional p-values in Gaussian linear model are valid even when the dimensionality is a non-vanishing fraction of the sample size, but can break down when the design matrix becomes singular in higher dimensions or when the error distribution deviates from Gaussianity. A natural question is when the conventional p-values in generalized linear models become invalid in diverging dimensions. We establish that such a breakdown can occur early in nonlinear models. Our theoretical characterizations are confirmed by simulation studies.},
 author = {Yingying Fan and Emre Demirkaya and Jinchi Lv},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2963097173},
 pages = {1--33},
 title = {Nonuniformity of P-values Can Occur Early in Diverging Dimensions},
 url = {http://jmlr.org/papers/v20/18-314.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-321,
 abstract = {We show that prediction performance for global-local shrinkage regression can overcome two major difficulties of global shrinkage regression: (i) the amount of relative shrinkage is monotone in the...},
 author = {Anindya Bhadra and Jyotishka Datta and Yunfan Li and Nicholas G. Polson and Brandon Willard},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W2963108479},
 pages = {1--39},
 title = {Prediction Risk for the Horseshoe Regression},
 url = {http://jmlr.org/papers/v20/18-321.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-329,
 author = {De Wen Soh and Sekhar Tatikonda},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W2956741580},
 pages = {1--30},
 title = {Learning Unfaithful $K$-separable Gaussian Graphical Models},
 url = {http://jmlr.org/papers/v20/18-329.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-339,
 abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.},
 author = {Ian Osband and Benjamin Van Roy and Daniel J. Russo and Zheng Wen},
 journal = {Journal of Machine Learning Research},
 number = {124},
 openalex = {W2974778612},
 pages = {1--62},
 title = {Deep Exploration via Randomized Value Functions},
 url = {http://jmlr.org/papers/v20/18-339.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-349,
 abstract = {Ordinal regression, also named ordinal classification, studies classification problems where there exist a natural order between class labels. This structured order of the labels is crucial in all steps of the learning process in order to take full advantage of the data. ORCA (Ordinal Regression and Classification Algorithms) is a Matlab/Octave framework that implements and integrates different ordinal classification algorithms and specifically designed performance metrics. The framework simplifies the task of experimental comparison to a great extent, allowing the user to: (i) describe experiments by simple configuration files; (ii) automatically run different data partitions; (iii) parallelize the executions; (iv) generate a variety of performance reports and (v) include new algorithms by using its intuitive interface. Source code, binaries, documentation, descriptions and links to data sets and tutorials (including examples of educational purpose) are available at https://github.com/ayrna/orca.},
 author = {Javier S{{\'a}}nchez-Monedero and Pedro A. Guti{{\'e}}rrez and Mar{{\'i}}a P{{\'e}}rez-Ortiz},
 journal = {Journal of Machine Learning Research},
 number = {125},
 openalex = {W2974818429},
 pages = {1--5},
 title = {ORCA: A Matlab/Octave Toolbox for Ordinal Regression},
 url = {http://jmlr.org/papers/v20/18-349.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-358,
 author = {Christoph D. Hofer and Roland Kwitt and Marc Niethammer},
 journal = {Journal of Machine Learning Research},
 number = {126},
 openalex = {W2975769120},
 pages = {1--45},
 title = {Learning Representations of Persistence Barcodes},
 url = {http://jmlr.org/papers/v20/18-358.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-374,
 abstract = {Training Gaussian process-based models typically involves an $ O(N^3)$ computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of $K$ Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarrassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost.},
 author = {Michael Minyi Zhang and Sinead A. Williamson},
 journal = {Journal of Machine Learning Research},
 number = {169},
 openalex = {W2998223454},
 pages = {1--26},
 title = {Embarrassingly Parallel Inference for Gaussian Processes},
 url = {http://jmlr.org/papers/v20/18-374.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-383,
 abstract = {This paper frames causal structure estimation as a machine learning task. The idea is to treat indicators of causal relationships between variables as 'labels' and to exploit available data on the variables of interest to provide features for the labelling task. Background scientific knowledge or any available interventional data provide labels on some causal relationships and the remainder are treated as unlabelled. To illustrate the key ideas, we develop a distance-based approach (based on bivariate histograms) within a manifold regularization framework. We present empirical results on three different biological data sets (including examples where causal effects can be verified by experimental intervention), that together demonstrate the efficacy and general nature of the approach as well as its simplicity from a user's point of view.},
 author = {Steven M. Hill and Chris J. Oates and Duncan A. Blythe and Sach Mukherjee},
 journal = {Journal of Machine Learning Research},
 number = {127},
 openalex = {W2971719789},
 pages = {1--32},
 title = {Causal Learning via Manifold Regularization.},
 url = {http://jmlr.org/papers/v20/18-383.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-392,
 abstract = {When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture online and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice.},
 author = {Edward Barker and Charl Ras},
 journal = {Journal of Machine Learning Research},
 number = {128},
 openalex = {W2974372801},
 pages = {1--73},
 title = {Unsupervised Basis Function Adaptation for Reinforcement Learning},
 url = {http://jmlr.org/papers/v20/18-392.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-395,
 abstract = {This paper studies active learning in the context of robust statistics. Specifically, we propose a variant of the Best Arm Identification problem for \emph{contaminated bandits}, where each arm pull has probability $\varepsilon$ of generating a sample from an arbitrary contamination distribution instead of the true underlying distribution. The goal is to identify the best (or approximately best) true distribution with high probability, with a secondary goal of providing guarantees on the quality of this distribution. The primary challenge of the contaminated bandit setting is that the true distributions are only partially identifiable, even with infinite samples. To address this, we develop tight, non-asymptotic sample complexity bounds for high-probability estimation of the first two robust moments (median and median absolute deviation) from contaminated samples. These concentration inequalities are the main technical contributions of the paper and may be of independent interest. Using these results, we adapt several classical Best Arm Identification algorithms to the contaminated bandit setting and derive sample complexity upper bounds for our problem. Finally, we provide matching information-theoretic lower bounds on the sample complexity (up to a small logarithmic factor).},
 author = {Jason Altschuler and Victor-Emmanuel Brunel and Alan Malek},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2962948945},
 pages = {1--39},
 title = {Best Arm Identification for Contaminated Bandits},
 url = {http://jmlr.org/papers/v20/18-395.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-399,
 abstract = {We introduce coroICA, confounding-robust independent component analysis, a novel ICA algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. We show that our proposed general noise model allows to perform ICA in settings where other noisy ICA procedures fail. Additionally, it can be used for applications with grouped data by adjusting for different stationary noise within each group. Our proposed noise model has a natural relation to causality and we explain how it can be applied in the context of causal inference. In addition to our theoretical framework, we provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate the applicability to real-world scenarios by experiments on publicly available Antarctic ice core data as well as two EEG data sets. We provide a scikit-learn compatible pip-installable Python package coroICA as well as R and Matlab implementations accompanied by a documentation at https://sweichwald.de/coroICA/},
 author = {Niklas Pfister and Sebastian Weichwald and Peter B{{\"u}}hlmann and Bernhard Sch{{\"o}}lkopf},
 journal = {Journal of Machine Learning Research},
 number = {147},
 openalex = {W2903768887},
 pages = {1--50},
 title = {Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise},
 url = {http://jmlr.org/papers/v20/18-399.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-403,
 abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
 author = {Eli Bingham and Jonathan P. Chen and Martin Jankowiak and Fritz Obermeyer and Neeraj Pradhan and Theofanis Karaletsos and Rohit Singh and Paul Szerlip and Paul Horsfall and Noah D. Goodman},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2964321317},
 pages = {1--6},
 title = {Pyro: Deep Universal Probabilistic Programming},
 url = {http://jmlr.org/papers/v20/18-403.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-418,
 abstract = {We propose to optimize the activation functions of a deep neural network by adding a corresponding functional regularization to the cost function. We justify the use of a second-order total-variation criterion. This allows us to derive a general representer theorem for deep neural networks that makes a direct connection with splines and sparsity. Specifically, we show that the optimal network configuration can be achieved with activation functions that are nonuniform linear splines with adaptive knots. The bottom line is that the action of each neuron is encoded by a spline whose parameters (including the number of knots) are optimized during the training procedure. The scheme results in a computational structure that is compatible with the existing deep-ReLU, parametric ReLU, APL (adaptive piecewise-linear) and MaxOut architectures. It also suggests novel optimization challenges, while making the link with $\ell_1$ minimization and sparsity-promoting techniques explicit.},
 author = {Michael Unser},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W4301499895},
 pages = {1--30},
 title = {A representer theorem for deep neural networks},
 url = {http://jmlr.org/papers/v20/18-418.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-424,
 abstract = {New methods for time-to-event prediction are proposed by extending the Cox proportional hazards model with neural networks. Building on methodology from nested case-control studies, we propose a loss function that scales well to large data sets, and enables fitting of both proportional and non-proportional extensions of the Cox model. Through simulation studies, the proposed loss function is verified to be a good approximation for the Cox partial log-likelihood. The proposed methodology is compared to existing methodologies on real-world data sets, and is found to be highly competitive, typically yielding the best performance in terms of Brier score and binomial log-likelihood. A python package for the proposed methods is available at https://github.com/havakv/pycox.},
 author = {H{{\aa}}vard Kvamme and {{\O}}rnulf Borgan and Ida Scheel},
 journal = {Journal of Machine Learning Research},
 number = {129},
 openalex = {W2954512644},
 pages = {1--30},
 title = {Time-to-Event Prediction with Neural Networks and Cox Regression},
 url = {http://jmlr.org/papers/v20/18-424.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-444,
 abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to chose adequate hyperparameter spaces for tuning.},
 author = {Philipp Probst and Anne-Laure Boulesteix and Bernd Bischl},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2788079077},
 pages = {1--32},
 title = {Tunability: Importance of Hyperparameters of Machine Learning Algorithms},
 url = {http://jmlr.org/papers/v20/18-444.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-450,
 author = {Felipe Bravo-Marquez and Eibe Frank and Bernhard Pfahringer and Saif M. Mohammad},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W2950151462},
 pages = {1--6},
 title = {AffectiveTweets: a Weka package for analyzing affect in tweets},
 url = {http://jmlr.org/papers/v20/18-450.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-456,
 abstract = {The quantification problem consists of determining the prevalence of a given label in a target population. However, one often has access to the labels in a sample from the training population but not in the target population. A common assumption in this situation is that of prior probability shift, that is, once the labels are known, the distribution of the features is the same in the training and target populations. In this paper, we derive a new lower bound for the risk of the quantification problem under the prior shift assumption. Complementing this lower bound, we present a new approximately minimax class of estimators, ratio estimators, which generalize several previous proposals in the literature. Using a weaker version of the prior shift assumption, which can be tested, we show that ratio estimators can be used to build confidence intervals for the quantification problem. We also extend the ratio estimator so that it can: (i) incorporate labels from the target population, when they are available and (ii) estimate how the prevalence of positive labels varies according to a function of certain covariates.},
 author = {Afonso Fernandes Vaz and Rafael Izbicki and Rafael Bassi Stern},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2962804064},
 pages = {1--33},
 title = {Quantification under prior probability shift: the ratio estimator and its extensions},
 url = {http://jmlr.org/papers/v20/18-456.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-460,
 abstract = {We propose a two step algorithm based on $\ell_1/\ell_0$ regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only $2\cdotp$Lasso$(n,p)$, where Lasso$(n,p)$ represents the computational burden of one Lasso optimization in a model of size $(n,p)$. In comparison, existing grid search based approaches to this problem require a computational cost of at least $n\cdot {\rm Lasso}(n,p)$ optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of `no change', i.e., where no finite change point exists in the model. We work under a subgaussian random design where the underlying assumptions in our study are milder than those currently assumed in the high dimensional change point regression literature. We allow the true change point parameter $\tau_0$ to possibly move to the boundaries of its parametric space, and the jump size $\|\beta_0-\gamma_0\|_2$ to possibly diverge as $n$ increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.},
 author = {Abhishek Kaul and Venkata K. Jandhyala and Stergios B. Fotopoulos},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W2961511600},
 pages = {1--40},
 title = {An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search},
 url = {http://jmlr.org/papers/v20/18-460.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-470,
 author = {Daren Wang and Xinyang Lu and Alessandro Rinaldo},
 journal = {Journal of Machine Learning Research},
 number = {170},
 openalex = {W2997865178},
 pages = {1--50},
 title = {DBSCAN: Optimal Rates For Density-Based Cluster Estimation},
 url = {http://jmlr.org/papers/v20/18-470.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-476,
 abstract = {Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a c...},
 author = {Maximilian H{{\"u}}ttenrauch and Adrian {\v{S}}o{\v{s}}i{{\'c}} and Gerhard Neumann},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W3209101326},
 pages = {1--31},
 title = {Deep reinforcement learning for swarm systems},
 url = {http://jmlr.org/papers/v20/18-476.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-483,
 abstract = {Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries. Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system. However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one. In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution. Then, we relax the assumption of exponential family distribution for the noise and we investigate the distribution-free case. In this setting, we do not assume any specific model for the observations. The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.},
 author = {Mokhtar Z. Alaya and Olga Klopp},
 journal = {Journal of Machine Learning Research},
 number = {148},
 openalex = {W2983144912},
 pages = {1--43},
 title = {Collective Matrix Completion},
 url = {http://jmlr.org/papers/v20/18-483.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-484,
 abstract = {Author(s): Franks, AM; Hoff, P | Abstract: We develop a model-based method for evaluating heterogeneity among several p × p covariance matrices in the large p, small n setting. This is done by assuming a spiked covariance model for each group and sharing information about the space spanned by the group-level eigenvectors. We use an empirical Bayes method to identify a low-dimensional subspace which explains variation across all groups and use an MCMC algorithm to estimate the posterior uncertainty of eigenvectors and eigenvalues on this subspace. The implementation and utility of our model is illustrated with analyses of high-dimensional multivariate gene expression.},
 author = {Alexander M. Franks and Peter Hoff},
 journal = {Journal of Machine Learning Research},
 number = {171},
 openalex = {W2997355229},
 pages = {1--37},
 title = {Shared Subspace Models for Multi-Group Covariance Estimation},
 url = {http://jmlr.org/papers/v20/18-484.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-512,
 author = {Daniil Ryabko},
 journal = {Journal of Machine Learning Research},
 number = {149},
 openalex = {W2984852013},
 pages = {1--24},
 title = {On Asymptotic and Finite-Time Optimality of Bayesian Predictors},
 url = {http://jmlr.org/papers/v20/18-512.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-517,
 abstract = {Our interest in this paper is in the construction of symbolic explanations for predictions made by a deep neural network. We will focus attention on deep relational machines (DRMs, first proposed by H. Lodhi). A DRM is a deep network in which the input layer consists of Boolean-valued functions (features) that are defined in terms of relations provided as domain, or background, knowledge. Our DRMs differ from those proposed by Lodhi, which use an Inductive Logic Programming (ILP) engine to first select features (we use random selections from a space of features that satisfies some approximate constraints on logical relevance and non-redundancy). But why do the DRMs predict what they do? One way of answering this is the LIME setting, in which readable proxies for a black-box predictor. The proxies are intended only to model the predictions of the black-box in local regions of the instance-space. But readability alone may not enough: to be understandable, the local models must use relevant concepts in an meaningful manner. We investigate the use of a Bayes-like approach to identify logical proxies for local predictions of a DRM. We show: (a) DRM's with our randomised propositionalization method achieve state-of-the-art predictive performance; (b) Models in first-order logic can approximate the DRM's prediction closely in a small local region; and (c) Expert-provided relevance information can play the role of a prior to distinguish between logical explanations that perform equivalently on prediction alone.},
 author = {Ashwin Srinivasan and Lovekesh Vig and Michael Bain},
 journal = {Journal of Machine Learning Research},
 number = {130},
 openalex = {W4289765804},
 pages = {1--47},
 title = {Logical Explanations for Deep Relational Machines Using Relevance Information},
 url = {http://jmlr.org/papers/v20/18-517.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-539,
 author = {Bernard Chazelle and Chu Wang},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W2913716705},
 pages = {1--28},
 title = {Iterated Learning in Dynamic Social Networks},
 url = {http://jmlr.org/papers/v20/18-539.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-540,
 abstract = {In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.},
 author = {Maximilian Alber and Sebastian Lapuschkin and Philipp Seegerer and Miriam H{{\"a}}gele and Kristof T. Sch{{\"u}}tt and Gr{{\'e}}goire Montavon and Wojciech Samek and Klaus-Robert M{{\"u}}ller and Sven D{{\"a}}hne and Pieter-Jan Kindermans},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W4289699961},
 pages = {1--8},
 title = {iNNvestigate neural networks!},
 url = {http://jmlr.org/papers/v20/18-540.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-549,
 abstract = {We investigate the {\em direct-sum} problem in the context of differentially private PAC learning: What is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual example consists of a domain element x labeled by k unknown concepts (c1,...,ck). The goal of a multi-learner is to output k hypotheses (h1,...,hk) that generalize the input examples.},
 author = {Mark Bun and Kobbi Nissim and Uri Stemmer},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W3102341936},
 pages = {1--34},
 title = {Simultaneous Private Learning of Multiple Concepts},
 url = {http://jmlr.org/papers/v20/18-549.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-569,
 author = {Sophie Burkhardt and Stefan Kramer},
 journal = {Journal of Machine Learning Research},
 number = {131},
 openalex = {W2976420234},
 pages = {1--27},
 title = {Decoupling Sparsity and Smoothness in the Dirichlet Variational Autoencoder Topic Model},
 url = {http://jmlr.org/papers/v20/18-569.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-596,
 abstract = {In this paper, we propose improved estimation method for logistic regression based on subsamples taken according the optimal subsampling probabilities developed in Wang et al. (2018). Both asymptotic results and numerical results show that the new estimator has a higher estimation efficiency. We also develop a new algorithm based on Poisson subsampling, which does not require to approximate the optimal subsampling probabilities all at once. This is computationally advantageous when available random-access memory is not enough to hold the full data. Interestingly, asymptotic distributions also show that Poisson subsampling produces a more efficient estimator if the sampling ratio, the ratio of the subsample size to the full data sample size, does not converge to zero. We also obtain the unconditional asymptotic distribution for the estimator based on Poisson subsampling. Pilot estimators are required to calculate subsampling probabilities and to correct biases in un-weighted estimators; interestingly, even if pilot estimators are inconsistent, the proposed method still produce consistent and asymptotically normal estimators.},
 author = {HaiYing Wang},
 journal = {Journal of Machine Learning Research},
 number = {132},
 openalex = {W2975245449},
 pages = {1--59},
 title = {More Efficient Estimation for Logistic Regression with Optimal Subsamples},
 url = {http://jmlr.org/papers/v20/18-596.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-598,
 abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
 author = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2885311373},
 pages = {1--21},
 title = {Neural Architecture Search: A Survey},
 url = {http://jmlr.org/papers/v20/18-598.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-615,
 abstract = {Risk scores are simple classification models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are difficult to learn from data because they need to be calibrated, sparse, use small integer coefficients, and obey application-specific operational constraints. In this paper, we present a new machine learning approach to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm for non-convex settings to efficiently recover its optimal solution. We improve our algorithm with specialized techniques to generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our approach can fit risk scores in a way that scales linearly in the number of samples, provides a certificate of optimality, and obeys real-world constraints without parameter tuning or post-processing. We benchmark the performance benefits of this approach through an extensive set of numerical experiments, comparing to risk scores built using heuristic approaches. We also discuss its practical benefits through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.},
 author = {Berk Ustun and Cynthia Rudin},
 journal = {Journal of Machine Learning Research},
 number = {150},
 openalex = {W2985440697},
 pages = {1--75},
 title = {Learning Optimized Risk Scores},
 url = {http://jmlr.org/papers/v20/18-615.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-616,
 author = {Andrew Cotter and Heinrich Jiang and Maya Gupta and Serena Wang and Taman Narayan and Seungil You and Karthik Sridharan},
 journal = {Journal of Machine Learning Research},
 number = {172},
 openalex = {W2996860127},
 pages = {1--59},
 title = {Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals},
 url = {http://jmlr.org/papers/v20/18-616.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-618,
 author = {Vasileios Maroulas and Joshua L Mike and Christopher Oballe},
 journal = {Journal of Machine Learning Research},
 number = {151},
 openalex = {W2982638006},
 pages = {1--49},
 title = {Nonparametric Estimation of Probability Density Functions of Random Persistence Diagrams},
 url = {http://jmlr.org/papers/v20/18-618.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-659,
 abstract = {Multiple generalized additive models (GAMs) are a type of distributional regression wherein parameters of probability distributions depend on predictors through smooth functions, with selection of the degree of smoothness via $L_2$ regularization. Multiple GAMs allow finer statistical inference by incorporating explanatory information in any or all of the parameters of the distribution. Owing to their nonlinearity, flexibility and interpretability, GAMs are widely used, but reliable and fast methods for automatic smoothing in large datasets are still lacking, despite recent advances. We develop a general methodology for automatically learning the optimal degree of $L_2$ regularization for multiple GAMs using an empirical Bayes approach. The smooth functions are penalized by different amounts, which are learned simultaneously by maximization of a marginal likelihood through an approximate expectation-maximization algorithm that involves a double Laplace approximation at the E-step, and leads to an efficient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than all existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data.},
 author = {Yousra El-Bachir and Anthony C.  Davison},
 journal = {Journal of Machine Learning Research},
 number = {173},
 openalex = {W2892673301},
 pages = {1--27},
 title = {Fast Automatic Smoothing for Generalized Additive Models},
 url = {http://jmlr.org/papers/v20/18-659.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-674,
 abstract = {Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may explain such success. In this paper, we address this phenomenon by studying a key topological property of the loss: the presence or absence of spurious valleys, defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of two-layer neural networks defined by smooth (but generally non-linear) activation functions, we identify a notion of intrinsic dimension and show that it provides necessary and sufficient conditions for the absence of spurious valleys. More concretely, finite intrinsic dimension guarantees that for sufficiently overparametrised models no spurious valleys exist, independently of the data distribution. Conversely, infinite intrinsic dimension implies that spurious valleys do exist for certain data distributions, independently of model overparametrisation. Besides these positive and negative results, we show that, although spurious valleys may exist in general, they are confined to low risk levels and avoided with high probability on overparametrised models.},
 author = {Luca Venturi and Afonso S. Bandeira and Joan Bruna},
 journal = {Journal of Machine Learning Research},
 number = {133},
 openalex = {W2973737673},
 pages = {1--34},
 title = {Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes},
 url = {http://jmlr.org/papers/v20/18-674.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-680,
 abstract = {Can one reduce the size of a graph without significantly altering its basic properties? The graph reduction problem is hereby approached from the perspective of restricted spectral approximation, a modification of the spectral similarity measure used for graph sparsification. This choice is motivated by the observation that restricted approximation carries strong spectral and cut guarantees, and that it implies approximation results for unsupervised learning problems relying on spectral embeddings. The paper then focuses on coarsening---the most common type of graph reduction. Sufficient conditions are derived for a small graph to approximate a larger one in the sense of restricted similarity. These findings give rise to nearly-linear algorithms that, compared to both standard and advanced graph reduction methods, find coarse graphs of improved quality, often by a large margin, without sacrificing speed.},
 author = {Andreas Loukas},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W2960658350},
 pages = {1--42},
 title = {Graph reduction with spectral and cut guarantees},
 url = {http://jmlr.org/papers/v20/18-680.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-700,
 abstract = {We propose a unified data-driven framework based on inverse optimal transport that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts. We emphasize that the discrete optimal transport plays the role of a variational principle which gives rise to an optimization-based framework for modeling the observed empirical matching data. Our formulation leads to a non-convex optimization problem which can be solved efficiently by an alternating optimization method. A key novel aspect of our formulation is the incorporation of marginal relaxation via regularized Wasserstein distance, significantly improving the robustness of the method in the face of noisy or missing empirical matching data. Our model falls into the category of prescriptive models, which not only predict potential future matching, but is also able to explain what leads to empirical matching and quantifies the impact of changes in matching factors. The proposed approach has wide applicability including predicting matching in online dating, labor market, college application and crowdsourcing. We back up our claims with numerical experiments on both synthetic data and real world data sets.},
 author = {Ruilin Li and Xiaojing Ye and Haomin Zhou and Hongyuan Zha},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2804828897},
 pages = {1--37},
 title = {Learning to Match via Inverse Optimal Transport},
 url = {http://jmlr.org/papers/v20/18-700.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-703,
 abstract = {Finding overcomplete latent representations of data has applications in data analysis, signal processing, machine learning, theoretical neuroscience and many other fields. In an overcomplete representation, the number of latent features exceeds the data dimensionality, which is useful when the data is undersampled by the measurements (compressed sensing, information bottlenecks in neural systems) or composed from multiple complete sets of linear features, each spanning the data space. Independent Components Analysis (ICA) is a linear technique for learning sparse latent representations, which typically has a lower computational cost than sparse coding, its nonlinear, recurrent counterpart. While well suited for finding complete representations, we show that overcompleteness poses a challenge to existing ICA algorithms. Specifically, the coherence control in existing ICA algorithms, necessary to prevent the formation of duplicate dictionary features, is ill-suited in the overcomplete case. We show that in this case several existing ICA algorithms have undesirable global minima that maximize coherence. Further, by comparing ICA algorithms on synthetic data and natural images to the computationally more expensive sparse coding solution, we show that the coherence control biases the exploration of the data manifold, sometimes yielding suboptimal solutions. We provide a theoretical explanation of these failures and, based on the theory, propose improved overcomplete ICA algorithms. All told, this study contributes new insights into and methods for coherence control for linear ICA, some of which are applicable to many other, potentially nonlinear, unsupervised learning methods.},
 author = {Jesse A. Livezey and Alejandro F. Bujan and Friedrich T. Sommer},
 journal = {Journal of Machine Learning Research},
 number = {174},
 openalex = {W2997147428},
 pages = {1--42},
 title = {Learning overcomplete, low coherence dictionaries with linear inference},
 url = {http://jmlr.org/papers/v20/18-703.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-705,
 abstract = {We study the parameter estimation problem for a varying index coefficient model in high dimensions. Unlike the most existing works that iteratively estimate the parameters and link functions, based on the generalized Stein's identity, we propose computationally efficient estimators for the high-dimensional parameters without estimating the link functions. We consider two different setups where we either estimate each sparse parameter vector individually or estimate the parameters simultaneously as a sparse or low-rank matrix. For all these cases, our estimators are shown to achieve optimal statistical rates of convergence (up to logarithmic terms in the low-rank setting). Moreover, throughout our analysis, we only require the covariate to satisfy certain moment conditions, which is significantly weaker than the Gaussian or elliptically symmetric assumptions that are commonly made in the existing literature. Finally, we conduct extensive numerical experiments to corroborate the theoretical results.},
 author = {Sen Na and Zhuoran Yang and Zhaoran Wang and Mladen Kolar},
 journal = {Journal of Machine Learning Research},
 number = {152},
 openalex = {W2897873189},
 pages = {1--44},
 title = {High-dimensional Varying Index Coefficient Models via Stein's Identity},
 url = {http://jmlr.org/papers/v20/18-705.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-716,
 abstract = {We consider stochastic settings for clustering, and develop provably-good approximation algorithms for a number of these notions. These algorithms yield better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including clustering which is fairer and has better long-term behavior for each user. In particular, they ensure that *every user* is guaranteed to get good service (on average). We also complement some of these with impossibility results.},
 author = {David G. Harris and Shi Li and Thomas Pensyl and Aravind Srinivasan and Khoa Trinh},
 journal = {Journal of Machine Learning Research},
 number = {153},
 openalex = {W2984282532},
 pages = {1--33},
 title = {Approximation Algorithms for Stochastic Clustering},
 url = {http://jmlr.org/papers/v20/18-716.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-719,
 abstract = {The VC-dimension of a set system is a way to capture its complexity and has been a key parameter studied extensively in machine learning and geometry communities. In this paper, we resolve two longstanding open problems on bounding the VC-dimension of two fundamental set systems: k-fold unions/intersections of half-spaces and the simplices set system. Among other implications, it settles an open question in machine learning that was first studied in the foundational paper of Blumer et al. (1989) as well as by Eisenstat and Angluin (2007) and Johnson (2008).},
 author = {M{{\'o}}nika Csik{{\'o}}s and Nabil H. Mustafa and Andrey Kupavskii},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2945267489},
 pages = {1--8},
 title = {Tight Lower Bounds on the VC-dimension of Geometric Set Systems},
 url = {http://jmlr.org/papers/v20/18-719.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-753,
 author = {Felix Biessmann and Tammo Rukat and Phillipp Schmidt and Prathik Naidu and Sebastian Schelter and Andrey Taptunov and Dustin Lange and David Salinas},
 journal = {Journal of Machine Learning Research},
 number = {175},
 openalex = {W2997919412},
 pages = {1--6},
 title = {DataWig: Missing Value Imputation for Tables},
 url = {http://jmlr.org/papers/v20/18-753.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-759,
 abstract = {The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is violated for cases where the objective function is strongly convex. In Bottou et al. (2018), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. We show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime. We then move on to the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime in the case of diminished learning rate. It is well-known that SGD converges if a sequence of learning rates $\{\eta_t\}$ satisfies $\sum_{t=0}^\infty \eta_t \rightarrow \infty$ and $\sum_{t=0}^\infty \eta^2_t < \infty$. We show the convergence of SGD for strongly convex objective function without using bounded gradient assumption when $\{\eta_t\}$ is a diminishing sequence and $\sum_{t=0}^\infty \eta_t \rightarrow \infty$. In other words, we extend the current state-of-the-art class of learning rates satisfying the convergence of SGD.},
 author = {Lam M. Nguyen and Phuong Ha Nguyen and Peter Richt{{\'a}}rik and Katya Scheinberg and Martin Tak{{\'a}}{\v{c}} and Marten van Dijk},
 journal = {Journal of Machine Learning Research},
 number = {176},
 openalex = {W2997199030},
 pages = {1--49},
 title = {New Convergence Aspects of Stochastic Gradient Algorithms},
 url = {http://jmlr.org/papers/v20/18-759.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-760,
 abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model $f(\mathbf{x})=\mathbf{x}^{T}β$ with a fixed coefficient vector $β$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
 author = {Aaron Fisher and Cynthia Rudin and Francesca Dominici},
 journal = {Journal of Machine Learning Research},
 number = {177},
 openalex = {W2969476445},
 pages = {1--81},
 title = {All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously},
 url = {http://jmlr.org/papers/v20/18-760.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-762,
 abstract = {We consider the problem of finding critical points of functions that are non-convex and non-smooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simplification of the popular CCCP algorithm, used for optimizing functions that can be written as a difference of two convex functions. Our simplified algorithm retains all the convergence properties of CCCP, along with a significantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.},
 author = {Koulik Khamaru and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {154},
 openalex = {W2798332588},
 pages = {1--52},
 title = {Convergence guarantees for a class of non-convex and non-smooth optimization problems},
 url = {http://jmlr.org/papers/v20/18-762.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-789,
 abstract = {Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.},
 author = {Christopher J. Shallue and Jaehoon Lee and Joseph Antognini and Jascha Sohl-Dickstein and Roy Frostig and George E. Dahl},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W2900167092},
 pages = {1--49},
 title = {Measuring the Effects of Data Parallelism on Neural Network Training},
 url = {http://jmlr.org/papers/v20/18-789.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-801,
 abstract = {The growing size of modern data brings many new challenges to existing statistical inference methodologies and theories, and calls for the development of distributed inferential approaches. This paper studies distributed inference for linear support vector machine (SVM) for the binary classification task. Despite a vast literature on SVM, much less is known about the inferential properties of SVM, especially in a distributed setting. In this paper, we propose a multi-round distributed linear-type (MDL) estimator for conducting inference for linear SVM. The proposed estimator is computationally efficient. In particular, it only requires an initial SVM estimator and then successively refines the estimator by solving simple weighted least squares problem. Theoretically, we establish the Bahadur representation of the estimator. Based on the representation, the asymptotic normality is further derived, which shows that the MDL estimator achieves the optimal statistical efficiency, i.e., the same efficiency as the classical linear SVM applying to the entire data set in a single machine setup. Moreover, our asymptotic result avoids the condition on the number of machines or data batches, which is commonly assumed in distributed estimation literature, and allows the case of diverging dimension. We provide simulation studies to demonstrate the performance of the proposed MDL estimator.},
 author = {Xiaozhou Wang and Zhuoyi Yang and Xi Chen and Weidong Liu},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W2964291083},
 pages = {1--41},
 title = {Distributed Inference for Linear Support Vector Machine},
 url = {http://jmlr.org/papers/v20/18-801.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-819,
 author = {Gunwoong Park and Sion Park},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2951072839},
 pages = {1--41},
 title = {High-Dimensional Poisson Structural Equation Model Learning via $\ell_1$-Regularized Regression},
 url = {http://jmlr.org/papers/v20/18-819.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-859,
 abstract = {SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates inter-rater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website contains links to the code repository and extensive user documentation.},
 author = {Rob Chew and Michael Wenger and Caroline Kery and Jason Nance and Keith Richards and Emily Hadley and Peter Baumgartner},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W4382132564},
 pages = {1--5},
 title = {SMART: An Open Source Data Labeling Platform for Supervised Learning},
 url = {http://jmlr.org/papers/v20/18-859.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-869,
 abstract = {In this paper, we study the behavior of the Hedge algorithm in the online stochastic setting. We prove that anytime Hedge with decreasing learning rate, which is one of the simplest algorithm for the problem of prediction with expert advice, is surprisingly both worst-case optimal and adaptive to the easier stochastic and adversarial with a gap problems. This shows that, in spite of its small, non-adaptive learning rate, Hedge possesses the same optimal regret guarantee in the stochastic case as recently introduced adaptive algorithms. Moreover, our analysis exhibits qualitative differences with other variants of the Hedge algorithm, such as the fixed-horizon version (with constant learning rate) and the one based on the so-called "doubling trick", both of which fail to adapt to the easier stochastic setting. Finally, we discuss the limitations of anytime Hedge and the improvements provided by second-order regret bounds in the stochastic case.},
 author = {Jaouad Mourtada and St{{\'e}}phane Ga{{\"i}}ffas},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2952267903},
 pages = {1--28},
 title = {On the optimality of the Hedge algorithm in the stochastic regime},
 url = {http://jmlr.org/papers/v20/18-869.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-873,
 abstract = {This paper proposes and analyzes a novel clustering algorithm that combines graph-based diffusion geometry with techniques based on density and mode estimation. The proposed method is suitable for data generated from mixtures of distributions with densities that are both multimodal and have nonlinear shapes. A crucial aspect of this algorithm is the use of time of a data-adapted diffusion process as a scale parameter that is different from the local spatial scale parameter used in many clustering algorithms. We prove estimates for the behavior of diffusion distances with respect to this time parameter under a flexible nonparametric data model, identifying a range of times in which the mesoscopic equilibria of the underlying process are revealed, corresponding to a gap between within-cluster and between-cluster diffusion distances. These structures can be missed by the top eigenvectors of the graph Laplacian, commonly used in spectral clustering. This analysis is leveraged to prove sufficient conditions guaranteeing the accuracy of the proposed \emph{learning by unsupervised nonlinear diffusion (LUND)} procedure. We implement LUND and confirm its theoretical properties on illustrative datasets, demonstrating the theoretical and empirical advantages over both spectral clustering and density-based clustering techniques.},
 author = {Mauro Maggioni and James M. Murphy},
 journal = {Journal of Machine Learning Research},
 number = {160},
 openalex = {W2895913707},
 pages = {1--56},
 title = {Learning by Unsupervised Nonlinear Diffusion},
 url = {http://jmlr.org/papers/v20/18-873.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-875,
 author = {Zengfeng Huang},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2803931841},
 pages = {1--23},
 title = {Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices},
 url = {http://jmlr.org/papers/v20/18-875.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-006,
 abstract = {Accurately quantifying uncertainty in predictions is essential for the deployment of machine learning algorithms in critical applications where mistakes are costly. Most approaches to quantifying p ...},
 author = {Theodore Vasiloudis and Gianmarco De Francisci Morales and Henrik Bostr{{\"o}}m},
 journal = {Journal of Machine Learning Research},
 number = {155},
 openalex = {W2986740091},
 pages = {1--35},
 title = {Quantifying Uncertainty in Online Regression Forests},
 url = {http://jmlr.org/papers/v20/19-006.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-008,
 abstract = {Deep learning is built on the foundational guarantee that gradient descent on an objective function converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, that exhibit multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new tools to understand and control the dynamics in n-player differentiable games. 
The key result is to decompose the game Jacobian into two components. The first, symmetric component, is related to potential games, which reduce to gradient descent on an implicit function. The second, antisymmetric component, relates to Hamiltonian games, a new class of games that obey a conservation law akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in differentiable games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -- while at the same time being applicable to, and having guarantees in, much more general cases.},
 author = {Alistair Letcher and David Balduzzi and S{{\'e}}bastien Racani{{\`e}}re and James Martens and Jakob Foerster and Karl Tuyls and Thore Graepel},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2944678322},
 pages = {1--40},
 title = {Differentiable Game Mechanics},
 url = {http://jmlr.org/papers/v20/19-008.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-011,
 abstract = {PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod.},
 author = {Yue Zhao and Zain Nasrullah and Zheng Li},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W2963445059},
 pages = {1--7},
 title = {PyOD: A Python Toolbox for Scalable Outlier Detection},
 url = {http://jmlr.org/papers/v20/19-011.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-020,
 abstract = {Nonconvex matrix recovery is known to contain no spurious local minima under a restricted isometry property (RIP) with a sufficiently small RIP constant $δ$. If $δ$ is too large, however, then counterexamples containing spurious local minima are known to exist. In this paper, we introduce a proof technique that is capable of establishing sharp thresholds on $δ$ to guarantee the inexistence of spurious local minima. Using the technique, we prove that in the case of a rank-1 ground truth, an RIP constant of $δ&lt;1/2$ is both necessary and sufficient for exact recovery from any arbitrary initial point (such as a random point). We also prove a local recovery result: given an initial point $x_{0}$ satisfying $f(x_{0})\le(1-δ)^{2}f(0)$, any descent algorithm that converges to second-order optimality guarantees exact recovery.},
 author = {Richard Y. Zhang and Somayeh Sojoudi and Javad Lavaei},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W2964225338},
 pages = {1--34},
 title = {Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local Minima in Nonconvex Matrix Recovery},
 url = {http://jmlr.org/papers/v20/19-020.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-033,
 abstract = {Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims answer: to what extent has my model learned represent specific factors of variation in the data? We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at this https URL.},
 author = {Daniel C. Castro and Jeremy Tan and Bernhard Kainz and Ender Konukoglu and Ben Glocker},
 journal = {Journal of Machine Learning Research},
 number = {178},
 openalex = {W2997880344},
 pages = {1--29},
 title = {Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning},
 url = {http://jmlr.org/papers/v20/19-033.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-055,
 author = {Dongruo Zhou and Pan Xu and Quanquan Gu},
 journal = {Journal of Machine Learning Research},
 number = {134},
 openalex = {W2973370143},
 pages = {1--47},
 title = {Stochastic Variance-Reduced Cubic Regularization Methods},
 url = {http://jmlr.org/papers/v20/19-055.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-065,
 abstract = {This paper presents an approach for constrained Gaussian Process (GP) regression where we assume that a set of linear transformations of the process are bounded. It is motivated by machine learning applications for high-consequence engineering systems, where this kind of information is often made available from phenomenological knowledge. We consider a GP $f$ over functions on $\mathcal{X} \subset \mathbb{R}^{n}$ taking values in $\mathbb{R}$, where the process $\mathcal{L}f$ is still Gaussian when $\mathcal{L}$ is a linear operator. Our goal is to model $f$ under the constraint that realizations of $\mathcal{L}f$ are confined to a convex set of functions. In particular, we require that $a \leq \mathcal{L}f \leq b$, given two functions $a$ and $b$ where $a < b$ pointwise. This formulation provides a consistent way of encoding multiple linear constraints, such as shape-constraints based on e.g. boundedness, monotonicity or convexity. We adopt the approach of using a sufficiently dense set of virtual observation locations where the constraint is required to hold, and derive the exact posterior for a conjugate likelihood. The results needed for stable numerical implementation are derived, together with an efficient sampling scheme for estimating the posterior process.},
 author = {Christian Agrell},
 journal = {Journal of Machine Learning Research},
 number = {135},
 openalex = {W4288639456},
 pages = {1--36},
 title = {Gaussian processes with linear operator inequality constraints},
 url = {http://jmlr.org/papers/v20/19-065.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-150,
 abstract = {Much effort has been devoted in the last two decades to characterize the situations in which a reservoir computing system exhibits the so-called echo state (ESP) and fading memory (FMP) properties. These important features amount, in mathematical terms, to the existence and continuity of global reservoir system solutions. That research is complemented in this paper with the characterization of the differentiability of reservoir filters for very general classes of discrete-time deterministic inputs. This constitutes a novel strong contribution to the long line of research on the ESP and the FMP and, in particular, links to existing research on the input-dependence of the ESP. Differentiability has been shown in the literature to be a key feature in the learning of attractors of chaotic dynamical systems. A Volterra-type series representation for reservoir filters with semi-infinite discrete-time inputs is constructed in the analytic case using Taylor's theorem and corresponding approximation bounds are provided. Finally, it is shown as a corollary of these results that any fading memory filter can be uniformly approximated by a finite Volterra series with finite memory.},
 author = {Lyudmila Grigoryeva and Juan-Pablo Ortega},
 journal = {Journal of Machine Learning Research},
 number = {179},
 openalex = {W4288578125},
 pages = {1--62},
 title = {Differentiable reservoir computing},
 url = {http://jmlr.org/papers/v20/19-150.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-179,
 author = {Guillaume Gautier and Guillermo Polito and R{{\'e}}mi Bardenet and Michal Valko},
 journal = {Journal of Machine Learning Research},
 number = {180},
 openalex = {W2997033535},
 pages = {1--7},
 title = {DPPy: DPP Sampling with Python},
 url = {http://jmlr.org/papers/v20/19-179.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-197,
 abstract = {Structured latent attribute models (SLAMs) are a special family of discrete latent variable models widely used in social and biological sciences. This paper considers the problem of learning significant attribute patterns from a SLAM with potentially high-dimensional configurations of the latent attributes. We address the theoretical identifiability issue, propose a penalized likelihood method for the selection of the attribute patterns, and further establish the selection consistency in such an overfitted SLAM with diverging number of latent patterns. The good performance of the proposed methodology is illustrated by simulation studies and two real datasets in educational assessment.},
 author = {Yuqi Gu and Gongjun Xu},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W2961067567},
 pages = {1--58},
 title = {Learning Attribute Patterns in High-Dimensional Structured Latent Attribute Models},
 url = {http://jmlr.org/papers/v20/19-197.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-205,
 abstract = {Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet supports up-to-date detection models with best practice. SimpleDet also supports distributed training with near linear scaling out of box. Codes, examples and documents of SimpleDet can be found at https://github.com/tusimple/simpledet .},
 author = {Yuntao Chen and Chenxia Han and Yanghao Li and Zehao Huang and Yi Jiang and Naiyan Wang and Zhaoxiang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {156},
 openalex = {W2922315017},
 pages = {1--8},
 title = {SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition},
 url = {http://jmlr.org/papers/v20/19-205.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-216,
 abstract = {We unify $\textit{kernel density estimation}$ and $\textit{empirical Bayes}$ and address a set of problems in unsupervised learning with a geometric interpretation of those methods, rooted in the $\textit{concentration of measure}$ phenomenon. Kernel density is viewed symbolically as $X\rightharpoonup Y$ where the random variable $X$ is smoothed to $Y= X+N(0,\sigma^2 I_d)$, and empirical Bayes is the machinery to denoise in a least-squares sense, which we express as $X \leftharpoondown Y$. A learning objective is derived by combining these two, symbolically captured by $X \rightleftharpoons Y$. Crucially, instead of using the original nonparametric estimators, we parametrize $\textit{the energy function}$ with a neural network denoted by $\phi$; at optimality, $\nabla \phi \approx -\nabla \log f$ where $f$ is the density of $Y$. The optimization problem is abstracted as interactions of high-dimensional spheres which emerge due to the concentration of isotropic gaussians. We introduce two algorithmic frameworks based on this machinery: (i) a walk-jump sampling scheme that combines Langevin MCMC (walks) and empirical Bayes (jumps), and (ii) a probabilistic framework for $\textit{associative memory}$, called NEBULA, defined a la Hopfield by the $\textit{gradient flow}$ of the learned energy to a set of attractors. We finish the paper by reporting the emergence of very rich creative memories as attractors of NEBULA for highly-overlapping spheres.},
 author = {Saeed Saremi and Aapo Hyv{{\"a}}rinen},
 journal = {Journal of Machine Learning Research},
 number = {181},
 openalex = {W2997570648},
 pages = {1--23},
 title = {Neural Empirical Bayes},
 url = {http://jmlr.org/papers/v20/19-216.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-236,
 author = {Soumya Ghosh and Jiayu Yao and Finale Doshi-Velez},
 journal = {Journal of Machine Learning Research},
 number = {182},
 openalex = {W2998441233},
 pages = {1--46},
 title = {Model Selection in Bayesian Neural Networks via Horseshoe Priors},
 url = {http://jmlr.org/papers/v20/19-236.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-261,
 abstract = {Several data analysis techniques employ similarity relationships between data points to uncover the intrinsic dimension and geometric structure of the underlying data-generating mechanism. In this paper we work under the model assumption that the data is made of random perturbations of feature vectors lying on a low-dimensional manifold. We study two questions: how to define the similarity relationship over noisy data points, and what is the resulting impact of the choice of similarity in the extraction of global geometric information from the underlying manifold. We provide concrete mathematical evidence that using a local regularization of the noisy data to define the similarity improves the approximation of the hidden Euclidean distance between unperturbed points. Furthermore, graph-based objects constructed with the locally regularized similarity function satisfy better error bounds in their recovery of global geometric ones. Our theory is supported by numerical experiments that demonstrate that the gain in geometric understanding facilitated by local regularization translates into a gain in classification accuracy in simulated and real data.},
 author = {Nicol{{\'a}}s Garc{{\'i}}a Trillos and Daniel Sanz-Alonso and Ruiyi Yang},
 journal = {Journal of Machine Learning Research},
 number = {136},
 openalex = {W2976625731},
 pages = {1--37},
 title = {Local Regularization of Noisy Point Clouds: Improved Global Geometric Estimates and Data Analysis},
 url = {http://jmlr.org/papers/v20/19-261.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-306,
 abstract = {We consider the problem of sampling from a strongly log-concave density in $\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by simulating a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most $δ$ for a density with condition number $κ$, we show that MALA requires $\mathcal{O} \big(κd \log(1/δ) \big)$ steps, as compared to the $\mathcal{O} \big(κ^2 d/δ^2 \big)$ steps established in past work on ULA. We also demonstrate the gains of MALA over ULA for weakly log-concave densities. Furthermore, we derive mixing time bounds for the Metropolized random walk (MRW) and obtain $\mathcal{O}(κ)$ mixing time slower than MALA. We provide numerical examples that support our theoretical findings, and demonstrate the benefits of Metropolis-Hastings adjustment for Langevin-type sampling algorithms.},
 author = {Raaz Dwivedi and Yuansi Chen and Martin J. Wainwright and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {183},
 openalex = {W2783417494},
 pages = {1--42},
 title = {Log-concave sampling: Metropolis-Hastings algorithms are fast},
 url = {http://jmlr.org/papers/v20/19-306.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-332,
 abstract = {We consider the task of recovering two real or complex $m$-vectors from phaseless Fourier measurements of their circular convolution. Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a nontrivial convex relaxation of the bilinear measurements from convolution. We prove that if the two signals belong to known random subspaces of dimensions $k$ and $n$, then they can be recovered up to the inherent scaling ambiguity with $m \gg (k+n) \log^2 m$ phaseless measurements. Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based on Rademacher complexity estimates. Additionally, we provide an alternating direction method of multipliers (ADMM) implementation and provide numerical experiments that verify the theory.},
 author = {Ali Ahmed and Alireza Aghasi and Paul Hand},
 journal = {Journal of Machine Learning Research},
 number = {157},
 openalex = {W2985373544},
 pages = {1--28},
 title = {Simultaneous Phase Retrieval and Blind Deconvolution via Convex Programming},
 url = {http://jmlr.org/papers/v20/19-332.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-490,
 abstract = {We introduce GraSPy, a Python library devoted to statistical inference, machine learning, and visualization of random graphs and graph populations. This package provides flexible and easy-to-use algorithms for analyzing and understanding graphs with a scikit-learn compliant API. GraSPy can be downloaded from Python Package Index (PyPi), and is released under the Apache 2.0 open-source license. The documentation and all releases are available at this https URL.},
 author = {Jaewon Chung and Benjamin D. Pedigo and Eric W. Bridgeford and Bijan K. Varjavand and Hayden S. Helm and Joshua T. Vogelstein},
 journal = {Journal of Machine Learning Research},
 number = {158},
 openalex = {W2987403824},
 pages = {1--7},
 title = {GraSPy: Graph Statistics in Python},
 url = {http://jmlr.org/papers/v20/19-490.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-519,
 abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
 author = {Aharon Azulay and Yair Weiss},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W2996904338},
 pages = {1--25},
 title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
 url = {http://jmlr.org/papers/v20/19-519.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-543,
 abstract = {This work proposes a theoretical analysis of distributed optimization of convex functions using a network of computing units. We investigate this problem under two communication schemes (centralized and decentralized) and four classical regularity assumptions: Lipschitz continuity, strong convexity, smoothness, and a combination of strong convexity and smoothness. Under the decentralized communication scheme, we provide matching upper and lower bounds of complexity along with algorithms achieving this rate up to logarithmic constants. For non-smooth objective functions, while the dominant term of the error is in $O(1/\sqrt{t})$, the structure of the communication network only impacts a second-order term in $O(1/t)$, where $t$ is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly convex objective functions. Such a convergence rate is achieved by the novel multi-step primal-dual (MSPD) algorithm. Under the centralized communication scheme, we show that the naive distribution of standard optimization algorithms is optimal for smooth objective functions, and provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function for non-smooth functions. We then show that DRS is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is the underlying dimension.},
 author = {Kevin Scaman and Francis Bach and S{{\'e}}bastien Bubeck and Yin Tat Lee and Laurent Massouli{{\'e}}},
 journal = {Journal of Machine Learning Research},
 number = {159},
 openalex = {W2989361410},
 pages = {1--31},
 title = {Optimal Convergence Rates for Convex Distributed Optimization in Networks},
 url = {http://jmlr.org/papers/v20/19-543.html},
 volume = {20},
 year = {2019}
}
