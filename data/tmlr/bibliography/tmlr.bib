@comment{@article{,
  pdf = {https://openreview.net/pdf?id=ndw90pkNM9},
  review = {https://openreview.net/forum?id=ndw90pkNM9},
  code = {https://github.com/volvo-cars/eene-nav-bandit-sim},
}}

@article{abbott2024neural,
 abstract = {Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation. In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer. I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities.},
 author = {Vincent Abbott},
 code = {https://github.com/vtabbott/Neural-Circuit-Diagrams},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391709611},
 pdf = {https://openreview.net/pdf?id=RyZB4qXEgt},
 review = {https://openreview.net/forum?id=RyZB4qXEgt},
 title = {Neural Circuit Diagrams: Robust Diagrams for the Communication,
  Implementation, and Analysis of Deep Learning Architectures},
 url = {https://openreview.net/forum?id=RyZB4qXEgt},
 year = {2024}
}

@article{abdelhack2023a,
 abstract = {Data quality is a common problem in machine learning, especially in high-stakes settings such as healthcare. Missing data affects accuracy, calibration, and feature attribution in complex patterns. Developers often train models on carefully curated datasets to minimize missing data bias; however, this reduces the usability of such models in production environments, such as real-time healthcare records. Making machine learning models robust to missing data is therefore crucial for practical application. While some classifiers naturally handle missing data, others, such as deep neural networks, are not designed for unknown values. We propose a novel neural network modification to mitigate the impacts of missing data. The approach is inspired by neuromodulation that is performed by biological neural networks. Our proposal replaces the fixed weights of a fully-connected layer with a function of an additional input (reliability score) at each input, mimicking the ability of cortex to up- and down-weight inputs based on the presence of other data. The modulation function is jointly learned with the main task using a multi-layer perceptron. We tested our modulating fully connected layer on multiple classification, regression, and imputation problems, and it either improved performance or generated comparable performance to conventional neural network architectures concatenating reliability to the inputs. Models with modulating layers were more robust against degradation of data quality by introducing additional missingness at evaluation time. These results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time settings.},
 author = {Mohamed Abdelhack and Jiaming Zhang and Sandhya Tripathi and Bradley A Fritz and Daniel Felsky and Michael Avidan and Yixin Chen and Christopher Ryan King},
 code = {https://github.com/mabdelhack/mfcl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3186738100},
 pdf = {https://openreview.net/pdf?id=MRLHN4MSmA},
 review = {https://openreview.net/forum?id=MRLHN4MSmA},
 title = {A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues.},
 url = {https://openreview.net/forum?id=MRLHN4MSmA},
 year = {2023}
}

@article{abe2024pathologies,
 abstract = {Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training paradigms), we observe an opportunity cost associated with pursuing increased predictive diversity. Examining over 1000 ensembles, we observe that the performance benefits of diverse architectures/training procedures are easily dwarfed by the benefits of simply using higher-capacity models, despite the fact that such higher capacity models often yield significantly less predictive diversity. Overall, our findings demonstrate that standard intuitions around predictive diversity, originally developed for low-capacity ensembles, do not directly apply to modern high-capacity deep ensembles. This work clarifies fundamental challenges to the goal of improving deep ensembles by making them more diverse, while suggesting an alternative path: simply forming ensembles from ever more powerful (and less diverse) component models.},
 author = {Taiga Abe and E. Kelly Buchanan and Geoff Pleiss and John Patrick Cunningham},
 badge = {Featured},
 code = {https://github.com/cellistigs/ensemble_attention/tree/dkl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4319165519},
 pdf = {https://openreview.net/pdf?id=TQfQUksaC8},
 review = {https://openreview.net/forum?id=TQfQUksaC8},
 title = {Pathologies of Predictive Diversity in Deep Ensembles},
 url = {https://openreview.net/forum?id=TQfQUksaC8},
 year = {2024}
}

@article{abrevaya2024effective,
 abstract = {Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. GOKU-UI not only broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), but also integrates attention mechanisms and a novel multiple shooting training strategy in the latent space. These modifications have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 16-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stochastic Stuart-Landau oscillators into its dynamical core, our proposed enhancements markedly increased the model's effectiveness in capturing complex brain dynamics. This augmented version not only surpassed all baseline methods in the reconstruction task, but also demonstrated lower prediction error of future brain activity up to 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encoded whole-brain dynamics into a latent representation, learning a low-dimensional dynamical system model that could offer insights into brain functionality and open avenues for practical applications such as the classification of mental states or psychiatric conditions. Ultimately, our research provides further impetus for the field of Scientific Machine Learning, showcasing the potential for advancements when established scientific insights are interwoven with modern machine learning.},
 author = {Germ{\'a}n Abrevaya and Mahta Ramezanian-Panahi and Jean-Christophe Gagnon-Audet and Pablo Polosecki and Irina Rish and Silvina Ponce Dawson and Guillermo Cecchi and Guillaume Dumas},
 code = {https://github.com/gabrevaya/TMLR_GOKU-UI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384263505},
 pdf = {https://openreview.net/pdf?id=uxNfN2PU1W},
 review = {https://openreview.net/forum?id=uxNfN2PU1W},
 title = {Effective Latent Differential Equation Models via Attention and Multiple Shooting},
 url = {https://openreview.net/forum?id=uxNfN2PU1W},
 year = {2024}
}

@article{abuduweili2023an,
 abstract = {Deep reinforcement learning has the potential to address various scientific problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment captures the essence of nonconvexity, nonlinearity, and time-dependent noise inherent in optical systems, offering a more realistic setting. Subsequently, we provide the benchmark results of several reinforcement learning algorithms on the proposed simulation environment. The experimental findings demonstrate the superiority of off-policy reinforcement learning approaches over traditional control algorithms in navigating the intricacies of complex optical control environments. The code of the paper is available at https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking.},
 author = {ABULIKEMU ABUDUWEILI and Changliu Liu},
 code = {https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221140424},
 pdf = {https://openreview.net/pdf?id=61TKzU9B96},
 review = {https://openreview.net/forum?id=61TKzU9B96},
 title = {An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms},
 url = {https://openreview.net/forum?id=61TKzU9B96},
 year = {2023}
}

@article{achab2023onestep,
 abstract = {Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.},
 author = {Mastane Achab and Reda ALAMI and YASSER ABDELAZIZ DAHOU DJILALI and Kirill Fedyanin and Eric Moulines},
 code = {https://github.com/mastane/cleanrl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4367623555},
 pdf = {https://openreview.net/pdf?id=ZPMf53vE1L},
 review = {https://openreview.net/forum?id=ZPMf53vE1L},
 title = {One-Step Distributional Reinforcement Learning},
 url = {https://openreview.net/forum?id=ZPMf53vE1L},
 year = {2023}
}

@article{adaloglou2024adapting,
 abstract = {We present a comprehensive experimental study on pretrained feature extractors for visual out-of-distribution (OOD) detection, focusing on adapting contrastive language-image pretrained (CLIP) models. Without fine-tuning on the training data, we are able to establish a positive correlation ($R^2\geq0.92$) between in-distribution classification and unsupervised OOD detection for CLIP models in $4$ benchmarks. We further propose a new simple and scalable method called \textit{pseudo-label probing} (PLP) that adapts vision-language models for OOD detection. Given a set of label names of the training set, PLP trains a linear layer using the pseudo-labels derived from the text encoder of CLIP. To test the OOD detection robustness of pretrained models, we develop a novel feature-based adversarial OOD data manipulation approach to create adversarial samples. Intriguingly, we show that (i) PLP outperforms the previous state-of-the-art \citep{ming2022mcm} on all $5$ large-scale benchmarks based on ImageNet, specifically by an average AUROC gain of 3.4\% using the largest CLIP model (ViT-G), (ii) we show that linear probing outperforms fine-tuning by large margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of 7.3\% AUROC on average on all ImageNet-based benchmarks), and (iii) billion-parameter CLIP models still fail at detecting adversarially manipulated OOD images. The code and adversarially created datasets will be made publicly available.},
 author = {Nikolas Adaloglou and Felix Michels and Tim Kaiser and Markus Kollmann},
 code = {https://github.com/HHU-MMBS/plp-official-tmlr2024},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4324105789},
 pdf = {https://openreview.net/pdf?id=YCgX7sJRF1},
 review = {https://openreview.net/forum?id=YCgX7sJRF1},
 title = {Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection},
 url = {https://openreview.net/forum?id=YCgX7sJRF1},
 year = {2024}
}

@article{adib,
 code = {https://github.com/FarnazAdib/online_tracking_with_adversarial_disturbances},
 pdf = {https://openreview.net/pdf?id=5nVJlKgmxp},
 review = {https://openreview.net/forum?id=5nVJlKgmxp}
}

@article{adolphs2022boosting,
 abstract = {This paper presents first successful steps in designing search agents that learn meta-strategies for iterative query refinement in information-seeking tasks. Our approach uses machine reading to guide the selection of refinement terms from aggregated search results. Agents are then empowered with simple but effective search operators to exert fine-grained and transparent control over queries and search results. We develop a novel way of generating synthetic search sessions, which leverages the power of transformer-based language models through (self-)supervised learning. We also present a reinforcement learning agent with dynamically constrained actions that learns interactive search strategies from scratch. Our search agents obtain retrieval and answer quality performance comparable to recent neural methods, using only a traditional term-based BM25 ranking function and interpretable discrete reranking and filtering actions.},
 author = {Leonard Adolphs and Benjamin B{\"o}rschinger and Christian Buck and Michelle Chen Huebscher and Massimiliano Ciaramita and Lasse Espeholt and Thomas Hofmann and Yannic Kilcher and Sascha Rothe and Pier Giuseppe Sessa and Lierni Sestorain},
 code = {https://github.com/google-research/language/tree/master/language/search_agents},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287016601},
 pdf = {https://openreview.net/pdf?id=0ZbPmmB61g},
 review = {https://openreview.net/forum?id=0ZbPmmB61g},
 title = {Boosting Search Engines with Interactive Agents},
 url = {https://openreview.net/forum?id=0ZbPmmB61g},
 year = {2022}
}

@article{agarwal2022concave,
 abstract = {We consider the problem of tabular infinite horizon concave utility reinforcement learning (CURL) with convex constraints. For this, we propose a model-based learning algorithm that also achieves zero constraint violations. Assuming that the concave objective and the convex constraints have a solution interior to the set of feasible occupation measures, we solve a tighter optimization problem to ensure that the constraints are never violated despite the imprecise model knowledge and model stochasticity. We use Bellman error-based analysis for tabular infinite-horizon setups which allows analyzing stochastic policies. Combining the Bellman error-based analysis and tighter optimization equation, for $T$ interactions with the environment, we obtain a high-probability regret guarantee for objective which grows as $\Tilde{O}(1/\sqrt{T})$, excluding other factors. The proposed method can be applied for optimistic algorithms to obtain high-probability regret bounds and also be used for posterior sampling algorithms to obtain a loose Bayesian regret bounds but with significant improvement in computational complexity.},
 author = {Mridul Agarwal and Qinbo Bai and Vaneet Aggarwal},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286979736},
 pdf = {https://openreview.net/pdf?id=WXVkgkPXRk},
 review = {https://openreview.net/forum?id=WXVkgkPXRk},
 title = {Concave Utility Reinforcement Learning with Zero-Constraint Violations},
 url = {https://openreview.net/forum?id=WXVkgkPXRk},
 year = {2022}
}

@article{agarwal2023auxdrop,
 abstract = {Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This helps in better learning for scenarios where certain features disappear in time or when new features are to be modelled. The efficacy of Aux-Drop has been demonstrated through extensive numerical experiments on SOTA benchmarking datasets that include Italy Power Demand, HIGGS, SUSY and multiple UCI datasets. The code is available at https://github.com/Rohit102497/Aux-Drop.},
 author = {Rohit Agarwal and Deepak Gupta and Alexander Horsch and Dilip K. Prasad},
 badge = {Reproducibility},
 code = {https://github.com/Rohit102497/Aux-Drop},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4323924637},
 pdf = {https://openreview.net/pdf?id=R9CgBkeZ6Z},
 review = {https://openreview.net/forum?id=R9CgBkeZ6Z},
 title = {Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts},
 url = {https://openreview.net/forum?id=R9CgBkeZ6Z},
 year = {2023}
}

@article{agarwala2023temperature,
 abstract = {The softmax function combined with a cross-entropy loss is a principled approach to modeling probability distributions that has become ubiquitous in deep learning. The softmax function is defined by a lone hyperparameter, the temperature, that is commonly set to one or regarded as a way to tune model confidence after training; however, less is known about how the temperature impacts training dynamics or generalization performance. In this work we develop a theory of early learning for models trained with softmax-cross-entropy loss and show that the learning dynamics depend crucially on the inverse-temperature $β$ as well as the magnitude of the logits at initialization, $||β{\bf z}||_{2}$. We follow up these analytic results with a large-scale empirical study of a variety of model architectures trained on CIFAR10, ImageNet, and IMDB sentiment analysis. We find that generalization performance depends strongly on the temperature, but only weakly on the initial logit magnitude. We provide evidence that the dependence of generalization on $β$ is not due to changes in model confidence, but is a dynamical phenomenon. It follows that the addition of $β$ as a tunable hyperparameter is key to maximizing model performance. Although we find the optimal $β$ to be sensitive to the architecture, our results suggest that tuning $β$ over the range $10^{-2}$ to $10^1$ improves performance over all architectures studied. We find that smaller $β$ may lead to better peak performance at the cost of learning stability.},
 author = {Atish Agarwala and Samuel Stern Schoenholz and Jeffrey Pennington and Yann Dauphin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3093341604},
 pdf = {https://openreview.net/pdf?id=LBA2Jj5Gqn},
 review = {https://openreview.net/forum?id=LBA2Jj5Gqn},
 title = {Temperature check: theory and practice for training models with softmax-cross-entropy losses},
 url = {https://openreview.net/forum?id=LBA2Jj5Gqn},
 year = {2023}
}

@article{aghazadeh2023spectral,
 abstract = {Data-driven machine learning models are being increasingly employed in several important inference problems in biology, chemistry, and physics which require learning over combinatorial spaces. Recent empirical evidence (see, e.g., [1], [2], [3]) suggests that regularizing the spectral representation of such models improves their generalization power when labeled data is scarce. However, despite these empirical studies, the theoretical underpinning of when and how spectral regularization enables improved generalization is poorly understood. In this paper, we focus on learning pseudo-Boolean functions and demonstrate that regularizing the empirical mean squared error by the L_1 norm of the spectral transform of the learned function reshapes the loss landscape and allows for data-frugal learning, under a restricted secant condition on the learner's empirical error measured against the ground truth function. Under a weaker quadratic growth condition, we show that stationary points which also approximately interpolate the training data points achieve statistically optimal generalization performance. Complementing our theory, we empirically demonstrate that running gradient descent on the regularized loss results in a better generalization performance compared to baseline algorithms in several data-scarce real-world problems.},
 author = {Amirali Aghazadeh and Nived Rajaraman and Tony Tu and Kannan Ramchandran},
 code = {https://github.com/tonytu16/Walsh-Hadamard-Transform},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4303648785},
 pdf = {https://openreview.net/pdf?id=mySiFHCeAl},
 review = {https://openreview.net/forum?id=mySiFHCeAl},
 title = {Spectral Regularization Allows Data-frugal Learning over Combinatorial Spaces},
 url = {https://openreview.net/forum?id=mySiFHCeAl},
 year = {2023}
}

@article{ahmad2023constrained,
 abstract = {Learning in neural networks is often framed as a problem in which targeted error signals are directly propagated to parameters and used to produce updates that induce more optimal network behaviour. Backpropagation of error (BP) is an example of such an approach and has proven to be a highly successful application of stochastic gradient descent to deep neural networks. We propose constrained parameter inference (COPI) as a new principle for learning. The COPI approach assumes that learning can be set up in a manner where parameters infer their own values based upon observations of their local neuron activities. We find that this estimation of network parameters is possible under the constraints of decorrelated neural inputs and top-down perturbations of neural states for credit assignment. We show that the decorrelation required for COPI allows learning at extremely high learning rates, competitive with that of adaptive optimizers, as used by BP. We further demonstrate that COPI affords a new approach to feature analysis and network compression. Finally, we argue that COPI may shed new light on learning in biological networks given the evidence for decorrelation in the brain.},
 author = {Nasir Ahmad and Ellen Schrader and Marcel van Gerven},
 code = {https://github.com/nasiryahm/ConstrainedParameterInference},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221144241},
 pdf = {https://openreview.net/pdf?id=CUDdbTT1QC},
 review = {https://openreview.net/forum?id=CUDdbTT1QC},
 title = {Constrained Parameter Inference as a Principle for Learning},
 url = {https://openreview.net/forum?id=CUDdbTT1QC},
 year = {2023}
}

@article{aketi2023neighborhood,
 author = {Sai Aparna Aketi and Sangamesh Kodge and Kaushik Roy},
 code = {https://github.com/aparna-aketi/neighborhood_gradient_clustering},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=vkiKzK5G3e},
 review = {https://openreview.net/forum?id=vkiKzK5G3e},
 title = {Neighborhood Gradient Mean: An Efficient Decentralized Learning Method for Non-{IID} Data},
 url = {https://openreview.net/forum?id=vkiKzK5G3e},
 year = {2023}
}

@article{alabi2023bounded,
 abstract = {Estimating the quantiles of a large dataset is a fundamental problem in both the streaming algorithms literature and the differential privacy literature. However, all existing private mechanisms for distribution-independent quantile computation require space at least linear in the input size $n$. In this work, we devise a differentially private algorithm for the quantile estimation problem, with strongly sublinear space complexity, in the one-shot and continual observation settings. Our basic mechanism estimates any $\alpha$-approximate quantile of a length-$n$ stream over a data universe $\mathcal{X}$ with probability $1-\beta$ using $O\left( \frac{\log (|\mathcal{X}|/\beta) \log (\alpha \epsilon n)}{\alpha \epsilon} \right)$ space while satisfying $\epsilon$-differential privacy at a single time point. Our approach builds upon deterministic streaming algorithms for non-private quantile estimation instantiating the exponential mechanism using a utility function defined on sketch items, while (privately) sampling from intervals defined by the sketch. We also present another algorithm based on histograms that is especially suited to the multiple quantiles case. We implement our algorithms and experimentally evaluate them on synthetic and real-world datasets.},
 author = {Daniel Alabi and Omri Ben-Eliezer and Anamay Chaturvedi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226028843},
 pdf = {https://openreview.net/pdf?id=sixOD8YVvM},
 review = {https://openreview.net/forum?id=sixOD8YVvM},
 title = {Bounded Space Differentially Private Quantiles},
 url = {https://openreview.net/forum?id=sixOD8YVvM},
 year = {2023}
}

@article{alabi2023privacy,
 author = {Daniel Alabi and Chris Wiggins},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=SnPEhMyuYX},
 review = {https://openreview.net/forum?id=SnPEhMyuYX},
 title = {Privacy Budget Tailoring in Private Data Analysis},
 url = {https://openreview.net/forum?id=SnPEhMyuYX},
 year = {2023}
}

@article{alhamoud2023generalizability,
 abstract = {Recent progress in empirical and certified robustness promises to deliver reliable and deployable Deep Neural Networks (DNNs). Despite that success, most existing evaluations of DNN robustness have been done on images sampled from the same distribution on which the model was trained. However, in the real world, DNNs may be deployed in dynamic environments that exhibit significant distribution shifts. In this work, we take a first step towards thoroughly investigating the interplay between empirical and certified adversarial robustness on one hand and domain generalization on another. To do so, we train robust models on multiple domains and evaluate their accuracy and robustness on an unseen domain. We observe that: (1) both empirical and certified robustness generalize to unseen domains, and (2) the level of generalizability does not correlate well with input visual similarity, measured by the FID between source and target domains. We also extend our study to cover a real-world medical application, in which adversarial augmentation significantly boosts the generalization of robustness with minimal effect on clean data accuracy.},
 author = {Kumail Alhamoud and Hasan Abed Al Kader Hammoud and Motasem Alfarra and Bernard Ghanem},
 badge = {Featured},
 code = {https://github.com/m1k2zoo/RobustDG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4300978433},
 pdf = {https://openreview.net/pdf?id=XNFo3dQiCJ},
 review = {https://openreview.net/forum?id=XNFo3dQiCJ},
 title = {Generalizability of Adversarial Robustness Under Distribution Shifts},
 url = {https://openreview.net/forum?id=XNFo3dQiCJ},
 year = {2023}
}

@article{allingham2022sparse,
 abstract = {Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, but also provides better predictive performance and uncertainty estimates for larger models.},
 author = {James Urquhart Allingham and Florian Wenzel and Zelda E Mariet and Basil Mustafa and Joan Puigcerver and Neil Houlsby and Ghassen Jerfel and Vincent Fortuin and Balaji Lakshminarayanan and Jasper Snoek and Dustin Tran and Carlos Riquelme Ruiz and Rodolphe Jenatton},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/google-research/vmoe},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification, Expert Certification},
 openalex = {W4298204073},
 pdf = {https://openreview.net/pdf?id=i0ZM36d2qU},
 review = {https://openreview.net/forum?id=i0ZM36d2qU},
 title = {Sparse MoEs meet Efficient Ensembles},
 url = {https://openreview.net/forum?id=i0ZM36d2qU},
 year = {2022}
}

@article{alomrani2022deep,
 abstract = {The challenge in the widely applicable online matching problem lies in making irrevocable assignments while there is uncertainty about future inputs. Most theoretically-grounded policies are myopic or greedy in nature. In real-world applications where the matching process is repeated on a regular basis, the underlying data distribution can be leveraged for better decision-making. We present an end-to-end Reinforcement Learning framework for deriving better matching policies based on trial-and-error on historical data. We devise a set of neural network architectures, design feature representations, and empirically evaluate them across two online matching problems: Edge-Weighted Online Bipartite Matching and Online Submodular Bipartite Matching. We show that most of the learning approaches perform consistently better than classical baseline algorithms on four synthetic and real-world datasets. On average, our proposed models improve the matching quality by 3--10\% on a variety of synthetic and real-world datasets. Our code is publicly available at https://github.com/lyeskhalil/CORL.},
 author = {Mohammad Ali Alomrani and Reza Moravej and Elias Boutros Khalil},
 code = {https://github.com/lyeskhalil/CORL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286969368},
 pdf = {https://openreview.net/pdf?id=mbwm7NdkpO},
 review = {https://openreview.net/forum?id=mbwm7NdkpO},
 title = {Deep Policies for Online Bipartite Matching: A Reinforcement Learning Approach},
 url = {https://openreview.net/forum?id=mbwm7NdkpO},
 year = {2022}
}

@article{alomrani2024dygvec,
 abstract = {Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requiring 5-10x less training/inference time. Lastly, different aspects of the proposed framework are investigated through experimental analysis and ablation studies. The code is publicly available at https://github.com/huawei-noah/noah-research/tree/master/graph_atlas.},
 author = {Mohammad Alomrani and Mahdi Biparva and Yingxue Zhang and Mark Coates},
 code = {https://github.com/huawei-noah/noah-research/tree/master/graph_atlas},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307930292},
 pdf = {https://openreview.net/pdf?id=YRKS2J0x36},
 review = {https://openreview.net/forum?id=YRKS2J0x36},
 title = {DyG2Vec: Efficient Representation Learning for Dynamic Graphs},
 url = {https://openreview.net/forum?id=YRKS2J0x36},
 year = {2024}
}

@article{altekr,
 pdf = {https://openreview.net/pdf?id=Wcui061fxr},
 review = {https://openreview.net/forum?id=Wcui061fxr}
}

@article{alvarez-melis2022optimizing,
 abstract = {Gradient flows are a powerful tool for optimizing functionals in general metric spaces, including the space of probabilities endowed with the Wasserstein metric. A typical approach to solving this optimization problem relies on its connection to the dynamic formulation of optimal transport and the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation involves optimization over convex functions, which is challenging, especially in high dimensions. In this work, we propose an approach that relies on the recently introduced input-convex neural networks (ICNN) to parameterize the space of convex functions in order to approximate the JKO scheme, as well as in designing functionals over measures that enjoy convergence guarantees. We derive a computationally efficient implementation of this JKO-ICNN framework and use various experiments to demonstrate its feasibility and validity in approximating solutions of low-dimensional partial differential equations with known solutions. We also explore the use of our JKO-ICNN approach in high dimensions with an experiment in controlled generation for molecular discovery.},
 author = {David Alvarez-Melis and Yair Schiff and Youssef Mroueh},
 code = {https://github.com/dmelis/JKO-ICNN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3169286221},
 pdf = {https://openreview.net/pdf?id=dpOYN7o8Jm},
 review = {https://openreview.net/forum?id=dpOYN7o8Jm},
 title = {Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks.},
 url = {https://openreview.net/forum?id=dpOYN7o8Jm},
 year = {2022}
}

@article{amid2023layerwise,
 abstract = {In this work, we propose a novel approach for layerwise representation learning of a trained neural network. In particular, we form a Bregman divergence based on the layer's transfer function and construct an extension of the original Bregman PCA formulation by incorporating a mean vector and normalizing the principal directions with respect to the geometry of the local convex function around the mean. This generalization allows exporting the learned representation as a fixed layer with a non-linearity. As an application to knowledge distillation, we cast the learning problem for the student network as predicting the compression coefficients of the teacher's representations, which are passed as the input to the imported layer. Our empirical findings indicate that our approach is substantially more effective for transferring information between networks than typical teacher-student training using the teacher's penultimate layer representations and soft labels.},
 author = {Ehsan Amid and Rohan Anil and Christopher Fifty and Manfred K Warmuth},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297847123},
 pdf = {https://openreview.net/pdf?id=6dsvH7pQHH},
 review = {https://openreview.net/forum?id=6dsvH7pQHH},
 title = {Layerwise Bregman Representation Learning with Applications to Knowledge Distillation},
 url = {https://openreview.net/forum?id=6dsvH7pQHH},
 year = {2023}
}

@article{andersson2024extending,
 author = {William Andersson and Jakob Heiss and Florian Krach and Josef Teichmann},
 code = {https://github.com/FlorianKrach/PD-NJODE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=0T2OTVCCC1},
 review = {https://openreview.net/forum?id=0T2OTVCCC1},
 title = {Extending Path-Dependent {NJ}-{ODE}s to Noisy Observations and a Dependent Observation Framework},
 url = {https://openreview.net/forum?id=0T2OTVCCC1},
 year = {2024}
}

@article{andreassen2022the,
 abstract = {Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit "effective robustness" and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models.},
 author = {Anders Johan Andreassen and Yasaman Bahri and Behnam Neyshabur and Rebecca Roelofs},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3173182565},
 pdf = {https://openreview.net/pdf?id=Qs3EfpieOh},
 review = {https://openreview.net/forum?id=Qs3EfpieOh},
 title = {The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning},
 url = {https://openreview.net/forum?id=Qs3EfpieOh},
 year = {2022}
}

@article{antoran2023uncertainty,
 abstract = {Existing deep-learning based tomographic image reconstruction methods do not provide accurate estimates of reconstruction uncertainty, hindering their real-world deployment. This paper develops a method, termed as the linearised deep image prior (DIP), to estimate the uncertainty associated with reconstructions produced by the DIP with total variation regularisation (TV). Specifically, we endow the DIP with conjugate Gaussian-linear model type error-bars computed from a local linearisation of the neural network around its optimised parameters. To preserve conjugacy, we approximate the TV regulariser with a Gaussian surrogate. This approach provides pixel-wise uncertainty estimates and a marginal likelihood objective for hyperparameter optimisation. We demonstrate the method on synthetic data and real-measured high-resolution 2D $\mu$CT data, and show that it provides superior calibration of uncertainty estimates relative to previous probabilistic formulations of the DIP. Our code is available at https://github.com/educating-dip/bayes_dip.},
 author = {Javier Antoran and Riccardo Barbano and Johannes Leuschner and Jos{\'e} Miguel Hern{\'a}ndez-Lobato and Bangti Jin},
 code = {https://github.com/educating-dip/bayes_dip},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226468927},
 pdf = {https://openreview.net/pdf?id=FWyabz82fH},
 review = {https://openreview.net/forum?id=FWyabz82fH},
 title = {Uncertainty Estimation for Computed Tomography with a Linearised Deep Image Prior},
 url = {https://openreview.net/forum?id=FWyabz82fH},
 year = {2023}
}

@article{arani2022a,
 abstract = {Deep neural network based object detectors are continuously evolving and are used in a multitude of applications, each having its own set of requirements. While safety-critical applications need high accuracy and reliability, low-latency tasks need resource and energy-efficient networks. Real-time detectors, which are a necessity in high-impact real-world applications, are continuously proposed, but they overemphasize the improvements in accuracy and speed while other capabilities such as versatility, robustness, resource and energy efficiency are omitted. A reference benchmark for existing networks does not exist, nor does a standard evaluation guideline for designing new networks, which results in ambiguous and inconsistent comparisons. We, thus, conduct a comprehensive study on multiple real-time detectors (anchor-, keypoint-, and transformer-based) on a wide range of datasets and report results on an extensive set of metrics. We also study the impact of variables such as image size, anchor dimensions, confidence thresholds, and architecture layers on the overall performance. We analyze the robustness of detection networks against distribution shifts, natural corruptions, and adversarial attacks. Also, we provide a calibration analysis to gauge the reliability of the predictions. Finally, to highlight the real-world impact, we conduct two unique case studies, on autonomous driving and healthcare applications. To further gauge the capability of networks in critical real-time applications, we report the performance after deploying the detection networks on edge devices. Our extensive empirical study can act as a guideline for the industrial community to make an informed choice on the existing networks. We also hope to inspire the research community towards a new direction in the design and evaluation of networks that focuses on a bigger and holistic overview for a far-reaching impact.},
 author = {Elahe Arani and Shruthi Gowda and Ratnajit Mukherjee and Omar Magdy and Senthilkumar Sockalingam Kathiresan and Bahram Zonooz},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4293252055},
 pdf = {https://openreview.net/pdf?id=ywr5sWqQt4},
 review = {https://openreview.net/forum?id=ywr5sWqQt4},
 title = {A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey},
 url = {https://openreview.net/forum?id=ywr5sWqQt4},
 year = {2022}
}

@article{arenz2023a,
 abstract = {This package contains the raw data / logs (fetched from WandB) for the experiments of the following publication: O. Arenz, P. Dahlinger, Z. Ye, M. Volpp, and G. Neumann. A unified perspective on natural gradient variational inference with gaussian mixture models. Transactions on Machine Learning Research, 2023. URL: https://openreview.net/forum?id=tLBjsX4tjs.},
 author = {Oleg Arenz and Philipp Dahlinger and Zihan Ye and Michael Volpp and Gerhard Neumann},
 code = {https://github.com/OlegArenz/gmmvi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4393478636},
 pdf = {https://openreview.net/pdf?id=tLBjsX4tjs},
 review = {https://openreview.net/forum?id=tLBjsX4tjs},
 title = {Experimental Results for "A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models"},
 url = {https://openreview.net/forum?id=tLBjsX4tjs},
 year = {2023}
}

@article{aronsson2024correlation,
 author = {Linus Aronsson and Morteza Haghir Chehreghani},
 code = {https://github.com/Linusaronsson/Active-Correlation-Clustering},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Ryf1TVCjBz},
 review = {https://openreview.net/forum?id=Ryf1TVCjBz},
 title = {Correlation Clustering with Active Learning of Pairwise Similarities},
 url = {https://openreview.net/forum?id=Ryf1TVCjBz},
 year = {2024}
}

@article{asadi2022collaborative,
 abstract = {We consider an online estimation problem involving a set of agents. Each agent has access to a (personal) process that generates samples from a real-valued distribution and seeks to estimate its mean. We study the case where some of the distributions have the same mean, and the agents are allowed to actively query information from other agents. The goal is to design an algorithm that enables each agent to improve its mean estimate thanks to communication with other agents. The means as well as the number of distributions with same mean are unknown, which makes the task nontrivial. We introduce a novel collaborative strategy to solve this online personalized mean estimation problem. We analyze its time complexity and introduce variants that enjoy good performance in numerical experiments. We also extend our approach to the setting where clusters of agents with similar means seek to estimate the mean of their cluster.},
 author = {Mahsa Asadi and Aur{\'e}lien Bellet and Odalric-Ambrym Maillard and Marc Tommasi},
 code = {https://github.com/llvllahsa/CollaborativePersonalizedMeanEstimation},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318034629},
 pdf = {https://openreview.net/pdf?id=VipljNfZSZ},
 review = {https://openreview.net/forum?id=VipljNfZSZ},
 title = {Collaborative Algorithms for Online Personalized Mean Estimation},
 url = {https://openreview.net/forum?id=VipljNfZSZ},
 year = {2022}
}

@article{asiedu,
 pdf = {https://openreview.net/pdf?id=D3WI0QG7dC},
 review = {https://openreview.net/forum?id=D3WI0QG7dC}
}

@article{askari,
 badge = {Featured},
 code = {https://github.com/ReyhaneAskari/Least_action_dynamics_minmax},
 pdf = {https://openreview.net/pdf?id=vXSsTYs6ZB},
 review = {https://openreview.net/forum?id=vXSsTYs6ZB}
}

@article{assaad2023vntransformer,
 abstract = {Rotation equivariance is a desirable property in many practical applications such as motion forecasting and 3D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations. Vector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional "vector neurons." We introduce a novel "VN-Transformer" architecture to address several shortcomings of the current VN models. Our contributions are: $(i)$ we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; $(ii)$ we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; $(iii)$ we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; $(iv)$ we show that small tradeoffs in equivariance ($\epsilon$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models. Finally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results.},
 author = {Serge Assaad and Carlton Downey and Rami Al-Rfou' and Nigamaa Nayakanti and Benjamin Sapp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281877847},
 pdf = {https://openreview.net/pdf?id=EiX2L4sDPG},
 review = {https://openreview.net/forum?id=EiX2L4sDPG},
 title = {VN-Transformer: Rotation-Equivariant Attention for Vector Neurons},
 url = {https://openreview.net/forum?id=EiX2L4sDPG},
 year = {2023}
}

@article{asthana2024multiconditioned,
 abstract = {Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.},
 author = {Rohan Asthana and Joschua Conrad and Youssef Dawoud and Maurits Ortmanns and Vasileios Belagiannis},
 code = {https://github.com/rohanasthana/DiNAS},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4392737111},
 pdf = {https://openreview.net/pdf?id=5VotySkajV},
 review = {https://openreview.net/forum?id=5VotySkajV},
 title = {Multi-conditioned Graph Diffusion for Neural Architecture Search},
 url = {https://openreview.net/forum?id=5VotySkajV},
 year = {2024}
}

@article{atashgahi2023supervised,
 abstract = {Feature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention on feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named \enquote{NeuroFS}. By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on $11$ low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. The code is available on GitHub.},
 author = {Zahra Atashgahi and Xuhao Zhang and Neil Kichler and Shiwei Liu and Lu Yin and Mykola Pechenizkiy and Raymond Veldhuis and Decebal Constantin Mocanu},
 code = {https://github.com/zahraatashgahi/NeuroFS},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4327486070},
 pdf = {https://openreview.net/pdf?id=GcO6ugrLKp},
 review = {https://openreview.net/forum?id=GcO6ugrLKp},
 title = {Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks},
 url = {https://openreview.net/forum?id=GcO6ugrLKp},
 year = {2023}
}

@article{avalos2023local,
 author = {Rapha{\"e}l Avalos and Mathieu Reymond and Ann Nowe and Diederik M Roijers},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=adpKzWQunW},
 review = {https://openreview.net/forum?id=adpKzWQunW},
 title = {Local Advantage Networks for Multi-Agent Reinforcement Learning in Dec-{POMDP}s},
 url = {https://openreview.net/forum?id=adpKzWQunW},
 year = {2023}
}

@article{aydemir2022modeling,
 abstract = {Saliency prediction has made great strides over the past two decades, with current techniques modeling low-level information, such as color, intensity and size contrasts, and high-level ones, such as attention and gaze direction for entire objects. Despite this, these methods fail to account for the dissimilarity between objects, which affects human visual attention. In this paper, we introduce a detection-guided saliency prediction network that explicitly models the differences between multiple objects, such as their appearance and size dissimilarities. Our approach allows us to fuse our object dissimilarities with features extracted by any deep saliency prediction network. As evidenced by our experiments, this consistently boosts the accuracy of the baseline networks, enabling us to outperform the state-of-the-art models on three saliency benchmarks, namely SALICON, MIT300 and CAT2000. Our project page is at https://github.com/IVRL/DisSal.},
 author = {Bahar Aydemir and Deblina Bhattacharjee and Tong Zhang and Seungryong Kim and Mathieu Salzmann and Sabine S{\"u}sstrunk},
 code = {https://github.com/IVRL/DisSal},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287236249},
 pdf = {https://openreview.net/pdf?id=NmTMc3uD1G},
 review = {https://openreview.net/forum?id=NmTMc3uD1G},
 title = {Modeling Object Dissimilarity for Deep Saliency Prediction},
 url = {https://openreview.net/forum?id=NmTMc3uD1G},
 year = {2022}
}

@article{ayed2023data,
 abstract = {Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of $30\%$ or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; in [Sorscher et al.], the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law. In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate ``No Free Lunch" theorems for data pruning and present calibration protocols that enhance the performance of existing pruning algorithms in this high compression regime using randomization.},
 author = {Fadhel Ayed and Soufiane Hayou},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321012582},
 pdf = {https://openreview.net/pdf?id=iRTL4pDavo},
 review = {https://openreview.net/forum?id=iRTL4pDavo},
 title = {Data pruning and neural scaling laws: fundamental limitations of score-based algorithms},
 url = {https://openreview.net/forum?id=iRTL4pDavo},
 year = {2023}
}

@article{azizi2023synthetic,
 abstract = {Deep generative models are becoming increasingly powerful, now generating diverse high fidelity photo-realistic samples given text prompts. Have they reached the point where models of natural images can be used for generative data augmentation, helping to improve challenging discriminative tasks? We show that large-scale text-to image diffusion models can be fine-tuned to produce class conditional models with SOTA FID (1.76 at 256x256 resolution) and Inception Score (239 at 256x256). The model also yields a new SOTA in Classification Accuracy Scores (64.96 for 256x256 generative samples, improving to 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with samples from the resulting models yields significant improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines.},
 author = {Shekoofeh Azizi and Simon Kornblith and Chitwan Saharia and Mohammad Norouzi and David J. Fleet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4366330698},
 pdf = {https://openreview.net/pdf?id=DlRsoxjyPm},
 review = {https://openreview.net/forum?id=DlRsoxjyPm},
 title = {Synthetic Data from Diffusion Models Improves ImageNet Classification},
 url = {https://openreview.net/forum?id=DlRsoxjyPm},
 year = {2023}
}

@article{b,
 code = {https://github.com/usinedepain/projectively_equivariant_deep_nets},
 pdf = {https://openreview.net/pdf?id=Ls1E16bTj8},
 review = {https://openreview.net/forum?id=Ls1E16bTj8}
}

@article{b,
 code = {https://github.com/mariabankestad/VariationalEP},
 pdf = {https://openreview.net/pdf?id=djN3TaqbdA},
 review = {https://openreview.net/forum?id=djN3TaqbdA}
}

@article{b,
 code = {https://github.com/AxelBohm/OGDA_for_weak_Minty},
 pdf = {https://openreview.net/pdf?id=Gp0pHyUyrb},
 review = {https://openreview.net/forum?id=Gp0pHyUyrb}
}

@article{b,
 code = {https://github.com/gabriben/metrics-as-losses},
 pdf = {https://openreview.net/pdf?id=gvSHaaD2wQ},
 review = {https://openreview.net/forum?id=gvSHaaD2wQ}
}

@article{babakniya2023revisiting,
 author = {Sara Babakniya and Souvik Kundu and Saurav Prakash and Yue Niu and Salman Avestimehr},
 code = {https://github.com/SaraBabakN/flash_fl.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=iHyhdpsnyi},
 review = {https://openreview.net/forum?id=iHyhdpsnyi},
 title = {Revisiting Sparsity Hunting in Federated Learning: Why does Sparsity Consensus Matter?},
 url = {https://openreview.net/forum?id=iHyhdpsnyi},
 year = {2023}
}

@article{baby2023nonstationary,
 author = {Dheeraj Baby and Jianyu Xu and Yu-Xiang Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=fWIQ9Oaao0},
 review = {https://openreview.net/forum?id=fWIQ9Oaao0},
 title = {Non-Stationary Contextual Pricing with Safety Constraints},
 url = {https://openreview.net/forum?id=fWIQ9Oaao0},
 year = {2023}
}

@article{bach2023global,
 author = {Thong Bach and Anh Tong and Truong Son Hy and Vu Nguyen and Thanh Nguyen-Tang},
 code = {https://github.com/meobach/GloCo_ProCo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=xWrtiJwJj5},
 review = {https://openreview.net/forum?id=xWrtiJwJj5},
 title = {Global Contrastive Learning for Long-Tailed Classification},
 url = {https://openreview.net/forum?id=xWrtiJwJj5},
 year = {2023}
}

@article{bagatella2022sfp,
 abstract = {Efficient exploration is a crucial challenge in deep reinforcement learning. Several methods, such as behavioral priors, are able to leverage offline data in order to efficiently accelerate reinforcement learning on complex tasks. However, if the task at hand deviates excessively from the demonstrated task, the effectiveness of such methods is limited. In our work, we propose to learn features from offline data that are shared by a more diverse range of tasks, such as correlation between actions and directedness. Therefore, we introduce state-free priors, which directly model temporal consistency in demonstrated trajectories, and are capable of driving exploration in complex tasks, even when trained on data collected on simpler tasks. Furthermore, we introduce a novel integration scheme for action priors in off-policy reinforcement learning by dynamically sampling actions from a probabilistic mixture of policy and action prior. We compare our approach against strong baselines and provide empirical evidence that it can accelerate reinforcement learning in long-horizon continuous control tasks under sparse reward settings.},
 author = {Marco Bagatella and Sammy Joe Christen and Otmar Hilliges},
 code = {https://eth-ait.github.io/sfp/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4294406453},
 pdf = {https://openreview.net/pdf?id=qYNfwFCX9a},
 review = {https://openreview.net/forum?id=qYNfwFCX9a},
 title = {SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning},
 url = {https://openreview.net/forum?id=qYNfwFCX9a},
 year = {2022}
}

@article{baharlouei2023rifle,
 abstract = {The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample sizes, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for regression and classification in the presence of missing data without imputation. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments of the underlying data distribution with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and normal discriminant analysis, and we provide convergence and performance guarantees. This framework can also be adapted to impute missing data. In numerical experiments, we compare RIFLE to several state-of-the-art approaches (including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) for imputation and inference in the presence of missing values. Our experiments demonstrate that RIFLE outperforms other benchmark algorithms when the percentage of missing values is high and/or when the number of data points is relatively small. RIFLE is publicly available at https://github.com/optimization-for-data-driven-science/RIFLE.},
 author = {Sina Baharlouei and Sze-Chuan Suen and Meisam Razaviyayn},
 code = {https://github.com/optimization-for-data-driven-science/RIFLE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386754034},
 pdf = {https://openreview.net/pdf?id=oud7Ny0KQy},
 review = {https://openreview.net/forum?id=oud7Ny0KQy},
 title = {RIFLE: Imputation and Robust Inference from Low Order Marginals},
 url = {https://openreview.net/forum?id=oud7Ny0KQy},
 year = {2023}
}

@article{bahmani2023daware,
 abstract = {Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs.},
 author = {Sherwin Bahmani and Jeong Joon Park and Despoina Paschalidou and Hao Tang and Gordon Wetzstein and Leonidas Guibas and Luc Van Gool and Radu Timofte},
 code = {https://github.com/sherwinbahmani/3dvideogeneration/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283761699},
 pdf = {https://openreview.net/pdf?id=SwlfyDq6B3},
 review = {https://openreview.net/forum?id=SwlfyDq6B3},
 title = {3D-Aware Video Generation},
 url = {https://openreview.net/forum?id=SwlfyDq6B3},
 year = {2023}
}

@article{bahmani2023semantic,
 abstract = {The lack of out-of-domain generalization is a critical weakness of deep networks for semantic segmentation. Previous studies relied on the assumption of a static model, i. e., once the training process is complete, model parameters remain fixed at test time. In this work, we challenge this premise with a self-adaptive approach for semantic segmentation that adjusts the inference process to each input sample. Self-adaptation operates on two levels. First, it fine-tunes the parameters of convolutional layers to the input image using consistency regularization. Second, in Batch Normalization layers, self-adaptation interpolates between the training and the reference distribution derived from a single test sample. Despite both techniques being well known in the literature, their combination sets new state-of-the-art accuracy on synthetic-to-real generalization benchmarks. Our empirical study suggests that self-adaptation may complement the established practice of model regularization at training time for improving deep network generalization to out-of-domain data. Our code and pre-trained models are available at https://github.com/visinf/self-adaptive.},
 author = {Sherwin Bahmani and Oliver Hahn and Eduard Zamfir and Nikita Araslanov and Daniel Cremers and Stefan Roth},
 code = {https://github.com/visinf/self-adaptive},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4291238730},
 pdf = {https://openreview.net/pdf?id=ILNqQhGbLx},
 review = {https://openreview.net/forum?id=ILNqQhGbLx},
 title = {Semantic Self-adaptation: Enhancing Generalization with a Single Sample},
 url = {https://openreview.net/forum?id=ILNqQhGbLx},
 year = {2023}
}

@article{bai2023sequential,
 abstract = {Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and robust solution to CQA. In the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators, and finally, recursively execute these neuralized operators. However, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder. Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks. Motivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. Instead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and then uses a sequence encoder to compute its vector representation. Then this vector representation is used as a query embedding to retrieve answers from the embedding space according to similarity scores. Despite its simplicity, SQE demonstrates state-of-the-art neural query encoding performance on FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine types of in-distribution queries. Further experiment shows that SQE also demonstrates comparable knowledge inference capability on out-of-distribution queries, whose query types are not observed during the training process.},
 author = {Jiaxin Bai and Tianshi Zheng and Yangqiu Song},
 code = {https://github.com/HKUST-KnowComp/SQE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4322759472},
 pdf = {https://openreview.net/pdf?id=ERqGqZzSu5},
 review = {https://openreview.net/forum?id=ERqGqZzSu5},
 title = {Sequential Query Encoding For Complex Query Answering on Knowledge Graphs},
 url = {https://openreview.net/forum?id=ERqGqZzSu5},
 year = {2023}
}

@article{bai2024on,
 abstract = {We propose the framework of dual convexified convolutional neural networks (DCCNNs). In this framework, we first introduce a primal learning problem motivated by convexified convolutional neural networks (CCNNs), and then construct the dual convex training program through careful analysis of the Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach reduces the computational overhead of constructing a large kernel matrix and more importantly, eliminates the ambiguity of factorizing the matrix. Due to the low-rank structure in CCNNs and the related subdifferential of nuclear norms, there is no closed-form expression to recover the primal solution from the dual solution. To overcome this, we propose a highly novel weight recovery algorithm, which takes the dual solution and the kernel information as the input, and recovers the linear weight and the output of convolutional layer, instead of weight parameter. Furthermore, our recovery algorithm exploits the low-rank structure and imposes a small number of filters indirectly, which reduces the parameter size. As a result, DCCNNs inherit all the statistical benefits of CCNNs, while enjoying a more formal and efficient workflow.},
 author = {Site Bai and Chuyang Ke and Jean Honorio},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320458211},
 pdf = {https://openreview.net/pdf?id=0yMuNezwJ1},
 review = {https://openreview.net/forum?id=0yMuNezwJ1},
 title = {Dual Convexified Convolutional Neural Networks},
 url = {https://openreview.net/forum?id=0yMuNezwJ1},
 year = {2024}
}

@article{bairaktari2023fair,
 abstract = {As important decisions about the distribution of society's resources become increasingly automated, it is essential to consider the measurement and enforcement of fairness in these decisions. In this work we build on the results of Dwork and Ilvento ITCS'19, which laid the foundations for the study of fair algorithms under composition. In particular, we study the cohort selection problem, where we wish to use a fair classifier to select $k$ candidates from an arbitrarily ordered set of size $n>k$, while preserving individual fairness and maximizing utility. We define a linear utility function to measure performance relative to the behavior of the original classifier. We develop a fair, utility-optimal $O(n)$-time cohort selection algorithm for the offline setting, and our primary result, a solution to the problem in the streaming setting that keeps no more than $O(k)$ pending candidates at all time.},
 author = {Konstantina Bairaktari and Paul Tsela Langton and Huy Nguyen and Niklas Smedemark-Margulies and Jonathan Ullman},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3083578731},
 pdf = {https://openreview.net/pdf?id=wRepWp1KC7},
 review = {https://openreview.net/forum?id=wRepWp1KC7},
 title = {Fair and Useful Cohort Selection},
 url = {https://openreview.net/forum?id=wRepWp1KC7},
 year = {2023}
}

@article{balazadeh2022learning,
 abstract = {Reinforcement learning agents have been mostly developed and evaluated under the assumption that they will operate in a fully autonomous manner -- they will take all actions. In this work, our goal is to develop algorithms that, by learning to switch control between agents, allow existing reinforcement learning agents to operate under different automation levels. To this end, we first formally define the problem of learning to switch control among agents in a team via a 2-layer Markov decision process. Then, we develop an online learning algorithm that uses upper confidence bounds on the agents' policies and the environment's transition probabilities to find a sequence of switching policies. The total regret of our algorithm with respect to the optimal switching policy is sublinear in the number of learning steps and, whenever multiple teams of agents operate in a similar environment, our algorithm greatly benefits from maintaining shared confidence bounds for the environments' transition probabilities and it enjoys a better regret bound than problem-agnostic algorithms. Simulation experiments in an obstacle avoidance task illustrate our theoretical findings and demonstrate that, by exploiting the specific structure of the problem, our proposed algorithm is superior to problem-agnostic algorithms.},
 author = {Vahid Balazadeh and Abir De and Adish Singla and Manuel Gomez Rodriguez},
 code = {https://github.com/vdblm/Human-Machine-MDP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4383173612},
 pdf = {https://openreview.net/pdf?id=NT9zgedd3I},
 review = {https://openreview.net/forum?id=NT9zgedd3I},
 title = {Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes},
 url = {https://openreview.net/forum?id=NT9zgedd3I},
 year = {2022}
}

@article{baldassi2022systematically,
 abstract = {We present a meta-method for initializing (seeding) the $k$-means clustering algorithm called PNN-smoothing. It consists in splitting a given dataset into $J$ random subsets, clustering each of them individually, and merging the resulting clusterings with the pairwise-nearest-neighbor (PNN) method. It is a meta-method in the sense that when clustering the individual subsets any seeding algorithm can be used. If the computational complexity of that seeding algorithm is linear in the size of the data $N$ and the number of clusters $k$, PNN-smoothing is also almost linear with an appropriate choice of $J$, and quite competitive in practice. We show empirically, using several existing seeding methods and testing on several synthetic and real datasets, that this procedure results in systematically better costs. In particular, our method of enhancing $k$-means++ seeding proves superior in both effectiveness and speed compared to the popular "greedy" $k$-means++ variant. Our implementation is publicly available at https://github.com/carlobaldassi/KMeansPNNSmoothing.jl.},
 author = {Carlo Baldassi},
 code = {https://github.com/carlobaldassi/KMeansPNNSmoothing.jl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4289782752},
 pdf = {https://openreview.net/pdf?id=FTtFAg3pek},
 review = {https://openreview.net/forum?id=FTtFAg3pek},
 title = {Systematically and efficiently improving $k$-means initialization by pairwise-nearest-neighbor smoothing},
 url = {https://openreview.net/forum?id=FTtFAg3pek},
 year = {2022}
}

@article{balles2024on,
 author = {Lukas Balles and Prabhu Teja S and Cedric Archambeau},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=DPvwr4HJdt},
 review = {https://openreview.net/forum?id=DPvwr4HJdt},
 title = {On the Choice of Learning Rate for Local {SGD}},
 url = {https://openreview.net/forum?id=DPvwr4HJdt},
 year = {2024}
}

@article{banerjee2023mermaide,
 abstract = {We study how a principal can efficiently and effectively intervene on the rewards of a previously unseen learning agent in order to induce desirable outcomes. This is relevant to many real-world settings like auctions or taxation, where the principal may not know the learning behavior nor the rewards of real people. Moreover, the principal should be few-shot adaptable and minimize the number of interventions, because interventions are often costly. We introduce MERMAIDE, a model-based meta-learning framework to train a principal that can quickly adapt to out-of-distribution agents with different learning strategies and reward functions. We validate this approach step-by-step. First, in a Stackelberg setting with a best-response agent, we show that meta-learning enables quick convergence to the theoretically known Stackelberg equilibrium at test time, although noisy observations severely increase the sample complexity. We then show that our model-based meta-learning approach is cost-effective in intervening on bandit agents with unseen explore-exploit strategies. Finally, we outperform baselines that use either meta-learning or agent behavior modeling, in both $0$-shot and $K=1$-shot settings with partial agent information.},
 author = {Arundhati Banerjee and Soham Rajesh Phade and Stefano Ermon and Stephan Zheng},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4364387575},
 pdf = {https://openreview.net/pdf?id=H5VRvCXCzf},
 review = {https://openreview.net/forum?id=H5VRvCXCzf},
 title = {MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning},
 url = {https://openreview.net/forum?id=H5VRvCXCzf},
 year = {2023}
}

@article{banihashem2023defense,
 abstract = {We study defense strategies against reward poisoning attacks in reinforcement learning. As a threat model, we consider attacks that minimally alter rewards to make the attacker's target policy uniquely optimal under the poisoned rewards, with the optimality gap specified by an attack parameter. Our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true, unpoisoned, rewards while computing their policies under the poisoned rewards. We propose an optimization framework for deriving optimal defense policies, both when the attack parameter is known and unknown. Moreover, we show that defense policies that are solutions to the proposed optimization problems have provable performance guarantees. In particular, we provide the following bounds with respect to the true, unpoisoned, rewards: a) lower bounds on the expected return of the defense policies, and b) upper bounds on how suboptimal these defense policies are compared to the attacker's target policy. We conclude the paper by illustrating the intuitions behind our formal results, and showing that the derived bounds are non-trivial.},
 author = {Kiarash Banihashem and Adish Singla and Goran Radanovic},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3127643357},
 pdf = {https://openreview.net/pdf?id=goPsLn3RVo},
 review = {https://openreview.net/forum?id=goPsLn3RVo},
 title = {Defense Against Reward Poisoning Attacks in Reinforcement Learning},
 url = {https://openreview.net/forum?id=goPsLn3RVo},
 year = {2023}
}

@article{bao2023finding,
 author = {Xuchan Bao and Guodong Zhang},
 badge = {Featured},
 code = {https://github.com/XuchanBao/double-ftr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 pdf = {https://openreview.net/pdf?id=igdWKxK5RZ},
 review = {https://openreview.net/forum?id=igdWKxK5RZ},
 title = {Finding and Only Finding Differential Nash Equilibria by Both Pretending to be a Follower},
 url = {https://openreview.net/forum?id=igdWKxK5RZ},
 year = {2023}
}

@article{barbano2024image,
 abstract = {Deep learning has been widely used for solving image reconstruction tasks but its deployability has been held back due to the shortage of high-quality training data. Unsupervised learning methods, such as the deep image prior (DIP), naturally fill this gap, but bring a host of new issues: the susceptibility to overfitting due to a lack of robust early stopping strategies and unstable convergence. We present a novel approach to tackle these issues by restricting DIP optimisation to a sparse linear subspace of its parameters, employing a synergy of dimensionality reduction techniques and second order optimisation methods. The low-dimensionality of the subspace reduces DIP's tendency to fit noise and allows the use of stable second order optimisation methods, e.g., natural gradient descent or L-BFGS. Experiments across both image restoration and tomographic tasks of different geometry and ill-posedness show that second order optimisation within a low-dimensional subspace is favourable in terms of optimisation stability to reconstruction fidelity trade-off.},
 author = {Riccardo Barbano and Javier Antoran and Johannes Leuschner and Jos{\'e} Miguel Hern{\'a}ndez-Lobato and Bangti Jin and Zeljko Kereta},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321524629},
 pdf = {https://openreview.net/pdf?id=torWsEui9N},
 review = {https://openreview.net/forum?id=torWsEui9N},
 title = {Image Reconstruction via Deep Image Prior Subspaces},
 url = {https://openreview.net/forum?id=torWsEui9N},
 year = {2024}
}

@article{barik2024recovering,
 author = {Adarsh Barik and Jean Honorio},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=JdXzKSyqbH},
 review = {https://openreview.net/forum?id=JdXzKSyqbH},
 title = {Recovering Exact Support in Federated lasso without Optimization},
 url = {https://openreview.net/forum?id=JdXzKSyqbH},
 year = {2024}
}

@article{bastos2022how,
 abstract = {The recent works proposing transformer-based models for graphs have proven the inadequacy of Vanilla Transformer for graph representation learning. To understand this inadequacy, there is a need to investigate if spectral analysis of the transformer will reveal insights into its expressive power. Similar studies already established that spectral analysis of Graph neural networks (GNNs) provides extra perspectives on their expressiveness. In this work, we systematically study and establish the link between the spatial and spectral domain in the realm of the transformer. We further provide a theoretical analysis and prove that the spatial attention mechanism in the transformer cannot effectively capture the desired frequency response, thus, inherently limiting its expressiveness in spectral space. Therefore, we propose FeTA, a framework that aims to perform attention over the entire graph spectrum (i.e., actual frequency components of the graphs) analogous to the attention in spatial space. Empirical results suggest that FeTA provides homogeneous performance gain against vanilla transformer across all tasks on standard benchmarks and can easily be extended to GNN-based models with low-pass characteristics (e.g., GAT).},
 author = {Anson Bastos and Abhishek Nadgeri and Kuldeep Singh and Hiroki Kanezashi and Toyotaro Suzumura and Isaiah Onando Mulang'},
 code = {https://github.com/ansonb/FeTA_TMLR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286349576},
 pdf = {https://openreview.net/pdf?id=aRsLetumx1},
 review = {https://openreview.net/forum?id=aRsLetumx1},
 title = {How Expressive are Transformers in Spectral Domain for Graphs?},
 url = {https://openreview.net/forum?id=aRsLetumx1},
 year = {2022}
}

@article{basu2022equivariant,
 abstract = {Equivariance to symmetries has proven to be a powerful inductive bias in deep learning research. Recent works on mesh processing have concentrated on various kinds of natural symmetries, including translations, rotations, scaling, node permutations, and gauge transformations. To date, no existing architecture is equivariant to all of these transformations. In this paper, we present an attention-based architecture for mesh data that is provably equivariant to all transformations mentioned above. Our pipeline relies on the use of relative tangential features: a simple, effective, equivariance-friendly alternative to raw node positions as inputs. Experiments on the FAUST and TOSCA datasets confirm that our proposed architecture achieves improved performance on these benchmarks and is indeed equivariant, and therefore robust, to a wide variety of local/global transformations.},
 author = {Sourya Basu and Jose Gallego-Posada and Francesco Vigan{\`o} and James Rowbottom and Taco Cohen},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/gallego-posada/eman},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4281488743},
 pdf = {https://openreview.net/pdf?id=3IqqJh2Ycy},
 review = {https://openreview.net/forum?id=3IqqJh2Ycy},
 title = {Equivariant Mesh Attention Networks},
 url = {https://openreview.net/forum?id=3IqqJh2Ycy},
 year = {2022}
}

@article{bathla2023ibia,
 abstract = {Exact computation of the partition function is known to be intractable, necessitating approximate inference techniques. Existing methods for approximate inference are slow to converge for many benchmarks. The control of accuracy-complexity trade-off is also non-trivial in many of these methods. We propose a novel incremental build-infer-approximate (IBIA) framework for approximate inference that addresses these issues. In this framework, the probabilistic graphical model is converted into a sequence of clique tree forests (SCTF) with bounded clique sizes. We show that the SCTF can be used to efficiently compute the partition function. We propose two new algorithms which are used to construct the SCTF and prove the correctness of both. The first is an algorithm for incremental construction of CTFs that is guaranteed to give a valid CTF with bounded clique sizes and the second is an approximation algorithm that takes a calibrated CTF as input and yields a valid and calibrated CTF with reduced clique sizes as the output. We have evaluated our method using several benchmark sets from recent UAI competitions and our results show good accuracies with competitive runtimes.},
 author = {Shivani Bathla and Vinita Vasudevan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4365601047},
 pdf = {https://openreview.net/pdf?id=8L7Rh6FIXt},
 review = {https://openreview.net/forum?id=8L7Rh6FIXt},
 title = {IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function},
 url = {https://openreview.net/forum?id=8L7Rh6FIXt},
 year = {2023}
}

@article{baumann2022identifying,
 abstract = {We present a method for automatically identifying the causal structure of a dynamical control system. Through a suitable experiment design and subsequent causal analysis, the method reveals, which state and input variables of the system have a causal influence on each other. The experiment design builds on the concept of controllability, which provides a systematic way to compute input trajectories that steer the system to specific regions in its state space. For the causal analysis, we leverage powerful techniques from causal inference and extend them to control systems. Further, we derive conditions that guarantee discovery of the true causal structure of the system and show that the obtained knowledge of the causal structure reduces the complexity of model learning and yields improved generalization capabilities. Experiments on a robot arm demonstrate reliable causal identification from real-world data and extrapolation to regions outside the training domain.},
 author = {Dominik Baumann and Friedrich Solowjow and Karl Henrik Johansson and Sebastian Trimpe},
 code = {https://github.com/baumanndominik/identifying_causal_structure},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3033402248},
 pdf = {https://openreview.net/pdf?id=X2BodlyLvT},
 review = {https://openreview.net/forum?id=X2BodlyLvT},
 title = {Identifying Causal Structure in Dynamical Systems.},
 url = {https://openreview.net/forum?id=X2BodlyLvT},
 year = {2022}
}

@article{becker2022on,
 abstract = {Improved state space models, such as Recurrent State Space Models (RSSMs), are a key factor behind recent advances in model-based reinforcement learning (RL). Yet, despite their empirical success, many of the underlying design choices are not well understood. We show that RSSMs use a suboptimal inference scheme and that models trained using this inference overestimate the aleatoric uncertainty of the ground truth system. We find this overestimation implicitly regularizes RSSMs and allows them to succeed in model-based RL. We postulate that this implicit regularization fulfills the same functionality as explicitly modeling epistemic uncertainty, which is crucial for many other model-based RL approaches. Yet, overestimating aleatoric uncertainty can also impair performance in cases where accurately estimating it matters, e.g., when we have to deal with occlusions, missing observations, or fusing sensor modalities at different frequencies. Moreover, the implicit regularization is a side-effect of the inference scheme and not the result of a rigorous, principled formulation, which renders analyzing or improving RSSMs difficult. Thus, we propose an alternative approach building on well-understood components for modeling aleatoric and epistemic uncertainty, dubbed Variational Recurrent Kalman Network (VRKN). This approach uses Kalman updates for exact smoothing inference in a latent space and Monte Carlo Dropout to model epistemic uncertainty. Due to the Kalman updates, the VRKN can naturally handle missing observations or sensor fusion problems with varying numbers of observations per time step. Our experiments show that using the VRKN instead of the RSSM improves performance in tasks where appropriately capturing aleatoric uncertainty is crucial while matching it in the deterministic standard benchmarks.},
 author = {Philipp Becker and Gerhard Neumann},
 code = {https://github.com/pbecker93/vrkn},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306809098},
 pdf = {https://openreview.net/pdf?id=UQXdQyoRZh},
 review = {https://openreview.net/forum?id=UQXdQyoRZh},
 title = {On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning},
 url = {https://openreview.net/forum?id=UQXdQyoRZh},
 year = {2022}
}

@article{behjati2023inducing,
 abstract = {Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaningful units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images. We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers. These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction.},
 author = {Melika Behjati and James Henderson},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287348678},
 pdf = {https://openreview.net/pdf?id=m8U9rSs6gU},
 review = {https://openreview.net/forum?id=m8U9rSs6gU},
 title = {Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention},
 url = {https://openreview.net/forum?id=m8U9rSs6gU},
 year = {2023}
}

@article{ben,
 pdf = {https://openreview.net/pdf?id=VcXNAr5Rur},
 review = {https://openreview.net/forum?id=VcXNAr5Rur}
}

@article{ben-iwhiwhu2023lifelong,
 abstract = {Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previously learned masks to exploit previous knowledge when learning new tasks: not only is learning faster, the algorithm solves tasks that we could not otherwise solve from scratch due to extremely sparse rewards. The results suggest that RL with modulating masks is a promising approach to lifelong learning, to the composition of knowledge to learn increasingly complex tasks, and to knowledge reuse for efficient and faster learning.},
 author = {Eseoghene Ben-Iwhiwhu and Saptarshi Nath and Praveen Kumar Pilly and Soheil Kolouri and Andrea Soltoggio},
 code = {https://github.com/dlpbc/mask-lrl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4312108280},
 pdf = {https://openreview.net/pdf?id=V7tahqGrOq},
 review = {https://openreview.net/forum?id=V7tahqGrOq},
 title = {Lifelong Reinforcement Learning with Modulating Masks},
 url = {https://openreview.net/forum?id=V7tahqGrOq},
 year = {2023}
}

@article{ben-shaul2023exploring,
 abstract = {Multiplication layers are a key component in various influential neural network modules, including self-attention and hypernetwork layers. In this paper, we investigate the approximation capabilities of deep neural networks with intermediate neurons connected by simple multiplication operations. We consider two classes of target functions: generalized bandlimited functions, which are frequently used to model real-world signals with finite bandwidth, and Sobolev-Type balls, which are embedded in the Sobolev Space $\mathcal{W}^{r,2}$. Our results demonstrate that multiplicative neural networks can approximate these functions with significantly fewer layers and neurons compared to standard ReLU neural networks, with respect to both input dimension and approximation error. These findings suggest that multiplicative gates can outperform standard feed-forward layers and have potential for improving neural network design.},
 author = {Ido Ben-Shaul and Tomer Galanti and Shai Dekel},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4315882030},
 pdf = {https://openreview.net/pdf?id=sWQJfb2GSk},
 review = {https://openreview.net/forum?id=sWQJfb2GSk},
 title = {Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions},
 url = {https://openreview.net/forum?id=sWQJfb2GSk},
 year = {2023}
}

@article{benjamins2023contextualize,
 author = {Carolin Benjamins and Theresa Eimer and Frederik Schubert and Aditya Mohan and Sebastian D{\"o}hler and Andr{\'e} Biedenkapp and Bodo Rosenhahn and Frank Hutter and Marius Lindauer},
 code = {https://github.com/automl/carl/tree/train},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Y42xVBQusn},
 review = {https://openreview.net/forum?id=Y42xVBQusn},
 title = {Contextualize Me {\textendash} The Case for Context in Reinforcement Learning},
 url = {https://openreview.net/forum?id=Y42xVBQusn},
 year = {2023}
}

@article{benton2024error,
 abstract = {Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDE). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODE) rather than SDE. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions.},
 author = {Joe Benton and George Deligiannidis and Arnaud Doucet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4378718194},
 pdf = {https://openreview.net/pdf?id=uqQPyWFDhY},
 review = {https://openreview.net/forum?id=uqQPyWFDhY},
 title = {Error Bounds for Flow Matching Methods},
 url = {https://openreview.net/forum?id=uqQPyWFDhY},
 year = {2024}
}

@article{berahas2024nonuniform,
 abstract = {The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods.},
 author = {Albert S. Berahas and Lindon Roberts and Fred Roosta},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4388747860},
 pdf = {https://openreview.net/pdf?id=17ESEjETbP},
 review = {https://openreview.net/forum?id=17ESEjETbP},
 title = {Non-Uniform Smoothness for Gradient Descent},
 url = {https://openreview.net/forum?id=17ESEjETbP},
 year = {2024}
}

@article{bergou2023personalized,
 abstract = {In contrast to training traditional machine learning (ML) models in data centers, federated learning (FL) trains ML models over local datasets contained on resource-constrained heterogeneous edge devices. Existing FL algorithms aim to learn a single global model for all participating devices, which may not be helpful to all devices participating in the training due to the heterogeneity of the data across the devices. Recently, Hanzely and Richt\'{a}rik (2020) proposed a new formulation for training personalized FL models aimed at balancing the trade-off between the traditional global model and the local models that could be trained by individual devices using their private data only. They derived a new algorithm, called Loopless Gradient Descent (L2GD), to solve it and showed that this algorithms leads to improved communication complexity guarantees in regimes when more personalization is required. In this paper, we equip their L2GD algorithm with a bidirectional compression mechanism to further reduce the communication bottleneck between the local devices and the server. Unlike other compression-based algorithms used in the FL-setting, our compressed L2GD algorithm operates on a probabilistic communication protocol, where communication does not happen on a fixed schedule. Moreover, our compressed L2GD algorithm maintains a similar convergence rate as vanilla SGD without compression. To empirically validate the efficiency of our algorithm, we perform diverse numerical experiments on both convex and non-convex problems and using various compression techniques.},
 author = {El houcine Bergou and Konstantin Pavlovich Burlachenko and Aritra Dutta and Peter Richt{\'a}rik},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4295681203},
 pdf = {https://openreview.net/pdf?id=dZugyhbNFY},
 review = {https://openreview.net/forum?id=dZugyhbNFY},
 title = {Personalized Federated Learning with Communication Compression},
 url = {https://openreview.net/forum?id=dZugyhbNFY},
 year = {2023}
}

@article{bernhardt2022failure,
 abstract = {Failure detection in automated image classification is a critical safeguard for clinical deployment. Detected failure cases can be referred to human assessment, ensuring patient safety in computer-aided clinical decision making. Despite its paramount importance, there is insufficient evidence about the ability of state-of-the-art confidence scoring methods to detect test-time failures of classification models in the context of medical imaging. This paper provides a reality check, establishing the performance of in-domain misclassification detection methods, benchmarking 9 widely used confidence scores on 6 medical imaging datasets with different imaging modalities, in multiclass and binary classification settings. Our experiments show that the problem of failure detection is far from being solved. We found that none of the benchmarked advanced methods proposed in the computer vision and machine learning literature can consistently outperform a simple softmax baseline, demonstrating that improved out-of-distribution detection or model calibration do not necessarily translate to improved in-domain misclassification detection. Our developed testbed facilitates future work in this important area},
 author = {M{\'e}lanie Bernhardt and Fabio De Sousa Ribeiro and Ben Glocker},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320458345},
 pdf = {https://openreview.net/pdf?id=VBHuLfnOMf},
 review = {https://openreview.net/forum?id=VBHuLfnOMf},
 title = {Failure Detection in Medical Image Classification: A Reality Check and Benchmarking Testbed},
 url = {https://openreview.net/forum?id=VBHuLfnOMf},
 year = {2022}
}

@article{bie2023private,
 author = {Alex Bie and Gautam Kamath and Guojun Zhang},
 badge = {Survey},
 code = {https://github.com/alexbie98/dpgan-revisit},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 pdf = {https://openreview.net/pdf?id=9sVCIngrhP},
 review = {https://openreview.net/forum?id=9sVCIngrhP},
 title = {Private {GAN}s, Revisited},
 url = {https://openreview.net/forum?id=9sVCIngrhP},
 year = {2023}
}

@article{blancard2024statistical,
 abstract = {Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it performs surprisingly well in terms of peak signal-to-noise ratio on full signal reconstruction. In comparison, representation 2) appears less suitable for image denoising. Finally, we extend this method by introducing a diffusive stepwise algorithm which gives a new perspective to the initial method and leads to promising results for image denoising under specific circumstances.},
 author = {Bruno R{\'e}galdo-Saint Blancard and Michael Eickenberg},
 code = {https://github.com/bregaldo/stat_comp_sep},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4382490769},
 pdf = {https://openreview.net/pdf?id=OUWG6O4yo9},
 review = {https://openreview.net/forum?id=OUWG6O4yo9},
 title = {Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures},
 url = {https://openreview.net/forum?id=OUWG6O4yo9},
 year = {2024}
}

@article{bleeker2023reducing,
 abstract = {To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encoder from suppressing predictive features. We implement the LTD objective as an optimization constraint, to ensure that the reconstruction loss is below a bound value while primarily optimizing for the contrastive loss. Importantly, LTD does not depend on additional training data or expensive (hard) negative mining strategies. Our experiments show that, unlike reconstructing the input caption in the input space, LTD reduces predictive feature suppression, measured by obtaining higher recall@k, r-precision, and nDCG scores than a contrastive ICR baseline. Moreover, we show that LTD should be implemented as an optimization constraint instead of a dual optimization objective. Finally, we show that LTD can be used with different contrastive learning losses and a wide variety of resource-constrained ICR methods.},
 author = {Maurits Bleeker and Andrew Yates and Maarten de Rijke},
 code = {https://github.com/MauritsBleeker/reducing-predictive-feature-suppression},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4225153330},
 pdf = {https://openreview.net/pdf?id=T1XtOqrVKn},
 review = {https://openreview.net/forum?id=T1XtOqrVKn},
 title = {Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval},
 url = {https://openreview.net/forum?id=T1XtOqrVKn},
 year = {2023}
}

@article{boehm2022probabilistic,
 abstract = {Principal Component Analysis (PCA) minimizes the reconstruction error given a class of linear models of fixed component dimensionality. Probabilistic PCA adds a probabilistic structure by learning the probability distribution of the PCA latent space weights, thus creating a generative model. Autoencoders (AE) minimize the reconstruction error in a class of nonlinear models of fixed latent space dimensionality and outperform PCA at fixed dimensionality. Here, we introduce the Probabilistic Autoencoder (PAE) that learns the probability distribution of the AE latent space weights using a normalizing flow (NF). The PAE is fast and easy to train and achieves small reconstruction errors, high sample quality, and good performance in downstream tasks. We compare the PAE to Variational AE (VAE), showing that the PAE trains faster, reaches a lower reconstruction error, and produces good sample quality without requiring special tuning parameters or training procedures. We further demonstrate that the PAE is a powerful model for performing the downstream tasks of probabilistic image reconstruction in the context of Bayesian inference of inverse problems for inpainting and denoising applications. Finally, we identify latent space density from NF as a promising outlier detection metric.},
 author = {Vanessa M Boehm and Uros Seljak},
 code = {https://github.com/VMBoehm/PAE-ablation},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296562951},
 pdf = {https://openreview.net/pdf?id=AEoYjvjKVA},
 review = {https://openreview.net/forum?id=AEoYjvjKVA},
 title = {Probabilistic Autoencoder},
 url = {https://openreview.net/forum?id=AEoYjvjKVA},
 year = {2022}
}

@article{bohdal2023metacalibration,
 abstract = {Calibration of neural networks is a topical problem that is becoming more and more important as neural networks increasingly underpin real-world applications. The problem is especially noticeable when using modern neural networks, for which there is a significant difference between the confidence of the model and the probability of correct prediction. Various strategies have been proposed to improve calibration, yet accurate calibration remains challenging. We propose a novel framework with two contributions: introducing a new differentiable surrogate for expected calibration error (DECE) that allows calibration quality to be directly optimised, and a meta-learning framework that uses DECE to optimise for validation set calibration with respect to model hyper-parameters. The results show that we achieve competitive performance with existing calibration approaches. Our framework opens up a new avenue and toolset for tackling calibration, which we believe will inspire further work on this important challenge.},
 author = {Ondrej Bohdal and Yongxin Yang and Timothy Hospedales},
 code = {https://github.com/ondrejbohdal/meta-calibration},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4285787685},
 pdf = {https://openreview.net/pdf?id=R2hUure38l},
 review = {https://openreview.net/forum?id=R2hUure38l},
 title = {Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error},
 url = {https://openreview.net/forum?id=R2hUure38l},
 year = {2023}
}

@article{boix-adser,
 pdf = {https://openreview.net/pdf?id=qM7JPBYROr},
 review = {https://openreview.net/forum?id=qM7JPBYROr}
}

@article{boldt2024a,
 author = {Brendon Boldt and David R Mortensen},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=jesKcQxQ7j},
 review = {https://openreview.net/forum?id=jesKcQxQ7j},
 title = {A Review of the Applications of Deep Learning-Based Emergent Communication},
 url = {https://openreview.net/forum?id=jesKcQxQ7j},
 year = {2024}
}

@article{bolland2023policy,
 abstract = {Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of the policy.},
 author = {Adrien Bolland and Gilles Louppe and Damien Ernst},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4376312377},
 pdf = {https://openreview.net/pdf?id=3Ba6Hd3nZt},
 review = {https://openreview.net/forum?id=3Ba6Hd3nZt},
 title = {Policy Gradient Algorithms Implicitly Optimize by Continuation},
 url = {https://openreview.net/forum?id=3Ba6Hd3nZt},
 year = {2023}
}

@article{bonet2022efficient,
 abstract = {Minimizing functionals in the space of probability distributions can be done with Wasserstein gradient flows. To solve them numerically, a possible approach is to rely on the Jordan-Kinderlehrer-Otto (JKO) scheme which is analogous to the proximal scheme in Euclidean spaces. However, it requires solving a nested optimization problem at each iteration, and is known for its computational challenges, especially in high dimension. To alleviate it, very recent works propose to approximate the JKO scheme leveraging Brenier's theorem, and using gradients of Input Convex Neural Networks to parameterize the density (JKO-ICNN). However, this method comes with a high computational cost and stability issues. Instead, this work proposes to use gradient flows in the space of probability measures endowed with the sliced-Wasserstein (SW) distance. We argue that this method is more flexible than JKO-ICNN, since SW enjoys a closed-form differentiable approximation. Thus, the density at each step can be parameterized by any generative model which alleviates the computational burden and makes it tractable in higher dimensions.},
 author = {Cl{\'e}ment Bonet and Nicolas Courty and Fran{\c{c}}ois Septier and Lucas Drumetz},
 code = {https://github.com/clbonet/Sliced-Wasserstein_Gradient_Flows},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226145743},
 pdf = {https://openreview.net/pdf?id=Au1LNKmRvh},
 review = {https://openreview.net/forum?id=Au1LNKmRvh},
 title = {Efficient Gradient Flows in Sliced-Wasserstein Space},
 url = {https://openreview.net/forum?id=Au1LNKmRvh},
 year = {2022}
}

@article{bonnaerens2023learned,
 abstract = {Vision transformers have demonstrated remarkable success in a wide range of computer vision tasks over the last years. However, their high computational costs remain a significant barrier to their practical deployment. In particular, the complexity of transformer models is quadratic with respect to the number of input tokens. Therefore techniques that reduce the number of input tokens that need to be processed have been proposed. This paper introduces Learned Thresholds token Merging and Pruning (LTMP), a novel approach that leverages the strengths of both token merging and token pruning. LTMP uses learned threshold masking modules that dynamically determine which tokens to merge and which to prune. We demonstrate our approach with extensive experiments on vision transformers on the ImageNet classification task. Our results demonstrate that LTMP achieves state-of-the-art accuracy across reduction rates while requiring only a single fine-tuning epoch, which is an order of magnitude faster than previous methods. Code is available at https://github.com/Mxbonn/ltmp .},
 author = {Maxim Bonnaerens and Joni Dambre},
 code = {https://github.com/Mxbonn/ltmp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385004455},
 pdf = {https://openreview.net/pdf?id=WYKTCKpImz},
 review = {https://openreview.net/forum?id=WYKTCKpImz},
 title = {Learned Thresholds Token Merging and Pruning for Vision Transformers},
 url = {https://openreview.net/forum?id=WYKTCKpImz},
 year = {2023}
}

@article{bordes2022high,
 abstract = {Discovering what is learned by neural networks remains a challenge. In self-supervised learning, classification is the most common task used to evaluate how good a representation is. However, relying only on such downstream task can limit our understanding of what information is retained in the representation of a given input. In this work, we showcase the use of a Representation Conditional Diffusion Model (RCDM) to visualize in data space the representations learned by self-supervised models. The use of RCDM is motivated by its ability to generate high-quality samples -- on par with state-of-the-art generative models -- while ensuring that the representations of those samples are faithful i.e. close to the one used for conditioning. By using RCDM to analyze self-supervised models, we are able to clearly show visually that i) SSL (backbone) representation are not invariant to the data augmentations they were trained with -- thus debunking an often restated but mistaken belief; ii) SSL post-projector embeddings appear indeed invariant to these data augmentation, along with many other data symmetries; iii) SSL representations appear more robust to small adversarial perturbation of their inputs than representations trained in a supervised manner; and iv) that SSL-trained representations exhibit an inherent structure that can be explored thanks to RCDM visualization and enables image manipulation.},
 author = {Florian Bordes and Randall Balestriero and Pascal Vincent},
 code = {https://github.com/facebookresearch/RCDM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226253083},
 pdf = {https://openreview.net/pdf?id=urfWb7VjmL},
 review = {https://openreview.net/forum?id=urfWb7VjmL},
 title = {High Fidelity Visualization of What Your Self-Supervised Representation Knows About},
 url = {https://openreview.net/forum?id=urfWb7VjmL},
 year = {2022}
}

@article{bordes2023guillotine,
 abstract = {One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few projector layers entirely removed. This trick of throwing away the projector is actually critical for SSL methods to display competitive performances on ImageNet for which more than 30 percentage points can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the SSL criterion during training (the last projector layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a generically applicable method that has been used to improve generalization performance in transfer learning scenarios. In this work, we identify the underlying reasons behind its success and show that the optimal layer to use might change significantly depending on the training setup, the data or the downstream task. Lastly, we give some insights on how to reduce the need for a projector in SSL by aligning the pretext SSL task and the downstream task.},
 author = {Florian Bordes and Randall Balestriero and Quentin Garrido and Adrien Bardes and Pascal Vincent},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283687789},
 pdf = {https://openreview.net/pdf?id=ZgXfXSz51n},
 review = {https://openreview.net/forum?id=ZgXfXSz51n},
 title = {Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning},
 url = {https://openreview.net/forum?id=ZgXfXSz51n},
 year = {2023}
}

@article{bordino2022infinitely,
 abstract = {There is a growing literature on the study of large-width properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed parameters or weights, and Gaussian stochastic processes. Motivated by some empirical and theoretical studies showing the potential of replacing Gaussian distributions with Stable distributions, namely distributions with heavy tails, in this paper we investigate large-width properties of deep Stable NNs, i.e. deep NNs with Stable-distributed parameters. For sub-linear activation functions, a recent work has characterized the infinitely wide limit of a suitable rescaled deep Stable NN in terms of a Stable stochastic process, both under the assumption of a ``joint growth" and under the assumption of a ``sequential growth" of the width over the NN's layers. Here, assuming a ``sequential growth" of the width, we extend such a characterization to a general class of activation functions, which includes sub-linear, asymptotically linear and super-linear functions. As a novelty with respect to previous works, our results rely on the use of a generalized central limit theorem for heavy tails distributions, which allows for an interesting unified treatment of infinitely wide limits for deep Stable NNs. Our study shows that the scaling of Stable NNs and the stability of their infinitely wide limits may depend on the choice of the activation function, bringing out a critical difference with respect to the Gaussian setting.},
 author = {Alberto Bordino and Stefano Favaro and Sandra Fortini},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4364382896},
 pdf = {https://openreview.net/pdf?id=A5tIluhDW6},
 review = {https://openreview.net/forum?id=A5tIluhDW6},
 title = {Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions},
 url = {https://openreview.net/forum?id=A5tIluhDW6},
 year = {2022}
}

@article{bose2022controllable,
 author = {Joey Bose and Ricardo Pio Monti and Aditya Grover},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Z44YAcLaGw},
 review = {https://openreview.net/forum?id=Z44YAcLaGw},
 title = {Controllable Generative Modeling via Causal Reasoning},
 url = {https://openreview.net/forum?id=Z44YAcLaGw},
 year = {2022}
}

@article{bose2023beyond,
 author = {Shirsha Bose and Ankit Jha and Hitesh Kandala and Biplab Banerjee},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=jpZmhiIys1},
 review = {https://openreview.net/forum?id=jpZmhiIys1},
 title = {Beyond Boundaries: A Novel Data-Augmentation Discourse for Open Domain Generalization},
 url = {https://openreview.net/forum?id=jpZmhiIys1},
 year = {2023}
}

@article{bosser2023on,
 abstract = {Temporal Point Processes (TPPs) serve as the standard mathematical framework for modeling asynchronous event sequences in continuous time. However, classical TPP models are often constrained by strong assumptions, limiting their ability to capture complex real-world event dynamics. To overcome this limitation, researchers have proposed Neural TPPs, which leverage neural network parametrizations to offer more flexible and efficient modeling. While recent studies demonstrate the effectiveness of Neural TPPs, they often lack a unified setup, relying on different baselines, datasets, and experimental configurations. This makes it challenging to identify the key factors driving improvements in predictive accuracy, hindering research progress. To bridge this gap, we present a comprehensive large-scale experimental study that systematically evaluates the predictive accuracy of state-of-the-art neural TPP models. Our study encompasses multiple real-world and synthetic event sequence datasets, following a carefully designed unified setup. We thoroughly investigate the influence of major architectural components such as event encoding, history encoder, and decoder parametrization on both time and mark prediction tasks. Additionally, we delve into the less explored area of probabilistic calibration for neural TPP models. By analyzing our results, we draw insightful conclusions regarding the significance of history size and the impact of architectural components on predictive accuracy. Furthermore, we shed light on the miscalibration of mark distributions in neural TPP models. Our study aims to provide valuable insights into the performance and characteristics of neural TPP models, contributing to a better understanding of their strengths and limitations.},
 author = {Tanguy Bosser and Souhaib Ben Taieb},
 badge = {Survey},
 code = {https://github.com/tanguybosser/ntpp-tmlr2023},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4382766410},
 pdf = {https://openreview.net/pdf?id=3OSISBQPrM},
 review = {https://openreview.net/forum?id=3OSISBQPrM},
 title = {On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data},
 url = {https://openreview.net/forum?id=3OSISBQPrM},
 year = {2023}
}

@article{bousmalis2024robocat,
 abstract = {The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100-1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.},
 author = {Konstantinos Bousmalis and Giulia Vezzani and Dushyant Rao and Coline Manon Devin and Alex X. Lee and Maria Bauza Villalonga and Todor Davchev and Yuxiang Zhou and Agrim Gupta and Akhil Raju and Antoine Laurens and Claudio Fantacci and Valentin Dalibard and Martina Zambelli and Murilo Fernandes Martins and Rugile Pevceviciute and Michiel Blokzijl and Misha Denil and Nathan Batchelor and Thomas Lampe and Emilio Parisotto and Konrad Zolna and Scott Reed and Sergio G{\'o}mez Colmenarejo and Jonathan Scholz and Abbas Abdolmaleki and Oliver Groth and Jean-Baptiste Regli and Oleg Sushkov and Thomas Roth{\"o}rl and Jose Enrique Chen and Yusuf Aytar and David Barker and Joy Ortiz and Martin Riedmiller and Jost Tobias Springenberg and Raia Hadsell and Francesco Nori and Nicolas Heess},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4381586832},
 pdf = {https://openreview.net/pdf?id=vsCpILiWHu},
 review = {https://openreview.net/forum?id=vsCpILiWHu},
 title = {RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation},
 url = {https://openreview.net/forum?id=vsCpILiWHu},
 year = {2024}
}

@article{boytsov2024inparslight,
 abstract = {We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/},
 author = {Leonid Boytsov and Preksha Patel and Vivek Sourabh and Riddhi Nisar and Sayani Kundu and Ramya Ramanathan and Eric Nyberg},
 badge = {Reproducibility},
 code = {https://github.com/searchivarius/inpars_light},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4315588641},
 pdf = {https://openreview.net/pdf?id=sHSKFYyINO},
 review = {https://openreview.net/forum?id=sHSKFYyINO},
 title = {InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers},
 url = {https://openreview.net/forum?id=sHSKFYyINO},
 year = {2024}
}

@article{breitholtz2022practicality,
 abstract = {Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment. However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions. In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain shift. To overcome this and arrive at the tightest possible results, we combine each bound with recent data-dependent PAC-Bayes analysis, greatly improving the guarantees. We find that, when domain overlap can be assumed, a simple importance weighting extension of previous work provides the tightest estimable bound. Finally, we study which terms dominate the bounds and identify possible directions for further improvement.},
 author = {Adam Breitholtz and Fredrik Daniel Johansson},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/Healthy-AI/pacbayes_bounds},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4327673270},
 pdf = {https://openreview.net/pdf?id=vUuHPRrWs2},
 review = {https://openreview.net/forum?id=vUuHPRrWs2},
 title = {Practicality of generalization guarantees for unsupervised domain adaptation with neural networks},
 url = {https://openreview.net/forum?id=vUuHPRrWs2},
 year = {2022}
}

@article{brekelmans2022your,
 abstract = {Policy regularization methods such as maximum entropy regularization are widely used in reinforcement learning to improve the robustness of a learned policy. In this paper, we show how this robustness arises from hedging against worst-case perturbations of the reward function, which are chosen from a limited set by an imagined adversary. Using convex duality, we characterize this robust set of adversarial reward perturbations under KL and alpha-divergence regularization, which includes Shannon and Tsallis entropy regularization as special cases. Importantly, generalization guarantees can be given within this robust set. We provide detailed discussion of the worst-case reward perturbations, and present intuitive empirical examples to illustrate this robustness and its relationship with generalization. Finally, we discuss how our analysis complements and extends previous results on adversarial reward robustness and path consistency optimality conditions.},
 author = {Rob Brekelmans and Tim Genewein and Jordi Grau-Moya and Gregoire Detetang and Markus Kunesch and Shane Legg and Pedro A Ortega},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4225948724},
 pdf = {https://openreview.net/pdf?id=berNQMTYWZ},
 review = {https://openreview.net/forum?id=berNQMTYWZ},
 title = {Your Policy Regularizer is Secretly an Adversary},
 url = {https://openreview.net/forum?id=berNQMTYWZ},
 year = {2022}
}

@article{brellmann2023fourier,
 author = {David Brellmann and David Filliat and Goran Frehse},
 code = {https://github.com/DavidBrellmann/Fourier_Features_in_RL_with_NN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4393028861},
 pdf = {https://openreview.net/pdf?id=LWotmCKC6Y},
 review = {https://openreview.net/forum?id=LWotmCKC6Y},
 title = {Fourier Features in Reinforcement Learning with Neural Networks},
 url = {https://openreview.net/forum?id=LWotmCKC6Y},
 year = {2023}
}

@article{brown2024biasvariance,
 author = {Gavin Brown and Riccardo Ali},
 code = {https://github.com/profgavinbrown/ondecompositions},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4TnFbv16hK},
 review = {https://openreview.net/forum?id=4TnFbv16hK},
 title = {Bias/Variance is not the same as Approximation/Estimation},
 url = {https://openreview.net/forum?id=4TnFbv16hK},
 year = {2024}
}

@article{bu2023differentially,
 abstract = {Machine learning models have shone in a variety of domains and attracted increasing attention from both the security and the privacy communities. One important yet worrying question is: Will training models under the differential privacy (DP) constraint have an unfavorable impact on their adversarial robustness? While previous works have postulated that privacy comes at the cost of worse robustness, we give the first theoretical analysis to show that DP models can indeed be robust and accurate, even sometimes more robust than their naturally-trained non-private counterparts. We observe three key factors that influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP optimizers are critical; (2) pre-training on public data significantly mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a difference. With these factors set properly, we achieve 90\% natural accuracy, 72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$ attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with $\epsilon=2$. In fact, we show both theoretically and empirically that DP models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the robustness of DP models is consistently observed across various datasets and models. We believe our encouraging results are a significant step towards training models that are private as well as robust.},
 author = {Zhiqi Bu and Yuan Zhang},
 code = {https://github.com/woodyx218/private_vision/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320855482},
 pdf = {https://openreview.net/pdf?id=o8VgRNYh6n},
 review = {https://openreview.net/forum?id=o8VgRNYh6n},
 title = {Differentially Private Optimizers Can Learn Adversarially Robust Models},
 url = {https://openreview.net/forum?id=o8VgRNYh6n},
 year = {2023}
}

@article{bu2023on,
 abstract = {Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration. Furthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and similar accuracy, but are significantly more calibrated. Our code can be found at https://github.com/woodyx218/opacus_global_clipping.},
 author = {Zhiqi Bu and Hua Wang and Zongyu Dai and Qi Long},
 code = {https://github.com/woodyx218/opacus_global_clipping},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3205413540},
 pdf = {https://openreview.net/pdf?id=K0CAGgjYS1},
 review = {https://openreview.net/forum?id=K0CAGgjYS1},
 title = {On the Convergence and Calibration of Deep Learning with Differential Privacy},
 url = {https://openreview.net/forum?id=K0CAGgjYS1},
 year = {2023}
}

@article{buchholz2023some,
 author = {Simon Buchholz},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=REtKapdkyI},
 review = {https://openreview.net/forum?id=REtKapdkyI},
 title = {Some Remarks on Identifiability of Independent Component Analysis in Restricted Function Classes},
 url = {https://openreview.net/forum?id=REtKapdkyI},
 year = {2023}
}

@article{bui2023generating,
 abstract = {Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to address this issue, in the context where we can explicitly define the goal achievement for a task. Our principle is to only maintain the goal-achieved tasks, while letting the optimizer spend more effort on improving the goal-unachieved tasks. We conduct comprehensive experiments for our Task Oriented MOO on various adversarial example generation schemes. The experimental results firmly demonstrate the merit of our proposed approach. Our code is available at \url{https://github.com/tuananhbui89/TAMOO}.},
 author = {Anh Tuan Bui and Trung Le and He Zhao and Quan Hung Tran and Paul Montague and Dinh Phung},
 code = {https://github.com/tuananhbui89/TAMOO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4367189721},
 pdf = {https://openreview.net/pdf?id=2f81Q622ww},
 review = {https://openreview.net/forum?id=2f81Q622ww},
 title = {Generating Adversarial Examples with Task Oriented Multi-Objective Optimization},
 url = {https://openreview.net/forum?id=2f81Q622ww},
 year = {2023}
}

@article{buisson-fenet2023recognition,
 author = {Mona Buisson-Fenet and Valery Morgenthaler and Sebastian Trimpe and Florent Di Meglio},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LTAdaRM29K},
 review = {https://openreview.net/forum?id=LTAdaRM29K},
 title = {Recognition Models to Learn Dynamics from Partial Observations with Neural {ODE}s},
 url = {https://openreview.net/forum?id=LTAdaRM29K},
 year = {2023}
}

@article{bumin2022stochastic,
 author = {Aysegul Bumin and Kejun Huang},
 code = {https://github.com/aysegulbumin/SDRS-minibatch},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=uvDD9rN6Zz},
 review = {https://openreview.net/forum?id=uvDD9rN6Zz},
 title = {Stochastic Douglas-Rachford Splitting for Regularized Empirical Risk Minimization: Convergence, Mini-batch, and Implementation},
 url = {https://openreview.net/forum?id=uvDD9rN6Zz},
 year = {2022}
}

@article{burg2023image,
 abstract = {Many approaches have been proposed to use diffusion models to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large datasets, often with noisy annotations, and it remains an open question to which extent these models contribute to downstream classification performance. In particular, it remains unclear if they generalize enough to improve over directly using the additional data of their pre-training process for augmentation. We systematically evaluate a range of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. Personalizing diffusion models towards the target data outperforms simpler prompting strategies. However, using the pre-training data of the diffusion model alone, via a simple nearest-neighbor retrieval procedure, leads to even stronger downstream performance. Our study explores the potential of diffusion models in generating new training data, and surprisingly finds that these sophisticated models are not yet able to beat a simple and strong image retrieval baseline on simple downstream vision tasks.},
 author = {Max F Burg and Florian Wenzel and Dominik Zietlow and Max Horn and Osama Makansi and Francesco Locatello and Chris Russell},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4366766679},
 pdf = {https://openreview.net/pdf?id=xflYdGZMpv},
 review = {https://openreview.net/forum?id=xflYdGZMpv},
 title = {Image retrieval outperforms diffusion models on data augmentation},
 url = {https://openreview.net/forum?id=xflYdGZMpv},
 year = {2023}
}

@article{burns2023detecting,
 author = {Thomas F Burns and Robert Tang},
 code = {https://github.com/tfburns/State-Complexes-of-Gridworlds},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=t4p612DftO},
 review = {https://openreview.net/forum?id=t4p612DftO},
 title = {Detecting danger in gridworlds using Gromov{\textquoteright}s Link Condition},
 url = {https://openreview.net/forum?id=t4p612DftO},
 year = {2023}
}

@article{burroni2023ustatistics,
 abstract = {We propose the use of U-statistics to reduce variance for gradient estimation in importance-weighted variational inference. The key observation is that, given a base gradient estimator that requires $m > 1$ samples and a total of $n > m$ samples to be used for estimation, lower variance is achieved by averaging the base estimator on overlapping batches of size $m$ than disjoint batches, as currently done. We use classical U-statistic theory to analyze the variance reduction, and propose novel approximations with theoretical guarantees to ensure computational efficiency. We find empirically that U-statistic variance reduction can lead to modest to significant improvements in inference performance on a range of models, with little computational cost.},
 author = {Javier Burroni and Kenta Takatsu and Justin Domke and Daniel Sheldon},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4322717963},
 pdf = {https://openreview.net/pdf?id=oXmwAPlbVw},
 review = {https://openreview.net/forum?id=oXmwAPlbVw},
 title = {U-Statistics for Importance-Weighted Variational Inference},
 url = {https://openreview.net/forum?id=oXmwAPlbVw},
 year = {2023}
}

@article{bykov2023dora,
 abstract = {Deep Neural Networks (DNNs) excel at learning complex abstractions within their internal representations. However, the concepts they learn remain opaque, a problem that becomes particularly acute when models unintentionally learn spurious correlations. In this work, we present DORA (Data-agnOstic Representation Analysis), the first data-agnostic framework for analyzing the representational space of DNNs. Central to our framework is the proposed Extreme-Activation (EA) distance measure, which assesses similarities between representations by analyzing their activation patterns on data points that cause the highest level of activation. As spurious correlations often manifest in features of data that are anomalous to the desired task, such as watermarks or artifacts, we demonstrate that internal representations capable of detecting such artifactual concepts can be found by analyzing relationships within neural representations. We validate the EA metric quantitatively, demonstrating its effectiveness both in controlled scenarios and real-world applications. Finally, we provide practical examples from popular Computer Vision models to illustrate that representations identified as outliers using the EA metric often correspond to undesired and spurious concepts.},
 author = {Kirill Bykov and Mayukh Deb and Dennis Grinwald and Klaus Robert Muller and Marina MC H{\"o}hne},
 code = {https://github.com/lapalap/dora},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281874990},
 pdf = {https://openreview.net/pdf?id=nfYwRIezvg},
 review = {https://openreview.net/forum?id=nfYwRIezvg},
 title = {DORA: Exploring Outlier Representations in Deep Neural Networks},
 url = {https://openreview.net/forum?id=nfYwRIezvg},
 year = {2023}
}

@article{cabrera,
 pdf = {https://openreview.net/pdf?id=VrvGHDSzZ7},
 review = {https://openreview.net/forum?id=VrvGHDSzZ7}
}

@article{cai2023on,
 abstract = {In this paper, we study error bounds for {\em Bayesian quadrature} (BQ), with an emphasis on noisy settings, randomized algorithms, and average-case performance measures. We seek to approximate the integral of functions in a {\em Reproducing Kernel Hilbert Space} (RKHS), particularly focusing on the Mat\'ern-$\nu$ and squared exponential (SE) kernels, with samples from the function potentially being corrupted by Gaussian noise. We provide a two-step meta-algorithm that serves as a general tool for relating the average-case quadrature error with the $L^2$-function approximation error. When specialized to the Mat\'ern kernel, we recover an existing near-optimal error rate while avoiding the existing method of repeatedly sampling points. When specialized to other settings, we obtain new average-case results for settings including the SE kernel with noise and the Mat\'ern kernel with misspecification. Finally, we present algorithm-independent lower bounds that have greater generality and/or give distinct proofs compared to existing ones.},
 author = {Xu Cai and Thanh Lam and Jonathan Scarlett},
 code = {https://github.com/caitree/Kernelized-Bayesian-Quadrature},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221157887},
 pdf = {https://openreview.net/pdf?id=JJrKbq35l4},
 review = {https://openreview.net/forum?id=JJrKbq35l4},
 title = {On Average-Case Error Bounds for Kernel-Based Bayesian Quadrature},
 url = {https://openreview.net/forum?id=JJrKbq35l4},
 year = {2023}
}

@article{canatar2023bandwidth,
 abstract = {Quantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of the bandwidth can take a model from provably not being able to generalize to any target function to good generalization for well-aligned targets. Our analysis shows how the bandwidth controls the spectrum of the kernel integral operator and thereby the inductive bias of the model. We demonstrate empirically that our theory correctly predicts how varying the bandwidth affects generalization of quantum models on challenging datasets, including those far outside our theoretical assumptions. We discuss the implications of our results for quantum advantage in machine learning.},
 author = {Abdulkadir Canatar and Evan Peters and Cengiz Pehlevan and Stefan M. Wild and Ruslan Shaydulin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283020809},
 pdf = {https://openreview.net/pdf?id=A1N2qp4yAq},
 review = {https://openreview.net/forum?id=A1N2qp4yAq},
 title = {Bandwidth Enables Generalization in Quantum Kernel Models},
 url = {https://openreview.net/forum?id=A1N2qp4yAq},
 year = {2023}
}

@article{cao2022mvsformer,
 author = {Chenjie Cao and Xinlin Ren and Yanwei Fu},
 code = {https://github.com/ewrfcas/MVSFormer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=2VWR6JfwNo},
 review = {https://openreview.net/forum?id=2VWR6JfwNo},
 title = {{MVSF}ormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth},
 url = {https://openreview.net/forum?id=2VWR6JfwNo},
 year = {2022}
}

@article{caron2022counterfactual,
 abstract = {In this paper, we address the challenge of performing counterfactual inference with observational data via Bayesian nonparametric regression adjustment, with a focus on high-dimensional settings featuring multiple actions and multiple correlated outcomes. We present a general class of counterfactual multi-task deep kernels models that estimate causal effects and learn policies proficiently thanks to their sample efficiency gains, while scaling well with high dimensions. In the first part of the work, we rely on Structural Causal Models (SCM) to formally introduce the setup and the problem of identifying counterfactual quantities under observed confounding. We then discuss the benefits of tackling the task of causal effects estimation via stacked coregionalized Gaussian Processes and Deep Kernels. Finally, we demonstrate the use of the proposed methods on simulated experiments that span individual causal effects estimation, off-policy evaluation and optimization.},
 author = {Alberto Caron and Ioanna Manolopoulou and Gianluca Baio},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309801498},
 pdf = {https://openreview.net/pdf?id=iGREAJdULX},
 review = {https://openreview.net/forum?id=iGREAJdULX},
 title = {Counterfactual Learning with Multioutput Deep Kernels},
 url = {https://openreview.net/forum?id=iGREAJdULX},
 year = {2022}
}

@article{carvalho2023featureattending,
 abstract = {Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce "Feature-Attending Recurrent Modules" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generalizes compared to competing architectures that leverage attention or multiple modules.},
 author = {Wilka Torrico Carvalho and Andrew Kyle Lampinen and Kyriacos Nikiforou and Felix Hill and Murray Shanahan},
 code = {https://github.com/wcarvalho/farm},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4225550826},
 pdf = {https://openreview.net/pdf?id=j4y3gN7VtW},
 review = {https://openreview.net/forum?id=j4y3gN7VtW},
 title = {Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning},
 url = {https://openreview.net/forum?id=j4y3gN7VtW},
 year = {2023}
}

@article{casper2023open,
 abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
 author = {Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and J{\'e}r{\'e}my Scheurer and Javier Rando and Rachel Freedman and Tomasz Korbak and David Lindner and Pedro Freire and Tony Tong Wang and Samuel Marks and Charbel-Raphael Segerie and Micah Carroll and Andi Peng and Phillip Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem Biyik and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4385436553},
 pdf = {https://openreview.net/pdf?id=bx24KpJ4Eb},
 review = {https://openreview.net/forum?id=bx24KpJ4Eb},
 title = {Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
 url = {https://openreview.net/forum?id=bx24KpJ4Eb},
 year = {2023}
}

@article{castro2023a,
 abstract = {Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.},
 author = {Pablo Samuel Castro and Tyler Kastner and Prakash Panangaden and Mark Rowland},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/google-research/google-research/tree/master/ksme},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4388184262},
 pdf = {https://openreview.net/pdf?id=nHfPXl1ly7},
 review = {https://openreview.net/forum?id=nHfPXl1ly7},
 title = {A Kernel Perspective on Behavioural Metrics for Markov Decision Processes},
 url = {https://openreview.net/forum?id=nHfPXl1ly7},
 year = {2023}
}

@article{cesa-bianchi2022multitask,
 abstract = {We introduce and analyze MT-OMD, a multitask generalization of Online Mirror Descent (OMD) which operates by sharing updates between tasks. We prove that the regret of MT-OMD is of order $\sqrt{1 + \sigma^2(N-1)}\sqrt{T}$, where $\sigma^2$ is the task variance according to the geometry induced by the regularizer, $N$ is the number of tasks, and $T$ is the time horizon. Whenever tasks are similar, that is, $\sigma^2 \le 1$, this improves upon the $\sqrt{NT}$ bound obtained by running independent OMDs on each task. Our multitask extensions of Online Gradient Descent and Exponentiated Gradient, two important instances of OMD, are shown to enjoy closed-form updates, making them easy to use in practice. Finally, we provide numerical experiments on four real-world datasets which support our theoretical findings.},
 author = {Nicol{\`o} Cesa-Bianchi and Pierre Laforgue and Andrea Paudice and massimiliano pontil},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3170706428},
 pdf = {https://openreview.net/pdf?id=zwRX9kkKzj},
 review = {https://openreview.net/forum?id=zwRX9kkKzj},
 title = {Multitask Online Mirror Descent.},
 url = {https://openreview.net/forum?id=zwRX9kkKzj},
 year = {2022}
}

@article{chakrabarty2023santa,
 author = {Goirik Chakrabarty and Manogna Sreenivas and Soma Biswas},
 code = {https://github.com/goirik-chakrabarty/SANTA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=V7guVYzvE4},
 review = {https://openreview.net/forum?id=V7guVYzvE4},
 title = {{SANTA}: Source Anchoring Network and Target Alignment for Continual Test Time Adaptation},
 url = {https://openreview.net/forum?id=V7guVYzvE4},
 year = {2023}
}

@article{chang2022on,
 abstract = {It has been shown that learning audiovisual features can lead to improved speech recognition performance over audio-only features, especially for noisy speech. However, in many common applications, the visual features are partially or entirely missing, e.g.~the speaker might move off screen. Multi-modal models need to be robust: missing video frames should not degrade the performance of an audiovisual model to be worse than that of a single-modality audio-only model. While there have been many attempts at building robust models, there is little consensus on how robustness should be evaluated. To address this, we introduce a framework that allows claims about robustness to be evaluated in a precise and testable way. We also conduct a systematic empirical study of the robustness of common audiovisual speech recognition architectures on a range of acoustic noise conditions and test suites. Finally, we show that an architecture-agnostic solution based on cascades can consistently achieve robustness to missing video, even in settings where existing techniques for robustness like dropout fall short.},
 author = {Oscar Chang and Otavio Braga and Hank Liao and Dmitriy Serdyuk and Olivier Siohan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4390040710},
 pdf = {https://openreview.net/pdf?id=fXorxxbDvO},
 review = {https://openreview.net/forum?id=fXorxxbDvO},
 title = {On Robustness to Missing Video for Audiovisual Speech Recognition},
 url = {https://openreview.net/forum?id=fXorxxbDvO},
 year = {2022}
}

@article{chang2023dynamic,
 abstract = {This paper addresses safe distributed online optimization over an unknown set of linear safety constraints. A network of agents aims at jointly minimizing a global, time-varying function, which is only partially observable to each individual agent. Therefore, agents must engage in local communications to generate a safe sequence of actions competitive with the best minimizer sequence in hindsight, and the gap between the two sequences is quantified via dynamic regret. We propose distributed safe online gradient descent (D-Safe-OGD) with an exploration phase, where all agents estimate the constraint parameters collaboratively to build estimated feasible sets, ensuring the action selection safety during the optimization phase. We prove that for convex functions, D-Safe-OGD achieves a dynamic regret bound of $O(T^{2/3} \sqrt{\log T} + T^{1/3}C_T^*)$, where $C_T^*$ denotes the path-length of the best minimizer sequence. We further prove a dynamic regret bound of $O(T^{2/3} \sqrt{\log T} + T^{2/3}C_T^*)$ for certain non-convex problems, which establishes the first dynamic regret bound for a safe distributed algorithm in the non-convex setting.},
 author = {Ting-Jui Chang and Sapana Chaudhary and Dileep Kalathil and Shahin Shahrampour},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4322715975},
 pdf = {https://openreview.net/pdf?id=xiQXHvL1eN},
 review = {https://openreview.net/forum?id=xiQXHvL1eN},
 title = {Dynamic Regret Analysis of Safe Distributed Online Optimization for Convex and Non-convex Problems},
 url = {https://openreview.net/forum?id=xiQXHvL1eN},
 year = {2023}
}

@article{chard2024temporally,
 author = {Tim Chard and Mark Dras and Paul Sowman and Steve Cassidy and Jia Wu},
 code = {https://github.com/tim-chard/DeepLearningForMEG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=zSeoG5dRHK},
 review = {https://openreview.net/forum?id=zSeoG5dRHK},
 title = {Temporally Rich Deep Learning Models for Magnetoencephalography},
 url = {https://openreview.net/forum?id=zSeoG5dRHK},
 year = {2024}
}

@article{charton2022linear,
 abstract = {Transformers can learn to perform numerical computations from examples only. I study nine problems of linear algebra, from basic matrix operations to eigenvalue decomposition and inversion, and introduce and discuss four encoding schemes to represent real numbers. On all problems, transformers trained on sets of random matrices achieve high accuracies (over 90%). The models are robust to noise, and can generalize out of their training distribution. In particular, models trained to predict Laplace-distributed eigenvalues generalize to different classes of matrices: Wigner matrices or matrices with positive eigenvalues. The reverse is not true.},
 author = {Francois Charton},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4225777046},
 pdf = {https://openreview.net/pdf?id=Hp4g7FAXXG},
 review = {https://openreview.net/forum?id=Hp4g7FAXXG},
 title = {Linear algebra with transformers},
 url = {https://openreview.net/forum?id=Hp4g7FAXXG},
 year = {2022}
}

@article{chatterji2023undersampling,
 abstract = {While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of group-covariate shift we show that there is an undersampling algorithm that is minimax optimal when the overlap between the group distributions is small. We also perform an experimental case study on a label shift dataset and find that in line with our theory, the test accuracy of robust neural network classifiers is constrained by the number of minority samples.},
 author = {Niladri S. Chatterji and Saminul Haque and Tatsunori Hashimoto},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281762929},
 pdf = {https://openreview.net/pdf?id=r6oHDYOZ6p},
 review = {https://openreview.net/forum?id=r6oHDYOZ6p},
 title = {Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification},
 url = {https://openreview.net/forum?id=r6oHDYOZ6p},
 year = {2023}
}

@article{chatzipantazis2023learning,
 abstract = {Adapting to the structure of data distributions (such as symmetry and transformation invariances) is an important challenge in machine learning. Invariances can be built into the learning process by architecture design, or by augmenting the dataset. Both require a priori knowledge about the exact nature of the symmetries. Absent this knowledge, practitioners resort to expensive and time-consuming tuning. To address this problem, we propose a new approach to learn distributions of augmentation transforms, in a new \emph{Transformed Risk Minimization} (TRM) framework. In addition to predictive models, we also optimize over transformations chosen from a hypothesis space. As an algorithmic framework, our TRM method is (1) efficient (jointly learns augmentations and models in a \emph{single training loop}), (2) modular (works with \emph{any} training algorithm), and (3) general (handles \emph{both discrete and continuous} augmentations). We theoretically compare TRM with standard risk minimization, and give a PAC-Bayes upper bound on its generalization error. We propose to optimize this bound over a rich augmentation space via a new parametrization over compositions of blocks, leading to the new \emph{Stochastic Compositional Augmentation Learning} (SCALE) algorithm. We compare SCALE experimentally with prior methods (Fast AutoAugment and Augerino) on CIFAR10/100, SVHN . Additionally, we show that SCALE can correctly learn certain symmetries in the data distribution (recovering rotations on rotated MNIST) and can also improve calibration of the learned model.},
 author = {Evangelos Chatzipantazis and Stefanos Pertigkiozoglou and Kostas Daniilidis and Edgar Dobriban},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3214507387},
 pdf = {https://openreview.net/pdf?id=LRYtNj8Xw0},
 review = {https://openreview.net/forum?id=LRYtNj8Xw0},
 title = {Learning Augmentation Distributions using Transformed Risk Minimization.},
 url = {https://openreview.net/forum?id=LRYtNj8Xw0},
 year = {2023}
}

@article{chen2022adversarial,
 abstract = {Recent advances in computer vision take advantage of adversarial data augmentation to ameliorate the generalization ability of classification models. Here, we present an effective and efficient alternative that advocates adversarial augmentation on intermediate feature embeddings, instead of relying on computationally-expensive pixel-level perturbations. We propose Adversarial Feature Augmentation and Normalization (A-FAN), which (i) first augments visual recognition models with adversarial features that integrate flexible scales of perturbation strengths, (ii) then extracts adversarial feature statistics from batch normalization, and re-injects them into clean features through feature normalization. We validate the proposed approach across diverse visual recognition tasks with representative backbone networks, including ResNets and EfficientNets for classification, Faster-RCNN for detection, and Deeplab V3+ for segmentation. Extensive experiments show that A-FAN yields consistent generalization improvement over strong baselines across various datasets for classification, detection and segmentation tasks, such as CIFAR-10, CIFAR-100, ImageNet, Pascal VOC2007, Pascal VOC2012, COCO2017, and Cityspaces. Comprehensive ablation studies and detailed analyses also demonstrate that adding perturbations to specific modules and layers of classification/detection/segmentation backbones yields optimal performance. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV_A-FAN.},
 author = {Tianlong Chen and Yu Cheng and Zhe Gan and Jianfeng Wang and Lijuan Wang and Jingjing Liu and Zhangyang Wang},
 code = {https://github.com/VITA-Group/CV_A-FAN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3136140966},
 pdf = {https://openreview.net/pdf?id=2VEUIq9Yff},
 review = {https://openreview.net/forum?id=2VEUIq9Yff},
 title = {Adversarial Feature Augmentation and Normalization for Visual Recognition},
 url = {https://openreview.net/forum?id=2VEUIq9Yff},
 year = {2022}
}

@article{chen2022can,
 author = {Tianlong Chen and Zhenyu Zhang and Jun Wu and Randy Huang and Sijia Liu and Shiyu Chang and Zhangyang Wang},
 code = {https://github.com/VITA-Group/LTH-Pass},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=JL6MU9XFzW},
 review = {https://openreview.net/forum?id=JL6MU9XFzW},
 title = {Can You Win Everything with A Lottery Ticket?},
 url = {https://openreview.net/forum?id=JL6MU9XFzW},
 year = {2022}
}

@article{chen2022interpretable,
 abstract = {Variational Graph Autoencoders (VGAEs) are powerful models for unsupervised learning of node representations from graph data. In this work, we systematically analyze modeling node attributes in VGAEs and show that attribute decoding is important for node representation learning. We further propose a new learning model, interpretable NOde Representation with Attribute Decoding (NORAD). The model encodes node representations in an interpretable approach: node representations capture community structures in the graph and the relationship between communities and node attributes. We further propose a rectifying procedure to refine node representations of isolated notes, improving the quality of these nodes' representations. Our empirical results demonstrate the advantage of the proposed model when learning graph data in an interpretable approach.},
 author = {Xiaohui Chen and Xi Chen and Liping Liu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310823706},
 pdf = {https://openreview.net/pdf?id=AZIfC91hjM},
 review = {https://openreview.net/forum?id=AZIfC91hjM},
 title = {Interpretable Node Representation with Attribute Decoding},
 url = {https://openreview.net/forum?id=AZIfC91hjM},
 year = {2022}
}

@article{chen2022multiagent,
 abstract = {The finite-time convergence of off-policy TD learning has been comprehensively studied recently. However, such a type of convergence has not been well established for off-policy TD learning in the multi-agent setting, which covers broader applications and is fundamentally more challenging. This work develops a decentralized TD with correction (TDC) algorithm for multi-agent off-policy TD learning under Markovian sampling. In particular, our algorithms preserve full privacy of the actions, policies and rewards of the agents, and adopt mini-batch sampling to reduce the sampling variance and communication frequency. Under Markovian sampling and linear function approximation, we proved that the finite-time sample complexity of the algorithm for achieving an ϵ-accurate solution is in the order of ${\mathcal{O}}\left({{ \in ^{ - 1}}\ln { \in ^{ - 1}}}\right)$, matching the near-optimal sample complexity of centralized TD(0) and TDC. Importantly, the communication complexity of our algorithm is in the order of ${\mathcal{O}}\left({\ln { \in ^{ - 1}}}\right)$, which is significantly lower than the communication complexity ${\mathcal{O}}\left({{ \in ^{ - 1}}\ln { \in ^{ - 1}}}\right)$ of the existing decentralized TD(0). Experiments corroborate our theoretical findings.},
 author = {Ziyi Chen and Yi Zhou and Rong-Rong Chen},
 code = {https://github.com/changy12/Decentralized_TDC_TMLR2022},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4214881442},
 pdf = {https://openreview.net/pdf?id=tnPjQpYk7D},
 review = {https://openreview.net/forum?id=tnPjQpYk7D},
 title = {Multi-Agent Off-Policy TDC with Near-Optimal Sample and Communication Complexity},
 url = {https://openreview.net/forum?id=tnPjQpYk7D},
 year = {2022}
}

@article{chen2022optimal,
 abstract = {It is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participating clients compute their updates, but only the ones with "important" updates communicate back to the master. We show that importance can be measured using only the norm of the update and give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation, which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that for Distributed SGD (DSGD) and Federated Averaging (FedAvg), the performance of our approach can be close to full participation and superior to the baseline where participating clients are sampled uniformly. Moreover, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.},
 author = {Wenlin Chen and Samuel Horv{\'a}th and Peter Richt{\'a}rik},
 code = {https://github.com/SamuelHorvath/FL-optimal-client-sampling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3095276777},
 pdf = {https://openreview.net/pdf?id=8GvRCWKHIL},
 review = {https://openreview.net/forum?id=8GvRCWKHIL},
 title = {Optimal Client Sampling for Federated Learning},
 url = {https://openreview.net/forum?id=8GvRCWKHIL},
 year = {2022}
}

@article{chen2022queried,
 abstract = {Class-incremental learning (CIL) suffers from the notorious dilemma between learning newly added classes and preserving previously learned class knowledge. That catastrophic forgetting issue could be mitigated by storing historical data for replay, which yet would cause memory overheads as well as imbalanced prediction updates. To address this dilemma, we propose to leverage "free" external unlabeled data querying in continual learning. We first present a CIL with Queried Unlabeled Data (CIL-QUD) scheme, where we only store a handful of past training samples as anchors and use them to query relevant unlabeled examples each time. Along with new and past stored data, the queried unlabeled are effectively utilized, through learning-without-forgetting (LwF) regularizers and class-balance training. Besides preserving model generalization over past and current tasks, we next study the problem of adversarial robustness for CIL-QUD. Inspired by the recent success of learning robust models with unlabeled data, we explore a new robustness-aware CIL setting, where the learned adversarial robustness has to resist forgetting and be transferred as new tasks come in continually. While existing options easily fail, we show queried unlabeled data can continue to benefit, and seamlessly extend CIL-QUD into its robustified versions, RCIL-QUD. Extensive experiments demonstrate that CIL-QUD achieves substantial accuracy gains on CIFAR-10 and CIFAR-100, compared to previous state-of-the-art CIL approaches. Moreover, RCIL-QUD establishes the first strong milestone for robustness-aware CIL. Codes are available in https://github.com/VITA-Group/CIL-QUD.},
 author = {Tianlong Chen and Sijia Liu and Shiyu Chang and Lisa Amini and Zhangyang Wang},
 badge = {Featured},
 code = {https://github.com/VITA-Group/CIL-QUD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4283075480},
 pdf = {https://openreview.net/pdf?id=oLvlPJheCD},
 review = {https://openreview.net/forum?id=oLvlPJheCD},
 title = {Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning},
 url = {https://openreview.net/forum?id=oLvlPJheCD},
 year = {2022}
}

@article{chen2023a,
 abstract = {Gradient descent-ascent (GDA) is a widely used algorithm for minimax optimization. However, GDA has been proved to converge to stationary points for nonconvex minimax optimization, which are suboptimal compared with local minimax points. In this work, we develop cubic regularization (CR) type algorithms that globally converge to local minimax points in nonconvex-strongly-concave minimax optimization. We first show that local minimax points are equivalent to second-order stationary points of a certain envelope function. Then, inspired by the classic cubic regularization algorithm, we propose an algorithm named Cubic-LocalMinimax for finding local minimax points, and provide a comprehensive convergence analysis by leveraging its intrinsic potential function. Specifically, we establish the global convergence of Cubic-LocalMinimax to a local minimax point at a sublinear convergence rate and characterize its iteration complexity. Also, we propose a GDA-based solver for solving the cubic subproblem involved in Cubic-LocalMinimax up to certain pre-defined accuracy, and analyze the overall gradient and Hessian-vector product computation complexities of such an inexact Cubic-LocalMinimax algorithm. Moreover, we propose a stochastic variant of Cubic-LocalMinimax for large-scale minimax optimization, and characterize its sample complexity under stochastic sub-sampling. Experimental results demonstrate faster convergence of our stochastic Cubic-LocalMinimax than some existing algorithms.},
 author = {Ziyi Chen and Zhengyang Hu and Qunwei Li and Zhe Wang and Yi Zhou},
 code = {https://github.com/datou30/Cubic-Localminimax-experiment},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4285079688},
 pdf = {https://openreview.net/pdf?id=jVMMdg31De},
 review = {https://openreview.net/forum?id=jVMMdg31De},
 title = {A Cubic Regularization Approach for Finding Local Minimax Points in Nonconvex Minimax Optimization},
 url = {https://openreview.net/forum?id=jVMMdg31De},
 year = {2023}
}

@article{chen2023assisted,
 abstract = {In the era of big data, many big organizations are integrating machine learning into their work pipelines to facilitate data analysis. However, the performance of their trained models is often restricted by limited and imbalanced data available to them. In this work, we develop an assisted learning framework for assisting organizations to improve their learning performance. The organizations have sufficient computation resources but are subject to stringent data-sharing and collaboration policies. Their limited imbalanced data often cause biased inference and sub-optimal decision-making. In assisted learning, an organizational learner purchases assistance service from an external service provider and aims to enhance its model performance within only a few assistance rounds. We develop effective stochastic training algorithms for both assisted deep learning and assisted reinforcement learning. Different from existing distributed algorithms that need to frequently transmit gradients or models, our framework allows the learner to only occasionally share information with the service provider, but still obtain a model that achieves near-oracle performance as if all the data were centralized.},
 author = {Cheng Chen and Jiaying Zhou and Jie Ding and Yi Zhou},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3199690024},
 pdf = {https://openreview.net/pdf?id=SEDWlhcFWA},
 review = {https://openreview.net/forum?id=SEDWlhcFWA},
 title = {Assisted Learning for Organizations with Limited Data},
 url = {https://openreview.net/forum?id=SEDWlhcFWA},
 year = {2023}
}

@article{chen2023bag,
 abstract = {Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning.},
 author = {Yubei Chen and Adrien Bardes and ZENGYI LI and Yann LeCun},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283319089},
 pdf = {https://openreview.net/pdf?id=r06xREo3QG},
 review = {https://openreview.net/forum?id=r06xREo3QG},
 title = {Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning},
 url = {https://openreview.net/forum?id=r06xREo3QG},
 year = {2023}
}

@article{chen2023beyond,
 author = {Jiamin Chen and Xuhong Li and Lei Yu and Dejing Dou and Haoyi Xiong},
 code = {https://github.com/jiaminchen-1031/transformerinterp & https://github.com/PaddlePaddle/InterpretDL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=rm0zIzlhcX},
 review = {https://openreview.net/forum?id=rm0zIzlhcX},
 title = {Beyond Intuition: Rethinking Token Attributions inside Transformers},
 url = {https://openreview.net/forum?id=rm0zIzlhcX},
 year = {2023}
}

@article{chen2023calibrate,
 abstract = {Multiple sampling-based methods have been developed for approximating and accelerating node embedding aggregation in graph convolutional networks (GCNs) training. Among them, a layer-wise approach recursively performs importance sampling to select neighbors jointly for existing nodes in each layer. This paper revisits the approach from a matrix approximation perspective, and identifies two issues in the existing layer-wise sampling methods: suboptimal sampling probabilities and estimation biases induced by sampling without replacement. To address these issues, we accordingly propose two remedies: a new principle for constructing sampling probabilities and an efficient debiasing algorithm. The improvements are demonstrated by extensive analyses of estimation variance and experiments on common benchmarks. Code and algorithm implementations are publicly available at https://github.com/ychen-stat-ml/GCN-layer-wise-sampling .},
 author = {Yifan Chen and Tianning Xu and Dilek Hakkani-Tur and Di Jin and Yun Yang and Ruoqing Zhu},
 code = {https://github.com/ychen-stat-ml/GCN-layer-wise-sampling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282011383},
 pdf = {https://openreview.net/pdf?id=JyKNuoZGux},
 review = {https://openreview.net/forum?id=JyKNuoZGux},
 title = {Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks},
 url = {https://openreview.net/forum?id=JyKNuoZGux},
 year = {2023}
}

@article{chen2023containing,
 abstract = {The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-19), is contained through testing and isolation of infected nodes. The temporal and spatial evolution of the process (along with containment through isolation) render such detection as fundamentally different from active search detection strategies. In this work, through an active learning approach, we design testing and isolation strategies to contain the spread and minimize the cumulative infections under a given test budget. We prove that the objective can be optimized, with performance guarantees, by greedily selecting the nodes to test. We further design reward-based methodologies that effectively minimize an upper bound on the cumulative infections and are computationally more tractable in large networks. These policies, however, need knowledge about the nodes' infection probabilities which are dynamically changing and have to be learned by sequential testing. We develop a message-passing framework for this purpose and, building on that, show novel tradeoffs between exploitation of knowledge through reward-based heuristics and exploration of the unknown through a carefully designed probabilistic testing. The tradeoffs are fundamentally distinct from the classical counterparts under active search or multi-armed bandit problems (MABs). We provably show the necessity of exploration in a stylized network and show through simulations that exploration can outperform exploitation in various synthetic and real-data networks depending on the parameters of the network and the spread.},
 author = {Xingran Chen and Hesam Nikpey and Jungyeol Kim and Saswati Sarkar and Shirin Saeedi Bidokhti},
 code = {https://github.com/xingranchen/PaperCodesTalks/tree/main/2023_Feb_TMLR_Codes},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4322827143},
 pdf = {https://openreview.net/pdf?id=qvRWcDXBam},
 review = {https://openreview.net/forum?id=qvRWcDXBam},
 title = {Containing a spread through sequential learning: to exploit or to explore?},
 url = {https://openreview.net/forum?id=qvRWcDXBam},
 year = {2023}
}

@article{chen2023deep,
 author = {Ke Chen and Chunmei Wang and Haizhao Yang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=zmBFzuT2DN},
 review = {https://openreview.net/forum?id=zmBFzuT2DN},
 title = {Deep Operator Learning Lessens the Curse of Dimensionality for {PDE}s},
 url = {https://openreview.net/forum?id=zmBFzuT2DN},
 year = {2023}
}

@article{chen2023learning,
 author = {Yatong Chen and Jialu Wang and Yang Liu},
 code = {https://github.com/UCSC-REAL/ConstructiveAdaptation},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=W98AEKQ38Y},
 review = {https://openreview.net/forum?id=W98AEKQ38Y},
 title = {Learning to Incentivize Improvements from Strategic Agents},
 url = {https://openreview.net/forum?id=W98AEKQ38Y},
 year = {2023}
}

@article{chen2023machine,
 abstract = {Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.},
 author = {Chacha Chen and Shi Feng and Amit Sharma and Chenhao Tan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380318885},
 pdf = {https://openreview.net/pdf?id=y4CGF1A8VG},
 review = {https://openreview.net/forum?id=y4CGF1A8VG},
 title = {Machine Explanations and Human Understanding},
 url = {https://openreview.net/forum?id=y4CGF1A8VG},
 year = {2023}
}

@article{chen2023program,
 abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts},
 author = {Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
 code = {https://github.com/wenhuchen/Program-of-Thoughts},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309953112},
 pdf = {https://openreview.net/pdf?id=YfZ4ZPt8zd},
 review = {https://openreview.net/forum?id=YfZ4ZPt8zd},
 title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
 url = {https://openreview.net/forum?id=YfZ4ZPt8zd},
 year = {2023}
}

@article{chen2023tsmixer,
 author = {Si-An Chen and Chun-Liang Li and Sercan O Arik and Nathanael Christian Yoder and Tomas Pfister},
 code = {https://github.com/google-research/google-research/tree/master/tsmixer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=wbpxTuXgm0},
 review = {https://openreview.net/forum?id=wbpxTuXgm0},
 title = {{TSM}ixer: An All-{MLP} Architecture for Time Series Forecast-ing},
 url = {https://openreview.net/forum?id=wbpxTuXgm0},
 year = {2023}
}

@article{chen2023turning,
 abstract = {Many backdoor removal techniques in machine learning models require clean in-distribution data, which may not always be available due to proprietary datasets. Model inversion techniques, often considered privacy threats, can reconstruct realistic training samples, potentially eliminating the need for in-distribution data. Prior attempts to combine backdoor removal and model inversion yielded limited results. Our work is the first to provide a thorough understanding of leveraging model inversion for effective backdoor removal by addressing key questions about reconstructed samples' properties, perceptual similarity, and the potential presence of backdoor triggers. We establish that relying solely on perceptual similarity is insufficient for robust defenses, and the stability of model predictions in response to input and parameter perturbations is also crucial. To tackle this, we introduce a novel bi-level optimization-based framework for model inversion, promoting stability and visual quality. Interestingly, we discover that reconstructed samples from a pre-trained generator's latent space are backdoor-free, even when utilizing signals from a backdoored model. We provide a theoretical analysis to support this finding. Our evaluation demonstrates that our stabilized model inversion technique achieves state-of-the-art backdoor removal performance without clean in-distribution data, matching or surpassing performance using the same amount of clean samples.},
 author = {Si Chen and Yi Zeng and Won Park and Jiachen T. Wang and Xun Chen and Lingjuan Lyu and Zhuoqing Mao and Ruoxi Jia},
 code = {https://github.com/SCccc21/FRED.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282977235},
 pdf = {https://openreview.net/pdf?id=XuOE99cmST},
 review = {https://openreview.net/forum?id=XuOE99cmST},
 title = {Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion},
 url = {https://openreview.net/forum?id=XuOE99cmST},
 year = {2023}
}

@article{chen2024aspest,
 abstract = {Selective prediction aims to learn a reliable model that abstains from making predictions when uncertain. These predictions can then be deferred to humans for further evaluation. As an everlasting challenge for machine learning, in many real-world scenarios, the distribution of test data is different from the training data. This results in more inaccurate predictions, and often increased dependence on humans, which can be difficult and expensive. Active learning aims to lower the overall labeling effort, and hence human dependence, by querying the most informative examples. Selective prediction and active learning have been approached from different angles, with the connection between them missing. In this work, we introduce a new learning paradigm, active selective prediction, which aims to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new paradigm, we propose a simple yet effective approach, ASPEST, that utilizes ensembles of model snapshots with self-training with their aggregated outputs as pseudo labels. Extensive experiments on numerous image, text and structured datasets, which suffer from domain shifts, demonstrate that ASPEST can significantly outperform prior work on selective prediction and active learning (e.g. on the MNIST$\to$SVHN benchmark with the labeling budget of 100, ASPEST improves the AUACC metric from 79.36% to 88.84%) and achieves more optimal utilization of humans in the loop.},
 author = {Jiefeng Chen and Jinsung Yoon and Sayna Ebrahimi and Sercan O Arik and Somesh Jha and Tomas Pfister},
 code = {https://github.com/google-research/google-research/tree/master/active_selective_prediction},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4365143428},
 pdf = {https://openreview.net/pdf?id=3nprbNR3HB},
 review = {https://openreview.net/forum?id=3nprbNR3HB},
 title = {ASPEST: Bridging the Gap Between Active Learning and Selective Prediction},
 url = {https://openreview.net/forum?id=3nprbNR3HB},
 year = {2024}
}

@article{chen2024learning,
 author = {Angelica Chen and J{\'e}r{\'e}my Scheurer and Jon Ander Campos and Tomasz Korbak and Jun Shern Chan and Samuel R. Bowman and Kyunghyun Cho and Ethan Perez},
 code = {Code + data for code generation portion: https://github.com/nyu-mll/ILF-for-code-generation; Data for text summarization portion: https://huggingface.co/datasets/JeremyAlain/SLF5K; Code for text summarization portion: https://github.com/JeremyAlain/imitation_learning_from_language_feedback},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=xo3hI5MwvU},
 review = {https://openreview.net/forum?id=xo3hI5MwvU},
 title = {Learning from Natural Language Feedback},
 url = {https://openreview.net/forum?id=xo3hI5MwvU},
 year = {2024}
}

@article{chen2024two,
 author = {Angelica Chen and Jason Phang and Alicia Parrish and Vishakh Padmakumar and Chen Zhao and Samuel R. Bowman and Kyunghyun Cho},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=5nBqY1y96B},
 review = {https://openreview.net/forum?id=5nBqY1y96B},
 title = {Two Failures of Self-Consistency in the Multi-Step Reasoning of {LLM}s},
 url = {https://openreview.net/forum?id=5nBqY1y96B},
 year = {2024}
}

@article{chhabra2023towards,
 author = {Anshuman Chhabra and Kartik Patwari and Chandana Kuntala and Sristi and Deepak Kumar Sharma and Prasant Mohapatra},
 code = {https://github.com/anshuman23/fair_video_summarization_tmlr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Uj6MRfR1P5},
 review = {https://openreview.net/forum?id=Uj6MRfR1P5},
 title = {Towards Fair Video Summarization},
 url = {https://openreview.net/forum?id=Uj6MRfR1P5},
 year = {2023}
}

@article{chiang2023named,
 abstract = {We propose a notation for tensors with named axes, which relieves the author, reader, and future implementers of machine learning models from the burden of keeping track of the order of axes and the purpose of each. The notation makes it easy to lift operations on low-order tensors to higher order ones, for example, from images to minibatches of images, or from an attention mechanism to multiple attention heads. After a brief overview and formal definition of the notation, we illustrate it through several examples from modern machine learning, from building blocks like attention and convolution to full models like Transformers and LeNet. We then discuss differential calculus in our notation and compare with some alternative notations. Our proposals build on ideas from many previous papers and software libraries. We hope that our notation will encourage more authors to use named tensors, resulting in clearer papers and more precise implementations.},
 author = {David Chiang and Alexander M Rush and Boaz Barak},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4312106776},
 pdf = {https://openreview.net/pdf?id=hVT7SHlilx},
 review = {https://openreview.net/forum?id=hVT7SHlilx},
 title = {Named Tensor Notation},
 url = {https://openreview.net/forum?id=hVT7SHlilx},
 year = {2023}
}

@article{chitsaz2023training,
 author = {Kamran Chitsaz and Goncalo Mordido and Jean-Pierre David and Fran{\c{c}}ois Leduc-Primeau},
 code = {https://github.com/kmchiti/WCAT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=BxjHMPwZIH},
 review = {https://openreview.net/forum?id=BxjHMPwZIH},
 title = {Training {DNN}s Resilient to Adversarial and Random Bit-Flips by Learning Quantization Ranges},
 url = {https://openreview.net/forum?id=BxjHMPwZIH},
 year = {2023}
}

@article{chizat2022meanfield,
 abstract = {Noisy particle gradient descent (NPGD) is an algorithm to minimize convex functions over the space of measures that include an entropy term. In the many-particle limit, this algorithm is described by a Mean-Field Langevin dynamics - a generalization of the Langevin dynamics with a non-linear drift - which is our main object of study. Previous work have shown its convergence to the unique minimizer via non-quantitative arguments. We prove that this dynamics converges at an exponential rate, under the assumption that a certain family of Log-Sobolev inequalities holds. This assumption holds for instance for the minimization of the risk of certain two-layer neural networks, where NPGD is equivalent to standard noisy gradient descent. We also study the annealed dynamics, and show that for a noise decaying at a logarithmic rate, the dynamics converges in value to the global minimizer of the unregularized objective function.},
 author = {L{\'e}na{\"\i}c Chizat},
 code = {https://github.com/lchizat/2022-mean-field-langevin-rate},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297904236},
 pdf = {https://openreview.net/pdf?id=BDqzLH1gEm},
 review = {https://openreview.net/forum?id=BDqzLH1gEm},
 title = {Mean-Field Langevin Dynamics: Exponential Convergence and Annealing},
 url = {https://openreview.net/forum?id=BDqzLH1gEm},
 year = {2022}
}

@article{cho2024maximizing,
 abstract = {Federated learning typically considers collaboratively training a global model using local data at edge clients. Clients may have their own individual requirements, such as having a minimal training loss threshold, which they expect to be met by the global model. However, due to client heterogeneity, the global model may not meet each client's requirements, and only a small subset may find the global model appealing. In this work, we explore the problem of the global model lacking appeal to the clients due to not being able to satisfy local requirements. We propose MaxFL, which aims to maximize the number of clients that find the global model appealing. We show that having a high global model appeal is important to maintain an adequate pool of clients for training, and can directly improve the test accuracy on both seen and unseen clients. We provide convergence guarantees for MaxFL and show that MaxFL achieves a $22$-$40\%$ and $18$-$50\%$ test accuracy improvement for the training clients and unseen clients respectively, compared to a wide range of FL modeling approaches, including those that tackle data heterogeneity, aim to incentivize clients, and learn personalized or fair models.},
 author = {Yae Jee Cho and Divyansh Jhunjhunwala and Tian Li and Virginia Smith and Gauri Joshi},
 code = {https://openreview.net/attachment?id=8GI1SXqJBk&name=supplementary_material},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281625219},
 pdf = {https://openreview.net/pdf?id=8GI1SXqJBk},
 review = {https://openreview.net/forum?id=8GI1SXqJBk},
 title = {Maximizing Global Model Appeal in Federated Learning},
 url = {https://openreview.net/forum?id=8GI1SXqJBk},
 year = {2024}
}

@article{christianos2023pareto,
 abstract = {This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal Nash equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and, therefore, is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a Pareto-optimal equilibrium in a range of matrix games. Finally, we propose PACDCG, a graph neural network extension of Pareto-AC, which is shown to efficiently scale in games with a large number of agents.},
 author = {Filippos Christianos and Georgios Papoudakis and Stefano V Albrecht},
 code = {https://github.com/uoe-agents/epymarl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4298181618},
 pdf = {https://openreview.net/pdf?id=3AzqYa18ah},
 review = {https://openreview.net/forum?id=3AzqYa18ah},
 title = {Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning},
 url = {https://openreview.net/forum?id=3AzqYa18ah},
 year = {2023}
}

@article{chuck2024granger,
 author = {Caleb Chuck and Kevin Black and Aditya Arjun and Yuke Zhu and Scott Niekum},
 code = {https://github.com/CalCharles/object-options},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=iA2KQyoun1},
 review = {https://openreview.net/forum?id=iA2KQyoun1},
 title = {Granger Causal Interaction Skill Chains},
 url = {https://openreview.net/forum?id=iA2KQyoun1},
 year = {2024}
}

@article{connor2023learning,
 abstract = {Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However, these approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitations by introducing a learning strategy that does not require transformation labels and developing a method that learns the local regions where each operator is likely to be used while preserving the identity of inputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to learn identity-preserving transformations on multi-class datasets. Additionally, we train on CelebA to showcase our model's ability to learn semantically meaningful transformations on complex datasets in an unsupervised manner.},
 author = {Marissa Catherine Connor and Kion Fallah and Christopher John Rozell},
 code = {https://github.com/Sensory-Information-Processing-Lab/manifold-autoencoder-extended},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4361805504},
 pdf = {https://openreview.net/pdf?id=gyhiZYrk5y},
 review = {https://openreview.net/forum?id=gyhiZYrk5y},
 title = {Learning Identity-Preserving Transformations on Data Manifolds},
 url = {https://openreview.net/forum?id=gyhiZYrk5y},
 year = {2023}
}

@article{conserva2022the,
 abstract = {Many algorithms for ranked data become computationally intractable as the number of objects grows due to the complex geometric structure induced by rankings. An additional challenge is posed by partial rankings, i.e. rankings in which the preference is only known for a subset of all objects. For these reasons, state-of-the-art methods cannot scale to real-world applications, such as recommender systems. We address this challenge by exploiting the geometric structure of ranked data and additional available information about the objects to derive a kernel for ranking based on the graph cut function. The graph cut kernel combines the efficiency of submodular optimization with the theoretical properties of kernel-based methods. The graph cut kernel combines the efficiency of submodular optimization with the theoretical properties of kernel-based methods.},
 author = {Michelangelo Conserva and Marc Peter Deisenroth and K S Sesh Kumar},
 code = {https://github.com/MichelangeloConserva/CutFunctionKernel/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286382509},
 pdf = {https://openreview.net/pdf?id=SEUGkraMPi},
 review = {https://openreview.net/forum?id=SEUGkraMPi},
 title = {The Graph Cut Kernel for Ranked Data},
 url = {https://openreview.net/forum?id=SEUGkraMPi},
 year = {2022}
}

@article{corcoll2022did,
 abstract = {Identifying controllable aspects of the environment has proven to be an extraordinary intrinsic motivator to reinforcement learning agents. Despite repeatedly achieving State-of-the-Art results, this approach has only been studied as a proxy to a reward-based task and has not yet been evaluated on its own. Current methods are based on action-prediction. Humans, on the other hand, assign blame to their actions to decide what they controlled. This work proposes Controlled Effect Network (CEN), an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. Moreover, we demonstrate CEN's capabilities as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models.},
 author = {Oriol Corcoll and Youssef Sherif Mansour Mohamed and Raul Vicente},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287160677},
 pdf = {https://openreview.net/pdf?id=NL2L3XjVFx},
 review = {https://openreview.net/forum?id=NL2L3XjVFx},
 title = {Did I do that? Blame as a means to identify controlled effects in reinforcement learning},
 url = {https://openreview.net/forum?id=NL2L3XjVFx},
 year = {2022}
}

@article{cuesta-albertos2023on,
 author = {Juan Cuesta-Albertos and Subhajit Dutta},
 code = {https://www.dropbox.com/sh/ont3ggvz44g5j07/AAC1PRuzIWx9_yFUiR5mk4Zna?dl=0},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=igDOV2KBwM},
 review = {https://openreview.net/forum?id=igDOV2KBwM},
 title = {On Perfect Clustering for Gaussian Processes},
 url = {https://openreview.net/forum?id=igDOV2KBwM},
 year = {2023}
}

@article{d,
 pdf = {https://openreview.net/pdf?id=SQnPE63jtA},
 review = {https://openreview.net/forum?id=SQnPE63jtA}
}

@article{d,
 code = {https://github.com/IssamLaradji/mirror-sps},
 pdf = {https://openreview.net/pdf?id=28bQiPWxHl},
 review = {https://openreview.net/forum?id=28bQiPWxHl}
}

@article{d,
 code = {https://github.com/OfficiallyDAC/mcsl},
 pdf = {https://openreview.net/pdf?id=Ub6XILEF9x},
 review = {https://openreview.net/forum?id=Ub6XILEF9x}
}

@article{d,
 badge = {Reproducibility},
 pdf = {https://openreview.net/pdf?id=ivCd8z8zR2},
 review = {https://openreview.net/forum?id=ivCd8z8zR2}
}

@article{d,
 pdf = {https://openreview.net/pdf?id=ZPQhzTSWA7},
 review = {https://openreview.net/forum?id=ZPQhzTSWA7}
}

@article{d,
 code = {https://github.com/facebookresearch/diffq/},
 pdf = {https://openreview.net/pdf?id=DijnKziche},
 review = {https://openreview.net/forum?id=DijnKziche}
}

@article{dadsetan2023robust,
 abstract = {Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from brain MRI. We also show that pretraining on extended (but not labeled) brain MRI data outperforms pretraining on natural images. We further observe that the highest performance is achieved when both natural images and extended brain-MRI data are used for pretraining.},
 author = {Saba Dadsetan and Mohsen Hejrati and Shandong Wu and Somaye Hashemifar},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309302603},
 pdf = {https://openreview.net/pdf?id=HVAeM6sNo8},
 review = {https://openreview.net/forum?id=HVAeM6sNo8},
 title = {Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning},
 url = {https://openreview.net/forum?id=HVAeM6sNo8},
 year = {2023}
}

@article{dai2023an,
 author = {Yutong Dai and Tianyi Chen and Guanyi Wang and Daniel Robinson},
 code = {https://github.com/tianyic/adahspg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=KBhSyBBeeO},
 review = {https://openreview.net/forum?id=KBhSyBBeeO},
 title = {An Adaptive Half-Space Projection Method for Stochastic Optimization Problems with Group Sparse Regularization},
 url = {https://openreview.net/forum?id=KBhSyBBeeO},
 year = {2023}
}

@article{dalm2023gradientadjusted,
 abstract = {Many of the recent advances in the field of artificial intelligence have been fueled by the highly successful backpropagation of error (BP) algorithm, which efficiently solves the credit assignment problem in artificial neural networks. However, it is unlikely that BP is implemented in its usual form within biological neural networks, because of its reliance on non-local information in propagating error gradients. Since biological neural networks are capable of highly efficient learning and responses from BP trained models can be related to neural responses, it seems reasonable that a biologically viable approximation of BP underlies synaptic plasticity in the brain. Gradient-adjusted incremental target propagation (GAIT-prop or GP for short) has recently been derived directly from BP and has been shown to successfully train networks in a more biologically plausible manner. However, so far, GP has only been shown to work on relatively low-dimensional problems, such as handwritten-digit recognition. This work addresses some of the scaling issues in GP and shows it to perform effective multi-layer credit assignment in deeper networks and on the much more challenging ImageNet dataset.},
 author = {Sander Dalm and Nasir Ahmad and Luca Ambrogioni and Marcel van Gerven},
 code = {https://github.com/artcogsys/GAIT_prop_scaling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287324487},
 pdf = {https://openreview.net/pdf?id=Lx19EyKX77},
 review = {https://openreview.net/forum?id=Lx19EyKX77},
 title = {Gradient-adjusted Incremental Target Propagation Provides Effective Credit Assignment in Deep Neural Networks},
 url = {https://openreview.net/forum?id=Lx19EyKX77},
 year = {2023}
}

@article{dangel2023vivit,
 author = {Felix Dangel and Lukas Tatzel and Philipp Hennig},
 code = {https://github.com/f-dangel/vivit},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=DzJ7JfPXkE},
 review = {https://openreview.net/forum?id=DzJ7JfPXkE},
 title = {ViViT: Curvature Access Through The Generalized Gauss-Newton{\textquoteright}s Low-Rank Structure},
 url = {https://openreview.net/forum?id=DzJ7JfPXkE},
 year = {2023}
}

@article{daniel2024ddlp,
 abstract = {We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation. In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web},
 author = {Tal Daniel and Aviv Tamar},
 code = {https://github.com/taldatech/ddlp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380362641},
 pdf = {https://openreview.net/pdf?id=Wqn8zirthg},
 review = {https://openreview.net/forum?id=Wqn8zirthg},
 title = {DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles},
 url = {https://openreview.net/forum?id=Wqn8zirthg},
 year = {2024}
}

@article{daras2023soft,
 abstract = {We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.},
 author = {Giannis Daras and Mauricio Delbracio and Hossein Talebi and Alex Dimakis and Peyman Milanfar},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297812225},
 pdf = {https://openreview.net/pdf?id=W98rebBxlQ},
 review = {https://openreview.net/forum?id=W98rebBxlQ},
 title = {Soft Diffusion: Score Matching for General Corruptions},
 url = {https://openreview.net/forum?id=W98rebBxlQ},
 year = {2023}
}

@article{darvish2023multimodal,
 author = {Kasra Darvish and Edward Raff and Francis Ferraro and Cynthia Matuszek},
 code = {https://github.com/kasraprime/EMMA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=cXa6Xdm0v7},
 review = {https://openreview.net/forum?id=cXa6Xdm0v7},
 title = {Multimodal Language Learning for Object Retrieval in Low Data Regimes in the Face of Missing Modalities},
 url = {https://openreview.net/forum?id=cXa6Xdm0v7},
 year = {2023}
}

@article{das2023accelerating,
 abstract = {A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm Continual Active Learning (CAL). We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse, uncertain points from the history. We conduct experiments across many data domains, including natural language, vision, medical imaging, and computational biology, each with different neural architectures and dataset sizes. CAL consistently provides a 3x reduction in training time, while retaining performance.},
 author = {Arnav Mohanty Das and Gantavya Bhatt and Megh Manoj Bhalerao and Vianne R. Gao and Rui Yang and Jeff Bilmes},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4376311847},
 pdf = {https://openreview.net/pdf?id=T55dLSgsEf},
 review = {https://openreview.net/forum?id=T55dLSgsEf},
 title = {Accelerating Batch Active Learning Using Continual Learning Techniques},
 url = {https://openreview.net/forum?id=T55dLSgsEf},
 year = {2023}
}

@article{das2023longterm,
 author = {Abhimanyu Das and Weihao Kong and Andrew Leach and Shaan K Mathur and Rajat Sen and Rose Yu},
 code = {https://github.com/google-research/google-research/tree/master/tide},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=pCbC3aQB5W},
 review = {https://openreview.net/forum?id=pCbC3aQB5W},
 title = {Long-term Forecasting with Ti{DE}: Time-series Dense Encoder},
 url = {https://openreview.net/forum?id=pCbC3aQB5W},
 year = {2023}
}

@article{david2023hermes,
 abstract = {Developing models and algorithms to predict nonstationary time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is twofold. We first provide publicly a dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviours of influencers. Secondly, to leverage such a dataset, we propose a new hybrid forecasting model. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed fashion dataset, on the weekly time series of the M4 competition, and illustrates the benefit of the contribution of external weak signals.},
 author = {Etienne David and Jean Bellot and Sylvain Le Corff},
 code = {https://github.com/etidav/HERMES},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4285817388},
 pdf = {https://openreview.net/pdf?id=4ofFo7D5GL},
 review = {https://openreview.net/forum?id=4ofFo7D5GL},
 title = {HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series},
 url = {https://openreview.net/forum?id=4ofFo7D5GL},
 year = {2023}
}

@article{davydov2024using,
 abstract = {When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.},
 author = {Andrey Davydov and Alexey Sidnev and Artsiom Sanakoyeu and Yuhua Chen and Mathieu Salzmann and Pascal Fua},
 code = {https://github.com/cvlab-epfl/of4hmr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391591316},
 pdf = {https://openreview.net/pdf?id=fUhOb14sQv},
 review = {https://openreview.net/forum?id=fUhOb14sQv},
 title = {Using Motion Cues to Supervise Single-Frame Body Pose and Shape
  Estimation in Low Data Regimes},
 url = {https://openreview.net/forum?id=fUhOb14sQv},
 year = {2024}
}

@article{daxberger2023improving,
 author = {Erik Daxberger and Siddharth Swaroop and Kazuki Osawa and Rio Yokota and Richard E Turner and Jos{\'e} Miguel Hern{\'a}ndez-Lobato and Mohammad Emtiyaz Khan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=b1fpfCjja1},
 review = {https://openreview.net/forum?id=b1fpfCjja1},
 title = {Improving Continual Learning by Accurate Gradient Reconstructions of the Past},
 url = {https://openreview.net/forum?id=b1fpfCjja1},
 year = {2023}
}

@article{de,
 badge = {Written by Expert Reviewer},
 pdf = {https://openreview.net/pdf?id=MhK5aXo3gB},
 review = {https://openreview.net/forum?id=MhK5aXo3gB}
}

@article{deac2023equivariant,
 abstract = {Deep reinforcement learning repeatedly succeeds in closed, well-defined domains such as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are numerous and varied. For this, agents need to learn the underlying rules governing the environment, so as to robustly generalise to conditions that differ from those they were trained on. Model-based reinforcement learning algorithms, such as the highly successful MuZero, aim to accomplish this by learning a world model. However, leveraging a world model has not consistently shown greater generalisation capabilities compared to model-free alternatives. In this work, we propose improving the data efficiency and generalisation capabilities of MuZero by explicitly incorporating the symmetries of the environment in its world-model architecture. We prove that, so long as the neural networks used by MuZero are equivariant to a particular symmetry group acting on the environment, the entirety of MuZero's action-selection algorithm will also be equivariant to that group. We evaluate Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then testing on unseen rotated versions, demonstrating the benefits of equivariance. Further, we verify that our performance improvements hold even when only some of the components of Equivariant MuZero obey strict equivariance, which highlights the robustness of our construction.},
 author = {Andreea Deac and Theophane Weber and George Papamakarios},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320165921},
 pdf = {https://openreview.net/pdf?id=ExbGarTbLE},
 review = {https://openreview.net/forum?id=ExbGarTbLE},
 title = {Equivariant MuZero},
 url = {https://openreview.net/forum?id=ExbGarTbLE},
 year = {2023}
}

@article{delbracio2023inversion,
 abstract = {Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called "regression to the mean" effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models. Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality. While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising.},
 author = {Mauricio Delbracio and Peyman Milanfar},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4353113087},
 pdf = {https://openreview.net/pdf?id=VmyFF5lL3F},
 review = {https://openreview.net/forum?id=VmyFF5lL3F},
 title = {Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration},
 url = {https://openreview.net/forum?id=VmyFF5lL3F},
 year = {2023}
}

@article{demange-chryst2024variational,
 abstract = {Probability density function estimation with weighted samples is the main foundation of all adaptive importance sampling algorithms. Classically, a target distribution is approximated either by a non-parametric model or within a parametric family. However, these models suffer from the curse of dimensionality or from their lack of flexibility. In this contribution, we suggest to use as the approximating model a distribution parameterised by a variational autoencoder. We extend the existing framework to the case of weighted samples by introducing a new objective function. The flexibility of the obtained family of distributions makes it as expressive as a non-parametric model, and despite the very high number of parameters to estimate, this family is much more efficient in high dimension than the classical Gaussian or Gaussian mixture families. Moreover, in order to add flexibility to the model and to be able to learn multimodal distributions, we consider a learnable prior distribution for the variational autoencoder latent variables. We also introduce a new pre-training procedure for the variational autoencoder to find good starting weights of the neural networks to prevent as much as possible the posterior collapse phenomenon to happen. At last, we explicit how the resulting distribution can be combined with importance sampling, and we exploit the proposed procedure in existing adaptive importance sampling algorithms to draw points from a target distribution and to estimate a rare event probability in high dimension on two multimodal problems.},
 author = {Julien Demange-Chryst and Francois Bachoc and J{\'e}r{\^o}me Morio and Timoth{\'e} Krauth},
 code = {https://github.com/Julien6431/Importance-Sampling-VAE.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387688006},
 pdf = {https://openreview.net/pdf?id=nzG9KGssSe},
 review = {https://openreview.net/forum?id=nzG9KGssSe},
 title = {Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling},
 url = {https://openreview.net/forum?id=nzG9KGssSe},
 year = {2024}
}

@article{demirel2022attentive,
 abstract = {Graph neural networks (GNNs) have been shown to possess strong representation power, which can be exploited for downstream prediction tasks on graph-structured data, such as molecules and social networks. They typically learn representations by aggregating information from the $K$-hop neighborhood of individual vertices or from the enumerated walks in the graph. Prior studies have demonstrated the effectiveness of incorporating weighting schemes into GNNs; however, this has been primarily limited to $K$-hop neighborhood GNNs so far. In this paper, we aim to design an algorithm incorporating weighting schemes into walk-aggregating GNNs and analyze their effect. We propose a novel GNN model, called AWARE, that aggregates information about the walks in the graph using attention schemes. This leads to an end-to-end supervised learning method for graph-level prediction tasks in the standard setting where the input is the adjacency and vertex information of a graph, and the output is a predicted label for the graph. We then perform theoretical, empirical, and interpretability analyses of AWARE. Our theoretical analysis in a simplified setting identifies successful conditions for provable guarantees, demonstrating how the graph information is encoded in the representation, and how the weighting schemes in AWARE affect the representation and learning performance. Our experiments demonstrate the strong performance of AWARE in graph-level prediction tasks in the standard setting in the domains of molecular property prediction and social networks. Lastly, our interpretation study illustrates that AWARE can successfully capture the important substructures of the input graph. The code is available on $\href{https://github.com/mehmetfdemirel/aware}{GitHub}$.},
 author = {Mehmet F Demirel and Shengchao Liu and Siddhant Garg and Zhenmei Shi and Yingyu Liang},
 code = {https://github.com/mehmetfdemirel/aware},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4293307820},
 pdf = {https://openreview.net/pdf?id=TWSTyYd2Rl},
 review = {https://openreview.net/forum?id=TWSTyYd2Rl},
 title = {Attentive Walk-Aggregating Graph Neural Networks},
 url = {https://openreview.net/forum?id=TWSTyYd2Rl},
 year = {2022}
}

@article{deng2023causal,
 abstract = {Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.},
 author = {Zhihong Deng and Jing Jiang and Guodong Long and Chengqi Zhang},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4383468900},
 pdf = {https://openreview.net/pdf?id=qqnttX9LPo},
 review = {https://openreview.net/forum?id=qqnttX9LPo},
 title = {Causal Reinforcement Learning: A Survey},
 url = {https://openreview.net/forum?id=qqnttX9LPo},
 year = {2023}
}

@article{deng2024normatch,
 abstract = {Semi-Supervised Learning (SSL) aims to learn a model using a tiny labeled set and massive amounts of unlabeled data. To better exploit the unlabeled data the latest SSL methods use pseudo-labels predicted from a single discriminative classifier. However, the generated pseudo-labels are inevitably linked to inherent confirmation bias and noise which greatly affects the model performance. In this work we introduce a new framework for SSL named NorMatch. Firstly, we introduce a new uncertainty estimation scheme based on normalizing flows, as an auxiliary classifier, to enforce highly certain pseudo-labels yielding a boost of the discriminative classifiers. Secondly, we introduce a threshold-free sample weighting strategy to exploit better both high and low confidence pseudo-labels. Furthermore, we utilize normalizing flows to model, in an unsupervised fashion, the distribution of unlabeled data. This modelling assumption can further improve the performance of generative classifiers via unlabeled data, and thus, implicitly contributing to training a better discriminative classifier. We demonstrate, through numerical and visual results, that NorMatch achieves state-of-the-art performance on several datasets.},
 author = {Zhongying Deng and Rihuan Ke and Carola-Bibiane Sch{\"o}nlieb and Angelica I Aviles-Rivero},
 code = {https://github.com/Zhongying-Deng/NorMatch},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309396668},
 pdf = {https://openreview.net/pdf?id=ebiAFpQ0Lw},
 review = {https://openreview.net/forum?id=ebiAFpQ0Lw},
 title = {NorMatch: Matching Normalizing Flows with Discriminative Classifiers for Semi-Supervised Learning},
 url = {https://openreview.net/forum?id=ebiAFpQ0Lw},
 year = {2024}
}

@article{deshpande2024are,
 abstract = {Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.},
 author = {Sameer Deshpande and Soumya Ghosh and Tin D. Nguyen and Tamara Broderick},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310629428},
 pdf = {https://openreview.net/pdf?id=n2YifD4Dxo},
 review = {https://openreview.net/forum?id=n2YifD4Dxo},
 title = {Are you using test log-likelihood correctly?},
 url = {https://openreview.net/forum?id=n2YifD4Dxo},
 year = {2024}
}

@article{dhawan2024leveraging,
 abstract = {The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local training epochs increase. Further, FedFish results in global networks that are more amenable to efficient personalization via local fine-tuning on the same or shifted data distributions. For instance, federated pretraining on the C4 dataset, followed by few-shot personalization on Stack Overflow, results in a 7% improvement in next-token prediction by FedFish over FedAvg.},
 author = {Nikita Dhawan and Nicole Elyse Mitchell and Zachary Charles and Zachary Garrett and Gintare Karolina Dziugaite},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4388843343},
 pdf = {https://openreview.net/pdf?id=Ytp9KFKZfZ},
 review = {https://openreview.net/forum?id=Ytp9KFKZfZ},
 title = {Leveraging Function Space Aggregation for Federated Learning at Scale},
 url = {https://openreview.net/forum?id=Ytp9KFKZfZ},
 year = {2024}
}

@article{dhuliawala2024variational,
 author = {Shehzaad Zuzar Dhuliawala and Mrinmaya Sachan and Carl Allen},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=EWv9XGOpB3},
 review = {https://openreview.net/forum?id=EWv9XGOpB3},
 title = {Variational Classification: A Probabilistic Generalization of the Softmax Classifier},
 url = {https://openreview.net/forum?id=EWv9XGOpB3},
 year = {2024}
}

@article{di,
 code = { https://github.com/JRowbottomGit/graff},
 pdf = {https://openreview.net/pdf?id=v5ew3FPTgb},
 review = {https://openreview.net/forum?id=v5ew3FPTgb}
}

@article{diao2023blackbox,
 abstract = {The increasing scale of general-purpose Pre-trained Language Models (PLMs) necessitates the study of more efficient adaptation across different downstream tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL) to resonate with pragmatic interactions between the cloud infrastructure and edge devices. Particularly, instead of fine-tuning the model in the cloud, we adapt PLMs by prompt learning, which efficiently optimizes only a few parameters of the discrete prompts. Moreover, we consider the scenario that we do not have access to the parameters and gradients of the pre-trained models, except for its outputs given inputs. This black-box setting secures the cloud infrastructure from potential attack and misuse to cause a single-point failure, which is preferable to the white-box counterpart by current infrastructures. Under this black-box constraint, we apply a variance-reduced policy gradient algorithm to estimate the gradients of parameters in the categorical distribution of each discrete prompt. In light of our method, the user devices can efficiently tune their tasks by querying the PLMs bounded by a range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the proposed algorithm achieves significant improvement on eight benchmarks in a cloud-device collaboration manner. Finally, we conduct in-depth case studies to comprehensively analyze our method in terms of various data sizes, prompt lengths, training budgets, optimization objectives, prompt transferability, and explanations of the learned prompts. Our code will be available at https://github.com/shizhediao/Black-Box-Prompt-Learning.},
 author = {Shizhe Diao and Zhichao Huang and Ruijia Xu and Xuechun Li and LIN Yong and Xiao Zhou and Tong Zhang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221142421},
 pdf = {https://openreview.net/pdf?id=IvsGP7xRvm},
 review = {https://openreview.net/forum?id=IvsGP7xRvm},
 title = {Black-box Prompt Learning for Pre-trained Language Models},
 url = {https://openreview.net/forum?id=IvsGP7xRvm},
 year = {2023}
}

@article{dimitrov2022data,
 abstract = {Recent attacks have shown that user data can be recovered from FedSGD updates, thus breaking privacy. However, these attacks are of limited practical relevance as federated learning typically uses the FedAvg algorithm. Compared to FedSGD, recovering data from FedAvg updates is much harder as: (i) the updates are computed at unobserved intermediate network weights, (ii) a large number of batches are used, and (iii) labels and network weights vary simultaneously across client steps. In this work, we propose a new optimization-based attack which successfully attacks FedAvg by addressing the above challenges. First, we solve the optimization problem using automatic differentiation that forces a simulation of the client's update that generates the unobserved parameters for the recovered labels and inputs to match the received client update. Second, we address the large number of batches by relating images from different epochs with a permutation invariant prior. Third, we recover the labels by estimating the parameters of existing FedSGD attacks at every FedAvg step. On the popular FEMNIST dataset, we demonstrate that on average we successfully recover >45% of the client's images from realistic FedAvg updates computed on 10 local epochs of 10 batches each with 5 images, compared to only <10% using the baseline. Our findings show many real-world federated learning implementations based on FedAvg are vulnerable.},
 author = {Dimitar Iliev Dimitrov and Mislav Balunovic and Nikola Konstantinov and Martin Vechev},
 code = {https://github.com/eth-sri/fedavg_leakage},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283711440},
 pdf = {https://openreview.net/pdf?id=e7A0B99zJf},
 review = {https://openreview.net/forum?id=e7A0B99zJf},
 title = {Data Leakage in Federated Averaging},
 url = {https://openreview.net/forum?id=e7A0B99zJf},
 year = {2022}
}

@article{dinh2022online,
 abstract = {Solving strategic games with huge action space is a critical yet under-explored topic in economics, operations research and artificial intelligence. This paper proposes new learning algorithms for solving two-player zero-sum normal-form games where the number of pure strategies is prohibitively large. Specifically, we combine no-regret analysis from online learning with Double Oracle (DO) methods from game theory. Our method -- \emph{Online Double Oracle (ODO)} -- is provably convergent to a Nash equilibrium (NE). Most importantly, unlike normal DO methods, ODO is \emph{rationale} in the sense that each agent in ODO can exploit strategic adversary with a regret bound of $\mathcal{O}(\sqrt{T k \log(k)})$ where $k$ is not the total number of pure strategies, but rather the size of \emph{effective strategy set} that is linearly dependent on the support size of the NE. On tens of different real-world games, ODO outperforms DO, PSRO methods, and no-regret algorithms such as Multiplicative Weight Update by a significant margin, both in terms of convergence rate to a NE and average payoff against strategic adversaries.},
 author = {Le Cong Dinh and Stephen Marcus McAleer and Zheng Tian and Nicolas Perez-Nieves and Oliver Slumbers and David Henry Mguni and Jun Wang and Haitham Bou Ammar and Yaodong Yang},
 code = {https://github.com/npvoid/OnlineDoubleOracle},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3137572139},
 pdf = {https://openreview.net/pdf?id=rrMK6hYNSx},
 review = {https://openreview.net/forum?id=rrMK6hYNSx},
 title = {Online Double Oracle.},
 url = {https://openreview.net/forum?id=rrMK6hYNSx},
 year = {2022}
}

@article{dockhorn2023differentially,
 abstract = {While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been demonstrated before for DP generative models. Project page and code: https://nv-tlabs.github.io/DPDM.},
 author = {Tim Dockhorn and Tianshi Cao and Arash Vahdat and Karsten Kreis},
 code = {https://github.com/nv-tlabs/DPDM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306893236},
 pdf = {https://openreview.net/pdf?id=ZPpQk7FJXF},
 review = {https://openreview.net/forum?id=ZPpQk7FJXF},
 title = {Differentially Private Diffusion Models},
 url = {https://openreview.net/forum?id=ZPpQk7FJXF},
 year = {2023}
}

@article{domingo-enrich2023an,
 abstract = {Let $V_* : \mathbb{R}^d \to \mathbb{R}$ be some (possibly non-convex) potential function, and consider the probability measure $\pi \propto e^{-V_*}$. When $\pi$ exhibits multiple modes, it is known that sampling techniques based on Wasserstein gradient flows of the Kullback-Leibler (KL) divergence (e.g. Langevin Monte Carlo) suffer poorly in the rate of convergence, where the dynamics are unable to easily traverse between modes. In stark contrast, the work of Lu et al. (2019; 2022) has shown that the gradient flow of the KL with respect to the Fisher-Rao (FR) geometry exhibits a convergence rate to $\pi$ is that \textit{independent} of the potential function. In this short note, we complement these existing results in the literature by providing an explicit expansion of $\text{KL}(\rho_t^{\text{FR}}\|\pi)$ in terms of $e^{-t}$, where $(\rho_t^{\text{FR}})_{t\geq 0}$ is the FR gradient flow of the KL divergence. In turn, we are able to provide a clean asymptotic convergence rate, where the burn-in time is guaranteed to be finite. Our proof is based on observing a similarity between FR gradient flows and simulated annealing with linear scaling, and facts about cumulant generating functions. We conclude with simple synthetic experiments that demonstrate our theoretical findings are indeed tight. Based on our numerics, we conjecture that the asymptotic rates of convergence for Wasserstein-Fisher-Rao gradient flows are possibly related to this expansion in some cases.},
 author = {Carles Domingo-Enrich and Aram-Alexandre Pooladian},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321855276},
 pdf = {https://openreview.net/pdf?id=9pWjgQ3y85},
 review = {https://openreview.net/forum?id=9pWjgQ3y85},
 title = {An Explicit Expansion of the Kullback-Leibler Divergence along its Fisher-Rao Gradient Flow},
 url = {https://openreview.net/forum?id=9pWjgQ3y85},
 year = {2023}
}

@article{dong2022algorithms,
 abstract = {The phenomenon of data distribution evolving over time has been observed in a range of applications, calling the needs of adaptive learning algorithms. We thus study the problem of supervised gradual domain adaptation, where labeled data from shifting distributions are available to the learner along the trajectory, and we aim to learn a classifier on a target data distribution of interest. Under this setting, we provide the first generalization upper bound on the learning error under mild assumptions. Our results are algorithm agnostic, general for a range of loss functions, and only depend linearly on the averaged learning error across the trajectory. This shows significant improvement compared to the previous upper bound for unsupervised gradual domain adaptation, where the learning error on the target domain depends exponentially on the initial error on the source domain. Compared with the offline setting of learning from multiple domains, our results also suggest the potential benefits of the temporal structure among different domains in adapting to the target one. Empirically, our theoretical results imply that learning proper representations across the domains will effectively mitigate the learning errors. Motivated by these theoretical insights, we propose a min-max learning objective to learn the representation and classifier simultaneously. Experimental results on both semi-synthetic and large-scale real datasets corroborate our findings and demonstrate the effectiveness of our objectives.},
 author = {Jing Dong and Shiji Zhou and Baoxiang Wang and Han Zhao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4224928220},
 pdf = {https://openreview.net/pdf?id=35y5hv9fbb},
 review = {https://openreview.net/forum?id=35y5hv9fbb},
 title = {Algorithms and Theory for Supervised Gradual Domain Adaptation},
 url = {https://openreview.net/forum?id=35y5hv9fbb},
 year = {2022}
}

@article{dong2023raft,
 abstract = {Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.},
 author = {Hanze Dong and Wei Xiong and Deepanshu Goyal and Yihan Zhang and Winnie Chow and Rui Pan and Shizhe Diao and Jipeng Zhang and KaShun SHUM and Tong Zhang},
 code = {https://github.com/OptimalScale/LMFlow},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4366196653},
 pdf = {https://openreview.net/pdf?id=m7p5O7zblY},
 review = {https://openreview.net/forum?id=m7p5O7zblY},
 title = {RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment},
 url = {https://openreview.net/forum?id=m7p5O7zblY},
 year = {2023}
}

@article{drappo2023an,
 author = {Gianluca Drappo and Alberto Maria Metelli and Marcello Restelli},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=VP9p4u9jAo},
 review = {https://openreview.net/forum?id=VP9p4u9jAo},
 title = {An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-{MDP}},
 url = {https://openreview.net/forum?id=VP9p4u9jAo},
 year = {2023}
}

@article{dresdner2023learning,
 abstract = {Despite their ubiquity throughout science and engineering, only a handful of partial differential equations (PDEs) have analytical, or closed-form solutions. This motivates a vast amount of classical work on numerical simulation of PDEs and more recently, a whirlwind of research into data-driven techniques leveraging machine learning (ML). A recent line of work indicates that a hybrid of classical numerical techniques and machine learning can offer significant improvements over either approach alone. In this work, we show that the choice of the numerical scheme is crucial when incorporating physics-based priors. We build upon Fourier-based spectral methods, which are known to be more efficient than other numerical schemes for simulating PDEs with smooth and periodic solutions. Specifically, we develop ML-augmented spectral solvers for three common PDEs of fluid dynamics. Our models are more accurate (2-4x) than standard spectral solvers at the same resolution but have longer overall runtimes (~2x), due to the additional runtime cost of the neural network component. We also demonstrate a handful of key design principles for combining machine learning and numerical methods for solving PDEs.},
 author = {Gideon Dresdner and Dmitrii Kochkov and Peter Christian Norgaard and Leonardo Zepeda-Nunez and Jamie Smith and Michael Brenner and Stephan Hoyer},
 code = {https://github.com/google/jax-cfd/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283813711},
 pdf = {https://openreview.net/pdf?id=wNBARGxoJn},
 review = {https://openreview.net/forum?id=wNBARGxoJn},
 title = {Learning to correct spectral methods for simulating turbulent flows},
 url = {https://openreview.net/forum?id=wNBARGxoJn},
 year = {2023}
}

@article{du2023bidirectional,
 author = {Yuntao Du and å¨ æ± and Hongtao Luo and Haiyang Yang and MingCai Chen and Chongjun Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=WVwnccBJLz},
 review = {https://openreview.net/forum?id=WVwnccBJLz},
 title = {Bidirectional View based Consistency Regularization for Semi-Supervised Domain Adaptation},
 url = {https://openreview.net/forum?id=WVwnccBJLz},
 year = {2023}
}

@article{du2023chemspace,
 abstract = {Discovering meaningful molecules in the vast combinatorial chemical space has been a long-standing challenge in many fields from materials science to drug discovery. Recent advances in machine learning, especially generative models, have made remarkable progress and demonstrate considerable promise for automated molecule design. Nevertheless, most molecule generative models remain black-box systems, whose utility is limited by a lack of interpretability and human participation in the generation process. In this work we propose Chemical Space Explorer (ChemSpacE), a simple yet effective method for exploring the chemical space with pre-trained deep generative models. It enables users to interact with existing generative models and inform the molecule generation process. We demonstrate the efficacy of ChemSpacE on the molecule optimization task and the molecule manipulation task in single property and multi-property settings. On the molecule optimization task, the performance of ChemSpacE is on par with previous black-box optimization methods yet is considerably faster and more sample efficient. Furthermore, the interface from ChemSpacE facilitates human-in-the-loop chemical space exploration and interactive molecule design.},
 author = {Yuanqi Du and Xian Liu and Nilay Mahesh Shah and Shengchao Liu and Jieyu Zhang and Bolei Zhou},
 code = {https://github.com/yuanqidu/ChemSpacE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4301430631},
 pdf = {https://openreview.net/pdf?id=C1Xl8dYCBn},
 review = {https://openreview.net/forum?id=C1Xl8dYCBn},
 title = {ChemSpacE: Interpretable and Interactive Chemical Space Exploration},
 url = {https://openreview.net/forum?id=C1Xl8dYCBn},
 year = {2023}
}

@article{du2023noiserobust,
 abstract = {Teaching Graph Neural Networks (GNNs) to accurately classify nodes under severely noisy labels is an important problem in real-world graph learning applications, but is currently underexplored. Although pairwise training methods have demonstrated promise in supervised metric learning and unsupervised contrastive learning, they remain less studied on noisy graphs, where the structural pairwise interactions (PI) between nodes are abundant and thus might benefit label noise learning rather than the pointwise methods. This paper bridges the gap by proposing a pairwise framework for noisy node classification on graphs, which relies on the PI as a primary learning proxy in addition to the pointwise learning from the noisy node class labels. Our proposed framework PI-GNN contributes two novel components: (1) a confidence-aware PI estimation model that adaptively estimates the PI labels, which are defined as whether the two nodes share the same node labels, and (2) a decoupled training approach that leverages the estimated PI labels to regularize a node classification model for robust node classification. Extensive experiments on different datasets and GNN architectures demonstrate the effectiveness of PI-GNN, yielding a promising improvement over the state-of-the-art methods. Code is publicly available at https://github.com/TianBian95/pi-gnn.},
 author = {Xuefeng Du and Tian Bian and Yu Rong and Bo Han and Tongliang Liu and Tingyang Xu and Wenbing Huang and Yixuan Li and Junzhou Huang},
 code = {https://github.com/TianBian95/pi-gnn},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4379534685},
 pdf = {https://openreview.net/pdf?id=r7imkFEAQb},
 review = {https://openreview.net/forum?id=r7imkFEAQb},
 title = {Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions},
 url = {https://openreview.net/forum?id=r7imkFEAQb},
 year = {2023}
}

@article{duan2024layerdiverse,
 abstract = {Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing.},
 author = {Wei Duan and Jie Lu and Yu Guang Wang and Junyu Xuan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4392972246},
 pdf = {https://openreview.net/pdf?id=WOrdoKbxh6},
 review = {https://openreview.net/forum?id=WOrdoKbxh6},
 title = {Layer-diverse Negative Sampling for Graph Neural Networks},
 url = {https://openreview.net/forum?id=WOrdoKbxh6},
 year = {2024}
}

@article{dumoulin2024a,
 abstract = {Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.},
 author = {Vincent Dumoulin and Daniel D. Johnson and Pablo Samuel Castro and Hugo Larochelle and Yann Dauphin},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/google-deepmind/pbde},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4389072749},
 pdf = {https://openreview.net/pdf?id=YH3oERVYjF},
 review = {https://openreview.net/forum?id=YH3oERVYjF},
 title = {A density estimation perspective on learning from pairwise human preferences},
 url = {https://openreview.net/forum?id=YH3oERVYjF},
 year = {2024}
}

@article{dupont2022coin,
 abstract = {Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the feasibility of our method by compressing various data modalities, from images and audio to medical and climate data.},
 author = {Emilien Dupont and Hrushikesh Loya and Milad Alizadeh and Adam Golinski and Yee Whye Teh and Arnaud Doucet},
 code = {https://github.com/EmilienDupont/coinpp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320169939},
 pdf = {https://openreview.net/pdf?id=NXB0rEM2Tq},
 review = {https://openreview.net/forum?id=NXB0rEM2Tq},
 title = {COIN++: Neural Compression Across Modalities},
 url = {https://openreview.net/forum?id=NXB0rEM2Tq},
 year = {2022}
}

@article{dwaracherla2023ensembles,
 abstract = {In machine learning, an agent needs to estimate uncertainty to efficiently explore and adapt and to make effective decisions. A common approach to uncertainty estimation maintains an ensemble of models. In recent years, several approaches have been proposed for training ensembles, and conflicting views prevail with regards to the importance of various ingredients of these approaches. In this paper, we aim to address the benefits of two ingredients -- prior functions and bootstrapping -- which have come into question. We show that prior functions can significantly improve an ensemble agent's joint predictions across inputs and that bootstrapping affords additional benefits if the signal-to-noise ratio varies across inputs. Our claims are justified by both theoretical and experimental results.},
 author = {Vikranth Dwaracherla and Zheng Wen and Ian Osband and Xiuyuan Lu and Seyed Mohammad Asghari and Benjamin Van Roy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281876281},
 pdf = {https://openreview.net/pdf?id=IqJsyulDUX},
 review = {https://openreview.net/forum?id=IqJsyulDUX},
 title = {Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping},
 url = {https://openreview.net/forum?id=IqJsyulDUX},
 year = {2023}
}

@article{ebrahimi2023testtime,
 abstract = {For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \textit{source} domain to an unlabeled \textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\% in (F1 score), 3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark datasets are available at \url{https://saynaebrahimi.github.io/DocTTA.html}.},
 author = {Sayna Ebrahimi and Sercan O Arik and Tomas Pfister},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283034476},
 pdf = {https://openreview.net/pdf?id=zshemTAa6U},
 review = {https://openreview.net/forum?id=zshemTAa6U},
 title = {Test-Time Adaptation for Visual Document Understanding},
 url = {https://openreview.net/forum?id=zshemTAa6U},
 year = {2023}
}

@article{eduardo2023bayesian,
 abstract = {Bayesian optimization is a methodology for global optimization of unknown and expensive objectives. It combines a surrogate Bayesian regression model with an acquisition function to decide where to evaluate the objective. Typical regression models are given by Gaussian processes with stationary covariance functions. However, these functions are unable to express prior input-dependent information, including possible locations of the optimum. The ubiquity of stationary models has led to the common practice of exploiting prior information via informative mean functions. In this paper, we highlight that these models can perform poorly, especially in high dimensions. We propose novel informative covariance functions for optimization, leveraging nonstationarity to encode preferences for certain regions of the search space and adaptively promote local exploration during optimization. We demonstrate that the proposed functions can increase the sample efficiency of Bayesian optimization in high dimensions, even under weak prior information.},
 author = {Afonso Eduardo and Michael U. Gutmann},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4290056608},
 pdf = {https://openreview.net/pdf?id=JwgVBv18RG},
 review = {https://openreview.net/forum?id=JwgVBv18RG},
 title = {Bayesian Optimization with Informative Covariance},
 url = {https://openreview.net/forum?id=JwgVBv18RG},
 year = {2023}
}

@article{ehrlich2023a,
 abstract = {In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of "Representational Complexity", which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representational complexity decreases both through successive hidden layers and over training, and compare the results to related measures. Overall, we propose representational complexity as a principled and interpretable summary statistic for analyzing the structure and evolution of neural representations and complex systems in general.},
 author = {David Alexander Ehrlich and Andreas Christian Schneider and Viola Priesemann and Michael Wibral and Abdullah Makkeh},
 code = {https://github.com/Priesemann-Group/nninfo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297900009},
 pdf = {https://openreview.net/pdf?id=R8TU3pfzFr},
 review = {https://openreview.net/forum?id=R8TU3pfzFr},
 title = {A Measure of the Complexity of Neural Representations based on Partial Information Decomposition},
 url = {https://openreview.net/forum?id=R8TU3pfzFr},
 year = {2023}
}

@article{eikema2022an,
 author = {Bryan Eikema and Germ{\'a}n Kruszewski and Christopher R Dance and Hady Elsahar and Marc Dymetman},
 code = {https://disco.europe.naverlabs.com/QRS/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=VW4IrC0n0M},
 review = {https://openreview.net/forum?id=VW4IrC0n0M},
 title = {An approximate sampler for energy-based models with divergence diagnostics},
 url = {https://openreview.net/forum?id=VW4IrC0n0M},
 year = {2022}
}

@article{eiras2022ancer,
 abstract = {Randomized smoothing has recently emerged as an effective tool that enables certification of deep neural network classifiers at scale. All prior art on randomized smoothing has focused on isotropic $\ell_p$ certification, which has the advantage of yielding certificates that can be easily compared among isotropic methods via $\ell_p$-norm radius. However, isotropic certification limits the region that can be certified around an input to worst-case adversaries, i.e., it cannot reason about other "close", potentially large, constant prediction safe regions. To alleviate this issue, (i) we theoretically extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to their generalized anisotropic counterparts following a simplified analysis. Moreover, (ii) we propose evaluation metrics allowing for the comparison of general certificates - a certificate is superior to another if it certifies a superset region - with the quantification of each certificate through the volume of the certified region. We introduce ANCER, a framework for obtaining anisotropic certificates for a given test set sample via volume maximization. We achieve it by generalizing memory-based certification of data-dependent classifiers. Our empirical results demonstrate that ANCER achieves state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on CIFAR-10 and ImageNet in the data-dependence setting, while certifying larger regions in terms of volume, highlighting the benefits of moving away from isotropic analysis. Our code is available in https://github.com/MotasemAlfarra/ANCER.},
 author = {Francisco Eiras and Motasem Alfarra and Philip Torr and M. Pawan Kumar and Puneet K. Dokania and Bernard Ghanem and Adel Bibi},
 code = {https://github.com/MotasemAlfarra/ANCER},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287081309},
 pdf = {https://openreview.net/pdf?id=7j0GI6tPYi},
 review = {https://openreview.net/forum?id=7j0GI6tPYi},
 title = {ANCER: Anisotropic Certification via Sample-wise Volume Maximization},
 url = {https://openreview.net/forum?id=7j0GI6tPYi},
 year = {2022}
}

@article{ek2023offpolicy,
 abstract = {We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finite-sample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy - including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.},
 author = {Sofia Ek and Dave Zachariah and Fredrik D. Johansson and Peter Stoica},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/sofiaek/off-policy-evaluation},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4317838029},
 pdf = {https://openreview.net/pdf?id=XnYtGPgG9p},
 review = {https://openreview.net/forum?id=XnYtGPgG9p},
 title = {Off-Policy Evaluation with Out-of-Sample Guarantees},
 url = {https://openreview.net/forum?id=XnYtGPgG9p},
 year = {2023}
}

@article{el,
 code = {https://github.com/tamim-el/Fast-Kernel-Methods-for-Generic-Lipschitz-Losses-via-p-Sparsified-Sketches},
 pdf = {https://openreview.net/pdf?id=ry2qgRqTOw},
 review = {https://openreview.net/forum?id=ry2qgRqTOw}
}

@article{el,
 badge = {Featured},
 code = {https://github.com/ServiceNow/workflow-discovery},
 pdf = {https://openreview.net/pdf?id=L9othQvPks},
 review = {https://openreview.net/forum?id=L9othQvPks}
}

@article{el-nouby2023image,
 abstract = {Recent neural compression methods have been based on the popular hyperprior framework. It relies on Scalar Quantization and offers a very strong compression performance. This contrasts from recent advances in image generation and representation learning, where Vector Quantization is more commonly employed. In this work, we attempt to bring these lines of research closer by revisiting vector quantization for image compression. We build upon the VQ-VAE framework and introduce several modifications. First, we replace the vanilla vector quantizer by a product quantizer. This intermediate solution between vector and scalar quantization allows for a much wider set of rate-distortion points: It implicitly defines high-quality quantizers that would otherwise require intractably large codebooks. Second, inspired by the success of Masked Image Modeling (MIM) in the context of self-supervised learning and generative image models, we propose a novel conditional entropy model which improves entropy coding by modelling the co-dependencies of the quantized latent codes. The resulting PQ-MIM model is surprisingly effective: its compression performance on par with recent hyperprior methods. It also outperforms HiFiC in terms of FID and KID metrics when optimized with perceptual losses (e.g. adversarial). Finally, since PQ-MIM is compatible with image generation frameworks, we show qualitatively that it can operate under a hybrid mode between compression and generation, with no further training or finetuning. As a result, we explore the extreme compression regime where an image is compressed into 200 bytes, i.e., less than a tweet.},
 author = {Alaaeldin El-Nouby and Matthew J. Muckley and Karen Ullrich and Ivan Laptev and Jakob Verbeek and Herve Jegou},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4311638103},
 pdf = {https://openreview.net/pdf?id=Z2L5d9ay4B},
 review = {https://openreview.net/forum?id=Z2L5d9ay4B},
 title = {Image Compression with Product Quantized Masked Image Modeling},
 url = {https://openreview.net/forum?id=Z2L5d9ay4B},
 year = {2023}
}

@article{elahi2023contextual,
 abstract = {In federated multi-armed bandit problems, maximizing global reward while satisfying minimum privacy requirements to protect clients is the main goal. To formulate such problems, we consider a combinatorial contextual bandit setting with groups and changing action sets, where similar base arms arrive in groups and a set of base arms, called a super arm, must be chosen in each round to maximize super arm reward while satisfying the constraints of the rewards of groups from which base arms were chosen. To allow for greater flexibility, we let each base arm have two outcomes, modeled as the output of a two-output Gaussian process (GP), where one outcome is used to compute super arm reward and the other for group reward. We then propose a novel double-UCB GP-bandit algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB), which balances between maximizing cumulative super arm reward and satisfying group reward constraints and can be tuned to prefer one over the other. We also define a new notion of regret that combines super arm regret with group reward constraint satisfaction and prove that TCGP-UCB incurs $\tilde{O}(\sqrt{\lambda^*(K)KT\overline{\gamma}_{T}} )$ regret with high probability, where $\overline{\gamma}_{T}$ is the maximum information gain associated with the set of base arm contexts that appeared in the first $T$ rounds and $K$ is the maximum super arm cardinality over all rounds. We lastly show in experiments using synthetic and real-world data and based on a federated learning setup as well as a content-recommendation one that our algorithm performs better then the current non-GP state-of-the-art combinatorial bandit algorithm, while satisfying group constraints.},
 author = {Sepehr Elahi and Baran Atalar and Sevda {\"O}{\u{g}}{\"u}t and Cem Tekin},
 code = {https://github.com/Bilkent-CYBORG/TCGP-UCB},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286849951},
 pdf = {https://openreview.net/pdf?id=OqbGu3hdQb},
 review = {https://openreview.net/forum?id=OqbGu3hdQb},
 title = {Contextual Combinatorial Multi-output GP Bandits with Group Constraints},
 url = {https://openreview.net/forum?id=OqbGu3hdQb},
 year = {2023}
}

@article{englesson2023logisticnormal,
 abstract = {A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This formulation has desirable loss attenuation properties, as it reduces the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. Furthermore, we discuss and address some practical challenges of this extension. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and other insightful analyses.},
 author = {Erik Englesson and Amir Mehrpanah and Hossein Azizpour},
 code = {https://github.com/ErikEnglesson/Logistic-Normal},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4362706561},
 pdf = {https://openreview.net/pdf?id=7wA65zL3B3},
 review = {https://openreview.net/forum?id=7wA65zL3B3},
 title = {Logistic-Normal Likelihoods for Heteroscedastic Label Noise},
 url = {https://openreview.net/forum?id=7wA65zL3B3},
 year = {2023}
}

@article{espinosa,
 code = {https://github.com/mateoespinosa/tabcbm},
 pdf = {https://openreview.net/pdf?id=TIsrnWpjQ0},
 review = {https://openreview.net/forum?id=TIsrnWpjQ0}
}

@article{everett2023protocaps,
 abstract = {Capsule Networks have emerged as a powerful class of deep learning architectures, known for robust performance with relatively few parameters compared to Convolutional Neural Networks (CNNs). However, their inherent efficiency is often overshadowed by their slow, iterative routing mechanisms which establish connections between Capsule layers, posing computational challenges resulting in an inability to scale. In this paper, we introduce a novel, non-iterative routing mechanism, inspired by trainable prototype clustering. This innovative approach aims to mitigate computational complexity, while retaining, if not enhancing, performance efficacy. Furthermore, we harness a shared Capsule subspace, negating the need to project each lower-level Capsule to each higher-level Capsule, thereby significantly reducing memory requisites during training. Our approach demonstrates superior results compared to the current best non-iterative Capsule Network and tests on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches. Our findings underscore the potential of our proposed methodology in enhancing the operational efficiency and performance of Capsule Networks, paving the way for their application in increasingly complex computational scenarios. Code is available at https://github.com/mileseverett/ProtoCaps.},
 author = {Miles Everett and Mingjun Zhong and Georgios Leontidis},
 code = {https://github.com/mileseverett/ProtoCaps},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384919619},
 pdf = {https://openreview.net/pdf?id=Id10mlBjcx},
 review = {https://openreview.net/forum?id=Id10mlBjcx},
 title = {ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method},
 url = {https://openreview.net/forum?id=Id10mlBjcx},
 year = {2023}
}

@article{evtimova2022sparse,
 abstract = {Sparse representations of images are useful in many computer vision applications. Sparse coding with an $l_1$ penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the $l_1$ norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements. In this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries. In our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime.},
 author = {Katrina Evtimova and Yann LeCun},
 code = {https://github.com/kevtimova/deep-sparse},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226383559},
 pdf = {https://openreview.net/pdf?id=4GuIi1jJ74},
 review = {https://openreview.net/forum?id=4GuIi1jJ74},
 title = {Sparse Coding with Multi-Layer Decoders using Variance Regularization},
 url = {https://openreview.net/forum?id=4GuIi1jJ74},
 year = {2022}
}

@article{f,
 code = {https://github.com/im-ethz/pub-gdu4dg},
 pdf = {https://openreview.net/pdf?id=V7BvYJyTmM},
 review = {https://openreview.net/forum?id=V7BvYJyTmM}
}

@article{fakorede2023vulnerabilityaware,
 abstract = {Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiments, we show that our proposed method significantly improves over existing reweighting schemes, especially against strong white and black-box attacks.},
 author = {Olukorede Fakorede and Ashutosh Kumar Nirala and Modeste Atsague and Jin Tian},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384615640},
 pdf = {https://openreview.net/pdf?id=kdPcLdJbt1},
 review = {https://openreview.net/forum?id=kdPcLdJbt1},
 title = {Vulnerability-Aware Instance Reweighting For Adversarial Training},
 url = {https://openreview.net/forum?id=kdPcLdJbt1},
 year = {2023}
}

@article{fallah2024manifold,
 abstract = {Self-supervised learning of deep neural networks has become a prevalent paradigm for learning representations that transfer to a variety of downstream tasks. Similar to proposed models of the ventral stream of biological vision, it is observed that these networks lead to a separation of category manifolds in the representations of the penultimate layer. Although this observation matches the manifold hypothesis of representation learning, current self-supervised approaches are limited in their ability to explicitly model this manifold. Indeed, current approaches often only apply augmentations from a pre-specified set of "positive pairs" during learning. In this work, we propose a contrastive learning approach that directly models the latent manifold using Lie group operators parameterized by coefficients with a sparsity-promoting prior. A variational distribution over these coefficients provides a generative model of the manifold, with samples which provide feature augmentations applicable both during contrastive training and downstream tasks. Additionally, learned coefficient distributions provide a quantification of which transformations are most likely at each point on the manifold while preserving identity. We demonstrate benefits in self-supervised benchmarks for image datasets, as well as a downstream semi-supervised task. In the former case, we demonstrate that the proposed methods can effectively apply manifold feature augmentations and improve learning both with and without a projection head. In the latter case, we demonstrate that feature augmentations sampled from learned Lie group operators can improve classification performance when using few labels.},
 author = {Kion Fallah and Alec Helbling and Kyle A. Johnsen and Christopher John Rozell},
 code = {https://github.com/kfallah/manifold-contrastive},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4382132543},
 pdf = {https://openreview.net/pdf?id=lVE1VeGQwg},
 review = {https://openreview.net/forum?id=lVE1VeGQwg},
 title = {Manifold Contrastive Learning with Variational Lie Group Operators},
 url = {https://openreview.net/forum?id=lVE1VeGQwg},
 year = {2024}
}

@article{fan2023euclideannorminduced,
 abstract = {The nuclear norm and Schatten-$p$ quasi-norm are popular rank proxies in low-rank matrix recovery. However, computing the nuclear norm or Schatten-$p$ quasi-norm of a tensor is hard in both theory and practice, hindering their application to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose a new class of tensor rank regularizers based on the Euclidean norms of the CP component vectors of a tensor and show that these regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm. This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the nuclear norm. On the other hand, we study the generalization abilities of LRTC with the Schatten-$p$ quasi-norm regularizer and LRTC with the proposed regularizers. The theorems show that a relatively sharper regularizer leads to a tighter error bound, which is consistent with our numerical results. Particularly, we prove that for LRTC with Schatten-$p$ quasi-norm regularizer on $d$-order tensors, $p=1/d$ is always better than any $p>1/d$ in terms of the generalization ability. We also provide a recovery error bound to verify the usefulness of small $p$ in the Schatten-$p$ quasi-norm for TRPCA. Numerical results on synthetic data and real data demonstrate the effectiveness of the regularization methods and theorems.},
 author = {Jicong Fan and Lijun Ding and Chengrun Yang and Zhao Zhang and Madeleine Udell},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319451686},
 pdf = {https://openreview.net/pdf?id=Grhi800jVz},
 review = {https://openreview.net/forum?id=Grhi800jVz},
 title = {Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis},
 url = {https://openreview.net/forum?id=Grhi800jVz},
 year = {2023}
}

@article{fan2023federated,
 author = {Ziqing Fan and Jiangchao Yao and Ruipeng Zhang and Lingjuan Lyu and Yanfeng Wang and Ya Zhang},
 code = {https://github.com/MediaBrain-SJTU/FedMR.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=jLJTqJXAG7},
 review = {https://openreview.net/forum?id=jLJTqJXAG7},
 title = {Federated Learning under Partially Disjoint Data via Manifold Reshaping},
 url = {https://openreview.net/forum?id=jLJTqJXAG7},
 year = {2023}
}

@article{fan2023neural,
 abstract = {Monge map refers to the optimal transport map between two probability distributions and provides a principled approach to transform one distribution to another. Neural network based optimal transport map solver has gained great attention in recent years. Along this line, we present a scalable algorithm for computing the neural Monge map between two probability distributions. Our algorithm is based on a weak form of the optimal transport problem, thus it only requires samples from the marginals instead of their analytic expressions, and can accommodate optimal transport between two distributions with different dimensions. Our algorithm is suitable for general cost functions, compared with other existing methods for estimating Monge maps using samples, which are usually for quadratic costs. The performance of our algorithms is demonstrated through a series of experiments with both synthetic and realistic data, including text-to-image generation and image inpainting tasks.},
 author = {Jiaojiao Fan and Shu Liu and Shaojun Ma and Hao-Min Zhou and Yongxin Chen},
 badge = {Featured},
 code = {https://github.com/sbyebss/monge_map_solver},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W3172997368},
 pdf = {https://openreview.net/pdf?id=2mZSlQscj3},
 review = {https://openreview.net/forum?id=2mZSlQscj3},
 title = {Neural Monge Map estimation and its applications},
 url = {https://openreview.net/forum?id=2mZSlQscj3},
 year = {2023}
}

@article{fan2024transfer,
 abstract = {Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.},
 author = {Zhou Fan and Xinran Han and Zi Wang},
 code = {https://github.com/Evensgn/hyperbo-mphd},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387225845},
 pdf = {https://openreview.net/pdf?id=emXh4M7TyH},
 review = {https://openreview.net/forum?id=emXh4M7TyH},
 title = {Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces},
 url = {https://openreview.net/forum?id=emXh4M7TyH},
 year = {2024}
}

@article{fang2023optimal,
 abstract = {Convolutional neural networks have shown impressive abilities in many applications, especially those related to the classification tasks. However, for the regression problem, the abilities of convolutional structures have not been fully understood, and further investigation is needed. In this paper, we consider the mean squared error analysis for deep convolutional neural networks. We show that, for additive ridge functions, convolutional neural networks followed by one fully connected layer with ReLU activation functions can reach optimal mini-max rates (up to a log factor). The input dimension only appears in the constant of convergence rates. This work shows the statistical optimality of convolutional neural networks and may shed light on why convolutional neural networks are able to behave well for high dimensional input.},
 author = {Zhiying Fang and Guang Cheng},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221157738},
 pdf = {https://openreview.net/pdf?id=Q6ZXm7VBFY},
 review = {https://openreview.net/forum?id=Q6ZXm7VBFY},
 title = {Optimal Convergence Rates of Deep Convolutional Neural Networks: Additive Ridge Functions},
 url = {https://openreview.net/forum?id=Q6ZXm7VBFY},
 year = {2023}
}

@article{fawzi2023on,
 author = {Omar Fawzi and Nicolas Flammarion and Aur{\'e}lien Garivier and Aadil Oufkir},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Hf95zFnQ7H},
 review = {https://openreview.net/forum?id=Hf95zFnQ7H},
 title = {On Adaptivity in Quantum Testing},
 url = {https://openreview.net/forum?id=Hf95zFnQ7H},
 year = {2023}
}

@article{fay2023adaptive,
 author = {Dominik Fay and Sindri Magn{\'u}sson and Jens Sj{\"o}lund and Mikael Johansson},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LLKI5Lq2YN},
 review = {https://openreview.net/forum?id=LLKI5Lq2YN},
 title = {Adaptive Hyperparameter Selection for Differentially Private Gradient Descent},
 url = {https://openreview.net/forum?id=LLKI5Lq2YN},
 year = {2023}
}

@article{feeney2023novelcraft,
 abstract = {In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects of varying size within the complex 3D scene that may impact gameplay. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. Further multimodal novelty detection experiments suggest that methods that fuse both visual and symbolic information can improve time until detection as well as overall discrimination. Finally, our evaluation of recent generalized category discovery methods suggests that adapting to new imbalanced categories in complex scenes remains an exciting open problem.},
 author = {Patrick Feeney and Sarah Schneider and Panagiotis Lymperopoulos and Liping Liu and Matthias Scheutz and Michael C Hughes},
 code = {https://novelcraft.cs.tufts.edu/data_access.html},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283465628},
 pdf = {https://openreview.net/pdf?id=4eL6z9ziw7},
 review = {https://openreview.net/forum?id=4eL6z9ziw7},
 title = {NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds},
 url = {https://openreview.net/forum?id=4eL6z9ziw7},
 year = {2023}
}

@article{feldman2023achieving,
 abstract = {To provide rigorous uncertainty quantification for online learning models, we develop a framework for constructing uncertainty sets that provably control risk -- such as coverage of confidence intervals, false negative rate, or F1 score -- in the online setting. This extends conformal prediction to apply to a larger class of online learning problems. Our method guarantees risk control at any user-specified level even when the underlying data distribution shifts drastically, even adversarially, over time in an unknown fashion. The technique we propose is highly flexible as it can be applied with any base online learning algorithm (e.g., a deep neural network trained online), requiring minimal implementation effort and essentially zero additional computational cost. We further extend our approach to control multiple risks simultaneously, so the prediction sets we generate are valid for all given risks. To demonstrate the utility of our method, we conduct experiments on real-world tabular time-series data sets showing that the proposed method rigorously controls various natural risks. Furthermore, we show how to construct valid intervals for an online image-depth estimation problem that previous sequential calibration schemes cannot handle.},
 author = {Shai Feldman and Liran Ringel and Stephen Bates and Yaniv Romano},
 code = {https://github.com/Shai128/rrc},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281257214},
 pdf = {https://openreview.net/pdf?id=5Y04GWvoJu},
 review = {https://openreview.net/forum?id=5Y04GWvoJu},
 title = {Achieving Risk Control in Online Learning Settings},
 url = {https://openreview.net/forum?id=5Y04GWvoJu},
 year = {2023}
}

@article{feldman2023weisfeiler,
 abstract = {Graph isomorphism testing is usually approached via the comparison of graph invariants. Two popular alternatives that offer a good trade-off between expressive power and computational efficiency are combinatorial (i.e., obtained via the Weisfeiler-Leman (WL) test) and spectral invariants. While the exact power of the latter is still an open question, the former is regularly criticized for its limited power, when a standard configuration of uniform pre-coloring is used. This drawback hinders the applicability of Message Passing Graph Neural Networks (MPGNNs), whose expressive power is upper bounded by the WL test. Relaxing the assumption of uniform pre-coloring, we show that one can increase the expressive power of the WL test ad infinitum. Following that, we propose an efficient pre-coloring based on spectral features that provably increase the expressive power of the vanilla WL test. The above claims are accompanied by extensive synthetic and real data experiments. The code to reproduce our experiments is available at https://github.com/TPFI22/Spectral-and-Combinatorial},
 author = {Or Feldman and Amit Boyarski and Shai Feldman and Dani Kogan and Avi Mendelson and Chaim Baskin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221167693},
 pdf = {https://openreview.net/pdf?id=YJDqQSAuB6},
 review = {https://openreview.net/forum?id=YJDqQSAuB6},
 title = {Weisfeiler and Leman Go Infinite: Spectral and Combinatorial Pre-Colorings},
 url = {https://openreview.net/forum?id=YJDqQSAuB6},
 year = {2023}
}

@article{feng2023monotone,
 abstract = {Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference. In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference? In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular choice of activation results in a fixed-point iteration that gives a variational mean-field solution. While this approach is still largely conceptual, it is the first architecture that allows for efficient approximate inference in fully-general weight structures for DBMs. We apply this approach to simple deep convolutional Boltzmann architectures and demonstrate that it allows for tasks such as the joint completion and classification of images, within a single deep probabilistic setting, while avoiding the pitfalls of mean-field inference in traditional RBMs.},
 author = {Zhili Feng and Ezra Winston and J Zico Kolter},
 code = {https://github.com/locuslab/MonotoneDBM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384111941},
 pdf = {https://openreview.net/pdf?id=SgTKk6ryPr},
 review = {https://openreview.net/forum?id=SgTKk6ryPr},
 title = {Monotone deep Boltzmann machines},
 url = {https://openreview.net/forum?id=SgTKk6ryPr},
 year = {2023}
}

@article{feuer2023distributionally,
 abstract = {Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.},
 author = {Benjamin Feuer and Ameya Joshi and Minh Pham and Chinmay Hegde},
 code = {https://www.github.com/penfever/vlhub/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385714471},
 pdf = {https://openreview.net/pdf?id=D5Z2E8CNsD},
 review = {https://openreview.net/forum?id=D5Z2E8CNsD},
 title = {Distributionally Robust Classification on a Data Budget},
 url = {https://openreview.net/forum?id=D5Z2E8CNsD},
 year = {2023}
}

@article{fini2023improved,
 abstract = {Contrastive learning has emerged as an efficient framework to learn multimodal representations. CLIP, a seminal work in this area, achieved impressive results by training on paired image-text data using the contrastive loss. Recent work claims improvements over CLIP using additional non-contrastive losses inspired from self-supervised learning. However, it is sometimes hard to disentangle the contribution of these additional losses from other implementation details, e.g., data augmentation or regularization techniques, used to train the model. To shed light on this matter, in this paper, we first propose, implement and evaluate several baselines obtained by combining contrastive learning with recent advances in self-supervised learning. In particular, we use the loss functions that were proven successful for visual self-supervised learning to align image and text modalities. We find that these baselines outperform a basic implementation of CLIP. However, when a stronger training recipe is employed, the advantage disappears. Indeed, we find that a simple CLIP baseline can also be improved substantially, up to a 25% relative improvement on downstream zero-shot tasks, by using well-known training techniques that are popular in other subfields. Moreover, we discover that it is enough to apply image and text augmentations to make up for most of the improvement attained by prior works. With our improved training recipe for CLIP, we obtain state-of-the-art performance on four standard datasets, and consistently outperform prior work (up to +4% on the largest dataset), while being substantially simpler. The code is available at https://github.com/facebookresearch/clip-rocket},
 author = {Enrico Fini and Pietro Astolfi and Adriana Romero-Soriano and Jakob Verbeek and Michal Drozdzal},
 badge = {Featured},
 code = {https://github.com/facebookresearch/clip-rocket},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4376654228},
 pdf = {https://openreview.net/pdf?id=a7nvXxNmdV},
 review = {https://openreview.net/forum?id=a7nvXxNmdV},
 title = {Improved baselines for vision-language pre-training},
 url = {https://openreview.net/forum?id=a7nvXxNmdV},
 year = {2023}
}

@article{fisch2022calibrated,
 abstract = {Selective classification allows models to abstain from making predictions (e.g., say "I don't know") when in doubt in order to obtain better effective accuracy. While typical selective models can be effective at producing more accurate predictions on average, they may still allow for wrong predictions that have high confidence, or skip correct predictions that have low confidence. Providing calibrated uncertainty estimates alongside predictions -- probabilities that correspond to true frequencies -- can be as important as having predictions that are simply accurate on average. However, uncertainty estimates can be unreliable for certain inputs. In this paper, we develop a new approach to selective classification in which we propose a method for rejecting examples with "uncertain" uncertainties. By doing so, we aim to make predictions with {well-calibrated} uncertainty estimates over the distribution of accepted examples, a property we call selective calibration. We present a framework for learning selectively calibrated models, where a separate selector network is trained to improve the selective calibration error of a given base model. In particular, our work focuses on achieving robust calibration, where the model is intentionally designed to be tested on out-of-domain data. We achieve this through a training strategy inspired by distributionally robust optimization, in which we apply simulated input perturbations to the known, in-domain training data. We demonstrate the empirical effectiveness of our approach on multiple image classification and lung cancer risk assessment tasks.},
 author = {Adam Fisch and Tommi S. Jaakkola and Regina Barzilay},
 code = {https://github.com/ajfisch/calibrated-selective-classification},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4293352322},
 pdf = {https://openreview.net/pdf?id=zFhNBs8GaV},
 review = {https://openreview.net/forum?id=zFhNBs8GaV},
 title = {Calibrated Selective Classification},
 url = {https://openreview.net/forum?id=zFhNBs8GaV},
 year = {2022}
}

@article{fishman2023diffusion,
 abstract = {Denoising diffusion models are a novel class of generative algorithms that achieve state-of-the-art performance across a range of domains, including image generation and text-to-image tasks. Building on this success, diffusion models have recently been extended to the Riemannian manifold setting, broadening their applicability to a range of problems from the natural and engineering sciences. However, these Riemannian diffusion models are built on the assumption that their forward and backward processes are well-defined for all times, preventing them from being applied to an important set of tasks that consider manifolds defined via a set of inequality constraints. In this work, we introduce a principled framework to bridge this gap. We present two distinct noising processes based on (i) the logarithmic barrier metric and (ii) the reflected Brownian motion induced by the constraints. As existing diffusion model techniques cannot be applied in this setting, we derive new tools to define such models in our framework. We then demonstrate the practical utility of our methods on a number of synthetic and real-world tasks, including applications from robotics and protein design.},
 author = {Nic Fishman and Leo Klarner and Valentin De Bortoli and Emile Mathieu and Michael John Hutchinson},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/oxcsml/constrained-diffusion},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4365211705},
 pdf = {https://openreview.net/pdf?id=xuWTFQ4VGO},
 review = {https://openreview.net/forum?id=xuWTFQ4VGO},
 title = {Diffusion Models for Constrained Domains},
 url = {https://openreview.net/forum?id=xuWTFQ4VGO},
 year = {2023}
}

@article{fortuin2022deep,
 abstract = {Uncertainty estimation in deep learning has recently emerged as a crucial area of interest to advance reliability and robustness in safety-critical applications. While there have been many proposed methods that either focus on distance-aware model uncertainties for out-of-distribution detection or on input-dependent label uncertainties for in-distribution calibration, both of these types of uncertainty are often necessary. In this work, we propose the HetSNGP method for jointly modeling the model and data uncertainty. We show that our proposed model affords a favorable combination between these two types of uncertainty and thus outperforms the baseline methods on some challenging out-of-distribution datasets, including CIFAR-100C, ImageNet-C, and ImageNet-A. Moreover, we propose HetSNGP Ensemble, an ensembled version of our method which additionally models uncertainty over the network parameters and outperforms other ensemble baselines.},
 author = {Vincent Fortuin and Mark Collier and Florian Wenzel and James Urquhart Allingham and Jeremiah Zhe Liu and Dustin Tran and Balaji Lakshminarayanan and Jesse Berent and Rodolphe Jenatton and Effrosyni Kokiopoulou},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/google/edward2/blob/main/edward2/tensorflow/layers/hetsngp.py},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4286909125},
 pdf = {https://openreview.net/pdf?id=Id7hTt78FV},
 review = {https://openreview.net/forum?id=Id7hTt78FV},
 title = {Deep Classifiers with Label Noise Modeling and Distance Awareness},
 url = {https://openreview.net/forum?id=Id7hTt78FV},
 year = {2022}
}

@article{fr,
 pdf = {https://openreview.net/pdf?id=UntUoeLwwu},
 review = {https://openreview.net/forum?id=UntUoeLwwu}
}

@article{franks2023a,
 author = {Billy Joe Franks and Markus Anders and Marius Kloft and Pascal Schweitzer},
 code = {https://github.com/bjfranks/IRNI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=AXUtAIX0Fn},
 review = {https://openreview.net/forum?id=AXUtAIX0Fn},
 title = {A Systematic Approach to Universal Random Features in Graph Neural Networks},
 url = {https://openreview.net/forum?id=AXUtAIX0Fn},
 year = {2023}
}

@article{frick2024mc,
 author = {Thomas Frick and Diego Antognini and Ioana Giurgiu and Benjamin F Grewe and Cristiano Malossi and Rong J.B. Zhu and Mattia Rigotti},
 code = {https://github.com/IBM/mc-layernorm},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=bG3ICt3E0C},
 review = {https://openreview.net/forum?id=bG3ICt3E0C},
 title = {{MC} Layer Normalization for calibrated uncertainty in Deep Learning},
 url = {https://openreview.net/forum?id=bG3ICt3E0C},
 year = {2024}
}

@article{friedman2023the,
 abstract = {Diversity is an important criterion for many areas of machine learning (ML), including generative modeling and dataset curation. However, existing metrics for measuring diversity are often domain-specific and limited in flexibility. In this paper, we address the diversity evaluation problem by proposing the Vendi Score, which connects and extends ideas from ecology and quantum statistical mechanics to ML. The Vendi Score is defined as the exponential of the Shannon entropy of the eigenvalues of a similarity matrix. This matrix is induced by a user-defined similarity function applied to the sample to be evaluated for diversity. In taking a similarity function as input, the Vendi Score enables its user to specify any desired form of diversity. Importantly, unlike many existing metrics in ML, the Vendi Score does not require a reference dataset or distribution over samples or labels, it is therefore general and applicable to any generative model, decoding algorithm, and dataset from any domain where similarity can be defined. We showcase the Vendi Score on molecular generative modeling where we found it addresses shortcomings of the current diversity metric of choice in that domain. We also applied the Vendi Score to generative models of images and decoding algorithms of text where we found it confirms known results about diversity in those domains. Furthermore, we used the Vendi Score to measure mode collapse, a known shortcoming of generative adversarial networks (GANs). In particular, the Vendi Score revealed that even GANs that capture all the modes of a labeled dataset can be less diverse than the original dataset. Finally, the interpretability of the Vendi Score allowed us to diagnose several benchmark ML datasets for diversity, opening the door for diversity-informed data augmentation.},
 author = {Dan Friedman and Adji Bousso Dieng},
 code = {https://github.com/vertaix/Vendi-Score},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4303443407},
 pdf = {https://openreview.net/pdf?id=g97OHbQyk1},
 review = {https://openreview.net/forum?id=g97OHbQyk1},
 title = {The Vendi Score: A Diversity Evaluation Metric for Machine Learning},
 url = {https://openreview.net/forum?id=g97OHbQyk1},
 year = {2023}
}

@article{fu2023forces,
 abstract = {Molecular dynamics (MD) simulation techniques are widely used for various natural science applications. Increasingly, machine learning (ML) force field (FF) models begin to replace ab-initio simulations by predicting forces directly from atomic structures. Despite significant progress in this area, such techniques are primarily benchmarked by their force/energy prediction errors, even though the practical use case would be to produce realistic MD trajectories. We aim to fill this gap by introducing a novel benchmark suite for learned MD simulation. We curate representative MD systems, including water, organic molecules, a peptide, and materials, and design evaluation metrics corresponding to the scientific objectives of respective systems. We benchmark a collection of state-of-the-art (SOTA) ML FF models and illustrate, in particular, how the commonly benchmarked force accuracy is not well aligned with relevant simulation metrics. We demonstrate when and how selected SOTA methods fail, along with offering directions for further improvement. Specifically, we identify stability as a key metric for ML models to improve. Our benchmark suite comes with a comprehensive open-source codebase for training and simulation with ML FFs to facilitate future work.},
 author = {Xiang Fu and Zhenghao Wu and Wujie Wang and Tian Xie and Sinan Keten and Rafael Gomez-Bombarelli and Tommi S. Jaakkola},
 badge = {Survey},
 code = {https://github.com/kyonofx/MDsim},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4306313211},
 pdf = {https://openreview.net/pdf?id=A8pqQipwkt},
 review = {https://openreview.net/forum?id=A8pqQipwkt},
 title = {Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations},
 url = {https://openreview.net/forum?id=A8pqQipwkt},
 year = {2023}
}

@article{fu2023simulate,
 abstract = {Molecular dynamics (MD) simulation is essential for various scientific domains but computationally expensive. Learning-based force fields have made significant progress in accelerating ab-initio MD simulation but are not fast enough for many real-world applications due to slow inference for large systems and small time steps (femtosecond-level). We aim to address these challenges by learning a multi-scale graph neural network that directly simulates coarse-grained MD with a very large time step (nanosecond-level) and a novel refinement module based on diffusion models to mitigate simulation instability. The effectiveness of our method is demonstrated in two complex systems: single-chain coarse-grained polymers and multi-component Li-ion polymer electrolytes. For evaluation, we simulate trajectories much longer than the training trajectories for systems with different chemical compositions that the model is not trained on. Structural and dynamical properties can be accurately recovered at several orders of magnitude higher speed than classical force fields by getting out of the femtosecond regime.},
 author = {Xiang Fu and Tian Xie and Nathan J. Rebello and Bradley Olsen and Tommi S. Jaakkola},
 code = {https://github.com/kyonofx/mlcgmd},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4224457748},
 pdf = {https://openreview.net/pdf?id=y8RZoPjEUl},
 review = {https://openreview.net/forum?id=y8RZoPjEUl},
 title = {Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-Scale Graph Networks},
 url = {https://openreview.net/forum?id=y8RZoPjEUl},
 year = {2023}
}

@article{gabrielsson2023rewiring,
 abstract = {Several recent works use positional encodings to extend the receptive fields of graph neural network (GNN) layers equipped with attention mechanisms. These techniques, however, extend receptive fields to the complete graph, at substantial computational cost and risking a change in the inductive biases of conventional GNNs, or require complex architecture adjustments. As a conservative alternative, we use positional encodings to expand receptive fields to $r$-hop neighborhoods. More specifically, our method augments the input graph with additional nodes/edges and uses positional encodings as node and/or edge features. We thus modify graphs before inputting them to a downstream GNN model, instead of modifying the model itself. This makes our method model-agnostic, i.e., compatible with any of the existing GNN architectures. We also provide examples of positional encodings that are lossless with a one-to-one map between the original and the modified graphs. We demonstrate that extending receptive fields via positional encodings and a virtual fully-connected node significantly improves GNN performance and alleviates over-squashing using small $r$. We obtain improvements on a variety of models and datasets and reach competitive performance using traditional GNNs or graph Transformers.},
 author = {Rickard Br{\"u}el Gabrielsson and Mikhail Yurochkin and Justin Solomon},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221160941},
 pdf = {https://openreview.net/pdf?id=dn3ZkqG2YV},
 review = {https://openreview.net/forum?id=dn3ZkqG2YV},
 title = {Rewiring with Positional Encodings for Graph Neural Networks},
 url = {https://openreview.net/forum?id=dn3ZkqG2YV},
 year = {2023}
}

@article{gagnon-audet2023woods,
 abstract = {Machine learning models often fail to generalize well under distributional shifts. Understanding and overcoming these failures have led to a research field of Out-of-Distribution (OOD) generalization. Despite being extensively studied for static computer vision tasks, OOD generalization has been underexplored for time series tasks. To shine light on this gap, we present WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals. We revise the existing OOD generalization algorithms for time series tasks and evaluate them using our systematic framework. Our experiments show a large room for improvement for empirical risk minimization and OOD generalization algorithms on our datasets, thus underscoring the new challenges posed by time series tasks. Code and documentation are available at https://woods-benchmarks.github.io .},
 author = {Jean-Christophe Gagnon-Audet and Kartik Ahuja and Mohammad Javad Darvishi Bayazi and Pooneh Mousavi and Guillaume Dumas and Irina Rish},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4221167889},
 pdf = {https://openreview.net/pdf?id=mvftzofTYQ},
 review = {https://openreview.net/forum?id=mvftzofTYQ},
 title = {WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series},
 url = {https://openreview.net/forum?id=mvftzofTYQ},
 year = {2023}
}

@article{gala2023indictrans,
 abstract = {India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.},
 author = {Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},
 code = {https://github.com/AI4Bharat/IndicTrans2},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4378505287},
 pdf = {https://openreview.net/pdf?id=vfT4YuzAYA},
 review = {https://openreview.net/forum?id=vfT4YuzAYA},
 title = {IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},
 url = {https://openreview.net/forum?id=vfT4YuzAYA},
 year = {2023}
}

@article{gala2024enequivariant,
 abstract = {Cellular automata (CAs) are computational models exhibiting rich dynamics emerging from the local interaction of cells arranged in a regular lattice. Graph CAs (GCAs) generalise standard CAs by allowing for arbitrary graphs rather than regular lattices, similar to how Graph Neural Networks (GNNs) generalise Convolutional NNs. Recently, Graph Neural CAs (GNCAs) have been proposed as models built on top of standard GNNs that can be trained to approximate the transition rule of any arbitrary GCA. Existing GNCAs are anisotropic in the sense that their transition rules are not equivariant to translation, rotation, and reflection of the nodes' spatial locations. However, it is desirable for instances related by such transformations to be treated identically by the model. By replacing standard graph convolutions with E(n)-equivariant ones, we avoid anisotropy by design and propose a class of isotropic automata that we call E(n)-GNCAs. These models are lightweight, but can nevertheless handle large graphs, capture complex dynamics and exhibit emergent self-organising behaviours. We showcase the broad and successful applicability of E(n)-GNCAs on three different tasks: (i) pattern formation, (ii) graph auto-encoding, and (iii) simulation of E(n)-equivariant dynamical systems.},
 author = {Gennaro Gala and Daniele Grattarola and Erik Quaeghebeur},
 code = {https://github.com/gengala/egnca},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318239428},
 pdf = {https://openreview.net/pdf?id=7PNJzAxkij},
 review = {https://openreview.net/forum?id=7PNJzAxkij},
 title = {E(n)-equivariant Graph Neural Cellular Automata},
 url = {https://openreview.net/forum?id=7PNJzAxkij},
 year = {2024}
}

@article{galanti2023comparative,
 author = {Tomer Galanti and Liane Galanti and Ido Ben-Shaul},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=162TqkUNPO},
 review = {https://openreview.net/forum?id=162TqkUNPO},
 title = {Comparative Generalization Bounds for Deep Neural Networks},
 url = {https://openreview.net/forum?id=162TqkUNPO},
 year = {2023}
}

@article{galloway2023bounding,
 abstract = {Estimating the Generalization Error (GE) of Deep Neural Networks (DNNs) is an important task that often relies on availability of held-out data. The ability to better predict GE based on a single training set may yield overarching DNN design principles to reduce a reliance on trial-and-error, along with other performance assessment advantages. In search of a quantity relevant to GE, we investigate the Mutual Information (MI) between the input and final layer representations, using the infinite-width DNN limit to bound MI. An existing input compression-based GE bound is used to link MI and GE. To the best of our knowledge, this represents the first empirical study of this bound. In our attempt to empirically falsify the theoretical bound, we find that it is often tight for best-performing models. Furthermore, it detects randomization of training labels in many cases, reflects test-time perturbation robustness, and works well given only few training samples. These results are promising given that input compression is broadly applicable where MI can be estimated with confidence.},
 author = {Angus Galloway and Anna Golubeva and Mahmoud Salem and Mihai Nica and Yani Ioannou and Graham W. Taylor},
 code = {https://github.com/AngusG/input-compression-bound-study},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286224460},
 pdf = {https://openreview.net/pdf?id=jbZEUtULft},
 review = {https://openreview.net/forum?id=jbZEUtULft},
 title = {Bounding generalization error with input compression: An empirical study with infinite-width networks},
 url = {https://openreview.net/forum?id=jbZEUtULft},
 year = {2023}
}

@article{gamba2023deep,
 abstract = {The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing good generalization performance, has been recently characterized in terms of the double descent curve for the test error. Common intuition from polynomial regression suggests that overparameterized networks are able to sharply interpolate noisy data, without considerably deviating from the ground-truth signal, thus preserving generalization ability. At present, a precise characterization of the relationship between interpolation and generalization for deep networks is missing. In this work, we quantify sharpness of fit of the training data interpolated by neural network functions, by studying the loss landscape w.r.t. to the input variable locally to each training point, over volumes around cleanly- and noisily-labelled training samples, as we systematically increase the number of model parameters and training epochs. Our findings show that loss sharpness in the input space follows both model- and epoch-wise double descent, with worse peaks observed around noisy labels. While small interpolating models sharply fit both clean and noisy data, large interpolating models express a smooth loss landscape, where noisy targets are predicted over large volumes around training data points, in contrast to existing intuition.},
 author = {Matteo Gamba and Erik Englesson and M{\r{a}}rten Bj{\"o}rkman and Hossein Azizpour},
 code = {https://github.com/magamba/double_descent},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297899413},
 pdf = {https://openreview.net/pdf?id=fempQstMbV},
 review = {https://openreview.net/forum?id=fempQstMbV},
 title = {Deep Double Descent via Smooth Interpolation},
 url = {https://openreview.net/forum?id=fempQstMbV},
 year = {2023}
}

@article{gangal2023physics,
 abstract = {Physics informed neural network (PINN) based solution methods for differential equations have recently shown success in a variety of scientific computing applications. Several authors have reported difficulties, however, when using PINNs to solve equations with multiscale features. The objective of the present work is to illustrate and explain the difficulty of using standard PINNs for the particular case of divergence-form elliptic partial differential equations (PDEs) with oscillatory coefficients present in the differential operator. We show that if the coefficient in the elliptic operator $a^{\epsilon}(x)$ is of the form $a(x/\epsilon)$ for a 1-periodic coercive function $a(\cdot)$, then the Frobenius norm of the neural tangent kernel (NTK) matrix associated to the loss function grows as $1/\epsilon^2$. This implies that as the separation of scales in the problem increases, training the neural network with gradient descent based methods to achieve an accurate approximation of the solution to the PDE becomes increasingly difficult. Numerical examples illustrate the stiffness of the optimization problem.},
 author = {Arnav Gangal and Luis Kim and Sean Patrick Carney},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4313304929},
 pdf = {https://openreview.net/pdf?id=QfyVqvpg7u},
 review = {https://openreview.net/forum?id=QfyVqvpg7u},
 title = {Physics informed neural networks for elliptic equations with oscillatory differential operators},
 url = {https://openreview.net/forum?id=QfyVqvpg7u},
 year = {2023}
}

@article{gangwar2023semantic,
 abstract = {Mathematical notation makes up a large portion of STEM literature, yet finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. This work describes an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with a structural approach that considers visual layout to embed an expression and show that our proposed approach is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.},
 author = {Neeraj Gangwar and Nickvash Kani},
 code = {https://github.com/mlpgroup/expemb},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309209230},
 pdf = {https://openreview.net/pdf?id=EWPA9TZcUy},
 review = {https://openreview.net/forum?id=EWPA9TZcUy},
 title = {Semantic Representations of Mathematical Expressions in a Continuous Vector Space},
 url = {https://openreview.net/forum?id=EWPA9TZcUy},
 year = {2023}
}

@article{ganz2023bigroc,
 author = {Roy Ganz and Michael Elad},
 code = {https://github.com/royg27/BIGRoC},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=y7RGNXhGSR},
 review = {https://openreview.net/forum?id=y7RGNXhGSR},
 title = {{BIGR}oC: Boosting Image Generation via a Robust Classifier},
 url = {https://openreview.net/forum?id=y7RGNXhGSR},
 year = {2023}
}

@article{gao2022deformation,
 author = {Liyao Gao and Guang Lin and Wei Zhu},
 code = {https://github.com/gaoliyao/Roto-scale-translation-Equivariant-CNN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=yVkpxs77cD},
 review = {https://openreview.net/forum?id=yVkpxs77cD},
 title = {Deformation Robust Roto-Scale-Translation Equivariant {CNN}s},
 url = {https://openreview.net/forum?id=yVkpxs77cD},
 year = {2022}
}

@article{gao2023feddag,
 author = {Erdun Gao and Junjia Chen and Li Shen and Tongliang Liu and Mingming Gong and Howard Bondell},
 code = {https://github.com/ErdunGAO/FedDAG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=MzWgBjZ6Le},
 review = {https://openreview.net/forum?id=MzWgBjZ6Le},
 title = {Fed{DAG}: Federated {DAG} Structure Learning},
 url = {https://openreview.net/forum?id=MzWgBjZ6Le},
 year = {2023}
}

@article{gasse2023using,
 author = {Maxime Gasse and Damien GRASSET and Guillaume Gaudron and Pierre-Yves Oudeyer},
 code = {https://github.com/gasse/causal-rl-tmlr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391157052},
 pdf = {https://openreview.net/pdf?id=nFWRuJXPkU},
 review = {https://openreview.net/forum?id=nFWRuJXPkU},
 title = {Using Confounded Data in Latent Model-Based Reinforcement Learning},
 url = {https://openreview.net/forum?id=nFWRuJXPkU},
 year = {2023}
}

@article{gast2022learning,
 author = {Nicolas Gast and Bruno Gaujal and Kimang Khun},
 code = {https://gitlab.inria.fr/kkhun/learning-in-rested-markovian-bandit.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Sh3RF9JowK},
 review = {https://openreview.net/forum?id=Sh3RF9JowK},
 title = {Learning Algorithms for Markovian Bandits:{\textbackslash}{\textbackslash}Is Posterior Sampling more Scalable than Optimism?},
 url = {https://openreview.net/forum?id=Sh3RF9JowK},
 year = {2022}
}

@article{gasteiger2022gemnetoc,
 abstract = {Recent years have seen the advent of molecular simulation datasets that are orders of magnitude larger and more diverse. These new datasets differ substantially in four aspects of complexity: 1. Chemical diversity (number of different elements), 2. system size (number of atoms per sample), 3. dataset size (number of data samples), and 4. domain shift (similarity of the training and test set). Despite these large differences, benchmarks on small and narrow datasets remain the predominant method of demonstrating progress in graph neural networks (GNNs) for molecular simulation, likely due to cheaper training compute requirements. This raises the question -- does GNN progress on small and narrow datasets translate to these more complex datasets? This work investigates this question by first developing the GemNet-OC model based on the large Open Catalyst 2020 (OC20) dataset. GemNet-OC outperforms the previous state-of-the-art on OC20 by 16% while reducing training time by a factor of 10. We then compare the impact of 18 model components and hyperparameter choices on performance in multiple datasets. We find that the resulting model would be drastically different depending on the dataset used for making model choices. To isolate the source of this discrepancy we study six subsets of the OC20 dataset that individually test each of the above-mentioned four dataset aspects. We find that results on the OC-2M subset correlate well with the full OC20 dataset while being substantially cheaper to train on. Our findings challenge the common practice of developing GNNs solely on small datasets, but highlight ways of achieving fast development cycles and generalizable results via moderately-sized, representative datasets such as OC-2M and efficient models such as GemNet-OC. Our code and pretrained model weights are open-sourced.},
 author = {Johannes Gasteiger and Muhammed Shuaibi and Anuroop Sriram and Stephan G{\"u}nnemann and Zachary Ward Ulissi and C. Lawrence Zitnick and Abhishek Das},
 code = {https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/gemnet_oc},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4298178320},
 pdf = {https://openreview.net/pdf?id=u8tvSxm4Bs},
 review = {https://openreview.net/forum?id=u8tvSxm4Bs},
 title = {GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets},
 url = {https://openreview.net/forum?id=u8tvSxm4Bs},
 year = {2022}
}

@article{ge2023invariant,
 abstract = {Learning the causal structure behind data is invaluable for improving generalization and obtaining high-quality explanations. We propose a novel framework, Invariant Structure Learning (ISL), that is designed to improve causal structure discovery by utilizing generalization as an indication. ISL splits the data into different environments, and learns a structure that is invariant to the target across different environments by imposing a consistency constraint. An aggregation mechanism then selects the optimal classifier based on a graph structure that reflects the causal mechanisms in the data more accurately compared to the structures learnt from individual environments. Furthermore, we extend ISL to a self-supervised learning setting where accurate causal structure discovery does not rely on any labels. This self-supervised ISL utilizes invariant causality proposals by iteratively setting different nodes as targets. On synthetic and real-world datasets, we demonstrate that ISL accurately discovers the causal structure, outperforms alternative methods, and yields superior generalization for datasets with significant distribution shifts.},
 author = {Yunhao Ge and Sercan O Arik and Jinsung Yoon and Ao Xu and Laurent Itti and Tomas Pfister},
 code = {https://github.com/AaronXu9/ISL.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282961313},
 pdf = {https://openreview.net/pdf?id=A9yn7KTwsK},
 review = {https://openreview.net/forum?id=A9yn7KTwsK},
 title = {Invariant Structure Learning for Better Generalization and Causal Explainability},
 url = {https://openreview.net/forum?id=A9yn7KTwsK},
 year = {2023}
}

@article{ge2023lightweight,
 abstract = {In Lifelong Learning (LL), agents continually learn as they encounter new conditions and tasks. Most current LL is limited to a single agent that learns tasks sequentially. Dedicated LL machinery is then deployed to mitigate the forgetting of old tasks as new tasks are learned. This is inherently slow. We propose a new Shared Knowledge Lifelong Learning (SKILL) challenge, which deploys a decentralized population of LL agents that each sequentially learn different tasks, with all agents operating independently and in parallel. After learning their respective tasks, agents share and consolidate their knowledge over a decentralized communication network, so that, in the end, all agents can master all tasks. We present one solution to SKILL which uses Lightweight Lifelong Learning (LLL) agents, where the goal is to facilitate efficient sharing by minimizing the fraction of the agent that is specialized for any given task. Each LLL agent thus consists of a common task-agnostic immutable part, where most parameters are, and individual task-specific modules that contain fewer parameters but are adapted to each task. Agents share their task-specific modules, plus summary information ("task anchors") representing their tasks in the common task-agnostic latent space of all agents. Receiving agents register each received task-specific module using the corresponding anchor. Thus, every agent improves its ability to solve new tasks each time new task-specific modules and anchors are received. On a new, very challenging SKILL-102 dataset with 102 image classification tasks (5,033 classes in total, 2,041,225 training, 243,464 validation, and 243,464 test images), we achieve much higher (and SOTA) accuracy over 8 LL baselines, while also achieving near perfect parallelization. Code and data can be found at https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning},
 author = {Yunhao Ge and Yuecheng Li and Di Wu and Ao Xu and Adam M. Jones and Amanda Sofie Rios and Iordanis Fostiropoulos and shixian wen and Po-Hsuan Huang and Zachary William Murdock and Gozde Sahin and Shuo Ni and Kiran Lekkala and Sumedh Anand Sontakke and Laurent Itti},
 code = {https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4378498831},
 pdf = {https://openreview.net/pdf?id=Jjl2c8kWUc},
 review = {https://openreview.net/forum?id=Jjl2c8kWUc},
 title = {Lightweight Learner for Shared Knowledge Lifelong Learning},
 url = {https://openreview.net/forum?id=Jjl2c8kWUc},
 year = {2023}
}

@article{gehring2023leveraging,
 abstract = {Demonstrations provide insight into relevant state or action space regions, bearing great potential to boost the efficiency and practicality of reinforcement learning agents. In this work, we propose to leverage demonstration datasets by combining skill learning and sequence modeling. Starting with a learned joint latent space, we separately train a generative model of demonstration sequences and an accompanying low-level policy. The sequence model forms a latent space prior over plausible demonstration behaviors to accelerate learning of high-level policies. We show how to acquire such priors from state-only motion capture demonstrations and explore several methods for integrating them into policy learning on transfer tasks. Our experimental results confirm that latent space priors provide significant gains in learning speed and final performance. We benchmark our approach on a set of challenging sparse-reward environments with a complex, simulated humanoid, and on offline RL benchmarks for navigation and object manipulation. Videos, source code and pre-trained models are available at the corresponding project website at https://facebookresearch.github.io/latent-space-priors .},
 author = {Jonas Gehring and Deepak Gopinath and Jungdam Won and Andreas Krause and Gabriel Synnaeve and Nicolas Usunier},
 code = {https://facebookresearch.github.io/latent-space-priors},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307478194},
 pdf = {https://openreview.net/pdf?id=OzGIu4T4Cz},
 review = {https://openreview.net/forum?id=OzGIu4T4Cz},
 title = {Leveraging Demonstrations with Latent Space Priors},
 url = {https://openreview.net/forum?id=OzGIu4T4Cz},
 year = {2023}
}

@article{geifman2024controlling,
 author = {Amnon Geifman and Daniel Barzilai and Ronen Basri and Meirav Galun},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=aD0ExytnEK},
 review = {https://openreview.net/forum?id=aD0ExytnEK},
 title = {Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel{\textquoteright}s Spectrum},
 url = {https://openreview.net/forum?id=aD0ExytnEK},
 year = {2024}
}

@article{geiger2022failsafe,
 abstract = {For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, safety and imitation performance of our approach.},
 author = {Philipp Geiger and Christoph-Nikolas Straehle},
 code = {https://github.com/boschresearch/fagil},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221148689},
 pdf = {https://openreview.net/pdf?id=e4Bb0b3QgJ},
 review = {https://openreview.net/forum?id=e4Bb0b3QgJ},
 title = {Fail-Safe Adversarial Generative Imitation Learning},
 url = {https://openreview.net/forum?id=e4Bb0b3QgJ},
 year = {2022}
}

@article{george2022lazy,
 abstract = {Among attempts at giving a theoretical account of the success of deep neural networks, a recent line of work has identified a so-called lazy training regime in which the network can be well approximated by its linearization around initialization. Here we investigate the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on subgroups of examples based on their difficulty. Specifically, we show that easier examples are given more weight in feature learning mode, resulting in faster training compared to more difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning of examples of increasing difficulty. We illustrate this phenomenon across different ways to quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn spurious correlations. Our results reveal a new understanding of how deep networks prioritize resources across example difficulty.},
 author = {Thomas George and Guillaume Lajoie and Aristide Baratin},
 code = {https://github.com/tfjgeorge/lazy_vs_hasty},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296595170},
 pdf = {https://openreview.net/pdf?id=lukVf4VrfP},
 review = {https://openreview.net/forum?id=lukVf4VrfP},
 title = {Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty},
 url = {https://openreview.net/forum?id=lukVf4VrfP},
 year = {2022}
}

@article{georgiev2022heat,
 abstract = {Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code. In this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality.},
 author = {Dobrik Georgiev Georgiev and Marc Brockschmidt and Miltiadis Allamanis},
 code = {https://github.com/microsoft/neurips21-self-supervised-bug-detection-and-repair/tree/heat},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226017958},
 pdf = {https://openreview.net/pdf?id=gCmQK6McbR},
 review = {https://openreview.net/forum?id=gCmQK6McbR},
 title = {HEAT: Hyperedge Attention Networks},
 url = {https://openreview.net/forum?id=gCmQK6McbR},
 year = {2022}
}

@article{ghari2024budgeted,
 abstract = {Online model selection involves selecting a model from a set of candidate models 'on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis proves that the proposed algorithm enjoys sub-linear regret with respect to the best model in hindsight. Experiments on real datasets demonstrate the effectiveness of the proposed algorithm.},
 author = {Pouya M. Ghari and Yanning Shen},
 code = {https://github.com/pouyamghari/OFMS-FT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391124005},
 pdf = {https://openreview.net/pdf?id=WeiRR8h87X},
 review = {https://openreview.net/forum?id=WeiRR8h87X},
 title = {Budgeted Online Model Selection and Fine-Tuning via Federated Learning},
 url = {https://openreview.net/forum?id=WeiRR8h87X},
 year = {2024}
}

@article{ghildyal2023attacking,
 abstract = {Perceptual similarity metrics have progressively become more correlated with human judgments on perceptual similarity; however, despite recent advances, the addition of an imperceptible distortion can still compromise these metrics. In our study, we systematically examine the robustness of these metrics to imperceptible adversarial perturbations. Following the two-alternative forced-choice experimental design with two distorted images and one reference image, we perturb the distorted image closer to the reference via an adversarial attack until the metric flips its judgment. We first show that all metrics in our study are susceptible to perturbations generated via common adversarial attacks such as FGSM, PGD, and the One-pixel attack. Next, we attack the widely adopted LPIPS metric using spatial-transformation-based adversarial perturbations (stAdv) in a white-box setting to craft adversarial examples that can effectively transfer to other similarity metrics in a black-box setting. We also combine the spatial attack stAdv with PGD ($\ell_\infty$-bounded) attack to increase transferability and use these adversarial examples to benchmark the robustness of both traditional and recently developed metrics. Our benchmark provides a good starting point for discussion and further research on the robustness of metrics to imperceptible adversarial perturbations.},
 author = {Abhijay Ghildyal and Feng Liu},
 badge = {Featured},
 code = {https://github.com/abhijay9/attacking_perceptual_similarity_metrics},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4376654498},
 pdf = {https://openreview.net/pdf?id=r9vGSpbbRO},
 review = {https://openreview.net/forum?id=r9vGSpbbRO},
 title = {Attacking Perceptual Similarity Metrics},
 url = {https://openreview.net/forum?id=r9vGSpbbRO},
 year = {2023}
}

@article{ghosal2023a,
 abstract = {Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of AI-generated text detection. This is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. Despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. In this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of AI-generated text detection. To enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on AI-generated text detection.},
 author = {Soumya Suvra Ghosal and Souradip Chakraborty and Jonas Geiping and Furong Huang and Dinesh Manocha and Amrit Bedi},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4387946978},
 pdf = {https://openreview.net/pdf?id=AXtFeYjboj},
 review = {https://openreview.net/forum?id=AXtFeYjboj},
 title = {Towards Possibilities &amp; Impossibilities of AI-generated Text Detection: A Survey},
 url = {https://openreview.net/forum?id=AXtFeYjboj},
 year = {2023}
}

@article{gidaris2024moca,
 abstract = {Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.},
 author = {Spyros Gidaris and Andrei Bursuc and Oriane Sim{\'e}oni and Anton{\'\i}n Vobeck{\'y} and Nikos Komodakis and Matthieu Cord and Patrick Perez},
 code = {https://github.com/valeoai/MOCA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384811734},
 pdf = {https://openreview.net/pdf?id=OdDsCaacZ0},
 review = {https://openreview.net/forum?id=OdDsCaacZ0},
 title = {MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments},
 url = {https://openreview.net/forum?id=OdDsCaacZ0},
 year = {2024}
}

@article{giovanni2024how,
 author = {Francesco Di Giovanni and T. Konstantin Rusch and Michael Bronstein and Andreea Deac and Marc Lackenby and Siddhartha Mishra and Petar Veli{\v{c}}kovi{\'c}},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=KJRoQvRWNs},
 review = {https://openreview.net/forum?id=KJRoQvRWNs},
 title = {How does over-squashing affect the power of {GNN}s?},
 url = {https://openreview.net/forum?id=KJRoQvRWNs},
 year = {2024}
}

@article{girrbach2023addressing,
 abstract = {Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .},
 author = {Leander Girrbach and Anders Christensen and Ole Winther and Zeynep Akata and A. Sophia Koepke},
 code = {https://github.com/ExplainableML/Deep-Graph-Persistence},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385005394},
 pdf = {https://openreview.net/pdf?id=oyfRWeoUJY},
 review = {https://openreview.net/forum?id=oyfRWeoUJY},
 title = {Addressing caveats of neural persistence with deep graph persistence},
 url = {https://openreview.net/forum?id=oyfRWeoUJY},
 year = {2023}
}

@article{godichon-baggioni2023learning,
 abstract = {This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data.},
 author = {Antoine Godichon-Baggioni and Nicklas Werge and Olivier Wintenberger},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281569244},
 pdf = {https://openreview.net/pdf?id=kdfiEu1ul6},
 review = {https://openreview.net/forum?id=kdfiEu1ul6},
 title = {Learning from time-dependent streaming data with online stochastic algorithms},
 url = {https://openreview.net/forum?id=kdfiEu1ul6},
 year = {2023}
}

@article{gontier2022does,
 abstract = {We study the utility of incorporating entity type abstractions into pre-trained Transformers and test these methods on four NLP tasks requiring different forms of logical reasoning: (1) compositional language understanding with text-based relational reasoning (CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question answering (HotpotQA), and (4) conversational question answering (CoQA). We propose and empirically explore three ways to add such abstraction: (i) as additional input embeddings, (ii) as a separate sequence to encode, and (iii) as an auxiliary prediction task for the model. Overall, our analysis demonstrates that models with abstract entity knowledge performs better than without it. The best abstraction aware models achieved an overall accuracy of 88.8% and 91.8% compared to the baseline model achieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for HotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our results suggest that the benefit of explicit abstraction is significant in formally defined logical reasoning settings requiring many reasoning hops, but point to the notion that it is less beneficial for NLP tasks having less formal logical structure.},
 author = {Nicolas Gontier and Siva Reddy and Christopher Pal},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221146446},
 pdf = {https://openreview.net/pdf?id=9nhmKwLAWV},
 review = {https://openreview.net/forum?id=9nhmKwLAWV},
 title = {Does Entity Abstraction Help Generative Transformers Reason?},
 url = {https://openreview.net/forum?id=9nhmKwLAWV},
 year = {2022}
}

@article{gopalani2024global,
 abstract = {In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are "Villani functions" and thus be able to build on recent progress with analyzing SGD on such objectives.},
 author = {Pulkit Gopalani and Samyak Jha and Anirbit Mukherjee},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386875474},
 pdf = {https://openreview.net/pdf?id=9TqAUYB6tC},
 review = {https://openreview.net/forum?id=9TqAUYB6tC},
 title = {Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets},
 url = {https://openreview.net/forum?id=9TqAUYB6tC},
 year = {2024}
}

@article{gowda2023dual,
 abstract = {Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. The inductive bias learner within DUCA is instrumental in encoding shape information, effectively countering the tendency of ANNs to learn local textures. Simultaneously, the inclusion of a semantic memory submodule facilitates the gradual consolidation of knowledge, replicating the dynamics observed in fast and slow learning systems, reminiscent of the principles underpinning the complementary learning system in human cognition. DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information. To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset.},
 author = {Shruthi Gowda and Bahram Zonooz and Elahe Arani},
 code = {https://github.com/NeurAI-Lab/DUCA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387797304},
 pdf = {https://openreview.net/pdf?id=PEyVq0hlO3},
 review = {https://openreview.net/forum?id=PEyVq0hlO3},
 title = {Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning},
 url = {https://openreview.net/forum?id=PEyVq0hlO3},
 year = {2023}
}

@article{graziani2023uncovering,
 abstract = {Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data.},
 author = {Mara Graziani and Laura O'Mahony and An-phi Nguyen and Henning M{\"u}ller and Vincent Andrearczyk},
 code = {https://github.com/maragraziani/concept_discovery_svd},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384392944},
 pdf = {https://openreview.net/pdf?id=LT4DXqUJTD},
 review = {https://openreview.net/forum?id=LT4DXqUJTD},
 title = {Uncovering Unique Concept Vectors through Latent Space Decomposition},
 url = {https://openreview.net/forum?id=LT4DXqUJTD},
 year = {2023}
}

@article{greenewald2023kmixup,
 abstract = {Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup in a simple, broadly applicable way to \emph{$k$-mixup}, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets considered, the performance gains of $k$-mixup over standard mixup are similar to or larger than the gains of mixup itself over standard ERM after hyperparameter optimization. In several instances, in fact, $k$-mixup achieves gains in settings where standard mixup has negligible to zero improvement over ERM.},
 author = {Kristjan Greenewald and Anming Gu and Mikhail Yurochkin and Justin Solomon and Edward Chien},
 code = {https://github.com/AnmingGu/kmixup-cifar10},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387558965},
 pdf = {https://openreview.net/pdf?id=lOegPKSu04},
 review = {https://openreview.net/forum?id=lOegPKSu04},
 title = {k-Mixup Regularization for Deep Learning via Optimal Transport},
 url = {https://openreview.net/forum?id=lOegPKSu04},
 year = {2023}
}

@article{grimes2023learning,
 author = {Matthew Koichi Grimes and Joseph Varughese Modayil and Piotr W Mirowski and Dushyant Rao and Raia Hadsell},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=9aXKUJEKwV},
 review = {https://openreview.net/forum?id=9aXKUJEKwV},
 title = {Learning to Look by Self-Prediction},
 url = {https://openreview.net/forum?id=9aXKUJEKwV},
 year = {2023}
}

@article{grinwald2023visualizing,
 abstract = {Explainable Artificial Intelligence (XAI) aims to make learning machines less opaque, and offers researchers and practitioners various tools to reveal the decision-making strategies of neural networks. In this work, we investigate how XAI methods can be used for exploring and visualizing the diversity of feature representations learned by Bayesian Neural Networks (BNNs). Our goal is to provide a global understanding of BNNs by making their decision-making strategies a) visible and tangible through feature visualizations and b) quantitatively measurable with a distance measure learned by contrastive learning. Our work provides new insights into the \emph{posterior} distribution in terms of human-understandable feature information with regard to the underlying decision making strategies. The main findings of our work are the following: 1) global XAI methods can be applied to explain the diversity of decision-making strategies of BNN instances, 2) Monte Carlo dropout with commonly used Dropout rates exhibit increased diversity in feature representations compared to the multimodal posterior approximation of MultiSWAG, 3) the diversity of learned feature representations highly correlates with the uncertainty estimate for the output and 4) the inter-mode diversity of the multimodal posterior decreases as the network width increases, while the intra mode diversity increases. These findings are consistent with the recent Deep Neural Networks theory, providing additional intuitions about what the theory implies in terms of humanly understandable concepts.},
 author = {Dennis Grinwald and Kirill Bykov and Shinichi Nakajima and Marina MC H{\"o}hne},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221167470},
 pdf = {https://openreview.net/pdf?id=ZSxvyWrX6k},
 review = {https://openreview.net/forum?id=ZSxvyWrX6k},
 title = {Visualizing the Diversity of Representations Learned by Bayesian Neural Networks},
 url = {https://openreview.net/forum?id=ZSxvyWrX6k},
 year = {2023}
}

@article{grosse2023optimistic,
 abstract = {Bayesian optimization is a popular formalism for global optimization, but its computational costs limit it to expensive-to-evaluate functions. A competing, computationally more efficient, global optimization framework is optimistic optimization, which exploits prior knowledge about the geometry of the search space in form of a dissimilarity function. We investigate to which degree the conceptual advantages of Bayesian Optimization can be combined with the computational efficiency of optimistic optimization. By mapping the kernel to a dissimilarity, we obtain an optimistic optimization algorithm for the Bayesian Optimization setting with a run-time of up to $\mathcal{O}(N \log N)$. As a high-level take-away we find that, when using stationary kernels on objectives of relatively low evaluation cost, optimistic optimization can be strongly preferable over Bayesian optimization, while for strongly coupled and parametric models, good implementations of Bayesian optimization can perform much better, even at low evaluation cost. We argue that there is a new research domain between geometric and probabilistic search, i.e. methods that run drastically faster than traditional Bayesian optimization, while retaining some of the crucial functionality of Bayesian optimization.},
 author = {Julia Grosse and Cheng Zhang and Philipp Hennig},
 code = {https://github.com/JuliaGrosse/Optimistic-Optimization-of-Gaussian-Process-Samples},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4294790506},
 pdf = {https://openreview.net/pdf?id=KQ5jI19kF3},
 review = {https://openreview.net/forum?id=KQ5jI19kF3},
 title = {Optimistic Optimization of Gaussian Process Samples},
 url = {https://openreview.net/forum?id=KQ5jI19kF3},
 year = {2023}
}

@article{gugglberger2022momentum,
 abstract = {Capsule networks are a class of neural networks that achieved promising results on many computer vision tasks. However, baseline capsule networks have failed to reach state-of-the-art results on more complex datasets due to the high computation and memory requirements. We tackle this problem by proposing a new network architecture, called Momentum Capsule Network (MoCapsNet). MoCapsNets are inspired by Momentum ResNets, a type of network that applies reversible residual building blocks. Reversible networks allow for recalculating activations of the forward pass in the backpropagation algorithm, so those memory requirements can be drastically reduced. In this paper, we provide a framework on how invertible residual building blocks can be applied to capsule networks. We will show that MoCapsNet beats the accuracy of baseline capsule networks on MNIST, SVHN, CIFAR-10 and CIFAR-100 while using considerably less memory. The source code is available on https://github.com/moejoe95/MoCapsNet.},
 author = {Josef Gugglberger and Antonio Rodriguez-sanchez and David Peer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226402761},
 pdf = {https://openreview.net/pdf?id=Su290sknyQ},
 review = {https://openreview.net/forum?id=Su290sknyQ},
 title = {Momentum Capsule Networks},
 url = {https://openreview.net/forum?id=Su290sknyQ},
 year = {2022}
}

@article{gui2023training,
 abstract = {Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes or patches, assumes that the visual backbone must first be trained on ImageNet class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders that does not require this supervision. In fact, in a head-to-head comparison between ViLT, the current state-of-the-art patch-based vision-language transformer which is pretrained with supervised object classification, and our model, VLC, we find that our approach 1. outperforms ViLT on standard benchmarks, 2. provides more interpretable and intuitive patch visualizations, and 3. is competitive with many larger models that utilize ROIs trained on annotated bounding-boxes.},
 author = {Liangke Gui and Yingshan Chang and Qiuyuan Huang and Subhojit Som and Alexander G Hauptmann and Jianfeng Gao and Yonatan Bisk},
 code = {https://github.com/guilk/VLC/tree/main},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281255798},
 pdf = {https://openreview.net/pdf?id=xLnbSpozWS},
 review = {https://openreview.net/forum?id=xLnbSpozWS},
 title = {Training Vision-Language Transformers from Captions},
 url = {https://openreview.net/forum?id=xLnbSpozWS},
 year = {2023}
}

@article{gulcehre2022an,
 abstract = {Deep neural networks are the most commonly used function approximators in offline reinforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under-parameterization of these networks. Specifically, the rank of the penultimate feature layer, also called \textit{effective rank}, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model's ability to further adapt in later stages of learning, leading to the diminished final performance. Such an association between the effective rank and performance makes effective rank compelling for offline RL, primarily for offline policy evaluation. In this work, we conduct a careful empirical study on the relation between effective rank and performance on three offline RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insufficient to explain the collapse of the effective rank. Further, we show that several other factors could confound the relationship between effective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading.},
 author = {Caglar Gulcehre and Srivatsan Srinivasan and Jakub Sygnowski and Georg Ostrovski and Mehrdad Farajtabar and Matthew Hoffman and Razvan Pascanu and Arnaud Doucet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4284688079},
 pdf = {https://openreview.net/pdf?id=HFfJWx60IT},
 review = {https://openreview.net/forum?id=HFfJWx60IT},
 title = {An Empirical Study of Implicit Regularization in Deep Offline RL},
 url = {https://openreview.net/forum?id=HFfJWx60IT},
 year = {2022}
}

@article{gunn2023regularized,
 abstract = {Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors when solving inverse problems. One challenge of using them is overcoming representation error, the fundamental limitation of the network in representing any particular signal. Recently, multiple proposed inversion algorithms reduce representation error by optimizing over intermediate layer representations. These methods are typically applied to generative models that were trained agnostic of the downstream inversion algorithm. In our work, we introduce a principle that if a generative model is intended for inversion using an algorithm based on optimization of intermediate layers, it should be trained in a way that regularizes those intermediate layers. We instantiate this principle for two notable recent inversion algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of these inversion algorithms, we introduce a new regularized GAN training algorithm and demonstrate that the learned generative model results in lower reconstruction errors across a wide range of under sampling ratios when solving compressed sensing, inpainting, and super-resolution problems.},
 author = {Sean Gunn and Jorio Cocola and PAul HAnd},
 code = {https://github.com/g33sean/RTIL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221160451},
 pdf = {https://openreview.net/pdf?id=cKsKXR28cG},
 review = {https://openreview.net/forum?id=cKsKXR28cG},
 title = {Regularized Training of Intermediate Layers for Generative Models for Inverse Problems},
 url = {https://openreview.net/forum?id=cKsKXR28cG},
 year = {2023}
}

@article{guo2022multisource,
 author = {Wenshuo Guo and Serena Lutong Wang and Peng Ding and Yixin Wang and Michael Jordan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=CrimIjBa64},
 review = {https://openreview.net/forum?id=CrimIjBa64},
 title = {Multi-Source Causal Inference Using Control Variates under Outcome Selection Bias},
 url = {https://openreview.net/forum?id=CrimIjBa64},
 year = {2022}
}

@article{guo2023towards,
 author = {Nianhui Guo and Joseph Bethge and Hong Guo and Christoph Meinel and Haojin Yang},
 code = {https://github.com/hpi-xnor/BNext},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4Hq816XDDG},
 review = {https://openreview.net/forum?id=4Hq816XDDG},
 title = {Towards Optimization-Friendly Binary Neural Network},
 url = {https://openreview.net/forum?id=4Hq816XDDG},
 year = {2023}
}

@article{gupta2022ensembles,
 abstract = {Ensembles are a straightforward, remarkably effective method for improving the accuracy,calibration, and robustness of models on classification tasks; yet, the reasons that underlie their success remain an active area of research. We build upon the extension to the bias-variance decomposition by Pfau (2013) in order to gain crucial insights into the behavior of ensembles of classifiers. Introducing a dual reparameterization of the bias-variance tradeoff, we first derive generalized laws of total expectation and variance for nonsymmetric losses typical of classification tasks. Comparing conditional and bootstrap bias/variance estimates, we then show that conditional estimates necessarily incur an irreducible error. Next, we show that ensembling in dual space reduces the variance and leaves the bias unchanged, whereas standard ensembling can arbitrarily affect the bias. Empirically, standard ensembling reducesthe bias, leading us to hypothesize that ensembles of classifiers may perform well in part because of this unexpected reduction.We conclude by an empirical analysis of recent deep learning methods that ensemble over hyperparameters, revealing that these techniques indeed favor bias reduction. This suggests that, contrary to classical wisdom, targeting bias reduction may be a promising direction for classifier ensembles.},
 author = {Neha Gupta and Jamie Smith and Ben Adlam and Zelda E Mariet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283332114},
 pdf = {https://openreview.net/pdf?id=lIOQFVncY9},
 review = {https://openreview.net/forum?id=lIOQFVncY9},
 title = {Ensembling over Classifiers: a Bias-Variance Perspective},
 url = {https://openreview.net/forum?id=lIOQFVncY9},
 year = {2022}
}

@article{gupta2022lookback,
 abstract = {The expressive and computationally inexpensive bipartite Graph Neural Networks (GNN) have been shown to be an important component of deep learning based Mixed-Integer Linear Program (MILP) solvers. Recent works have demonstrated the effectiveness of such GNNs in replacing the branching (variable selection) heuristic in branch-and-bound (B&B) solvers. These GNNs are trained, offline and on a collection of MILPs, to imitate a very good but computationally expensive branching heuristic, strong branching. Given that B&B results in a tree of sub-MILPs, we ask (a) whether there are strong dependencies exhibited by the target heuristic among the neighboring nodes of the B&B tree, and (b) if so, whether we can incorporate them in our training procedure. Specifically, we find that with the strong branching heuristic, a child node's best choice was often the parent's second-best choice. We call this the "lookback" phenomenon. Surprisingly, the typical branching GNN of Gasse et al. (2019) often misses this simple "answer". To imitate the target behavior more closely by incorporating the lookback phenomenon in GNNs, we propose two methods: (a) target smoothing for the standard cross-entropy loss function, and (b) adding a Parent-as-Target (PAT) Lookback regularizer term. Finally, we propose a model selection framework to incorporate harder-to-formulate objectives such as solving time in the final models. Through extensive experimentation on standard benchmark instances, we show that our proposal results in up to 22% decrease in the size of the B&B tree and up to 15% improvement in the solving times.},
 author = {Prateek Gupta and Elias Boutros Khalil and Didier Ch{\'e}telat and Maxime Gasse and Andrea Lodi and Yoshua Bengio and M. Pawan Kumar},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4283770434},
 pdf = {https://openreview.net/pdf?id=EQpGkw5rvL},
 review = {https://openreview.net/forum?id=EQpGkw5rvL},
 title = {Lookback for Learning to Branch},
 url = {https://openreview.net/forum?id=EQpGkw5rvL},
 year = {2022}
}

@article{gupta2023quantization,
 abstract = {Federated Learning (FL) is a machine learning paradigm to distributively learn machine learning models from decentralized data that remains on-device. Despite the success of standard Federated optimization methods, such as Federated Averaging (FedAvg) in FL, the energy demands and hardware induced constraints for on-device learning have not been considered sufficiently in the literature. Specifically, an essential demand for on-device learning is to enable trained models to be quantized to various bit-widths based on the energy needs and heterogeneous hardware designs across the federation. In this work, we introduce multiple variants of federated averaging algorithm that train neural networks robust to quantization. Such networks can be quantized to various bit-widths with only limited reduction in full precision model accuracy. We perform extensive experiments on standard FL benchmarks to evaluate our proposed FedAvg variants for quantization robustness and provide a convergence analysis for our Quantization-Aware variants in FL. Our results demonstrate that integrating quantization robustness results in FL models that are significantly more robust to different bit-widths during quantized on-device inference.},
 author = {Kartik Gupta and Marios Fournarakis and Matthias Reisser and Christos Louizos and Markus Nagel},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283368760},
 pdf = {https://openreview.net/pdf?id=lvevdX6bxm},
 review = {https://openreview.net/forum?id=lvevdX6bxm},
 title = {Quantization Robust Federated Learning for Efficient Inference on Heterogeneous Devices},
 url = {https://openreview.net/forum?id=lvevdX6bxm},
 year = {2023}
}

@article{gupta2023towards,
 abstract = {Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. Various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local & global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, generalizing across different equation parameters or time-scales still remains a challenge. In this work, we make a comprehensive comparison between various FNO, ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. We further analyze the design considerations for using FNO layers to improve performance of U-Net architectures without major degradation of computational cost. Finally, we show promising results on generalization to different PDE parameters and time-scales with a single surrogate model. Source code for our PyTorch benchmark framework is available at https://github.com/microsoft/pdearena.},
 author = {Jayesh K Gupta and Johannes Brandstetter},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4300980636},
 pdf = {https://openreview.net/pdf?id=dPSTDbGtBY},
 review = {https://openreview.net/forum?id=dPSTDbGtBY},
 title = {Towards Multi-spatiotemporal-scale Generalized PDE Modeling},
 url = {https://openreview.net/forum?id=dPSTDbGtBY},
 year = {2023}
}

@article{gupta2024pnerv,
 author = {Sonam Gupta and Snehal Singh Tomar and Grigorios Chrysos and Sukhendu Das and Rajagopalan N Ambasamduram},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=oCBsxCov2g},
 review = {https://openreview.net/forum?id=oCBsxCov2g},
 title = {{PN}e{RV}: A Polynomial Neural Representation for Videos},
 url = {https://openreview.net/forum?id=oCBsxCov2g},
 year = {2024}
}

@article{gurbuzbalaban2023cyclic,
 abstract = {Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called "tail-index") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show through deep learning experiments that Markovian stepsizes can achieve even a heavier tail and be a viable alternative to cyclic and i.i.d. randomized stepsize rules.},
 author = {Mert Gurbuzbalaban and Yuanhan Hu and Umut Simsekli and Lingjiong Zhu},
 code = {https://github.com/YhHu96/Cyclic-and-Randomized-Stepsizes-Invoke-Heavier-Tails-in-SGD-than-Constant-Stepsize},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320830237},
 pdf = {https://openreview.net/pdf?id=lNB5EHx8uC},
 review = {https://openreview.net/forum?id=lNB5EHx8uC},
 title = {Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize},
 url = {https://openreview.net/forum?id=lNB5EHx8uC},
 year = {2023}
}

@article{gurnee2023finding,
 abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
 author = {Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
 code = {https://github.com/wesg52/sparse-probing-paper},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4368304611},
 pdf = {https://openreview.net/pdf?id=JYs1R9IMJr},
 review = {https://openreview.net/forum?id=JYs1R9IMJr},
 title = {Finding Neurons in a Haystack: Case Studies with Sparse Probing},
 url = {https://openreview.net/forum?id=JYs1R9IMJr},
 year = {2023}
}

@article{gurukar2022benchmarking,
 author = {Saket Gurukar and Priyesh Vijayan and srinivasan parthasarathy and Balaraman Ravindran and Aakash Srinivasan and Goonmeet Bajaj and Chen Cai and Moniba Keymanesh and Saravana Kumar and Pranav Maneriker and Anasua Mitra and Vedang Patel},
 code = {https://github.com/PriyeshV/NRL_Benchmark},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=GvF9ktXI1V},
 review = {https://openreview.net/forum?id=GvF9ktXI1V},
 title = {Benchmarking and Analyzing Unsupervised Network Representation Learning and the Illusion of Progress},
 url = {https://openreview.net/forum?id=GvF9ktXI1V},
 year = {2022}
}

@article{gustafsson2023how,
 abstract = {Many important computer vision applications are naturally formulated as regression problems. Within medical imaging, accurate regression models have the potential to automate various tasks, helping to lower costs and improve patient outcomes. Such safety-critical deployment does however require reliable estimation of model uncertainty, also under the wide variety of distribution shifts that might be encountered in practice. Motivated by this, we set out to investigate the reliability of regression uncertainty estimation methods under various real-world distribution shifts. To that end, we propose an extensive benchmark of 8 image-based regression datasets with different types of challenging distribution shifts. We then employ our benchmark to evaluate many of the most common uncertainty estimation methods, as well as two state-of-the-art uncertainty scores from the task of out-of-distribution detection. We find that while methods are well calibrated when there is no distribution shift, they all become highly overconfident on many of the benchmark datasets. This uncovers important limitations of current uncertainty estimation methods, and the proposed benchmark therefore serves as a challenge to the research community. We hope that our benchmark will spur more work on how to develop truly reliable regression uncertainty estimation methods. Code is available at https://github.com/fregu856/regression_uncertainty.},
 author = {Fredrik K. Gustafsson and Martin Danelljan and Thomas B. Sch{\"o}n},
 code = {https://github.com/fregu856/regression_uncertainty},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319654033},
 pdf = {https://openreview.net/pdf?id=WJt2Pc3qtI},
 review = {https://openreview.net/forum?id=WJt2Pc3qtI},
 title = {How Reliable is Your Regression Model's Uncertainty Under Real-World Distribution Shifts?},
 url = {https://openreview.net/forum?id=WJt2Pc3qtI},
 year = {2023}
}

@article{h,
 pdf = {https://openreview.net/pdf?id=lf0lr4AYM6},
 review = {https://openreview.net/forum?id=lf0lr4AYM6}
}

@article{haas2023linking,
 abstract = {We propose a simple modification to standard ResNet architectures--L2 normalization over feature space--that substantially improves out-of-distribution (OoD) performance on the previously proposed Deep Deterministic Uncertainty (DDU) benchmark. We show that this change also induces early Neural Collapse (NC), an effect linked to better OoD performance. Our method achieves comparable or superior OoD detection scores and classification accuracy in a small fraction of the training time of the benchmark. Additionally, it substantially improves worst case OoD performance over multiple, randomly initialized models. Though we do not suggest that NC is the sole mechanism or a comprehensive explanation for OoD behaviour in deep neural networks (DNN), we believe NC's simple mathematical and geometric structure can provide a framework for analysis of this complex phenomenon in future work.},
 author = {Jarrod Haas and William Yolland and Bernhard T Rabus},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296549243},
 pdf = {https://openreview.net/pdf?id=fjkN5Ur2d6},
 review = {https://openreview.net/forum?id=fjkN5Ur2d6},
 title = {Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks},
 url = {https://openreview.net/forum?id=fjkN5Ur2d6},
 year = {2023}
}

@article{haas2024exploring,
 abstract = {We demonstrate that L2 normalization over feature space can produce capable performance for Out-of-Distribution (OoD) detection for some models and datasets. Although it does not demonstrate outright state-of-the-art performance, this method is notable for its extreme simplicity: it requires only two addition lines of code, and does not need specialized loss functions, image augmentations, outlier exposure or extra parameter tuning. We also observe that training may be more efficient for some datasets and architectures. Notably, only 60 epochs with ResNet18 on CIFAR10 (or 100 epochs with ResNet50) can produce performance within two percentage points (AUROC) of several state-of-the-art methods for some near and far OoD datasets. We provide theoretical and empirical support for this method, and demonstrate viability across five architectures and three In-Distribution (ID) datasets.},
 author = {Jarrod Haas and William Yolland and Bernhard T Rabus},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380032859},
 pdf = {https://openreview.net/pdf?id=daX2UkLMS0},
 review = {https://openreview.net/forum?id=daX2UkLMS0},
 title = {Exploring Simple, High Quality Out-of-Distribution Detection with L2 Normalization},
 url = {https://openreview.net/forum?id=daX2UkLMS0},
 year = {2024}
}

@article{haddouche2023pacbayes,
 abstract = {While PAC-Bayes is now an established learning framework for light-tailed losses (\emph{e.g.}, subgaussian or subexponential), its extension to the case of heavy-tailed losses remains largely uncharted and has attracted a growing interest in recent years. We contribute PAC-Bayes generalisation bounds for heavy-tailed losses under the sole assumption of bounded variance of the loss function. Under that assumption, we extend previous results from \citet{kuzborskij2019efron}. Our key technical contribution is exploiting an extention of Markov's inequality for supermartingales. Our proof technique unifies and extends different PAC-Bayesian frameworks by providing bounds for unbounded martingales as well as bounds for batch and online learning with heavy-tailed losses.},
 author = {Maxime Haddouche and Benjamin Guedj},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4302012848},
 pdf = {https://openreview.net/pdf?id=qxrwt6F3sf},
 review = {https://openreview.net/forum?id=qxrwt6F3sf},
 title = {PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales},
 url = {https://openreview.net/forum?id=qxrwt6F3sf},
 year = {2023}
}

@article{haghighi2023selfsupervised,
 author = {Fatemeh Haghighi and soumitra ghosh and Sarah Chu and Hai Ngu and Mohsen Hejrati and Han Hui Lin and Baris Bingol and Somaye Hashemifar},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=izFnURFG3f},
 review = {https://openreview.net/forum?id=izFnURFG3f},
 title = {Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson{\textquoteright}s Disease},
 url = {https://openreview.net/forum?id=izFnURFG3f},
 year = {2023}
}

@article{hall2024dig,
 abstract = {The unprecedented photorealistic results achieved by recent text-to-image generative systems and their increasing use as plug-and-play content creation solutions make it crucial to understand their potential biases. In this work, we introduce three indicators to evaluate the realism, diversity and prompt-generation consistency of text-to-image generative systems when prompted to generate objects from across the world. Our indicators complement qualitative analysis of the broader impact of such systems by enabling automatic and efficient benchmarking of geographic disparities, an important step towards building responsible visual content creation systems. We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems and find that: (1) models have less realism and diversity of generations when prompting for Africa and West Asia than Europe, (2) prompting with geographic information comes at a cost to prompt-consistency and diversity of generated images, and (3) models exhibit more region-level disparities for some objects than others. Perhaps most interestingly, our indicators suggest that progress in image generation quality has come at the cost of real-world geographic representation. Our comprehensive evaluation constitutes a crucial step towards ensuring a positive experience of visual content creation for everyone.},
 author = {Melissa Hall and Candace Ross and Adina Williams and Nicolas Carion and Michal Drozdzal and Adriana Romero-Soriano},
 badge = {Featured},
 code = {https://github.com/facebookresearch/DIG-In},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4385825705},
 pdf = {https://openreview.net/pdf?id=FDt2UGM1Nz},
 review = {https://openreview.net/forum?id=FDt2UGM1Nz},
 title = {DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity},
 url = {https://openreview.net/forum?id=FDt2UGM1Nz},
 year = {2024}
}

@article{hamid2023bayesian,
 abstract = {Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -- tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -- that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.},
 author = {Saad Hamid and Xingchen Wan and Martin J{\o}rgensen and Binxin Ru and Michael A Osborne},
 code = {https://github.com/saadhamidml/bq-nes},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4327810306},
 pdf = {https://openreview.net/pdf?id=T5sXdAO3EQ},
 review = {https://openreview.net/forum?id=T5sXdAO3EQ},
 title = {Bayesian Quadrature for Neural Ensemble Search},
 url = {https://openreview.net/forum?id=T5sXdAO3EQ},
 year = {2023}
}

@article{hamidi2024adafed,
 abstract = {Federated learning (FL) is a promising technology via which some edge devices/clients collaboratively train a machine learning model orchestrated by a server. Learning an unfair model is known as a critical problem in federated learning, where the trained model may unfairly advantage or disadvantage some of the devices. To tackle this problem, in this work, we propose AdaFed. The goal of AdaFed is to find an updating direction for the server along which (i) all the clients' loss functions are decreasing; and (ii) more importantly, the loss functions for the clients with larger values decrease with a higher rate. AdaFed adaptively tunes this common direction based on the values of local gradients and loss functions. We validate the effectiveness of AdaFed on a suite of federated datasets, and demonstrate that AdaFed outperforms state-of-the-art fair FL methods.},
 author = {Shayan Mohajer Hamidi and EN-HUI YANG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4390810355},
 pdf = {https://openreview.net/pdf?id=rFecyFpFUp},
 review = {https://openreview.net/forum?id=rFecyFpFUp},
 title = {AdaFed: Fair Federated Learning via Adaptive Common Descent Direction},
 url = {https://openreview.net/forum?id=rFecyFpFUp},
 year = {2024}
}

@article{hammouamri2022mitigating,
 author = {Ilyass Hammouamri and Timoth{\'e}e Masquelier and Dennis George Wilson},
 code = {https://github.com/Thvnvtos/Nm-SNN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310833807},
 pdf = {https://openreview.net/pdf?id=15SoThZmtU},
 review = {https://openreview.net/forum?id=15SoThZmtU},
 title = {Mitigating Catastrophic Forgetting in Spiking Neural Networks through Threshold Modulation},
 url = {https://openreview.net/forum?id=15SoThZmtU},
 year = {2022}
}

@article{han2022local,
 author = {Mingxuan Han and Chenglong Ye and Jeff Phillips},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=EDAk6F8yMM},
 review = {https://openreview.net/forum?id=EDAk6F8yMM},
 title = {Local Kernel Ridge Regression for Scalable, Interpolating, Continuous Regression},
 url = {https://openreview.net/forum?id=EDAk6F8yMM},
 year = {2022}
}

@article{han2022on,
 abstract = {When there are unlabeled Out-Of-Distribution (OOD) data from other classes, Semi-Supervised Learning (SSL) methods suffer from severe performance degradation and even get worse than merely training on labeled data. In this paper, we empirically analyze Pseudo-Labeling (PL) in class-mismatched SSL. PL is a simple and representative SSL method that transforms SSL problems into supervised learning by creating pseudo-labels for unlabeled data according to the model's prediction. We aim to answer two main questions: (1) How do OOD data influence PL? (2) What is the proper usage of OOD data with PL? First, we show that the major problem of PL is imbalanced pseudo-labels on OOD data. Second, we find that OOD data can help classify In-Distribution (ID) data given their OOD ground truth labels. Based on the findings, we propose to improve PL in class-mismatched SSL with two components -- Re-balanced Pseudo-Labeling (RPL) and Semantic Exploration Clustering (SEC). RPL re-balances pseudo-labels of high-confidence data, which simultaneously filters out OOD data and addresses the imbalance problem. SEC uses balanced clustering on low-confidence data to create pseudo-labels on extra classes, simulating the process of training with ground truth. Experiments show that our method achieves steady improvement over supervised baseline and state-of-the-art performance under all class mismatch ratios on different benchmarks.},
 author = {Lu Han and Han-Jia Ye and De-Chuan Zhan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4317437991},
 pdf = {https://openreview.net/pdf?id=tLG26QxoD8},
 review = {https://openreview.net/forum?id=tLG26QxoD8},
 title = {On Pseudo-Labeling for Class-Mismatch Semi-Supervised Learning},
 url = {https://openreview.net/forum?id=tLG26QxoD8},
 year = {2022}
}

@article{han2023nonconvexnonconcave,
 author = {Andi Han and Bamdev Mishra and Pratik Jawanpuria and Junbin Gao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=EDVIHPZhFo},
 review = {https://openreview.net/forum?id=EDVIHPZhFo},
 title = {Nonconvex-nonconcave min-max optimization on Riemannian manifolds},
 url = {https://openreview.net/forum?id=EDVIHPZhFo},
 year = {2023}
}

@article{han2023retiring,
 author = {Xiaotian Han and Zhimeng Jiang and Hongye Jin and Zirui Liu and Na Zou and Qifan Wang and Xia Hu},
 code = {https://github.com/ahxt/new_metric_for_demographic_parity},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LjDFIWWVVa},
 review = {https://openreview.net/forum?id=LjDFIWWVVa},
 title = {Retiring \${\textbackslash}Delta {\textbackslash}text\{{DP}\}\$: New Distribution-Level Metrics for Demographic Parity},
 url = {https://openreview.net/forum?id=LjDFIWWVVa},
 year = {2023}
}

@article{han2024from,
 abstract = {Graph neural networks (GNNs) have demonstrated significant promise in modelling relational data and have been widely applied in various fields of interest. The key mechanism behind GNNs is the so-called message passing where information is being iteratively aggregated to central nodes from their neighbourhood. Such a scheme has been found to be intrinsically linked to a physical process known as heat diffusion, where the propagation of GNNs naturally corresponds to the evolution of heat density. Analogizing the process of message passing to the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and consequently informs better model design. Recently, there emerges a plethora of works that proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we provide the first systematic and comprehensive review of studies that leverage the continuous perspective of GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics to GNNs, along with a general framework for the design of graph neural dynamics. We then review and categorize existing works based on their driven mechanisms and underlying dynamics. We also summarize how the limitations of classic GNNs can be addressed under the continuous framework. We conclude by identifying multiple open research directions.},
 author = {Andi Han and Dai Shi and Lequan Lin and Junbin Gao},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4387724735},
 pdf = {https://openreview.net/pdf?id=fPQSxjqa2o},
 review = {https://openreview.net/forum?id=fPQSxjqa2o},
 title = {From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond},
 url = {https://openreview.net/forum?id=fPQSxjqa2o},
 year = {2024}
}

@article{han2024what,
 abstract = {Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate different solution concepts of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertainties. Our experiments demonstrate that our algorithm outperforms existing methods when faced with state perturbations and greatly improves the robustness of MARL policies. Our code is public on https://songyanghan.github.io/what_is_solution/.},
 author = {Songyang Han and Sanbao Su and Sihong He and Shuo Han and Haizhao Yang and Shaofeng Zou and Fei Miao},
 code = {https://songyanghan.github.io/what_is_solution/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310920802},
 pdf = {https://openreview.net/pdf?id=HyqSwNhM3x},
 review = {https://openreview.net/forum?id=HyqSwNhM3x},
 title = {What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?},
 url = {https://openreview.net/forum?id=HyqSwNhM3x},
 year = {2024}
}

@article{hanzely2023personalized,
 abstract = {We investigate the optimization aspects of personalized Federated Learning (FL). We propose general optimizers that can be applied to numerous existing personalized FL objectives, specifically a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. By examining a general personalized objective capable of recovering many existing personalized FL objectives as special cases, we develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. We showcase the practicality and/or optimality of our methods in terms of communication and local computation. Remarkably, our general optimization solvers and theory can recover the best-known communication and computation guarantees for addressing specific personalized FL objectives. Consequently, our proposed methods can serve as universal optimizers, rendering the design of task-specific optimizers unnecessary in many instances.},
 author = {Filip Hanzely and Boxin Zhao and mladen kolar},
 code = {https://github.com/boxinz17/PFL-Unified-Framework},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287326337},
 pdf = {https://openreview.net/pdf?id=ilHM31lXC4},
 review = {https://openreview.net/forum?id=ilHM31lXC4},
 title = {Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques},
 url = {https://openreview.net/forum?id=ilHM31lXC4},
 year = {2023}
}

@article{hao2023bridging,
 abstract = {In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation learning for the first time.},
 author = {Botao Hao and Rahul Jain and Dengwang Tang and Zheng Wen},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4353113064},
 pdf = {https://openreview.net/pdf?id=lanGfX0M6C},
 review = {https://openreview.net/forum?id=lanGfX0M6C},
 title = {Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale},
 url = {https://openreview.net/forum?id=lanGfX0M6C},
 year = {2023}
}

@article{harder2023pretrained,
 abstract = {Training even moderately-sized generative models with differentially-private stochastic gradient descent (DP-SGD) is difficult: the required level of noise for reasonable levels of privacy is simply too large. We advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation. In particular, we minimize the maximum mean discrepancy (MMD) between private target data and a generator's distribution, using a kernel based on perceptual features learned from a public dataset. With the MMD, we can simply privatize the data-dependent term once and for all, rather than introducing noise at each step of optimization as in DP-SGD. Our algorithm allows us to generate CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive features in the distribution, far surpassing the current state of the art, which mostly focuses on datasets such as MNIST and FashionMNIST at a large $\epsilon \approx 10$. Our work introduces simple yet powerful foundations for reducing the gap between private and non-private deep generative models. Our code is available at \url{https://github.com/ParkLabML/DP-MEPF}.},
 author = {Frederik Harder and Milad Jalali and Danica J. Sutherland and Mijung Park},
 code = {https://github.com/ParkLabML/DP-MEPF},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281984841},
 pdf = {https://openreview.net/pdf?id=R6W7zkMz0P},
 review = {https://openreview.net/forum?id=R6W7zkMz0P},
 title = {Pre-trained Perceptual Features Improve Differentially Private Image Generation},
 url = {https://openreview.net/forum?id=R6W7zkMz0P},
 year = {2023}
}

@article{hartill2023teaching,
 abstract = {We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show that performance can be significantly improved by adding retrieval-augmented training datasets which are designed to expose our models to a variety of heuristic reasoning strategies such as weighing partial evidence or ignoring an irrelevant context.},
 author = {Tim Hartill and Neset TAN and Michael Witbrock and Patricia J. Riddle},
 code = {https://github.com/timhartill/unseen_questions},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385965060},
 pdf = {https://openreview.net/pdf?id=d4Vr6E0jjm},
 review = {https://openreview.net/forum?id=d4Vr6E0jjm},
 title = {Teaching Smaller Language Models To Generalise To Unseen Compositional Questions},
 url = {https://openreview.net/forum?id=d4Vr6E0jjm},
 year = {2023}
}

@article{harun2023siesta,
 abstract = {In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we propose a novel continual learning method, SIESTA based on wake/sleep framework for training, which is well aligned to the needs of on-device learning. The major goal of SIESTA is to advance compute efficient continual learning so that DNNs can be updated efficiently using far less time and energy. The principal innovations of SIESTA are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and data-driven network update rule during its wake phase, and 2) expedited memory consolidation using a compute-restricted rehearsal policy during its sleep phase. For memory efficiency, SIESTA adapts latent rehearsal using memory indexing from REMIND. Compared to REMIND and prior arts, SIESTA is far more computationally efficient, enabling continual learning on ImageNet-1K in under 2 hours on a single GPU; moreover, in the augmentation-free setting it matches the performance of the offline learner, a milestone critical to driving adoption of continual learning in real-world applications.},
 author = {Md Yousuf Harun and Jhair Gallardo and Tyler L. Hayes and Ronald Kemker and Christopher Kanan},
 code = {https://yousuf907.github.io/siestasite},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4330339722},
 pdf = {https://openreview.net/pdf?id=MqDVlBWRRV},
 review = {https://openreview.net/forum?id=MqDVlBWRRV},
 title = {SIESTA: Efficient Online Continual Learning with Sleep},
 url = {https://openreview.net/forum?id=MqDVlBWRRV},
 year = {2023}
}

@article{hasan2023a,
 abstract = {The ability to understand causality from data is one of the major milestones of human-level intelligence. Causal Discovery (CD) algorithms can identify the cause-effect relationships among the variables of a system from related observational data with certain assumptions. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (I.I.D.) data and time series data. For this purpose, we first introduce the common terminologies used in causal discovery literature and then provide a comprehensive discussion of the algorithms designed to identify causal relations in different settings. We further discuss some of the benchmark datasets available for evaluating the algorithmic performance, off-the-shelf tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also evaluate some widely used causal discovery algorithms on multiple benchmark datasets and compare their performances. Finally, we conclude by discussing the research challenges and the applications of causal discovery algorithms in multiple areas of interest.},
 author = {Uzma Hasan and Emam Hossain and Md Osman Gani},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4361193818},
 pdf = {https://openreview.net/pdf?id=YdMrdhGx9y},
 review = {https://openreview.net/forum?id=YdMrdhGx9y},
 title = {A Survey on Causal Discovery Methods for I.I.D. and Time Series Data},
 url = {https://openreview.net/forum?id=YdMrdhGx9y},
 year = {2023}
}

@article{hayakawa2024policy,
 abstract = {Reward evaluation of episodes becomes a bottleneck in a broad range of reinforcement learning tasks. Our aim in this paper is to select a small but representative subset of a large batch of episodes, only on which we actually compute rewards for more efficient policy gradient iterations. We build a Gaussian process modeling of discounted returns or rewards to derive a positive definite kernel on the space of episodes, run an ``episodic" kernel quadrature method to compress the information of sample episodes, and pass the reduced episodes to the policy network for gradient updates. We present the theoretical background of this procedure as well as its numerical illustrations in MuJoCo tasks.},
 author = {Satoshi Hayakawa and Tetsuro Morimura},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387929449},
 pdf = {https://openreview.net/pdf?id=WFI9xhJrxF},
 review = {https://openreview.net/forum?id=WFI9xhJrxF},
 title = {Policy Gradient with Kernel Quadrature},
 url = {https://openreview.net/forum?id=WFI9xhJrxF},
 year = {2024}
}

@article{hayou2023on,
 abstract = {In this paper, we study the infinite-depth limit of finite-width residual neural networks with random Gaussian weights. With proper scaling, we show that by fixing the width and taking the depth to infinity, the pre-activations converge in distribution to a zero-drift diffusion process. Unlike the infinite-width limit where the pre-activation converge weakly to a Gaussian random variable, we show that the infinite-depth limit yields different distributions depending on the choice of the activation function. We document two cases where these distributions have closed-form (different) expressions. We further show an intriguing change of regime phenomenon of the post-activation norms when the width increases from 3 to 4. Lastly, we study the sequential limit infinite-depth-then-infinite-width and compare it with the more commonly studied infinite-width-then-infinite-depth limit.},
 author = {Soufiane Hayou},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4302011693},
 pdf = {https://openreview.net/pdf?id=RbLsYz1Az9},
 review = {https://openreview.net/forum?id=RbLsYz1Az9},
 title = {On the infinite-depth limit of finite-width neural networks},
 url = {https://openreview.net/forum?id=RbLsYz1Az9},
 year = {2023}
}

@article{he2023robust,
 abstract = {In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.},
 author = {Sihong He and Songyang Han and Sanbao Su and Shuo Han and Shaofeng Zou and Fei Miao},
 code = {https://github.com/sihongho/robust_marl_with_state_uncertainty},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385473944},
 pdf = {https://openreview.net/pdf?id=CqTkapZ6H9},
 review = {https://openreview.net/forum?id=CqTkapZ6H9},
 title = {Robust Multi-Agent Reinforcement Learning with State Uncertainty},
 url = {https://openreview.net/forum?id=CqTkapZ6H9},
 year = {2023}
}

@article{hedstr,
 code = {https://github.com/annahedstroem/MetaQuantus},
 pdf = {https://openreview.net/pdf?id=j3FK00HyfU},
 review = {https://openreview.net/forum?id=j3FK00HyfU}
}

@article{heikkil,
 code = {https://github.com/DPBayes/DPPVI},
 pdf = {https://openreview.net/pdf?id=55BcghgicI},
 review = {https://openreview.net/forum?id=55BcghgicI}
}

@article{heinrich2024hierarchical,
 abstract = {When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where "local" parameters impact individual events and "global" parameters influence the entire dataset. We introduce practical approaches for frequentist and Bayesian dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via a hierarchical forward model. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to significantly tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics and cosmology.},
 author = {Lukas Heinrich and Siddharth Mishra-Sharma and Chris Pollard and Philipp Windischhofer},
 code = {https://github.com/smsharma/hierarchical-inference},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4381826880},
 pdf = {https://openreview.net/pdf?id=Jy2IgzjoFH},
 review = {https://openreview.net/forum?id=Jy2IgzjoFH},
 title = {Hierarchical Neural Simulation-Based Inference Over Event Ensembles},
 url = {https://openreview.net/forum?id=Jy2IgzjoFH},
 year = {2024}
}

@article{herbold2024semantic,
 abstract = {Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.},
 author = {Steffen Herbold},
 code = {https://github.com/aieng-lab/stsscore},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387031514},
 pdf = {https://openreview.net/pdf?id=bfsNmgN5je},
 review = {https://openreview.net/forum?id=bfsNmgN5je},
 title = {Semantic similarity prediction is better than other semantic similarity measures},
 url = {https://openreview.net/forum?id=bfsNmgN5je},
 year = {2024}
}

@article{herde2023multiannotator,
 abstract = {Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowdworkers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A downstream ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of their potentially correlated annotations. Together with a weighted loss function, we improve the learning from correlated annotation patterns. In a comprehensive evaluation, we examine three research questions about multi-annotator supervised learning. Our findings show MaDL's state-of-the-art performance and robustness against many correlated, spamming annotators.},
 author = {Marek Herde and Denis Huseljic and Bernhard Sick},
 code = {https://github.com/ies-research/multi-annotator-deep-learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4362679609},
 pdf = {https://openreview.net/pdf?id=MgdoxzImlK},
 review = {https://openreview.net/forum?id=MgdoxzImlK},
 title = {Multi-annotator Deep Learning: A Probabilistic Framework for Classification},
 url = {https://openreview.net/forum?id=MgdoxzImlK},
 year = {2023}
}

@article{herley2023approximating,
 author = {Cormac Herley},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=KpElM2S9pw},
 review = {https://openreview.net/forum?id=KpElM2S9pw},
 title = {Approximating Naive Bayes on Unlabelled Categorical Data},
 url = {https://openreview.net/forum?id=KpElM2S9pw},
 year = {2023}
}

@article{hermans2022a,
 author = {Joeri Hermans and Arnaud Delaunoy and Fran{\c{c}}ois Rozet and Antoine Wehenkel and Volodimir Begy and Gilles Louppe},
 code = {https://github.com/montefiore-ai/trust-crisis-in-simulation-based-inference},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LHAbHkt6Aq},
 review = {https://openreview.net/forum?id=LHAbHkt6Aq},
 title = {A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful},
 url = {https://openreview.net/forum?id=LHAbHkt6Aq},
 year = {2022}
}

@article{herrera2023denise,
 abstract = {The robust PCA of covariance matrices plays an essential role when isolating key explanatory features. The currently available methods for performing such a low-rank plus sparse decomposition are matrix specific, meaning, those algorithms must re-run for every new matrix. Since these algorithms are computationally expensive, it is preferable to learn and store a function that nearly instantaneously performs this decomposition when evaluated. Therefore, we introduce Denise, a deep learning-based algorithm for robust PCA of covariance matrices, or more generally, of symmetric positive semidefinite matrices, which learns precisely such a function. Theoretical guarantees for Denise are provided. These include a novel universal approximation theorem adapted to our geometric deep learning problem and convergence to an optimal solution to the learning problem. Our experiments show that Denise matches state-of-the-art performance in terms of decomposition quality, while being approximately $2000\times$ faster than the state-of-the-art, principal component pursuit (PCP), and $200 \times$ faster than the current speed-optimized method, fast PCP.},
 author = {Calypso Herrera and Florian Krach and Anastasis Kratsios and Pierre Ruyssen and Josef Teichmann},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287811191},
 pdf = {https://openreview.net/pdf?id=D45gGvUZp2},
 review = {https://openreview.net/forum?id=D45gGvUZp2},
 title = {Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices},
 url = {https://openreview.net/forum?id=D45gGvUZp2},
 year = {2023}
}

@article{hibshman2023inherent,
 abstract = {Link prediction systems (e.g. recommender systems) typically use graph topology as one of their main sources of information. However, automorphisms and related properties of graphs beget inherent limits in predictability. We calculate hard upper bounds on how well graph topology alone enables link prediction for a wide variety of real-world graphs. We find that in the sparsest of these graphs the upper bounds are surprisingly low, thereby demonstrating that prediction systems on sparse graph data are inherently limited and require information in addition to the graph topology.},
 author = {Justus Isaiah Hibshman and Tim Weninger},
 code = {https://github.com/SteveWillowby/Link_Prediction_Limits},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4317939593},
 pdf = {https://openreview.net/pdf?id=izL3B8dPx1},
 review = {https://openreview.net/forum?id=izL3B8dPx1},
 title = {Inherent Limits on Topology-Based Link Prediction},
 url = {https://openreview.net/forum?id=izL3B8dPx1},
 year = {2023}
}

@article{hoang2023universal,
 abstract = {We address catastrophic forgetting issues in graph learning as incoming data transits from one to another graph distribution. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We propose a novel method that enables graph neural networks to excel in this universal setting. Our approach perseveres knowledge about past tasks through a rehearsal mechanism that maintains local and global structure consistency across the graphs. We benchmark our method against various continual learning baselines in real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks.},
 author = {Thanh Duc Hoang and Do Viet Tung and Duy-Hung Nguyen and Bao-Sinh Nguyen and Huy Hoang Nguyen and Hung Le},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386270384},
 pdf = {https://openreview.net/pdf?id=wzRE5kTnl3},
 review = {https://openreview.net/forum?id=wzRE5kTnl3},
 title = {Universal Graph Continual Learning},
 url = {https://openreview.net/forum?id=wzRE5kTnl3},
 year = {2023}
}

@article{hogan2023on,
 author = {Jack Hogan and Niall M. Adams},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 pdf = {https://openreview.net/pdf?id=FByH3qL87G},
 review = {https://openreview.net/forum?id=FByH3qL87G},
 title = {On Averaging {ROC} Curves},
 url = {https://openreview.net/forum?id=FByH3qL87G},
 year = {2023}
}

@article{hollenstein2022action,
 abstract = {Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\operatorname{X}_{\mathcal{U}\text{rel}}$ that is more robust to estimation artifacts caused by points close to the state-space boundary than previously-proposed measures. Larger noise scales generally increase state-space coverage. However, we found that increasing the space coverage using a larger noise scale is often not beneficial. On the contrary, reducing the noise scale over the training process reduces the variance and generally improves the learning performance. We conclude that the best noise type and scale are environment dependent, and based on our observations derive heuristic rules for guiding the choice of the action noise as a starting point for further optimization.},
 author = {Jakob Hollenstein and Sayantan Auddy and Matteo Saveriano and Erwan Renaudo and Justus Piater},
 badge = {Survey},
 code = {https://github.com/jkbjh/code-action_noise_in_off-policy_d-rl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4281784898},
 pdf = {https://openreview.net/pdf?id=NljBlZ6hmG},
 review = {https://openreview.net/forum?id=NljBlZ6hmG},
 title = {Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance},
 url = {https://openreview.net/forum?id=NljBlZ6hmG},
 year = {2022}
}

@article{horv,
 pdf = {https://openreview.net/pdf?id=Lgs5pQ1v30},
 review = {https://openreview.net/forum?id=Lgs5pQ1v30}
}

@article{houdard2023on,
 author = {Antoine Houdard and Arthur Leclaire and Nicolas Papadakis and Julien Rabin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4290854499},
 pdf = {https://openreview.net/pdf?id=FbztvhdCX9},
 review = {https://openreview.net/forum?id=FbztvhdCX9},
 title = {On the Gradient Formula for learning Generative Models with Regularized Optimal Transport Costs},
 url = {https://openreview.net/forum?id=FbztvhdCX9},
 year = {2023}
}

@article{hsu2023disco,
 author = {Joy Hsu and Jiayuan Mao and Jiajun Wu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=EgHnKOLaKW},
 review = {https://openreview.net/forum?id=EgHnKOLaKW},
 title = {DisCo: Improving Compositional Generalization in Visual Reasoning through Distribution Coverage},
 url = {https://openreview.net/forum?id=EgHnKOLaKW},
 year = {2023}
}

@article{hu2023latent,
 abstract = {The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent "detour" states that slow down convergence.},
 author = {Michael Y. Hu and Angelica Chen and Naomi Saphra and Kyunghyun Cho},
 code = {https://github.com/michahu/modeling-training},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386069016},
 pdf = {https://openreview.net/pdf?id=NE2xXWo0LF},
 review = {https://openreview.net/forum?id=NE2xXWo0LF},
 title = {Latent State Models of Training Dynamics},
 url = {https://openreview.net/forum?id=NE2xXWo0LF},
 year = {2023}
}

@article{hu2023private,
 abstract = {Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing, where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via joint differential privacy (JDP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.},
 author = {Shengyuan Hu and Steven Wu and Virginia Smith},
 code = {https://github.com/s-huu/PMTL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3197649764},
 pdf = {https://openreview.net/pdf?id=onufdyHvqN},
 review = {https://openreview.net/forum?id=onufdyHvqN},
 title = {Private Multi-Task Learning: Formulation and Applications to Federated Learning},
 url = {https://openreview.net/forum?id=onufdyHvqN},
 year = {2023}
}

@article{huang2023analyzing,
 author = {Wei Huang and Chunrui Liu and Yilan Chen and Richard Yi Da Xu and Miao Zhang and Tsui-Wei Weng},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=nEX2q5B2RQ},
 review = {https://openreview.net/forum?id=nEX2q5B2RQ},
 title = {Analyzing Deep {PAC}-Bayesian Learning with Neural Tangent Kernel: Convergence, Analytic Generalization Bound, and Efficient Hyperparameter Selection},
 url = {https://openreview.net/forum?id=nEX2q5B2RQ},
 year = {2023}
}

@article{huang2023bayesian,
 author = {Jireh Huang and Qing Zhou},
 code = {https://github.com/jirehhuang/bcb},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=sMsGv5Kfm3},
 review = {https://openreview.net/forum?id=sMsGv5Kfm3},
 title = {Bayesian Causal Bandits with Backdoor Adjustment Prior},
 url = {https://openreview.net/forum?id=sMsGv5Kfm3},
 year = {2023}
}

@article{huang2023fusion,
 abstract = {Personalized federated learning, as a variant of federated learning, trains customized models for clients using their heterogeneously distributed data. However, it is still inconclusive about how to design personalized models with better representation of shared global knowledge and personalized pattern. To bridge the gap, we in this paper explore personalized models with low-rank and sparse decomposition. Specifically, we employ proper regularization to extract a low-rank global knowledge representation (GKR), so as to distill global knowledge into a compact representation. Subsequently, we employ a sparse component over the obtained GKR to fuse the personalized pattern into the global knowledge. As a solution, we propose a two-stage proximal-based algorithm named \textbf{Fed}erated learning with mixed \textbf{S}parse and \textbf{L}ow-\textbf{R}ank representation (FedSLR) to efficiently search for the mixed models. Theoretically, under proper assumptions, we show that the GKR trained by FedSLR can at least sub-linearly converge to a stationary point of the regularized problem, and that the sparse component being fused can converge to its stationary point under proper settings. Extensive experiments also demonstrate the superior empirical performance of FedSLR. Moreover, FedSLR reduces the number of parameters, and lowers the down-link communication complexity, which are all desirable for federated learning algorithms. Source code is available in \url{https://github.com/huangtiansheng/fedslr}.},
 author = {Tiansheng Huang and Li Shen and Yan Sun and Weiwei Lin and Dacheng Tao},
 code = {https://github.com/huangtiansheng/fedslr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321648951},
 pdf = {https://openreview.net/pdf?id=QtrjqVIZna},
 review = {https://openreview.net/forum?id=QtrjqVIZna},
 title = {Fusion of Global and Local Knowledge for Personalized Federated Learning},
 url = {https://openreview.net/forum?id=QtrjqVIZna},
 year = {2023}
}

@article{huang2023on,
 author = {Julien Walden Huang and Stephen J. Roberts and Jan-Peter Calliess},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 pdf = {https://openreview.net/pdf?id=UIalYAHdBH},
 review = {https://openreview.net/forum?id=UIalYAHdBH},
 title = {On the Sample Complexity of Lipschitz Constant Estimation},
 url = {https://openreview.net/forum?id=UIalYAHdBH},
 year = {2023}
}

@article{huang2023online,
 author = {Yu Huang and Yuan Cheng and Yingbin Liang and Longbo Huang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=TdzQtbLeVw},
 review = {https://openreview.net/forum?id=TdzQtbLeVw},
 title = {Online Min-max Problems with Non-convexity and Non-stationarity},
 url = {https://openreview.net/forum?id=TdzQtbLeVw},
 year = {2023}
}

@article{huang2023robustness,
 abstract = {While deep learning through empirical risk minimization (ERM) has succeeded at achieving human-level performance at a variety of complex tasks, ERM is not robust to distribution shifts or adversarial attacks. Synthetic data augmentation followed by empirical risk minimization (DA-ERM) is a simple and widely used solution to improve robustness in ERM. In addition, consistency regularization can be applied to further improve the robustness of the model by forcing the representation of the original sample and the augmented one to be similar. However, existing consistency regularization methods are not applicable to covariant data augmentation, where the label in the augmented sample is dependent on the augmentation function. For example, dialog state covaries with named entity when we augment data with a new named entity. In this paper, we propose data augmented loss invariant regularization (DAIR), a simple form of consistency regularization that is applied directly at the loss level rather than intermediate features, making it widely applicable to both invariant and covariant data augmentation regardless of network architecture, problem setup, and task. We apply DAIR to real-world learning problems involving covariant data augmentation: robust neural task-oriented dialog state tracking and robust visual question answering. We also apply DAIR to tasks involving invariant data augmentation: robust regression, robust classification against adversarial attacks, and robust ImageNet classification under distribution shift. Our experiments show that DAIR consistently outperforms ERM and DA-ERM with little marginal computational cost and sets new state-of-the-art results in several benchmarks involving covariant data augmentation. Our code of all experiments is available at: https://github.com/optimization-for-data-driven-science/DAIR.git},
 author = {Tianjian Huang and Shaunak Ashish Halbe and Chinnadhurai Sankar and Pooyan Amini and Satwik Kottur and Alborz Geramifard and Meisam Razaviyayn and Ahmad Beirami},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/optimization-for-data-driven-science/DAIR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W3206072865},
 pdf = {https://openreview.net/pdf?id=a1meaRy1bN},
 review = {https://openreview.net/forum?id=a1meaRy1bN},
 title = {Robustness through Data Augmentation Loss Consistency},
 url = {https://openreview.net/forum?id=a1meaRy1bN},
 year = {2023}
}

@article{hubbert2023sobolev,
 abstract = {This work provides theoretical foundations for kernel methods in the hyperspherical context. Specifically, we characterise the native spaces (reproducing kernel Hilbert spaces) and the Sobolev spaces associated with kernels defined over hyperspheres. Our results have direct consequences for kernel cubature, determining the rate of convergence of the worst case error, and expanding the applicability of cubature algorithms based on Stein's method. We first introduce a suitable characterisation on Sobolev spaces on the $d$-dimensional hypersphere embedded in $(d+1)$-dimensional Euclidean spaces. Our characterisation is based on the Fourier--Schoenberg sequences associated with a given kernel. Such sequences are hard (if not impossible) to compute analytically on $d$-dimensional spheres, but often feasible over Hilbert spheres. We circumvent this problem by finding a projection operator that allows to Fourier mapping from Hilbert into finite dimensional hyperspheres. We illustrate our findings through some parametric families of kernels.},
 author = {Simon Hubbert and Emilio Porcu and Chris J. Oates and Mark Girolami},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309394032},
 pdf = {https://openreview.net/pdf?id=82hRiAbnnm},
 review = {https://openreview.net/forum?id=82hRiAbnnm},
 title = {Sobolev Spaces, Kernels and Discrepancies over Hyperspheres},
 url = {https://openreview.net/forum?id=82hRiAbnnm},
 year = {2023}
}

@article{huh2023the,
 abstract = {Modern deep neural networks are highly over-parameterized compared to the
data on which they are trained, yet they often generalize remarkably well. A
flurry of recent work has asked: why do deep networks not overfit to their
training data? In this work, we make a series of empirical observations that
investigate the hypothesis that deeper networks are inductively biased to find
solutions with lower rank embeddings. We conjecture that this bias exists
because the volume of functions that maps to low-rank embedding increases with
depth. We show empirically that our claim holds true on finite width linear and
non-linear models and show that these are the solutions that generalize well.
We then show that the low-rank simplicity bias exists even after training,
using a wide variety of commonly used optimizers. We found this phenomenon to
be resilient to initialization, hyper-parameters, and learning methods. We
further demonstrate how linear over-parameterization of deep non-linear models
can be used to induce low-rank bias, improving generalization performance
without changing the effective model capacity. Practically, we demonstrate that
simply linearly over-parameterizing standard models at training time can
improve performance on image classification tasks, including ImageNet.},
 author = {Minyoung Huh and Hossein Mobahi and Richard Zhang and Brian Cheung and Pulkit Agrawal and Phillip Isola},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3136618203},
 pdf = {https://openreview.net/pdf?id=bCiNWDmlY2},
 review = {https://openreview.net/forum?id=bCiNWDmlY2},
 title = {The Low-Rank Simplicity Bias in Deep Networks},
 url = {https://openreview.net/forum?id=bCiNWDmlY2},
 year = {2023}
}

@article{hult2023diagnostic,
 abstract = {Assessment of model fitness is a key part of machine learning. The standard paradigm is to learn models by minimizing a chosen loss function averaged over training data, with the aim of achieving small losses on future data. In this paper, we consider the use of a finite calibration data set to characterize the future, out-of-sample losses of a model. We propose a simple model diagnostic tool that provides finite-sample guarantees under weak assumptions. The tool is simple to compute and to interpret. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyper-parameter tuning.},
 author = {Ludvig Hult and Dave Zachariah and Peter Stoica},
 code = {https://github.com/el-hult/lal},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283395448},
 pdf = {https://openreview.net/pdf?id=Ulf3QZG9DC},
 review = {https://openreview.net/forum?id=Ulf3QZG9DC},
 title = {Diagnostic Tool for Out-of-Sample Model Evaluation},
 url = {https://openreview.net/forum?id=Ulf3QZG9DC},
 year = {2023}
}

@article{ibrahim2023the,
 author = {Mark Ibrahim and Quentin Garrido and Ari S. Morcos and Diane Bouchacourt},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=QhHLwn3D0Y},
 review = {https://openreview.net/forum?id=QhHLwn3D0Y},
 title = {The Robustness Limits of So{TA} Vision Models to Natural Variation},
 url = {https://openreview.net/forum?id=QhHLwn3D0Y},
 year = {2023}
}

@article{imani2022representation,
 author = {Ehsan Imani and Wei Hu and Martha White},
 badge = {Event: CoLLAs 2023},
 code = {https://github.com/EhsanEI/rep-align-demo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=fLIWMnZ9ij},
 review = {https://openreview.net/forum?id=fLIWMnZ9ij},
 title = {Representation Alignment in Neural Networks},
 url = {https://openreview.net/forum?id=fLIWMnZ9ij},
 year = {2022}
}

@article{iofinova2022flea,
 abstract = {Fairness-aware learning aims at constructing classifiers that not only make accurate predictions, but also do not discriminate against specific groups. It is a fast-growing area of machine learning with far-reaching societal impact. However, existing fair learning methods are vulnerable to accidental or malicious artifacts in the training data, which can cause them to unknowingly produce unfair classifiers. In this work we address the problem of fair learning from unreliable training data in the robust multisource setting, where the available training data comes from multiple sources, a fraction of which might not be representative of the true data distribution. We introduce FLEA, a filtering-based algorithm that identifies and suppresses those data sources that would have a negative impact on fairness or accuracy if they were used for training. As such, FLEA is not a replacement of prior fairness-aware learning methods but rather an augmentation that makes any of them robust against unreliable training data. We show the effectiveness of our approach by a diverse range of experiments on multiple datasets. Additionally, we prove formally that -- given enough data -- FLEA protects the learner against corruptions as long as the fraction of affected data sources is less than half. Our source code and documentation are available at https://github.com/ISTAustria-CVML/FLEA.},
 author = {Eugenia Iofinova and Nikola Konstantinov and Christoph H Lampert},
 code = {https://github.com/ISTAustria-CVML/FLEA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287115227},
 pdf = {https://openreview.net/pdf?id=XsPopigZXV},
 review = {https://openreview.net/forum?id=XsPopigZXV},
 title = {FLEA: Provably Robust Fair Multisource Learning from Unreliable Training Data},
 url = {https://openreview.net/forum?id=XsPopigZXV},
 year = {2022}
}

@article{islamov2023distributed,
 abstract = {Despite their high computation and communication costs, Newton-type methods remain an appealing option for distributed training due to their robustness against ill-conditioned convex problems. In this work, we study ommunication compression and aggregation mechanisms for curvature information in order to reduce these costs while preserving theoretically superior local convergence guarantees. We prove that the recently developed class of three point compressors (3PC) of Richtarik et al. [2022] for gradient communication can be generalized to Hessian communication as well. This result opens up a wide variety of communication strategies, such as contractive compression} and lazy aggregation, available to our disposal to compress prohibitively costly curvature information. Moreover, we discovered several new 3PC mechanisms, such as adaptive thresholding and Bernoulli aggregation, which require reduced communication and occasional Hessian computations. Furthermore, we extend and analyze our approach to bidirectional communication compression and partial device participation setups to cater to the practical considerations of applications in federated learning. For all our methods, we derive fast condition-number-independent local linear and/or superlinear convergence rates. Finally, with extensive numerical evaluations on convex optimization problems, we illustrate that our designed schemes achieve state-of-the-art communication complexity compared to several key baselines using second-order information.},
 author = {Rustem Islamov and Xun Qian and Slavomir Hanzely and Mher Safaryan and Peter Richt{\'a}rik},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281647776},
 pdf = {https://openreview.net/pdf?id=NekBTCKJ1H},
 review = {https://openreview.net/forum?id=NekBTCKJ1H},
 title = {Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation},
 url = {https://openreview.net/forum?id=NekBTCKJ1H},
 year = {2023}
}

@article{ismail2023interpretable,
 abstract = {The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has the premise of being integrated with existing DNNs to offer interpretability to a subset of samples while maintaining the accuracy of the DNNs. Through extensive experiments on 15 tabular and time-series datasets, IME is demonstrated to be more accurate than single interpretable models and perform comparably with existing state-of-the-art DNNs in accuracy. On most datasets, IME even outperforms DNNs, while providing faithful explanations. Lastly, IME's explanations are compared to commonly-used post-hoc explanations methods through a user study -- participants are able to better predict the model behavior when given IME explanations, while finding IME's explanations more faithful and trustworthy.},
 author = {Aya Abdelsalam Ismail and Sercan O Arik and Jinsung Yoon and Ankur Taly and Soheil Feizi and Tomas Pfister},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281612616},
 pdf = {https://openreview.net/pdf?id=DdZoPUPm0a},
 review = {https://openreview.net/forum?id=DdZoPUPm0a},
 title = {Interpretable Mixture of Experts},
 url = {https://openreview.net/forum?id=DdZoPUPm0a},
 year = {2023}
}

@article{issenhuth2023edibert,
 author = {Thibaut Issenhuth and Ugo Tanielian and Jeremie Mary and David Picard},
 code = {https://github.com/EdiBERT4ImageManipulation/EdiBERT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=GRBbtkW3Lp},
 review = {https://openreview.net/forum?id=GRBbtkW3Lp},
 title = {Edi{BERT}: a generative model for image editing},
 url = {https://openreview.net/forum?id=GRBbtkW3Lp},
 year = {2023}
}

@article{izacard2022unsupervised,
 abstract = {Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.},
 author = {Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
 code = {https://github.com/facebookresearch/contriever},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320813768},
 pdf = {https://openreview.net/pdf?id=jKN1pXi7b0},
 review = {https://openreview.net/forum?id=jKN1pXi7b0},
 title = {Unsupervised Dense Information Retrieval with Contrastive Learning},
 url = {https://openreview.net/forum?id=jKN1pXi7b0},
 year = {2022}
}

@article{j,
 badge = {Written by Expert Reviewer},
 pdf = {https://openreview.net/pdf?id=GlhM6XX1wv},
 review = {https://openreview.net/forum?id=GlhM6XX1wv}
}

@article{jain2024controlling,
 abstract = {A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide $\arg\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \textit{covert} or \textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated setting where an eavesdropper can use the optimal weights to generate toxic content, which is more easily misclassified. Numerical results show that when the learner uses the optimal policy, an eavesdropper can only achieve a validation accuracy of $52\%$ with no information and $69\%$ when it has a public dataset with 10\% positive samples compared to $83\%$ when the learner employs a greedy policy.},
 author = {Adit Jain and Vikram Krishnamurthy},
 code = {https://github.com/aditj/CovertHateSpeechClassification},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386044486},
 pdf = {https://openreview.net/pdf?id=g01OVahtN9},
 review = {https://openreview.net/forum?id=g01OVahtN9},
 title = {Controlling Federated Learning for Covertness},
 url = {https://openreview.net/forum?id=g01OVahtN9},
 year = {2024}
}

@article{jayaraman2023solidgen,
 abstract = {The Boundary representation (B-rep) format is the de-facto shape representation in computer-aided design (CAD) to model solid and sheet objects. Recent approaches to generating CAD models have focused on learning sketch-and-extrude modeling sequences that are executed by a solid modeling kernel in postprocess to recover a B-rep. In this paper we present a new approach that enables learning from and synthesizing B-reps without the need for supervision through CAD modeling sequence data. Our method SolidGen, is an autoregressive neural network that models the B-rep directly by predicting the vertices, edges, and faces using Transformer-based and pointer neural networks. Key to achieving this is our Indexed Boundary Representation that references B-rep vertices, edges and faces in a well-defined hierarchy to capture the geometric and topological relations suitable for use with machine learning. SolidGen can be easily conditioned on contexts e.g., class labels, images, and voxels thanks to its probabilistic modeling of the B-rep distribution. We demonstrate qualitatively, quantitatively, and through perceptual evaluation by human subjects that SolidGen can produce high quality, realistic CAD models.},
 author = {Pradeep Kumar Jayaraman and Joseph George Lambourne and Nishkrit Desai and Karl Willis and Aditya Sanghi and Nigel J. W. Morris},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4221162559},
 pdf = {https://openreview.net/pdf?id=ZR2CDgADRo},
 review = {https://openreview.net/forum?id=ZR2CDgADRo},
 title = {SolidGen: An Autoregressive Model for Direct B-rep Synthesis},
 url = {https://openreview.net/forum?id=ZR2CDgADRo},
 year = {2023}
}

@article{jeong2022ranking,
 author = {Minoh Jeong and Alex Dytso and Martina Cardone},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=2EOVIvRXlv},
 review = {https://openreview.net/forum?id=2EOVIvRXlv},
 title = {Ranking Recovery under Privacy Considerations},
 url = {https://openreview.net/forum?id=2EOVIvRXlv},
 year = {2022}
}

@article{jiang2023dplflow,
 author = {Dihong Jiang and Sun Sun},
 code = {https://github.com/dihjiang/DP-LFlow},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=GEcneTl9Mk},
 review = {https://openreview.net/forum?id=GEcneTl9Mk},
 title = {{DP}-{LF}low: Differentially Private Latent Flow for Scalable Sensitive Image Generation},
 url = {https://openreview.net/forum?id=GEcneTl9Mk},
 year = {2023}
}

@article{jiang2024crmoe,
 author = {Ziyu Jiang and Guoqing Zheng and Yu Cheng and Ahmed Hassan Awadallah and Zhangyang Wang},
 code = {https://github.com/VITA-Group/CRMoE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=qKIvn9xL1R},
 review = {https://openreview.net/forum?id=qKIvn9xL1R},
 title = {{CR}-MoE: Consistent Routed Mixture-of-Experts for Scaling Contrastive Learning},
 url = {https://openreview.net/forum?id=qKIvn9xL1R},
 year = {2024}
}

@article{jiang2024incorporating,
 abstract = {It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demonstrate that ICK has superior performance and flexibility on both synthetic and real-world data sets. We believe that ICK framework can be used to include prior information into neural networks in many applications.},
 author = {Ziyang Jiang and Tongshu Zheng and Yiling Liu and David Carlson},
 code = {https://github.com/jzy95310/ICK},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4280600660},
 pdf = {https://openreview.net/pdf?id=HhjSalvWVe},
 review = {https://openreview.net/forum?id=HhjSalvWVe},
 title = {Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel},
 url = {https://openreview.net/forum?id=HhjSalvWVe},
 year = {2024}
}

@article{jin2022weight,
 abstract = {While dropout is known to be a successful regularization technique, insights into the mechanisms that lead to this success are still lacking. We introduce the concept of \emph{weight expansion}, an increase in the signed volume of a parallelotope spanned by the column or row vectors of the weight covariance matrix, and show that weight expansion is an effective means of increasing the generalization in a PAC-Bayesian setting. We provide a theoretical argument that dropout leads to weight expansion and extensive empirical support for the correlation between dropout and weight expansion. To support our hypothesis that weight expansion can be regarded as an \emph{indicator} of the enhanced generalization capability endowed by dropout, and not just as a mere by-product, we have studied other methods that achieve weight expansion (resp.\ contraction), and found that they generally lead to an increased (resp.\ decreased) generalization ability. This suggests that dropout is an attractive regularizer, because it is a computationally cheap method for obtaining weight expansion. This insight justifies the role of dropout as a regularizer, while paving the way for identifying regularizers that promise improved generalization through weight expansion.},
 author = {Gaojie Jin and Xinping Yi and Pengfei Yang and Lijun Zhang and Sven Schewe and Xiaowei Huang},
 code = {https://github.com/Alexkael/Weight_Expansion},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221167689},
 pdf = {https://openreview.net/pdf?id=w3z3sN1b04},
 review = {https://openreview.net/forum?id=w3z3sN1b04},
 title = {Weight Expansion: A New Perspective on Dropout and Generalization},
 url = {https://openreview.net/forum?id=w3z3sN1b04},
 year = {2022}
}

@article{joshi2023learningtodefer,
 abstract = {Learning-to-defer is a framework to automatically defer decision-making to a human expert when ML-based decisions are deemed unreliable. Existing learning-to-defer frameworks are not designed for sequential settings. That is, they defer at every instance independently, based on immediate predictions, while ignoring the potential long-term impact of these interventions. As a result, existing frameworks are myopic. Further, they do not defer adaptively, which is crucial when human interventions are costly. In this work, we propose Sequential Learning-to-Defer (SLTD), a framework for learning-to-defer to a domain expert in sequential decision-making settings. Contrary to existing literature, we pose the problem of learning-to-defer as model-based reinforcement learning (RL) to i) account for long-term consequences of ML-based actions using RL and ii) adaptively defer based on the dynamics (model-based). Our proposed framework determines whether to defer (at each time step) by quantifying whether a deferral now will improve the value compared to delaying deferral to the next time step. To quantify the improvement, we account for potential future deferrals. As a result, we learn a pre-emptive deferral policy (i.e. a policy that defers early if using the ML-based policy could worsen long-term outcomes). Our deferral policy is adaptive to the non-stationarity in the dynamics. We demonstrate that adaptive deferral via SLTD provides an improved trade-off between long-term outcomes and deferral frequency on synthetic, semi-synthetic, and real-world data with non-stationary dynamics. Finally, we interpret the deferral decision by decomposing the propagated (long-term) uncertainty around the outcome, to justify the deferral decision.},
 author = {Shalmali Joshi and Sonali Parbhoo and Finale Doshi-Velez},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310815505},
 pdf = {https://openreview.net/pdf?id=0pn3KnbH5F},
 review = {https://openreview.net/forum?id=0pn3KnbH5F},
 title = {Learning-to-defer for sequential medical decision-making under uncertainty},
 url = {https://openreview.net/forum?id=0pn3KnbH5F},
 year = {2023}
}

@article{jovanovi,
 code = {https://github.com/eth-sri/paradox},
 pdf = {https://openreview.net/pdf?id=atJHLVyBi8},
 review = {https://openreview.net/forum?id=atJHLVyBi8}
}

@article{ju2023zeroshot,
 author = {Wei Ju and Yifang Qin and Siyu Yi and Zhengyang Mao and Kangjie Zheng and Luchen Liu and Xiao Luo and Ming Zhang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=8wGXnjRLSy},
 review = {https://openreview.net/forum?id=8wGXnjRLSy},
 title = {Zero-shot Node Classification with Graph Contrastive Embedding Network},
 url = {https://openreview.net/forum?id=8wGXnjRLSy},
 year = {2023}
}

@article{juliani2022on,
 abstract = {In popular media, there is often a connection drawn between the advent of awareness in artificial agents and those same agents simultaneously achieving human or superhuman level intelligence. In this work, we explore the validity and potential application of this seemingly intuitive link between consciousness and intelligence. We do so by examining the cognitive abilities associated with three contemporary theories of conscious function: Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST). We find that all three theories specifically relate conscious function to some aspect of domain-general intelligence in humans. With this insight, we turn to the field of Artificial Intelligence (AI) and find that, while still far from demonstrating general intelligence, many state-of-the-art deep learning methods have begun to incorporate key aspects of each of the three functional theories. Having identified this trend, we use the motivating example of mental time travel in humans to propose ways in which insights from each of the three theories may be combined into a single unified and implementable model. Given that it is made possible by cognitive abilities underlying each of the three functional theories, artificial agents capable of mental time travel would not only possess greater general intelligence than current approaches, but also be more consistent with our current understanding of the functional role of consciousness in humans, thus making it a promising near-term goal for AI research.},
 author = {Arthur Juliani and Kai Arulkumaran and Shuntaro Sasai and Ryota Kanai},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4223569766},
 pdf = {https://openreview.net/pdf?id=LTyqvLEv5b},
 review = {https://openreview.net/forum?id=LTyqvLEv5b},
 title = {On the link between conscious function and general intelligence in humans and machines},
 url = {https://openreview.net/forum?id=LTyqvLEv5b},
 year = {2022}
}

@article{jullien2023a,
 abstract = {In retail (e.g., grocery stores, apparel shops, online retailers), inventory managers have to balance short-term risk (no items to sell) with long-term-risk (over ordering leading to product waste). This balancing task is made especially hard due to the lack of information about future customer purchases. In this paper, we study the problem of restocking a grocery store's inventory with perishable items over time, from a distributional point of view. The objective is to maximize sales while minimizing waste, with uncertainty about the actual consumption by costumers. This problem is of a high relevance today, given the growing demand for food and the impact of food waste on the environment, the economy, and purchasing power. We frame inventory restocking as a new reinforcement learning task that exhibits stochastic behavior conditioned on the agent's actions, making the environment partially observable. We make two main contributions. First, we introduce a new reinforcement learning environment, RetaiL, based on real grocery store data and expert knowledge. This environment is highly stochastic, and presents a unique challenge for reinforcement learning practitioners. We show that uncertainty about the future behavior of the environment is not handled well by classical supply chain algorithms, and that distributional approaches are a good way to account for the uncertainty. Second, we introduce GTDQN, a distributional reinforcement learning algorithm that learns a generalized Tukey Lambda distribution over the reward space. GTDQN provides a strong baseline for our environment. It outperforms other distributional reinforcement learning approaches in this partially observable setting, in both overall reward and reduction of generated waste.},
 author = {Sami Jullien and Mozhdeh Ariannezhad and Paul Groth and Maarten de Rijke},
 code = {https://github.com/samijullien/GTDQN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320855359},
 pdf = {https://openreview.net/pdf?id=KSvr8A62MD},
 review = {https://openreview.net/forum?id=KSvr8A62MD},
 title = {A Simulation Environment and Reinforcement Learning Method for Waste Reduction},
 url = {https://openreview.net/forum?id=KSvr8A62MD},
 year = {2023}
}

@article{juodelyte2023revisiting,
 abstract = {While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to distinct intermediate representations, which appear to diverge further during fine-tuning. Despite these distinct representations, the predictions of the models remain similar. Our findings show that the similarity between networks before and after fine-tuning does not correlate with performance gains, suggesting that the advantages of transfer learning might not solely originate from the reuse of features in the early layers of a convolutional neural network.},
 author = {Dovile Juodelyte and Amelia Jim{\'e}nez-S{\'a}nchez and Veronika Cheplygina},
 code = {https://github.com/DovileDo/revisiting-transfer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321276990},
 pdf = {https://openreview.net/pdf?id=ScrEUZLxPr},
 review = {https://openreview.net/forum?id=ScrEUZLxPr},
 title = {Revisiting Hidden Representations in Transfer Learning for Medical Imaging},
 url = {https://openreview.net/forum?id=ScrEUZLxPr},
 year = {2023}
}

@article{kaili2023calibrating,
 abstract = {Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt the expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Contrast-Reg in enhancing the generalizability of the Graph Neural Network(GNN) model and improving the performance of graph contrastive algorithms with different similarity definitions and encoder backbones across various downstream tasks.},
 author = {MA KAILI and Garry YANG and Han Yang and Yongqiang Chen and James Cheng},
 code = {https://github.com/MaKaili/Contrast-Reg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3125831646},
 pdf = {https://openreview.net/pdf?id=LdSP6cvTS4},
 review = {https://openreview.net/forum?id=LdSP6cvTS4},
 title = {Calibrating and Improving Graph Contrastive Learning},
 url = {https://openreview.net/forum?id=LdSP6cvTS4},
 year = {2023}
}

@article{kalajdzievski2023transfer,
 abstract = {When presented with a data stream of two statistically dependent variables, predicting the future of one of the variables (the target stream) can benefit from information about both its history and the history of the other variable (the source stream). For example, fluctuations in temperature at a weather station can be predicted using both temperatures and barometric readings. However, a challenge when modelling such data is that it is easy for a neural network to rely on the greatest joint correlations within the target stream, which may ignore a crucial but small information transfer from the source to the target stream. As well, there are often situations where the target stream may have previously been modelled independently and it would be useful to use that model to inform a new joint model. Here, we develop an information bottleneck approach for conditional learning on two dependent streams of data. Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable, while quantifying this information transfer within the model. As such, TEB provides a useful new information bottleneck approach for modelling two statistically dependent streams of data in order to make predictions about one of them.},
 author = {Damjan Kalajdzievski and Ximeng Mao and Pascal Fortier-Poisson and Guillaume Lajoie and Blake Aaron Richards},
 code = {https://github.com/ximmao/TransferEntropyBottleneck},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310557244},
 pdf = {https://openreview.net/pdf?id=kJcwlP7BRs},
 review = {https://openreview.net/forum?id=kJcwlP7BRs},
 title = {Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer},
 url = {https://openreview.net/forum?id=kJcwlP7BRs},
 year = {2023}
}

@article{kalantidis2022tldr,
 abstract = {Dimensionality reduction methods are unsupervised approaches which learn low-dimensional spaces where some properties of the initial space, typically the notion of "neighborhood", are preserved. Such methods usually require propagation on large k-NN graphs or complicated optimization solvers. On the other hand, self-supervised learning approaches, typically used to learn representations from scratch, rely on simple and more scalable frameworks for learning. In this paper, we propose TLDR, a dimensionality reduction method for generic input spaces that is porting the recent self-supervised learning framework of Zbontar et al. (2021) to the specific task of dimensionality reduction, over arbitrary representations. We propose to use nearest neighbors to build pairs from a training set and a redundancy reduction loss to learn an encoder that produces representations invariant across such pairs. TLDR is a method that is simple, easy to train, and of broad applicability; it consists of an offline nearest neighbor computation step that can be highly approximated, and a straightforward learning process. Aiming for scalability, we focus on improving linear dimensionality reduction, and show consistent gains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on ROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining it with a 10x compression.},
 author = {Yannis Kalantidis and Carlos Eduardo Rosar Kos Lassance and Jon Almaz{\'a}n and Diane Larlus},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/naver/tldr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4286901374},
 pdf = {https://openreview.net/pdf?id=86fhqdBUbx},
 review = {https://openreview.net/forum?id=86fhqdBUbx},
 title = {TLDR: Twin Learning for Dimensionality Reduction},
 url = {https://openreview.net/forum?id=86fhqdBUbx},
 year = {2022}
}

@article{kalweit2022robust,
 author = {Gabriel Kalweit and Maria Kalweit and Joschka Boedecker},
 code = {https://github.com/NrLabFreiburg/composite-q-learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ak6Bds2DcI},
 review = {https://openreview.net/forum?id=ak6Bds2DcI},
 title = {Robust and Data-efficient Q-learning by Composite Value-estimation},
 url = {https://openreview.net/forum?id=ak6Bds2DcI},
 year = {2022}
}

@article{kamthe2022iterative,
 author = {Sanket Kamthe and So Takao and Shakir Mohamed and Marc Peter Deisenroth},
 code = {https://github.com/sanket-kamthe/EPyStateEstimator},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=xyt4wfdo4J},
 review = {https://openreview.net/forum?id=xyt4wfdo4J},
 title = {Iterative State Estimation in Non-linear Dynamical Systems Using Approximate Expectation Propagation},
 url = {https://openreview.net/forum?id=xyt4wfdo4J},
 year = {2022}
}

@article{kang2024the,
 author = {Justin Singh Kang and Ramtin Pedarsani and Kannan Ramchandran},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ynG5Ak7n7Q},
 review = {https://openreview.net/forum?id=ynG5Ak7n7Q},
 title = {The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning},
 url = {https://openreview.net/forum?id=ynG5Ak7n7Q},
 year = {2024}
}

@article{kareer2024were,
 abstract = {There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA},
 author = {Simar Kareer and Vivek Vijaykumar and Harsh Maheshwari and Judy Hoffman and Prithvijit Chattopadhyay and Viraj Uday Prabhu},
 badge = {Reproducibility},
 code = {https://github.com/SimarKareer/UnifiedVideoDA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4391506343},
 pdf = {https://openreview.net/pdf?id=10R6iX6JHm},
 review = {https://openreview.net/forum?id=10R6iX6JHm},
 title = {We're Not Using Videos Effectively: An Updated Domain Adaptive Video
  Segmentation Baseline},
 url = {https://openreview.net/forum?id=10R6iX6JHm},
 year = {2024}
}

@article{karthikeyan2022learning,
 abstract = {Decision trees provide a rich family of highly non-linear but efficient models, due to which they continue to be the go-to family of predictive models by practitioners across domains. But learning trees is a challenging problem due to their highly discrete and non-differentiable decision boundaries. The state-of-the-art techniques use greedy methods that exploit the discrete tree structure but are tailored to specific problem settings (say, categorical vs real-valued predictions). In this work, we propose a reformulation of the tree learning problem that provides better conditioned gradients, and leverages successful deep network learning techniques like overparameterization and straight-through estimators. Our reformulation admits an efficient and {\em accurate} gradient-based algorithm that allows us to deploy our solution in disparate tree learning settings like supervised batch learning and online bandit feedback based learning. 
Using extensive validation on standard benchmarks, we observe that in the supervised learning setting, our general method is competitive to, and in some cases more accurate than, existing methods that are designed {\em specifically} for the supervised settings. In contrast, for bandit settings, where most of the existing techniques are not applicable, our models are still accurate and significantly outperform the applicable state-of-the-art methods.},
 author = {Ajaykrishna Karthikeyan and Naman Jain and Nagarajan Natarajan and Prateek Jain},
 code = {https://github.com/microsoft/dgt},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3131777279},
 pdf = {https://openreview.net/pdf?id=u0n1chY0b6},
 review = {https://openreview.net/forum?id=u0n1chY0b6},
 title = {Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent.},
 url = {https://openreview.net/forum?id=u0n1chY0b6},
 year = {2022}
}

@article{karvonen2023a,
 abstract = {We study a class of Gaussian processes for which the posterior mean, for a particular choice of data, replicates a truncated Taylor expansion of any order. The data consist of derivative evaluations at the expansion point and the prior covariance kernel belongs to the class of Taylor kernels, which can be written in a certain power series form. We discuss and prove some results on maximum likelihood estimation of parameters of Taylor kernels. The proposed framework is a special case of Gaussian process regression based on data that is orthogonal in the reproducing kernel Hilbert space of the covariance kernel.},
 author = {Toni Karvonen and Jon Cockayne and Filip Tronarp and Simo S{\"a}rkk{\"a}},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287353428},
 pdf = {https://openreview.net/pdf?id=2TneniEIDB},
 review = {https://openreview.net/forum?id=2TneniEIDB},
 title = {A probabilistic Taylor expansion with Gaussian processes},
 url = {https://openreview.net/forum?id=2TneniEIDB},
 year = {2023}
}

@article{kawar2023enhancing,
 abstract = {Denoising diffusion probabilistic models (DDPMs) are a recent family of generative models that achieve state-of-the-art results. In order to obtain class-conditional generation, it was suggested to guide the diffusion process by gradients from a time-dependent classifier. While the idea is theoretically sound, deep learning-based classifiers are infamously susceptible to gradient-based adversarial attacks. Therefore, while traditional classifiers may achieve good accuracy scores, their gradients are possibly unreliable and might hinder the improvement of the generation results. Recent work discovered that adversarially robust classifiers exhibit gradients that are aligned with human perception, and these could better guide a generative process towards semantically meaningful images. We utilize this observation by defining and training a time-dependent adversarially robust classifier and use it as guidance for a generative diffusion model. In experiments on the highly challenging and diverse ImageNet dataset, our scheme introduces significantly more intelligible intermediate gradients, better alignment with theoretical findings, as well as improved generation results under several evaluation metrics. Furthermore, we conduct an opinion survey whose findings indicate that human raters prefer our method's results.},
 author = {Bahjat Kawar and Roy Ganz and Michael Elad},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4292419533},
 pdf = {https://openreview.net/pdf?id=tEVpz2xJWX},
 review = {https://openreview.net/forum?id=tEVpz2xJWX},
 title = {Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance},
 url = {https://openreview.net/forum?id=tEVpz2xJWX},
 year = {2023}
}

@article{kawashima2023minorizationmaximization,
 author = {Takahiro Kawashima and Hideitsu Hino},
 code = {https://github.com/ISMHinoLab/DPPMMEstimation},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=65AzNvY73Q},
 review = {https://openreview.net/forum?id=65AzNvY73Q},
 title = {Minorization-Maximization for Learning Determinantal Point Processes},
 url = {https://openreview.net/forum?id=65AzNvY73Q},
 year = {2023}
}

@article{kazemi2023tackling,
 abstract = {Representative Selection (RS) is the problem of finding a small subset of exemplars from a dataset that is representative of the dataset. In this paper, we study RS for attributed graphs, and focus on finding representative nodes that optimize the accuracy of a model trained on the selected representatives. Theoretically, we establish a new hardness result forRS (in the absence of a graph structure) by proving that a particular, highly practical variant of it (RS for Learning) is hard to approximate in polynomial time within any reasonable factor, which implies a significant potential gap between the optimum solution of widely-used surrogate functions and the actual accuracy of the model. We then study the setting where a (homophilous) graph structure is available, or can be constructed, between the data points.We show that with an appropriate modeling approach, the presence of such a structure can turn a hard RS (for learning) problem into one that can be effectively solved. To this end, we develop RS-GNN, a representation learning-based RS model based on Graph Neural Networks. Empirically, we demonstrate the effectiveness of RS-GNN on problems with predefined graph structures as well as problems with graphs induced from node feature similarities, by showing that RS-GNN achieves significant improvements over established baselines on a suite of eight benchmarks.},
 author = {Mehran Kazemi and Anton Tsitsulin and Hossein Esfandiari and Mohammadhossein Bateni and Deepak Ramachandran and Bryan Perozzi and Vahab Mirrokni},
 code = {https://github.com/google-research/google-research/tree/master/rs_gnn},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281485080},
 pdf = {https://openreview.net/pdf?id=3LzgOQ3eOb},
 review = {https://openreview.net/forum?id=3LzgOQ3eOb},
 title = {Tackling Provably Hard Representative Selection via Graph Neural Networks},
 url = {https://openreview.net/forum?id=3LzgOQ3eOb},
 year = {2023}
}

@article{ke2023neural,
 author = {Nan Rosemary Ke and Olexa Bilaniuk and Anirudh Goyal and Stefan Bauer and Hugo Larochelle and Bernhard Sch{\"o}lkopf and Michael Curtis Mozer and Christopher Pal and Yoshua Bengio},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=rdHVPPVuXa},
 review = {https://openreview.net/forum?id=rdHVPPVuXa},
 title = {Neural Causal Structure Discovery from Interventions},
 url = {https://openreview.net/forum?id=rdHVPPVuXa},
 year = {2023}
}

@article{ke2024provable,
 abstract = {We study the problem of consistently recovering the sparsity pattern of a regression parameter vector from correlated observations governed by deterministic missing data patterns using Lasso. We consider the case in which the observed dataset is censored by a deterministic, non-uniform filter. Recovering the sparsity pattern in datasets with deterministic missing structure can be arguably more challenging than recovering in a uniformly-at-random scenario. In this paper, we propose an efficient algorithm for missing value imputation by utilizing the topological property of the censorship filter. We then provide novel theoretical results for exact recovery of the sparsity pattern using the proposed imputation strategy. Our analysis shows that, under certain statistical and topological conditions, the hidden sparsity pattern can be recovered consistently with high probability in polynomial time and logarithmic sample complexity.},
 author = {Chuyang Ke and Jean Honorio},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282813535},
 pdf = {https://openreview.net/pdf?id=SSqOqAwpN7},
 review = {https://openreview.net/forum?id=SSqOqAwpN7},
 title = {Provable Guarantees for Sparsity Recovery with Deterministic Missing Data Patterns},
 url = {https://openreview.net/forum?id=SSqOqAwpN7},
 year = {2024}
}

@article{kehrenberg2024addressing,
 author = {Thomas Kehrenberg and Myles Bartlett and Viktoriia Sharmanska and Novi Quadrianto},
 code = {https://github.com/wearepal/support-matching},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=JYbnJ92TJf},
 review = {https://openreview.net/forum?id=JYbnJ92TJf},
 title = {Addressing Attribute Bias with Adversarial Support-Matching},
 url = {https://openreview.net/forum?id=JYbnJ92TJf},
 year = {2024}
}

@article{keith2023rct,
 abstract = {Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -- researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT -- which we release publicly -- consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.},
 author = {Katherine A. Keith and Sergey Feldman and David Jurgens and Jonathan Bragg and Rohit Bhattacharya},
 code = {https://github.com/kakeith/rct_rejection_sampling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385436414},
 pdf = {https://openreview.net/pdf?id=F74ZZk5hPa},
 review = {https://openreview.net/forum?id=F74ZZk5hPa},
 title = {RCT Rejection Sampling for Causal Estimation Evaluation},
 url = {https://openreview.net/forum?id=F74ZZk5hPa},
 year = {2023}
}

@article{kelkar2024ambientflow,
 abstract = {Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness of AmbientFlow in learning the object distribution. The utility of AmbientFlow in a downstream inference task of image reconstruction is demonstrated.},
 author = {Varun A. Kelkar and Rucha Deshpande and Arindam Banerjee and Mark Anastasio},
 code = {https://github.com/comp-imaging-sci/ambientflow},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386694316},
 pdf = {https://openreview.net/pdf?id=txpYITR8oa},
 review = {https://openreview.net/forum?id=txpYITR8oa},
 title = {AmbientFlow: Invertible generative models from incomplete, noisy measurements},
 url = {https://openreview.net/forum?id=txpYITR8oa},
 year = {2024}
}

@article{keller2023homomorphic,
 abstract = {In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.},
 author = {T. Anderson Keller and Xavier Suau and Luca Zappella},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309214087},
 pdf = {https://openreview.net/pdf?id=tEKqQgbwbf},
 review = {https://openreview.net/forum?id=tEKqQgbwbf},
 title = {Homomorphic Self-Supervised Learning},
 url = {https://openreview.net/forum?id=tEKqQgbwbf},
 year = {2023}
}

@article{kemertas2022approximate,
 abstract = {Bisimulation metrics define a distance measure between states of a Markov decision process (MDP) based on a comparison of reward sequences. Due to this property they provide theoretical guarantees in value function approximation (VFA). In this work we first prove that bisimulation and $\pi$-bisimulation metrics can be defined via a more general class of Sinkhorn distances, which unifies various state similarity metrics used in recent work. Then we describe an approximate policy iteration (API) procedure that uses a bisimulation-based discretization of the state space for VFA and prove asymptotic performance bounds. Next, we bound the difference between $\pi$-bisimulation metrics in terms of the change in the policies themselves. Based on these results, we design an API($\alpha$) procedure that employs conservative policy updates and enjoys better performance bounds than the naive API approach. We discuss how such API procedures map onto practical actor-critic methods that use bisimulation metrics for state representation learning. Lastly, we validate our theoretical results and investigate their practical implications via a controlled empirical analysis based on an implementation of bisimulation-based API for finite MDPs.},
 author = {Mete Kemertas and Allan Douglas Jepson},
 code = {https://github.com/metekemertas/api-bisim},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221166364},
 pdf = {https://openreview.net/pdf?id=Ii7UeHc0mO},
 review = {https://openreview.net/forum?id=Ii7UeHc0mO},
 title = {Approximate Policy Iteration with Bisimulation Metrics},
 url = {https://openreview.net/forum?id=Ii7UeHc0mO},
 year = {2022}
}

@article{khaled2022better,
 abstract = {Large-scale nonconvex optimization problems are ubiquitous in modern machine learning, and among practitioners interested in solving them, Stochastic Gradient Descent (SGD) reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new variant of the recently introduced expected smoothness assumption which governs the behaviour of the second moment of the stochastic gradient. We show that our assumption is both more general and more reasonable than assumptions made in all prior work. Moreover, our results yield the optimal $\mathcal{O}(\varepsilon^{-4})$ rate for finding a stationary point of nonconvex smooth functions, and recover the optimal $\mathcal{O}(\varepsilon^{-1})$ rate for finding a global solution if the Polyak-Łojasiewicz condition is satisfied. We compare against convergence rates under convexity and prove a theorem on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of independent interest. Moreover, we perform our analysis in a framework which allows for a detailed study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum optimization problems. We corroborate our theoretical results with experiments on real and synthetic data.},
 author = {Ahmed Khaled and Peter Richt{\'a}rik},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W3005128411},
 pdf = {https://openreview.net/pdf?id=AU4qHN2VkS},
 review = {https://openreview.net/forum?id=AU4qHN2VkS},
 title = {Better Theory for SGD in the Nonconvex World},
 url = {https://openreview.net/forum?id=AU4qHN2VkS},
 year = {2023}
}

@article{khetarpal2023pomrl,
 abstract = {We study the problem of planning under model uncertainty in an online meta-reinforcement learning (RL) setting where an agent is presented with a sequence of related tasks with limited interactions per task. The agent can use its experience in each task and across tasks to estimate both the transition model and the distribution over tasks. We propose an algorithm to meta-learn the underlying structure across tasks, utilize it to plan in each task, and upper-bound the regret of the planning loss. Our bound suggests that the average regret over tasks decreases as the number of tasks increases and as the tasks are more similar. In the classical single-task setting, it is known that the planning horizon should depend on the estimated model's accuracy, that is, on the number of samples within task. We generalize this finding to meta-RL and study this dependence of planning horizons on the number of tasks. Based on our theoretical findings, we derive heuristics for selecting slowly increasing discount factors, and we validate its significance empirically.},
 author = {Khimya Khetarpal and Claire Vernade and Brendan O'Donoghue and Satinder Singh and Tom Zahavy},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/kkhetarpal/pomrl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4313447142},
 pdf = {https://openreview.net/pdf?id=brGgOAXYtr},
 review = {https://openreview.net/forum?id=brGgOAXYtr},
 title = {POMRL: No-Regret Learning-to-Plan with Increasing Horizons},
 url = {https://openreview.net/forum?id=brGgOAXYtr},
 year = {2023}
}

@article{khosravani2024using,
 abstract = {The success of deep active learning hinges on the choice of an effective acquisition function, which ranks not yet labeled data points according to their expected informativeness. Many acquisition functions are (partly) based on the uncertainty that the current model has about the class label of a point, yet there is no generally agreed upon strategy for computing such uncertainty. This paper proposes a new and very simple approach to computing uncertainty in deep active learning with a Convolutional Neural Network (CNN). The main idea is to use the feature representation extracted by the CNN as data for training a Sum-Product Network (SPN). Since SPNs are typically used for estimating the distribution of a dataset, they are well suited to the task of estimating class probabilities that can be used directly by standard acquisition functions such as max entropy and variational ratio. The effectiveness of our method is demonstrated in an experimental study on several standard benchmark datasets for image classification, where we compare it to various state-of-the-art methods for assessing uncertainty in deep active learning.},
 author = {Mohamadsadegh Khosravani and Sandra Zilles},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283330514},
 pdf = {https://openreview.net/pdf?id=Ai9XpjGxjl},
 review = {https://openreview.net/forum?id=Ai9XpjGxjl},
 title = {Using Sum-Product Networks to Assess Uncertainty in Deep Active Learning},
 url = {https://openreview.net/forum?id=Ai9XpjGxjl},
 year = {2024}
}

@article{kilian2023mixed,
 author = {Pascal Kilian and Sangbeak Ye and Augustin Kelava},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=MKZyHtmfwH},
 review = {https://openreview.net/forum?id=MKZyHtmfwH},
 title = {Mixed effects in machine learning {\textendash} A flexible mixed{ML} framework to add random effects to supervised machine learning regression},
 url = {https://openreview.net/forum?id=MKZyHtmfwH},
 year = {2023}
}

@article{kilitcioglu2022nondeterministic,
 author = {Doruk Kilitcioglu and Serdar Kadioglu},
 badge = {Reproducibility},
 code = {https://github.com/fidelity/mabwiser/tree/master/examples/lints_reproducibility},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 pdf = {https://openreview.net/pdf?id=sX9d3gfwtE},
 review = {https://openreview.net/forum?id=sX9d3gfwtE},
 title = {Non-Deterministic Behavior of Thompson Sampling with Linear Payoffs and How to Avoid It},
 url = {https://openreview.net/forum?id=sX9d3gfwtE},
 year = {2022}
}

@article{killian2023risk,
 abstract = {In safety-critical decision-making scenarios being able to identify worst-case outcomes, or dead-ends is crucial in order to develop safe and reliable policies in practice. These situations are typically rife with uncertainty due to unknown or stochastic characteristics of the environment as well as limited offline training data. As a result, the value of a decision at any time point should be based on the distribution of its anticipated effects. We propose a framework to identify worst-case decision points, by explicitly estimating distributions of the expected return of a decision. These estimates enable earlier indication of dead-ends in a manner that is tunable based on the risk tolerance of the designed task. We demonstrate the utility of Distributional Dead-end Discovery (DistDeD) in a toy domain as well as when assessing the risk of severely ill patients in the intensive care unit reaching a point where death is unavoidable. We find that DistDeD significantly improves over prior discovery approaches, providing indications of the risk 10 hours earlier on average as well as increasing detection by 20%.},
 author = {Taylor W. Killian and Sonali Parbhoo and Marzyeh Ghassemi},
 code = {https://github.com/MLforHealth/DistDeD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4316843729},
 pdf = {https://openreview.net/pdf?id=oKlEOT83gI},
 review = {https://openreview.net/forum?id=oKlEOT83gI},
 title = {Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning},
 url = {https://openreview.net/forum?id=oKlEOT83gI},
 year = {2023}
}

@article{killingberg2023the,
 author = {Ludvig Killingberg and Helge Langseth},
 code = {https://github.com/ludvigk/MQ-MMDRL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=z49eaB8kiH},
 review = {https://openreview.net/forum?id=z49eaB8kiH},
 title = {The Multiquadric Kernel for Moment-Matching Distributional Reinforcement Learning},
 url = {https://openreview.net/forum?id=z49eaB8kiH},
 year = {2023}
}

@article{kim2022deep,
 abstract = {Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely a black-box. The data may have some known structure (e.g. symmetries) and/or the data generation process may be a composite process that yields useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and do not easily accommodate known structure. Instead, we use Bayesian neural networks, a class of scalable and flexible surrogate models with inductive biases, to extend BO to complex, structured problems with high dimensionality. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that neural networks often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.},
 author = {Samuel Kim and Peter Y Lu and Charlotte Loh and Jamie Smith and Jasper Snoek and Marin Soljacic},
 code = {https://github.com/samuelkim314/DeepBO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287202213},
 pdf = {https://openreview.net/pdf?id=tPMQ6Je2rB},
 review = {https://openreview.net/forum?id=tPMQ6Je2rB},
 title = {Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure},
 url = {https://openreview.net/forum?id=tPMQ6Je2rB},
 year = {2022}
}

@article{kim2023assisting,
 abstract = {Many practical applications, ranging from paper-reviewer assignment in peer review to job-applicant matching for hiring, require human decision makers to identify relevant matches by combining their expertise with predictions from machine learning models. In many such model-assisted document matching tasks, the decision makers have stressed the need for assistive information about the model outputs (or the data) to facilitate their decisions. In this paper, we devise a proxy matching task that allows us to evaluate which kinds of assistive information improve decision makers' performance (in terms of accuracy and time). Through a crowdsourced (N=271 participants) study, we find that providing black-box model explanations reduces users' accuracy on the matching task, contrary to the commonly-held belief that they can be helpful by allowing better understanding of the model. On the other hand, custom methods that are designed to closely attend to some task-specific desiderata are found to be effective in improving user performance. Surprisingly, we also find that the users' perceived utility of assistive information is misaligned with their objective utility (measured through their task performance).},
 author = {Joon Sik Kim and Valerie Chen and Danish Pruthi and Nihar B Shah and Ameet Talwalkar},
 code = {https://github.com/wnstlr/document-matching},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321277387},
 pdf = {https://openreview.net/pdf?id=5rq8iRzHAQ},
 review = {https://openreview.net/forum?id=5rq8iRzHAQ},
 title = {Assisting Human Decisions in Document Matching},
 url = {https://openreview.net/forum?id=5rq8iRzHAQ},
 year = {2023}
}

@article{kim2024generalizing,
 author = {Young Kyung Kim and Juan Matias Di Martino and Guillermo Sapiro},
 code = {https://github.com/youngkyungkim93/MNAM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=xLg8ljlEba},
 review = {https://openreview.net/forum?id=xLg8ljlEba},
 title = {Generalizing Neural Additive Models via Statistical Multimodal Analysis},
 url = {https://openreview.net/forum?id=xLg8ljlEba},
 year = {2024}
}

@article{kim2024when,
 abstract = {The extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence rate.},
 author = {Junhyung Lyle Kim and Gauthier Gidel and Anastasios Kyrillidis and Fabian Pedregosa},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308757011},
 pdf = {https://openreview.net/pdf?id=ZLVbQEu4Ab},
 review = {https://openreview.net/forum?id=ZLVbQEu4Ab},
 title = {When is Momentum Extragradient Optimal? A Polynomial-Based Analysis},
 url = {https://openreview.net/forum?id=ZLVbQEu4Ab},
 year = {2024}
}

@article{kirsch2022a,
 abstract = {Several recent works find empirically that the average test error of deep neural networks can be estimated via the prediction disagreement of models, which does not require labels. In particular, Jiang et al. (2022) show for the disagreement between two separately trained networks that this `Generalization Disagreement Equality' follows from the well-calibrated nature of deep ensembles under the notion of a proposed `class-aggregated calibration.' In this reproduction, we show that the suggested theory might be impractical because a deep ensemble's calibration can deteriorate as prediction disagreement increases, which is precisely when the coupling of test error and disagreement is of interest, while labels are needed to estimate the calibration on new datasets. Further, we simplify the theoretical statements and proofs, showing them to be straightforward within a probabilistic context, unlike the original hypothesis space view employed by Jiang et al. (2022).},
 author = {Andreas Kirsch and Yarin Gal},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/BlackHC/2202.01851},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4226340844},
 pdf = {https://openreview.net/pdf?id=oRP8urZ8Fx},
 review = {https://openreview.net/forum?id=oRP8urZ8Fx},
 title = {A Note on "Assessing Generalization of SGD via Disagreement"},
 url = {https://openreview.net/forum?id=oRP8urZ8Fx},
 year = {2022}
}

@article{kirsch2022unifying,
 abstract = {Recently proposed methods in data subset selection, that is active learning and active sampling, use Fisher information, Hessians, similarity matrices based on gradients, and gradient lengths to estimate how informative data is for a model's training. Are these different approaches connected, and if so, how? We revisit the fundamentals of Bayesian optimal experiment design and show that these recently proposed methods can be understood as approximations to information-theoretic quantities: among them, the mutual information between predictions and model parameters, known as expected information gain or BALD in machine learning, and the mutual information between predictions of acquisition candidates and test samples, known as expected predictive information gain. We develop a comprehensive set of approximations using Fisher information and observed information and derive a unified framework that connects seemingly disparate literature. Although Bayesian methods are often seen as separate from non-Bayesian ones, the sometimes fuzzy notion of "informativeness" expressed in various non-Bayesian objectives leads to the same couple of information quantities, which were, in principle, already known by Lindley (1956) and MacKay (1992).},
 author = {Andreas Kirsch and Yarin Gal},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/BlackHC/2208.00549},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4289598391},
 pdf = {https://openreview.net/pdf?id=UVDAKQANOW},
 review = {https://openreview.net/forum?id=UVDAKQANOW},
 title = {Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities},
 url = {https://openreview.net/forum?id=UVDAKQANOW},
 year = {2022}
}

@article{kirsch2023blackbox,
 abstract = {Batch active learning is a popular approach for efficiently training machine learning models on large, initially unlabelled datasets by repeatedly acquiring labels for batches of data points. However, many recent batch active learning methods are white-box approaches and are often limited to differentiable parametric models: they score unlabeled points using acquisition functions based on model embeddings or first- and second-order derivatives. In this paper, we propose black-box batch active learning for regression tasks as an extension of white-box approaches. Crucially, our method only relies on model predictions. This approach is compatible with a wide range of machine learning models, including regular and Bayesian deep learning models and non-differentiable models such as random forests. It is rooted in Bayesian principles and utilizes recent kernel-based approaches. This allows us to extend a wide range of existing state-of-the-art white-box batch active learning methods (BADGE, BAIT, LCMD) to black-box models. We demonstrate the effectiveness of our approach through extensive experimental evaluations on regression datasets, achieving surprisingly strong performance compared to white-box approaches for deep learning models.},
 author = {Andreas Kirsch},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/BlackHC/2302.08981},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4321392959},
 pdf = {https://openreview.net/pdf?id=fvEvDlKko6},
 review = {https://openreview.net/forum?id=fvEvDlKko6},
 title = {Black-Box Batch Active Learning for Regression},
 url = {https://openreview.net/forum?id=fvEvDlKko6},
 year = {2023}
}

@article{kirsch2023does,
 author = {Andreas Kirsch},
 badge = {Reproducibility},
 code = {https://github.com/blackhc/pytorch_datadiet https://github.com/blackhc/data_diet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification, Reproducibility Certification},
 pdf = {https://openreview.net/pdf?id=1dwXa9vmOI},
 review = {https://openreview.net/forum?id=1dwXa9vmOI},
 title = {Does {\textquoteleft}Deep Learning on a Data Diet{\textquoteright} reproduce? Overall yes, but GraNd at Initialization does not},
 url = {https://openreview.net/forum?id=1dwXa9vmOI},
 year = {2023}
}

@article{kirsch2023stochastic,
 abstract = {We examine a simple stochastic strategy for adapting well-known single-point acquisition functions to allow batch active learning. Unlike acquiring the top-K points from the pool set, score- or rank-based sampling takes into account that acquisition scores change as new data are acquired. This simple strategy for adapting standard single-sample acquisition strategies can even perform just as well as compute-intensive state-of-the-art batch acquisition functions, like BatchBALD or BADGE, while using orders of magnitude less compute. In addition to providing a practical option for machine learning practitioners, the surprising success of the proposed method in a wide range of experimental settings raises a difficult question for the field: when are these expensive batch acquisition methods pulling their weight?},
 author = {Andreas Kirsch and Sebastian Farquhar and Parmida Atighehchian and Andrew Jesson and Fr{\'e}d{\'e}ric Branchaud-Charron and Yarin Gal},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/baal-org/baal https://github.com/OATML/causal-bald https://github.com/BlackHC/active-bayesian-coresets https://github.com/BlackHC/active_learning_redux},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4286695170},
 pdf = {https://openreview.net/pdf?id=vcHwQyNBjW},
 review = {https://openreview.net/forum?id=vcHwQyNBjW},
 title = {Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning},
 url = {https://openreview.net/forum?id=vcHwQyNBjW},
 year = {2023}
}

@article{kiyasseh2023pcps,
 author = {Dani Kiyasseh and Tingting Zhu and David A. Clifton},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=X1pjWMCMB0},
 review = {https://openreview.net/forum?id=X1pjWMCMB0},
 title = {{PCP}s: Patient Cardiac Prototypes to Probe {AI}-based Medical Diagnoses, Distill Datasets, and Retrieve Patients},
 url = {https://openreview.net/forum?id=X1pjWMCMB0},
 year = {2023}
}

@article{klasson2023learn,
 abstract = {Replay methods are known to be successful at mitigating catastrophic forgetting in continual learning scenarios despite having limited access to historical data. However, storing historical data is cheap in many real-world settings, yet replaying all historical data is often prohibited due to processing time constraints. In such settings, we propose that continual learning systems should learn the time to learn and schedule which tasks to replay at different time steps. We first demonstrate the benefits of our proposal by using Monte Carlo tree search to find a proper replay schedule, and show that the found replay schedules can outperform fixed scheduling policies when combined with various replay methods in different continual learning settings. Additionally, we propose a framework for learning replay scheduling policies with reinforcement learning. We show that the learned policies can generalize better in new continual learning scenarios compared to equally replaying all seen tasks, without added computational cost. Our study reveals the importance of learning the time to learn in continual learning, which brings current research closer to real-world needs.},
 author = {Marcus Klasson and Hedvig Kjellstrom and Cheng Zhang},
 code = {https://github.com/marcusklasson/replay_scheduling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296544467},
 pdf = {https://openreview.net/pdf?id=Q4aAITDgdP},
 review = {https://openreview.net/forum?id=Q4aAITDgdP},
 title = {Learn the Time to Learn: Replay Scheduling in Continual Learning},
 url = {https://openreview.net/forum?id=Q4aAITDgdP},
 year = {2023}
}

@article{klepper2023relating,
 abstract = {Graph auto-encoders are widely used to construct graph representations in Euclidean vector spaces. However, it has already been pointed out empirically that linear models on many tasks can outperform graph auto-encoders. In our work, we prove that the solution space induced by graph auto-encoders is a subset of the solution space of a linear map. This demonstrates that linear embedding models have at least the representational power of graph auto-encoders based on graph convolutional networks. So why are we still using nonlinear graph auto-encoders? One reason could be that actively restricting the linear solution space might introduce an inductive bias that helps improve learning and generalization. While many researchers believe that the nonlinearity of the encoder is the critical ingredient towards this end, we instead identify the node features of the graph as a more powerful inductive bias. We give theoretical insights by introducing a corresponding bias in a linear model and analyzing the change in the solution space. Our experiments are aligned with other empirical work on this question and show that the linear encoder can outperform the nonlinear encoder when using feature information.},
 author = {Solveig Klepper and Ulrike von Luxburg},
 code = {https://github.com/tml-tuebingen/linear-gae},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308244652},
 pdf = {https://openreview.net/pdf?id=Y1eYplvxrE},
 review = {https://openreview.net/forum?id=Y1eYplvxrE},
 title = {Relating graph auto-encoders to linear models},
 url = {https://openreview.net/forum?id=Y1eYplvxrE},
 year = {2023}
}

@article{klindt2023controlling,
 author = {David A. Klindt},
 code = {https://github.com/david-klindt/tmlr_2023},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=JnsGy9uWtI},
 review = {https://openreview.net/forum?id=JnsGy9uWtI},
 title = {Controlling Neural Network Smoothness for Neural Algorithmic Reasoning},
 url = {https://openreview.net/forum?id=JnsGy9uWtI},
 year = {2023}
}

@article{knox2024models,
 abstract = {The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperforms the partial return preference model with finite training data in otherwise the same setting. Additionally, we find that our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research. We have open sourced our experimental code, the human preferences dataset we gathered, and our training and preference elicitation interfaces for gathering a such a dataset.},
 author = {W. Bradley Knox and Stephane Hatgis-Kessell and Serena Booth and Scott Niekum and Peter Stone and Alessandro G Allievi},
 code = {https://github.com/Stephanehk/Learning-OA-From-Prefs},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281963856},
 pdf = {https://openreview.net/pdf?id=hpKJkVoThY},
 review = {https://openreview.net/forum?id=hpKJkVoThY},
 title = {Models of human preference for learning reward functions},
 url = {https://openreview.net/forum?id=hpKJkVoThY},
 year = {2024}
}

@article{kocetkov2023the,
 abstract = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.},
 author = {Denis Kocetkov and Raymond Li and Loubna Ben allal and Jia LI and Chenghao Mou and Yacine Jernite and Margaret Mitchell and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro Von Werra and Harm de Vries},
 code = {https://github.com/bigcode-project},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310428868},
 pdf = {https://openreview.net/pdf?id=pxpbTdUEpD},
 review = {https://openreview.net/forum?id=pxpbTdUEpD},
 title = {The Stack: 3 TB of permissively licensed source code},
 url = {https://openreview.net/forum?id=pxpbTdUEpD},
 year = {2023}
}

@article{kojima2024a,
 abstract = {Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.},
 author = {Noriyuki Kojima and Hadar Averbuch-Elor and Yoav Artzi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386555936},
 pdf = {https://openreview.net/pdf?id=5G3PI1hEdw},
 review = {https://openreview.net/forum?id=5G3PI1hEdw},
 title = {A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models},
 url = {https://openreview.net/forum?id=5G3PI1hEdw},
 year = {2024}
}

@article{komiyama2022bridging,
 author = {Junpei Komiyama and Gustavo Malkomes and Bolong Cheng and Michael McCourt},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=XX8CEN815d},
 review = {https://openreview.net/forum?id=XX8CEN815d},
 title = {Bridging Offline and Online Experimentation: Constraint Active Search for Deployed Performance Optimization},
 url = {https://openreview.net/forum?id=XX8CEN815d},
 year = {2022}
}

@article{kompella2023event,
 abstract = {Experience replay (ER) is a crucial component of many deep reinforcement learning (RL) systems. However, uniform sampling from an ER buffer can lead to slow convergence and unstable asymptotic behaviors. This paper introduces Stratified Sampling from Event Tables (SSET), which partitions an ER buffer into Event Tables, each capturing important subsequences of optimal behavior. We prove a theoretical advantage over the traditional monolithic buffer approach and combine SSET with an existing prioritized sampling strategy to further improve learning speed and stability. Empirical results in challenging MiniGrid domains, benchmark RL environments, and a high-fidelity car racing simulator demonstrate the advantages and versatility of SSET over existing ER buffer sampling approaches.},
 author = {Varun Raj Kompella and Thomas Walsh and Samuel Barrett and Peter R. Wurman and Peter Stone},
 badge = {Event Tables for Efficient Experience Replay},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308022997},
 pdf = {https://openreview.net/pdf?id=XejzjAjKjv},
 review = {https://openreview.net/forum?id=XejzjAjKjv},
 title = {Event Tables for Efficient Experience Replay},
 url = {https://openreview.net/forum?id=XejzjAjKjv},
 year = {2023}
}

@article{kong2024improved,
 author = {Fang Kong and XiangCheng Zhang and Baoxiang Wang and Shuai Li},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=KcmWZSk53y},
 review = {https://openreview.net/forum?id=KcmWZSk53y},
 title = {Improved Regret Bounds for Linear Adversarial {MDP}s via Linear Optimization},
 url = {https://openreview.net/forum?id=KcmWZSk53y},
 year = {2024}
}

@article{kose2023fastfair,
 author = {Oyku Deniz Kose and Yanning Shen},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=nOk4XEB7Ke},
 review = {https://openreview.net/forum?id=nOk4XEB7Ke},
 title = {Fast\&Fair: Training Acceleration and Bias Mitigation for {GNN}s},
 url = {https://openreview.net/forum?id=nOk4XEB7Ke},
 year = {2023}
}

@article{koskela2023numerical,
 abstract = {Shuffle model of differential privacy is a novel distributed privacy model based on a combination of local privacy mechanisms and a secure shuffler. It has been shown that the additional randomisation provided by the shuffler improves privacy bounds compared to the purely local mechanisms. Accounting tight bounds, however, is complicated by the complexity brought by the shuffler. The recently proposed numerical techniques for evaluating $(\varepsilon,\delta)$-differential privacy guarantees have been shown to give tighter bounds than commonly used methods for compositions of various complex mechanisms. In this paper, we show how to obtain accurate bounds for adaptive compositions of general $\varepsilon$-LDP shufflers using the analysis by Feldman et al. (2021) and tight bounds for adaptive compositions of shufflers of $k$-randomised response mechanisms, using the analysis by Balle et al. (2019). We show how to speed up the evaluation of the resulting privacy loss distribution from $\mathcal{O}(n^2)$ to $\mathcal{O}(n)$, where $n$ is the number of users, without noticeable change in the resulting $\delta(\varepsilon)$-upper bounds. We also demonstrate looseness of the existing bounds and methods found in the literature, improving previous composition results significantly.},
 author = {Antti Koskela and Mikko A. Heikkil{\"a} and Antti Honkela},
 badge = {Featured},
 code = {https://github.com/DPBayes/numerical-shuffler-experiments},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4287145546},
 pdf = {https://openreview.net/pdf?id=11osftjEbF},
 review = {https://openreview.net/forum?id=11osftjEbF},
 title = {Tight Accounting in the Shuffle Model of Differential Privacy},
 url = {https://openreview.net/forum?id=11osftjEbF},
 year = {2023}
}

@article{kosma2023neural,
 author = {Chrysoula Kosma and Giannis Nikolentzos and George Panagopoulos and Jean-Marc Steyaert and Michalis Vazirgiannis},
 badge = {Featured},
 code = {https://github.com/sissykosm/GN-ODE-SIR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification, Expert Certification},
 pdf = {https://openreview.net/pdf?id=yrkJGne0vN},
 review = {https://openreview.net/forum?id=yrkJGne0vN},
 title = {Neural Ordinary Differential Equations for Modeling Epidemic Spreading},
 url = {https://openreview.net/forum?id=yrkJGne0vN},
 year = {2023}
}

@article{kossen2023active,
 abstract = {We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for state-of-the-art models. Applications of A2MT may be impactful in domains like medicine, robotics, or finance, where modalities differ in acquisition cost and informativeness.},
 author = {Jannik Kossen and C{\u{a}}t{\u{a}}lina Cangea and Eszter V{\'e}rtes and Andrew Jaegle and Viorica Patraucean and Ira Ktena and Nenad Tomasev and Danielle Belgrave},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308759651},
 pdf = {https://openreview.net/pdf?id=Gbu1bHQhEL},
 review = {https://openreview.net/forum?id=Gbu1bHQhEL},
 title = {Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task},
 url = {https://openreview.net/forum?id=Gbu1bHQhEL},
 year = {2023}
}

@article{kothapalli2023neural,
 abstract = {Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.},
 author = {Vignesh Kothapalli},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281638141},
 pdf = {https://openreview.net/pdf?id=QTXocpAP9p},
 review = {https://openreview.net/forum?id=QTXocpAP9p},
 title = {Neural Collapse: A Review on Modelling Principles and Generalization},
 url = {https://openreview.net/forum?id=QTXocpAP9p},
 year = {2023}
}

@article{kozachkov2023generalization,
 author = {Leo Kozachkov and Patrick Wensing and Jean-Jacques Slotine},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Sb6p5mcefw},
 review = {https://openreview.net/forum?id=Sb6p5mcefw},
 title = {Generalization as Dynamical Robustness--The Role of Riemannian Contraction in Supervised Learning},
 url = {https://openreview.net/forum?id=Sb6p5mcefw},
 year = {2023}
}

@article{krasowski2023provably,
 abstract = {Ensuring the safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL and most safe RL approaches do not guarantee safety. In recent years, several methods have been proposed to provide hard safety guarantees for RL, which is essential for applications where unsafe actions could have disastrous consequences. Nevertheless, there is no comprehensive comparison of these provably safe RL methods. Therefore, we introduce a categorization of existing provably safe RL methods, present the conceptual foundations for both continuous and discrete action spaces, and empirically benchmark existing methods. We categorize the methods based on how they adapt the action: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and a quadrotor stabilization task indicate that action replacement is the best-performing approach for these applications despite its comparatively simple realization. Furthermore, adding a reward penalty, every time the safety verification is engaged, improved training performance in our experiments. Finally, we provide practical guidance on selecting provably safe RL approaches depending on the safety specification, RL algorithm, and type of action space.},
 author = {Hanna Krasowski and Jakob Thumm and Marlon M{\"u}ller and Lukas Sch{\"a}fer and Xiao Wang and Matthias Althoff},
 badge = {Survey},
 code = {https://doi.org/10.24433/CO.9209121.v1},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4280525627},
 pdf = {https://openreview.net/pdf?id=mcN0ezbnzO},
 review = {https://openreview.net/forum?id=mcN0ezbnzO},
 title = {Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking},
 url = {https://openreview.net/forum?id=mcN0ezbnzO},
 year = {2023}
}

@article{kratsios2022do,
 author = {Anastasis Kratsios and Behnoosh Zamanlooy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=sNxNi54B8b},
 review = {https://openreview.net/forum?id=sNxNi54B8b},
 title = {Do Re{LU} Networks Have An Edge When Approximating Compactly-Supported Functions?},
 url = {https://openreview.net/forum?id=sNxNi54B8b},
 year = {2022}
}

@article{krishnamachari2022fourier,
 abstract = {Recent work has empirically shown that deep neural networks latch on to the Fourier statistics of training data and show increased sensitivity to Fourier-basis directions in the input. Understanding and modifying this Fourier-sensitivity of computer vision models may help improve their robustness. Hence, in this paper we study the frequency sensitivity characteristics of deep neural networks using a principled approach. We first propose a basis trick, proving that unitary transformations of the input-gradient of a function can be used to compute its gradient in the basis induced by the transformation. Using this result, we propose a general measure of any differentiable model's Fourier-sensitivity using the unitary Fourier-transform of its input-gradient. When applied to deep neural networks, we find that computer vision models are consistently sensitive to particular frequencies dependent on the dataset, training method and architecture. Based on this measure, we further propose a Fourier-regularization framework to modify the Fourier-sensitivities and frequency bias of models. Using our proposed regularizer-family, we demonstrate that deep neural networks obtain improved classification accuracy on robustness evaluations.},
 author = {Kiran Krishnamachari and See-Kiong Ng and Chuan-Sheng Foo},
 code = {https://github.com/kiranchari/Fourier-Sensitivity-Regularization},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318906579},
 pdf = {https://openreview.net/pdf?id=VmTYgjYloM},
 review = {https://openreview.net/forum?id=VmTYgjYloM},
 title = {Fourier Sensitivity and Regularization of Computer Vision Models},
 url = {https://openreview.net/forum?id=VmTYgjYloM},
 year = {2022}
}

@article{krishnamachari2023mitigating,
 author = {Kiran Krishnamachari and See-Kiong Ng and Chuan-Sheng Foo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=lu4oAq55iK},
 review = {https://openreview.net/forum?id=lu4oAq55iK},
 title = {Mitigating Real-World Distribution Shifts in the Fourier Domain},
 url = {https://openreview.net/forum?id=lu4oAq55iK},
 year = {2023}
}

@article{krishnan2022quarl,
 author = {Srivatsan Krishnan and Max Lam and Sharad Chitlangia and Zishen Wan and Gabriel Barth-maron and Aleksandra Faust and Vijay Janapa Reddi},
 code = {https://github.com/harvard-edge/QuaRL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=xwWsiFmUEs},
 review = {https://openreview.net/forum?id=xwWsiFmUEs},
 title = {Qua{RL}: Quantization for Fast and Environmentally Sustainable Reinforcement Learning},
 url = {https://openreview.net/forum?id=xwWsiFmUEs},
 year = {2022}
}

@article{kulkarni2024multitask,
 abstract = {In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes. Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness.},
 author = {Atharva Kulkarni and Lucio M. Dery and Amrith Setlur and Aditi Raghunathan and Ameet Talwalkar and Graham Neubig},
 code = {https://github.com/atharvajk98/MTL-group-robustness},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4389470931},
 pdf = {https://openreview.net/pdf?id=sPlhAIp6mk},
 review = {https://openreview.net/forum?id=sPlhAIp6mk},
 title = {Multitask Learning Can Improve Worst-Group Outcomes},
 url = {https://openreview.net/forum?id=sPlhAIp6mk},
 year = {2024}
}

@article{kumar2022decoding,
 author = {Neelesh Kumar and Guangzhi Tang and Raymond Yoo and Konstantinos P. Michmizos},
 code = {https://github.com/combra-lab/snn-eeg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ZPBJPGX3Bz},
 review = {https://openreview.net/forum?id=ZPBJPGX3Bz},
 title = {Decoding {EEG} With Spiking Neural Networks on Neuromorphic Hardware},
 url = {https://openreview.net/forum?id=ZPBJPGX3Bz},
 year = {2022}
}

@article{kumar2022do,
 abstract = {Perceptual distances between images, as measured in the space of pre-trained deep features, have outperformed prior low-level, pixel-based metrics on assessing perceptual similarity. While the capabilities of older and less accurate models such as AlexNet and VGG to capture perceptual similarity are well known, modern and more accurate models are less studied. In this paper, we present a large-scale empirical study to assess how well ImageNet classifiers perform on perceptual similarity. First, we observe a inverse correlation between ImageNet accuracy and Perceptual Scores of modern networks such as ResNets, EfficientNets, and Vision Transformers: that is better classifiers achieve worse Perceptual Scores. Then, we examine the ImageNet accuracy/Perceptual Score relationship on varying the depth, width, number of training steps, weight decay, label smoothing, and dropout. Higher accuracy improves Perceptual Score up to a certain point, but we uncover a Pareto frontier between accuracies and Perceptual Score in the mid-to-high accuracy regime. We explore this relationship further using a number of plausible hypotheses such as distortion invariance, spatial frequency sensitivity, and alternative perceptual functions. Interestingly we discover shallow ResNets and ResNets trained for less than 5 epochs only on ImageNet, whose emergent Perceptual Score matches the prior best networks trained directly on supervised human perceptual judgements. The checkpoints for the models in our study are available at https://console.cloud.google.com/storage/browser/gresearch/perceptual_similarity.},
 author = {Manoj Kumar and Neil Houlsby and Nal Kalchbrenner and Ekin Dogus Cubuk},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/tensorflow/tpu/pull/991},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4221157534},
 pdf = {https://openreview.net/pdf?id=qrGKGZZvH0},
 review = {https://openreview.net/forum?id=qrGKGZZvH0},
 title = {Do better ImageNet classifiers assess perceptual similarity better?},
 url = {https://openreview.net/forum?id=qrGKGZZvH0},
 year = {2022}
}

@article{kumar2023dual,
 abstract = {We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms), before and after the patch embedding layer in Vision Transformers. We demonstrate that Dual PatchNorm outperforms the result of exhaustive search for alternative LayerNorm placement strategies in the Transformer block itself. In our experiments, incorporating this trivial modification, often leads to improved accuracy over well-tuned Vision Transformers and never hurts.},
 author = {Manoj Kumar and Mostafa Dehghani and Neil Houlsby},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4319166702},
 pdf = {https://openreview.net/pdf?id=jgMqve6Qhw},
 review = {https://openreview.net/forum?id=jgMqve6Qhw},
 title = {Dual PatchNorm},
 url = {https://openreview.net/forum?id=jgMqve6Qhw},
 year = {2023}
}

@article{kumar2024introspective,
 abstract = {In reinforcement learning (RL), experience replay-based sampling techniques play a crucial role in promoting convergence by eliminating spurious correlations. However, widely used methods such as uniform experience replay (UER) and prioritized experience replay (PER) have been shown to have sub-optimal convergence and high seed sensitivity respectively. To address these issues, we propose a novel approach called IntrospectiveExperience Replay (IER) that selectively samples batches of data points prior to surprising events. Our method builds upon the theoretically sound reverse experience replay (RER) technique, which has been shown to reduce bias in the output of Q-learning-type algorithms with linear function approximation. However, this approach is not always practical or reliable when using neural function approximation. Through empirical evaluations, we demonstrate that IER with neural function approximation yields reliable and superior performance compared toUER, PER, and hindsight experience replay (HER) across most tasks.},
 author = {Ramnath Kumar and Dheeraj Mysore Nagaraj},
 code = {https://github.com/google-research/look-back-when-surprised/tree/main},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281636965},
 pdf = {https://openreview.net/pdf?id=vWTZO1RXZR},
 review = {https://openreview.net/forum?id=vWTZO1RXZR},
 title = {Introspective Experience Replay: Look Back When Surprised},
 url = {https://openreview.net/forum?id=vWTZO1RXZR},
 year = {2024}
}

@article{kuo2023mammut,
 author = {Weicheng Kuo and AJ Piergiovanni and Dahun Kim and xiyang luo and Benjamin Caine and Wei Li and Abhijit Ogale and Luowei Zhou and Andrew M. Dai and Zhifeng Chen and Claire Cui and Anelia Angelova},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=FqOG4osY7C},
 review = {https://openreview.net/forum?id=FqOG4osY7C},
 title = {Ma{MMUT}: A Simple Architecture for Joint Learning for MultiModal Tasks},
 url = {https://openreview.net/forum?id=FqOG4osY7C},
 year = {2023}
}

@article{kuric2023reusable,
 abstract = {Hierarchical methods in reinforcement learning have the potential to reduce the amount of decisions that the agent needs to perform when learning new tasks. However, finding reusable useful temporal abstractions that facilitate fast learning remains a challenging problem. Recently, several deep learning approaches were proposed to learn such temporal abstractions in the form of options in an end-to-end manner. In this work, we point out several shortcomings of these methods and discuss their potential negative consequences. Subsequently, we formulate the desiderata for reusable options and use these to frame the problem of learning options as a gradient-based meta-learning problem. This allows us to formulate an objective that explicitly incentivizes options which allow a higher-level decision maker to adjust in few steps to different tasks. Experimentally, we show that our method is able to learn transferable components which accelerate learning and performs better than existing prior methods developed for this setting. Additionally, we perform ablations to quantify the impact of using gradient-based meta-learning as well as other proposed changes.},
 author = {David Kuric and Herke van Hoof},
 code = {https://github.com/Kuroo/FAMP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4312122264},
 pdf = {https://openreview.net/pdf?id=qdDmxzGuzu},
 review = {https://openreview.net/forum?id=qdDmxzGuzu},
 title = {Reusable Options through Gradient-based Meta Learning},
 url = {https://openreview.net/forum?id=qdDmxzGuzu},
 year = {2023}
}

@article{kvalheim2024why,
 author = {Matthew Kvalheim and Eduardo Sontag},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=uGVFtjvI3v},
 review = {https://openreview.net/forum?id=uGVFtjvI3v},
 title = {Why should autoencoders work?},
 url = {https://openreview.net/forum?id=uGVFtjvI3v},
 year = {2024}
}

@article{kwon2022competition,
 abstract = {As machine learning (ML) is deployed by many competing service providers, the underlying ML predictors also compete against each other, and it is increasingly important to understand the impacts and biases from such competition. In this paper, we study what happens when the competing predictors can acquire additional labeled data to improve their prediction quality. We introduce a new environment that allows ML predictors to use active learning algorithms to purchase labeled data within their budgets while competing against each other to attract users. Our environment models a critical aspect of data acquisition in competing systems which has not been well-studied before. We found that the overall performance of an ML predictor improves when predictors can purchase additional labeled data. Surprisingly, however, the quality that users experience -- i.e. the accuracy of the predictor selected by each user -- can decrease even as the individual predictors get better. We show that this phenomenon naturally arises due to a trade-off whereby competition pushes each predictor to specialize in a subset of the population while data purchase has the effect of making predictors more uniform. We support our findings with both experiments and theories.},
 author = {Yongchan Kwon and Tony A Ginart and James Zou},
 code = {https://github.com/ykwon0407/data_purchase_in_comp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221143896},
 pdf = {https://openreview.net/pdf?id=63sJsCmq6Q},
 review = {https://openreview.net/forum?id=63sJsCmq6Q},
 title = {Competition over data: how does data purchase affect users?},
 url = {https://openreview.net/forum?id=63sJsCmq6Q},
 year = {2022}
}

@article{l,
 code = {https://github.com/crispchris/Active-Learning-of-Ordinal-Embeddings},
 pdf = {https://openreview.net/pdf?id=oq3tx5kinu},
 review = {https://openreview.net/forum?id=oq3tx5kinu}
}

@article{l,
 code = {https://github.com/loeweX/ComplexAutoEncoder},
 pdf = {https://openreview.net/pdf?id=1PfcmFTXoa},
 review = {https://openreview.net/forum?id=1PfcmFTXoa}
}

@article{lahlou2023deup,
 abstract = {Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias and can be applied in non-stationary learning environments arising in active learning or reinforcement learning. In addition to demonstrating these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.},
 author = {Salem Lahlou and Moksh Jain and Hadi Nekoei and Victor I Butoi and Paul Bertin and Jarrid Rector-Brooks and Maksym Korablyov and Yoshua Bengio},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/MJ10/DEUP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W3132395196},
 pdf = {https://openreview.net/pdf?id=eGLdVRvvfQ},
 review = {https://openreview.net/forum?id=eGLdVRvvfQ},
 title = {DEUP: Direct Epistemic Uncertainty Prediction},
 url = {https://openreview.net/forum?id=eGLdVRvvfQ},
 year = {2023}
}

@article{lalam2023ecg,
 author = {Sravan Kumar Lalam and Hari Krishna Kunderu and Shayan Ghosh and Harish Kumar A and Samir Awasthi and Ashim Prasad and Francisco Lopez-Jimenez and Zachi I Attia and Samuel Asirvatham and Paul Friedman and Rakesh Barve and Melwin Babu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=UxmvCwuTMG},
 review = {https://openreview.net/forum?id=UxmvCwuTMG},
 title = {{ECG} Representation Learning with Multi-Modal {EHR} Data},
 url = {https://openreview.net/forum?id=UxmvCwuTMG},
 year = {2023}
}

@article{lalande2023numerical,
 abstract = {Numerical data imputation algorithms replace missing values by estimates to leverage incomplete data sets. Current imputation methods seek to minimize the error between the unobserved ground truth and the imputed values. But this strategy can create artifacts leading to poor imputation in the presence of multimodal or complex distributions. To tackle this problem, we introduce the $k$NN$\times$KDE algorithm: a data imputation method combining nearest neighbor estimation ($k$NN) and density estimation with Gaussian kernels (KDE). We compare our method with previous data imputation methods using artificial and real-world data with different data missing scenarios and various data missing rates, and show that our method can cope with complex original data structure, yields lower data imputation errors, and provides probabilistic estimates with higher likelihood than current methods. We release the code in open-source for the community: https://github.com/DeltaFloflo/knnxkde},
 author = {Florian Lalande and Kenji Doya},
 badge = {Reproducibility},
 code = {https://github.com/DeltaFloflo/knnxkde},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4382766024},
 pdf = {https://openreview.net/pdf?id=KqR3rgooXb},
 review = {https://openreview.net/forum?id=KqR3rgooXb},
 title = {Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach},
 url = {https://openreview.net/forum?id=KqR3rgooXb},
 year = {2023}
}

@article{lalanne2023about,
 abstract = {We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber \& Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk, and under regular differential privacy, and we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman \& Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure differential privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation. With zero concentrated differential privacy, there is no need for relaxation, and we prove that the estimation is optimal.},
 author = {Cl{\'e}ment Lalanne and Aur{\'e}lien Garivier and R{\'e}mi Gribonval},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386943910},
 pdf = {https://openreview.net/pdf?id=uq29MIWvIV},
 review = {https://openreview.net/forum?id=uq29MIWvIV},
 title = {About the Cost of Central Privacy in Density Estimation},
 url = {https://openreview.net/forum?id=uq29MIWvIV},
 year = {2023}
}

@article{lalanne2023on,
 author = {Cl{\'e}ment Lalanne and Aur{\'e}lien Garivier and R{\'e}mi Gribonval},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4302028795},
 pdf = {https://openreview.net/pdf?id=OarsigVib0},
 review = {https://openreview.net/forum?id=OarsigVib0},
 title = {On the Statistical Complexity of Estimation and Testing under Privacy Constraints},
 url = {https://openreview.net/forum?id=OarsigVib0},
 year = {2023}
}

@article{lamb2023guaranteed,
 abstract = {In many sequential decision-making tasks, the agent is not able to model the full complexity of the world, which consists of multitudes of relevant and irrelevant information. For example, a person walking along a city street who tries to model all aspects of the world would quickly be overwhelmed by a multitude of shops, cars, and people moving in and out of view, each following their own complex and inscrutable dynamics. Is it possible to turn the agent's firehose of sensory information into a minimal latent state that is both necessary and sufficient for an agent to successfully act in the world? We formulate this question concretely, and propose the Agent Control-Endogenous State Discovery algorithm (AC-State), which has theoretical guarantees and is practically demonstrated to discover the minimal control-endogenous latent state which contains all of the information necessary for controlling the agent, while fully discarding all irrelevant information. This algorithm consists of a multi-step inverse model (predicting actions from distant observations) with an information bottleneck. AC-State enables localization, exploration, and navigation without reward or demonstrations. We demonstrate the discovery of the control-endogenous latent state in three domains: localizing a robot arm with distractions (e.g., changing lighting conditions and background), exploring a maze alongside other agents, and navigating in the Matterport house simulator.},
 author = {Alex Lamb and Riashat Islam and Yonathan Efroni and Aniket Rajiv Didolkar and Dipendra Misra and Dylan J Foster and Lekan P Molu and Rajan Chari and Akshay Krishnamurthy and John Langford},
 code = {https://controllable-latent-state.github.io/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286233210},
 pdf = {https://openreview.net/pdf?id=TNocbXm5MZ},
 review = {https://openreview.net/forum?id=TNocbXm5MZ},
 title = {Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models},
 url = {https://openreview.net/forum?id=TNocbXm5MZ},
 year = {2023}
}

@article{lambrechts2022recurrent,
 abstract = {Reinforcement learning aims to learn optimal policies from interaction with environments whose dynamics are unknown. Many methods rely on the approximation of a value function to derive near-optimal policies. In partially observable environments, these functions depend on the complete sequence of observations and past actions, called the history. In this work, we show empirically that recurrent neural networks trained to approximate such value functions internally filter the posterior probability distribution of the current state given the history, called the belief. More precisely, we show that, as a recurrent neural network learns the Q-function, its hidden states become more and more correlated with the beliefs of state variables that are relevant to optimal control. This correlation is measured through their mutual information. In addition, we show that the expected return of an agent increases with the ability of its recurrent architecture to reach a high mutual information between its hidden states and the beliefs. Finally, we show that the mutual information between the hidden states and the beliefs of variables that are irrelevant for optimal control decreases through the learning process. In summary, this work shows that in its hidden states, a recurrent neural network approximating the Q-function of a partially observable environment reproduces a sufficient statistic from the history that is correlated to the relevant part of the belief for taking optimal actions.},
 author = {Gaspard Lambrechts and Adrien Bolland and Damien Ernst},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4302068141},
 pdf = {https://openreview.net/pdf?id=dkHfV3wB2l},
 review = {https://openreview.net/forum?id=dkHfV3wB2l},
 title = {Recurrent networks, hidden states and beliefs in partially observable environments},
 url = {https://openreview.net/forum?id=dkHfV3wB2l},
 year = {2022}
}

@article{lan2023memoryefficient,
 abstract = {Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience replay buffers.},
 author = {Qingfeng Lan and Yangchen Pan and Jun Luo and A. Rupam Mahmood},
 badge = {Event: CoLLAs 2023},
 code = {https://github.com/qlan3/MeDQN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281493149},
 pdf = {https://openreview.net/pdf?id=zSDCvlaVBn},
 review = {https://openreview.net/forum?id=zSDCvlaVBn},
 title = {Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation},
 url = {https://openreview.net/forum?id=zSDCvlaVBn},
 year = {2023}
}

@article{lanctot2023populationbased,
 abstract = {Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.},
 author = {Marc Lanctot and John Schultz and Neil Burch and Max Olan Smith and Daniel Hennes and Thomas Anthony and Julien Perolat},
 code = {https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/examples/roshambo_population_example.py},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323570560},
 pdf = {https://openreview.net/pdf?id=gQnJ7ODIAx},
 review = {https://openreview.net/forum?id=gQnJ7ODIAx},
 title = {Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning},
 url = {https://openreview.net/forum?id=gQnJ7ODIAx},
 year = {2023}
}

@article{lang2024a,
 abstract = {Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.},
 author = {Hao Lang and Yinhe Zheng and Yixuan Li and Jian SUN and Fei Huang and Yongbin Li},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4376865102},
 pdf = {https://openreview.net/pdf?id=nYjSkOy8ij},
 review = {https://openreview.net/forum?id=nYjSkOy8ij},
 title = {A Survey on Out-of-Distribution Detection in NLP},
 url = {https://openreview.net/forum?id=nYjSkOy8ij},
 year = {2024}
}

@article{lange2022clustering,
 abstract = {It has been hypothesized that some form of "modular" structure in artificial neural networks should be useful for learning, compositionality, and generalization. However, defining and quantifying modularity remains an open problem. We cast the problem of detecting functional modules into the problem of detecting clusters of similar-functioning units. This begs the question of what makes two units functionally similar. For this, we consider two broad families of methods: those that define similarity based on how units respond to structured variations in inputs ("upstream"), and those based on how variations in hidden unit activations affect outputs ("downstream"). We conduct an empirical study quantifying modularity of hidden layer representations of simple feedforward, fully connected networks, across a range of hyperparameters. For each model, we quantify pairwise associations between hidden units in each layer using a variety of both upstream and downstream measures, then cluster them by maximizing their "modularity score" using established tools from network science. We find two surprising results: first, dropout dramatically increased modularity, while other forms of weight regularization had more modest effects. Second, although we observe that there is usually good agreement about clusters within both upstream methods and downstream methods, there is little agreement about the cluster assignments across these two families of methods. This has important implications for representation-learning, as it suggests that finding modular representations that reflect structure in inputs (e.g. disentanglement) may be a distinct goal from learning modular representations that reflect structure in outputs (e.g. compositionality).},
 author = {Richard D Lange and David Rolnick and Konrad Kording},
 code = {https://github.com/KordingLab/clustering-units-upstream-downstream},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4225552216},
 pdf = {https://openreview.net/pdf?id=Euf7KofunK},
 review = {https://openreview.net/forum?id=Euf7KofunK},
 title = {Clustering units in neural networks: upstream vs downstream information},
 url = {https://openreview.net/forum?id=Euf7KofunK},
 year = {2022}
}

@article{langley2022structured,
 abstract = {Variational autoencoders (VAEs) are a popular class of deep generative models with many variants and a wide range of applications. Improvements upon the standard VAE mostly focus on the modelling of the posterior distribution over the latent space and the properties of the neural network decoder. In contrast, improving the model for the observational distribution is rarely considered and typically defaults to a pixel-wise independent categorical or normal distribution. In image synthesis, sampling from such distributions produces spatially-incoherent results with uncorrelated pixel noise, resulting in only the sample mean being somewhat useful as an output prediction. In this paper, we aim to stay true to VAE theory by improving the samples from the observational distribution. We propose SOS-VAE, an alternative model for the observation space, encoding spatial dependencies via a low-rank parameterisation. We demonstrate that this new observational distribution has the ability to capture relevant covariance between pixels, resulting in spatially-coherent samples. In contrast to pixel-wise independent distributions, our samples seem to contain semantically-meaningful variations from the mean allowing the prediction of multiple plausible outputs with a single forward pass.},
 author = {James Langley and Miguel Monteiro and Charles Jones and Nick Pawlowski and Ben Glocker},
 code = {https://github.com/biomedia-mira/sos-vae},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281742342},
 pdf = {https://openreview.net/pdf?id=cxp7n9q5c4},
 review = {https://openreview.net/forum?id=cxp7n9q5c4},
 title = {Structured Uncertainty in the Observation Space of Variational Autoencoders},
 url = {https://openreview.net/forum?id=cxp7n9q5c4},
 year = {2022}
}

@article{lebovitz2023efficient,
 author = {Luzian Lebovitz and Lukas Cavigelli and Michele Magno and Lorenz K Muller},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=obB415rg8q},
 review = {https://openreview.net/forum?id=obB415rg8q},
 title = {Efficient Inference With Model Cascades},
 url = {https://openreview.net/forum?id=obB415rg8q},
 year = {2023}
}

@article{lee2023evaluating,
 abstract = {Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurassic-1), we find that better non-interactive performance does not always translate to better human-LM interaction. In particular, we highlight three cases where the results from non-interactive and interactive metrics diverge and underscore the importance of human-LM interaction for LM evaluation.},
 author = {Mina Lee and Megha Srivastava and Amelia Hardy and John Thickstun and Esin Durmus and Ashwin Paranjape and Ines Gerard-Ursin and Xiang Lisa Li and Faisal Ladhak and Frieda Rong and Rose E Wang and Minae Kwon and Joon Sung Park and Hancheng Cao and Tony Lee and Rishi Bommasani and Michael S. Bernstein and Percy Liang},
 code = {https://github.com/stanford-crfm/halie},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4312089323},
 pdf = {https://openreview.net/pdf?id=hjDYJUn9l1},
 review = {https://openreview.net/forum?id=hjDYJUn9l1},
 title = {Evaluating Human-Language Model Interaction},
 url = {https://openreview.net/forum?id=hjDYJUn9l1},
 year = {2023}
}

@article{lee2023towards,
 abstract = {Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs improve the OOD generalization on a variety of inference tasks in the direction of diverse structural features.},
 author = {HyunGeun Lee and Kijung Yoon},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297833820},
 pdf = {https://openreview.net/pdf?id=EYjfLeJL4l},
 review = {https://openreview.net/forum?id=EYjfLeJL4l},
 title = {Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks},
 url = {https://openreview.net/forum?id=EYjfLeJL4l},
 year = {2023}
}

@article{lee2023training,
 abstract = {When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment -- a mapping from all tensors (arising in training) to precision levels (high or low) -- that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classification tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides > 2x memory reduction over a baseline precision assignment while preserving training accuracy, and gives further reductions by trading off accuracy. Compared to other baselines which sometimes cause training to diverge, our method provides similar or better memory reduction while avoiding divergence.},
 author = {Wonyeol Lee and Rahul Sharma and Alex Aiken},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318906182},
 pdf = {https://openreview.net/pdf?id=ZoXi7n54OB},
 review = {https://openreview.net/forum?id=ZoXi7n54OB},
 title = {Training with Mixed-Precision Floating-Point Assignments},
 url = {https://openreview.net/forum?id=ZoXi7n54OB},
 year = {2023}
}

@article{lee2024estimating,
 author = {Jonathan Lee and Weihao Kong and Aldo Pacchiano and Vidya Muthukumar and Emma Brunskill},
 code = {https://drive.google.com/drive/folders/18k6rQdtjahBS1sHLHRVomwbFZddIaXak?usp=drive_link},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=RUNiIDU8P7},
 review = {https://openreview.net/forum?id=RUNiIDU8P7},
 title = {Estimating Optimal Policy Value in Linear Contextual Bandits Beyond Gaussianity},
 url = {https://openreview.net/forum?id=RUNiIDU8P7},
 year = {2024}
}

@article{lei2023variational,
 abstract = {Latent world models allow agents to reason about complex environments with high-dimensional observations. However, adapting to new environments and effectively leveraging previous knowledge remain significant challenges. We present variational causal dynamics (VCD), a structured world model that exploits the invariance of causal mechanisms across environments to achieve fast and modular adaptation. By causally factorising a transition model, VCD is able to identify reusable components across different environments. This is achieved by combining causal discovery and variational inference to learn a latent representation and transition model jointly in an unsupervised manner. Specifically, we optimise the evidence lower bound jointly over a representation model and a transition model structured as a causal graphical model. In evaluations on simulated environments with state and image observations, we show that VCD is able to successfully identify causal variables, and to discover consistent causal structures across different environments. Moreover, given a small number of observations in a previously unseen, intervened environment, VCD is able to identify the sparse changes in the dynamics and to adapt efficiently. In doing so, VCD significantly extends the capabilities of the current state-of-the-art in latent world models while also comparing favourably in terms of prediction accuracy.},
 author = {Anson Lei and Bernhard Sch{\"o}lkopf and Ingmar Posner},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283379183},
 pdf = {https://openreview.net/pdf?id=V9tQKYYNK1},
 review = {https://openreview.net/forum?id=V9tQKYYNK1},
 title = {Variational Causal Dynamics: Discovering Modular World Models from Interventions},
 url = {https://openreview.net/forum?id=V9tQKYYNK1},
 year = {2023}
}

@article{lei2023worstcase,
 author = {Jingshi Lei and Da Li and Chengming Xu and Liming Fang and Timothy Hospedales and Yanwei Fu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=czev0exHXT},
 review = {https://openreview.net/forum?id=czev0exHXT},
 title = {Worst-case Feature Risk Minimization for Data-Efficient Learning},
 url = {https://openreview.net/forum?id=czev0exHXT},
 year = {2023}
}

@article{leino2022degradation,
 author = {Klas Leino and Chi Zhang and Ravi Mangal and Matt Fredrikson and Bryan Parno and Corina Pasareanu},
 code = {https://github.com/ravimangal/degradation-attacks},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=P0XO5ZE98j},
 review = {https://openreview.net/forum?id=P0XO5ZE98j},
 title = {Degradation Attacks on Certifiably Robust Neural Networks},
 url = {https://openreview.net/forum?id=P0XO5ZE98j},
 year = {2022}
}

@article{leluc2023asymptotic,
 abstract = {In this paper, we investigate a general class of stochastic gradient descent (SGD) algorithms, called Conditioned SGD, based on a preconditioning of the gradient direction. Using a discrete-time approach with martingale tools, we establish under mild assumptions the weak convergence of the rescaled sequence of iterates for a broad class of conditioning matrices including stochastic first-order and second-order methods. Almost sure convergence results, which may be of independent interest, are also presented. Interestingly, the asymptotic normality result consists in a stochastic equicontinuity property so when the conditioning matrix is an estimate of the inverse Hessian, the algorithm is asymptotically optimal.},
 author = {R{\'e}mi Leluc and Fran{\c{c}}ois Portier},
 code = {https://github.com/RemiLELUC/ConditionedSGD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307072769},
 pdf = {https://openreview.net/pdf?id=U4XgzRjfF1},
 review = {https://openreview.net/forum?id=U4XgzRjfF1},
 title = {Asymptotic Analysis of Conditioned Stochastic Gradient Descent},
 url = {https://openreview.net/forum?id=U4XgzRjfF1},
 year = {2023}
}

@article{lesmana2022reinventing,
 author = {Nixie S Lesmana and Huangyuan Su and Chi Seng Pun},
 code = {https://github.com/Hither1/Time-Inconsistency-RL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=bN2vWLTh0P},
 review = {https://openreview.net/forum?id=bN2vWLTh0P},
 title = {Reinventing Policy Iteration under Time Inconsistency},
 url = {https://openreview.net/forum?id=bN2vWLTh0P},
 year = {2022}
}

@article{lester2022using,
 abstract = {Testing whether data breaks symmetries of interest can be important to many fields. This paper describes a simple way that machine learning algorithms (whose outputs have been appropriately symmetrised) can be used to detect symmetry breaking. The original motivation for the paper was an important question in Particle Physics: "Is parity violated at the LHC in some way that no-one has anticipated?" and so we illustrate the main idea with an example strongly related to that question. However, in order that the key ideas be accessible to readers who are not particle physicists but who are interesting in symmetry breaking, we choose to illustrate the method/approach with a 'toy' example which places a simple discrete source of symmetry breaking (the handedness of human handwriting) within a idealised particle-physics-like context. Readers interested in seeing extensions to continuous symmetries, non-ideal environments or more realistic particle-physics contexts are provided with links to separate papers which delve into such details.},
 author = {Christopher Gorham Lester},
 code = {https://github.com/kesterlester/ColabForParityPaper3},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306809379},
 pdf = {https://openreview.net/pdf?id=QFJ3gtbwHR},
 review = {https://openreview.net/forum?id=QFJ3gtbwHR},
 title = {Using unsupervised learning to detect broken symmetries, with relevance to searches for parity violation in nature. (Previously: "Stressed GANs snag desserts")},
 url = {https://openreview.net/forum?id=QFJ3gtbwHR},
 year = {2022}
}

@article{levi2022domain,
 abstract = {The phenomenon of adversarial examples illustrates one of the most basic vulnerabilities of deep neural networks. Among the variety of techniques introduced to surmount this inherent weakness, adversarial training has emerged as the most effective strategy for learning robust models. Typically, this is achieved by balancing robust and natural objectives. In this work, we aim to further optimize the trade-off between robust and standard accuracy by enforcing a domain-invariant feature representation. We present a new adversarial training method, Domain Invariant Adversarial Learning (DIAL), which learns a feature representation that is both robust and domain invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on the natural domain and its corresponding adversarial domain. In the case where the source domain consists of natural examples and the target domain is the adversarially perturbed examples, our method learns a feature representation constrained not to discriminate between the natural and adversarial examples, and can therefore achieve a more robust representation. DIAL is a generic and modular technique that can be easily incorporated into any adversarial training method. Our experiments indicate that incorporating DIAL in the adversarial training process improves both robustness and standard accuracy.},
 author = {Matan Levi and Idan Attias and Aryeh Kontorovich},
 code = {https://github.com/matanle51/DIAL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287247434},
 pdf = {https://openreview.net/pdf?id=U8uJAUMzj9},
 review = {https://openreview.net/forum?id=U8uJAUMzj9},
 title = {Domain Invariant Adversarial Learning},
 url = {https://openreview.net/forum?id=U8uJAUMzj9},
 year = {2022}
}

@article{li2023a,
 abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
 author = {Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4315588599},
 pdf = {https://openreview.net/pdf?id=r30yuDPvf2},
 review = {https://openreview.net/forum?id=r30yuDPvf2},
 title = {A Survey on Transformers in Reinforcement Learning},
 url = {https://openreview.net/forum?id=r30yuDPvf2},
 year = {2023}
}

@article{li2023can,
 abstract = {With the rapid development of deep learning, the sizes of neural networks become larger and larger so that the training and inference often overwhelm the hardware resources. Given the fact that neural networks are often over-parameterized, one effective way to reduce such computational overhead is neural network pruning, by removing redundant parameters from trained neural networks. It has been recently observed that pruning can not only reduce computational overhead but also can improve empirical robustness of deep neural networks (NNs), potentially owing to removing spurious correlations while preserving the predictive accuracies. This paper for the first time demonstrates that pruning can generally improve certified robustness for ReLU-based NNs under the complete verification setting. Using the popular Branch-and-Bound (BaB) framework, we find that pruning can enhance the estimated bound tightness of certified robustness verification, by alleviating linear relaxation and sub-domain split problems. We empirically verify our findings with off-the-shelf pruning methods and further present a new stability-based pruning method tailored for reducing neuron instability, that outperforms existing pruning methods in enhancing certified robustness. Our experiments show that by appropriately pruning an NN, its certified accuracy can be boosted up to 8.2% under standard training, and up to 24.5% under adversarial training on the CIFAR10 dataset. We additionally observe the existence of certified lottery tickets that can match both standard and certified robust accuracies of the original dense models across different datasets. Our findings offer a new angle to study the intriguing interaction between sparsity and robustness, i.e. interpreting the interaction of sparsity and certified robustness via neuron stability. Codes are available at: https://github.com/VITA-Group/CertifiedPruning.},
 author = {Zhangheng LI and Tianlong Chen and Linyi Li and Bo Li and Zhangyang Wang},
 code = {https://github.com/VITA-Group/CertifiedPruning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283027161},
 pdf = {https://openreview.net/pdf?id=6IFi2soduD},
 review = {https://openreview.net/forum?id=6IFi2soduD},
 title = {Can pruning improve certified robustness of neural networks?},
 url = {https://openreview.net/forum?id=6IFi2soduD},
 year = {2023}
}

@article{li2023dreamedit,
 abstract = {Subject-driven image generation aims at generating images containing customized subjects, which has recently drawn enormous attention from the research community. However, the previous works cannot precisely control the background and position of the target subject. In this work, we aspire to fill the void and propose two novel subject-driven sub-tasks, i.e., Subject Replacement and Subject Addition. The new tasks are challenging in multiple aspects: replacing a subject with a customized one can change its shape, texture, and color, while adding a target subject to a designated position in a provided scene necessitates a context-aware posture. To conquer these two novel tasks, we first manually curate a new dataset DreamEditBench containing 22 different types of subjects, and 440 source images with different difficulty levels. We plan to host DreamEditBench as a platform and hire trained evaluators for standard human evaluation. We also devise an innovative method DreamEditor to resolve these tasks by performing iterative generation, which enables a smooth adaptation to the customized subject. In this project, we conduct automatic and human evaluations to understand the performance of DreamEditor and baselines on DreamEditBench. For Subject Replacement, we found that the existing models are sensitive to the shape and color of the original subject. The model failure rate will dramatically increase when the source and target subjects are highly different. For Subject Addition, we found that the existing models cannot easily blend the customized subjects into the background smoothly, leading to noticeable artifacts in the generated image. We hope DreamEditBench can become a standard platform to enable future investigations toward building more controllable subject-driven image editing. Our project homepage is https://dreameditbenchteam.github.io/.},
 author = {Tianle Li and Max Ku and Cong Wei and Wenhu Chen},
 code = {https://dreameditbenchteam.github.io/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4381826954},
 pdf = {https://openreview.net/pdf?id=P9haooN9v2},
 review = {https://openreview.net/forum?id=P9haooN9v2},
 title = {DreamEdit: Subject-driven Image Editing},
 url = {https://openreview.net/forum?id=P9haooN9v2},
 year = {2023}
}

@article{li2023graphpnas,
 author = {Muchen Li and Jeffrey Yunfan Liu and Leonid Sigal and Renjie Liao},
 code = {https://github.com/DSL-Lab/GraphPNAS},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ok18jj7cam},
 review = {https://openreview.net/forum?id=ok18jj7cam},
 title = {Graph{PNAS}: Learning Probabilistic Graph Generators for Neural Architecture Search},
 url = {https://openreview.net/forum?id=ok18jj7cam},
 year = {2023}
}

@article{li2023identification,
 abstract = {Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection do not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances. Then, we approximate the precomputed performances with a linear regression model that can also predict the multitask performance of unseen task subsets. We show theoretically and empirically that fitting this model only requires sampling linearly many subsets in the number of source tasks. The fitted model provides a relevance score between each source and target task. We use the relevance scores to perform subset selection for multitask learning by thresholding. Through extensive experiments, we show that our approach predicts negative transfers from multiple source tasks to target tasks much more accurately than existing task affinity measures. Additionally, we demonstrate that for several weak supervision datasets, our approach consistently improves upon existing optimization methods for multitask learning.},
 author = {Dongyue Li and Huy Nguyen and Hongyang Ryan Zhang},
 badge = {Featured},
 code = {https://github.com/NEU-StatsML-Research/Task-Modeling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4361200537},
 pdf = {https://openreview.net/pdf?id=KgfFAI9f3E},
 review = {https://openreview.net/forum?id=KgfFAI9f3E},
 title = {Identification of Negative Transfers in Multitask Learning Using Surrogate Models},
 url = {https://openreview.net/forum?id=KgfFAI9f3E},
 year = {2023}
}

@article{li2023jiangjun,
 abstract = {This paper presents an empirical exploration of non-transitivity in perfect-information games, specifically focusing on Xiangqi, a traditional Chinese board game comparable in game-tree complexity to chess and shogi. By analyzing over 10,000 records of human Xiangqi play, we highlight the existence of both transitive and non-transitive elements within the game's strategic structure. To address non-transitivity, we introduce the JiangJun algorithm, an innovative combination of Monte-Carlo Tree Search (MCTS) and Policy Space Response Oracles (PSRO) designed to approximate a Nash equilibrium. We evaluate the algorithm empirically using a WeChat mini program and achieve a Master level with a 99.41\% win rate against human players. The algorithm's effectiveness in overcoming non-transitivity is confirmed by a plethora of metrics, such as relative population performance and visualization results. Our project site is available at \url{https://sites.google.com/view/jiangjun-site/}.},
 author = {Yang Li and Kun Xiong and Yingping Zhang and Jiangcheng Zhu and Stephen Marcus McAleer and Wei Pan and Jun Wang and Zonghong Dai and Yaodong Yang},
 code = {https://github.com/liyang619/JiangJun},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385750086},
 pdf = {https://openreview.net/pdf?id=MMsyqXIJuk},
 review = {https://openreview.net/forum?id=MMsyqXIJuk},
 title = {JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games},
 url = {https://openreview.net/forum?id=MMsyqXIJuk},
 year = {2023}
}

@article{li2023optimumstatistical,
 abstract = {In this paper, we make the key delineation on the roles of resolution and statistical uncertainty in hierarchical bandits-based black-box optimization algorithms, guiding a more general analysis and a more efficient algorithm design. We introduce the \textit{optimum-statistical collaboration}, an algorithm framework of managing the interaction between optimization error flux and statistical error flux evolving in the optimization process. We provide a general analysis of this framework without specifying the forms of statistical error and uncertainty quantifier. Our framework and its analysis, due to their generality, can be applied to a large family of functions and partitions that satisfy different local smoothness assumptions and have different numbers of local optimums, which is much richer than the class of functions studied in prior works. Our framework also inspires us to propose a better measure of the statistical uncertainty and consequently a variance-adaptive algorithm \texttt{VHCT}. In theory, we prove the algorithm enjoys rate-optimal regret bounds under different local smoothness assumptions; in experiments, we show the algorithm outperforms prior efforts in different settings.},
 author = {Wenjie Li and Chi-Hua Wang and Guang Cheng and Qifan Song},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3205037160},
 pdf = {https://openreview.net/pdf?id=ClIcmwdlxn},
 review = {https://openreview.net/forum?id=ClIcmwdlxn},
 title = {Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization},
 url = {https://openreview.net/forum?id=ClIcmwdlxn},
 year = {2023}
}

@article{li2023reclip,
 abstract = {We present RECLIP (Resource-efficient CLIP), a simple method that minimizes computational resource footprint for CLIP (Contrastive Language Image Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we leverage small images to learn from large-scale language supervision efficiently, and finetune the model with high-resolution data in the end. Since the complexity of the vision transformer heavily depends on input image size, our approach significantly reduces the training resource requirements both in theory and in practice. Using the same batch size and training epoch, RECLIP achieves highly competitive zero-shot classification and image-text retrieval accuracy with 6 to 8x less computational resources and 7 to 9x fewer FLOPs than the baseline. Compared to the state-of-the-art contrastive learning methods, RECLIP demonstrates 5 to 59x training resource savings while maintaining highly competitive zero-shot classification and retrieval performance. Finally, RECLIP matches the state of the art in transfer learning to open-vocabulary detection tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the broader research community to explore language supervised pretraining in resource-friendly settings.},
 author = {Runze Li and Dahun Kim and Bir Bhanu and Weicheng Kuo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4365475016},
 pdf = {https://openreview.net/pdf?id=Ufc5cWhHko},
 review = {https://openreview.net/forum?id=Ufc5cWhHko},
 title = {RECLIP: Resource-efficient CLIP by Training with Small Images},
 url = {https://openreview.net/forum?id=Ufc5cWhHko},
 year = {2023}
}

@article{li2023representations,
 author = {Yuxuan Li and James McClelland},
 code = {https://github.com/Effie-Li/transformer-structured-generalization-public},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=oFC2LAqS6Z},
 review = {https://openreview.net/forum?id=oFC2LAqS6Z},
 title = {Representations and Computations in Transformers that Support Generalization on Structured Tasks},
 url = {https://openreview.net/forum?id=oFC2LAqS6Z},
 year = {2023}
}

@article{li2023smile,
 abstract = {To improve the performance of deep learning, mixup has been proposed to force the neural networks favoring simple linear behaviors in-between training samples. Performing mixup for transfer learning with pre-trained models however is not that simple, a high capacity pre-trained model with a large fully-connected (FC) layer could easily overfit to the target dataset even with samples-to-labels mixed up. In this work, we propose SMILE - Self-Distilled Mixup for EffIcient Transfer LEarning. With mixed images as inputs, SMILE regularizes the outputs of CNN feature extractors to learn from the mixed feature vectors of inputs (sample-to-feature mixup), in addition to the mixed labels. Specifically, SMILE incorporates a mean teacher, inherited from the pre-trained model, to provide the feature vectors of input samples in a self-distilling fashion, and mixes up the feature vectors accordingly via a novel triplet regularizer. The triple regularizer balances the mixup effects in both feature and label spaces while bounding the linearity in-between samples for pre-training tasks. Extensive experiments have been done to verify the performance improvement made by SMILE, in comparisons with a wide spectrum of transfer learning algorithms, including fine-tuning, L2-SP, DELTA, and RIFLE, even with mixup strategies combined. Ablation studies show that the vanilla sample-to-label mixup strategies could marginally increase the linearity in-between training samples but lack of generalizability, while SMILE significantly improve the mixup effects in both label and feature spaces with both training and testing datasets. The empirical observations backup our design intuition and purposes.},
 author = {Xingjian Li and Haoyi Xiong and Cheng-zhong Xu and Dejing Dou},
 code = {https://github.com/lixingjian/SMILE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3137412700},
 pdf = {https://openreview.net/pdf?id=czgMCpvrDM},
 review = {https://openreview.net/forum?id=czgMCpvrDM},
 title = {SMILE: Self-Distilled MIxup for Efficient Transfer LEarning},
 url = {https://openreview.net/forum?id=czgMCpvrDM},
 year = {2023}
}

@article{li2023starcoder,
 abstract = {The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.},
 author = {Raymond Li and Loubna Ben allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia LI and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Joel Lamy-Poirier and Joao Monteiro and Nicolas Gontier and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Ben Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason T Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Urvashi Bhattacharyya and Wenhao Yu and Sasha Luccioni and Paulo Villegas and Fedor Zhdanov and Tony Lee and Nadav Timor and Jennifer Ding and Claire S Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro Von Werra and Harm de Vries},
 badge = {Reproducibility},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4376167329},
 pdf = {https://openreview.net/pdf?id=KoFOg41haE},
 review = {https://openreview.net/forum?id=KoFOg41haE},
 title = {StarCoder: may the source be with you!},
 url = {https://openreview.net/forum?id=KoFOg41haE},
 year = {2023}
}

@article{li2023supervised,
 abstract = {Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset by leveraging prior knowledge of a labeled set comprising disjoint but related classes. Given that most existing literature focuses primarily on utilizing supervised knowledge from a labeled set at the methodology level, this paper considers the question: Is supervised knowledge always helpful at different levels of semantic relevance? To proceed, we first establish a novel metric, so-called transfer flow, to measure the semantic similarity between labeled/unlabeled datasets. To show the validity of the proposed metric, we build up a large-scale benchmark with various degrees of semantic similarities between labeled/unlabeled datasets on ImageNet by leveraging its hierarchical class structure. The results based on the proposed benchmark show that the proposed transfer flow is in line with the hierarchical class structure; and that NCD performance is consistent with the semantic similarities (measured by the proposed metric). Next, by using the proposed transfer flow, we conduct various empirical experiments with different levels of semantic similarity, yielding that supervised knowledge may hurt NCD performance. Specifically, using supervised information from a low-similarity labeled set may lead to a suboptimal result as compared to using pure self-supervised knowledge. These results reveal the inadequacy of the existing NCD literature which usually assumes that supervised knowledge is beneficial. Finally, we develop a pseudo-version of the transfer flow as a practical reference to decide if supervised knowledge should be used in NCD. Its effectiveness is supported by our empirical studies, which show that the pseudo transfer flow (with or without supervised knowledge) is consistent with the corresponding accuracy based on various datasets. Code is released at https://github.com/J-L-O/SK-Hurt-NCD},
 author = {ZIYUN LI and Jona Otholt and Ben Dai and Di Hu and Christoph Meinel and Haojin Yang},
 badge = {Event: CoLLAs 2023},
 code = {https://github.com/J-L-O/SK-Hurt-NCD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4379924823},
 pdf = {https://openreview.net/pdf?id=oqOBTo5uWD},
 review = {https://openreview.net/forum?id=oqOBTo5uWD},
 title = {Supervised Knowledge May Hurt Novel Class Discovery Performance},
 url = {https://openreview.net/forum?id=oqOBTo5uWD},
 year = {2023}
}

@article{li2023transformer,
 author = {Zijie Li and Kazem Meidani and Amir Barati Farimani},
 code = {https://github.com/BaratiLab/OFormer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=EPPqt3uERT},
 review = {https://openreview.net/forum?id=EPPqt3uERT},
 title = {Transformer for Partial Differential Equations{\textquoteright} Operator Learning},
 url = {https://openreview.net/forum?id=EPPqt3uERT},
 year = {2023}
}

@article{li2023triproma,
 abstract = {Contrastive self-supervised learning (SSL) methods, such as MoCo and SimCLR, have achieved great success in unsupervised visual representation learning. They rely on a large number of negative pairs and thus require either large memory banks or large batches. Some recent non-contrastive SSL methods, such as BYOL and SimSiam, attempt to discard negative pairs and have also shown remarkable performance. To avoid collapsed solutions caused by not using negative pairs, these methods require non-trivial asymmetry designs. However, in small data regimes, we can not obtain a sufficient number of negative pairs or effectively avoid the over-fitting problem when negatives are not used at all. To address this situation, we argue that negative pairs are still important but one is generally sufficient for each positive pair. We show that a simple Triplet-based loss (Trip) can achieve surprisingly good performance without requiring large batches or asymmetry designs. Moreover, to alleviate the over-fitting problem in small data regimes and further enhance the effect of Trip, we propose a simple plug-and-play RandOm MApping (ROMA) strategy by randomly mapping samples into other spaces and requiring these randomly projected samples to satisfy the same relationship indicated by the triplets. Integrating the triplet-based loss with random mapping, we obtain the proposed method Trip-ROMA. Extensive experiments, including unsupervised representation learning and unsupervised few-shot learning, have been conducted on ImageNet-1K and seven small datasets. They successfully demonstrate the effectiveness of Trip-ROMA and consistently show that ROMA can further effectively boost other SSL methods. Code is available at https://github.com/WenbinLee/Trip-ROMA.},
 author = {Wenbin Li and Xuesong Yang and Meihao Kong and Lei Wang and Jing Huo and Yang Gao and Jiebo Luo},
 code = {https://github.com/WenbinLee/Trip-ROMA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386184820},
 pdf = {https://openreview.net/pdf?id=MR4glug5GU},
 review = {https://openreview.net/forum?id=MR4glug5GU},
 title = {Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings},
 url = {https://openreview.net/forum?id=MR4glug5GU},
 year = {2023}
}

@article{li2023uncovering,
 abstract = {Spiking Neural Networks (SNNs) are recognized as the candidate for the next-generation neural networks due to their bio-plausibility and energy efficiency. Recently, researchers have demonstrated that SNNs are able to achieve nearly state-of-the-art performance in image recognition tasks using surrogate gradient training. However, some essential questions exist pertaining to SNNs that are little studied: Do SNNs trained with surrogate gradient learn different representations from traditional Artificial Neural Networks (ANNs)? Does the time dimension in SNNs provide unique representation power? In this paper, we aim to answer these questions by conducting a representation similarity analysis between SNNs and ANNs using Centered Kernel Alignment (CKA). We start by analyzing the spatial dimension of the networks, including both the width and the depth. Furthermore, our analysis of residual connections shows that SNNs learn a periodic pattern, which rectifies the representations in SNNs to be ANN-like. We additionally investigate the effect of the time dimension on SNN representation, finding that deeper layers encourage more dynamics along the time dimension. We also investigate the impact of input data such as event-stream data and adversarial attacks. Our work uncovers a host of new findings of representations in SNNs. We hope this work will inspire future research to fully comprehend the representation power of SNNs. Code is released at https://github.com/Intelligent-Computing-Lab-Yale/SNNCKA.},
 author = {Yuhang Li and Youngeun Kim and Hyoungseob Park and Priyadarshini Panda},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4367189470},
 pdf = {https://openreview.net/pdf?id=s9efQF3QW1},
 review = {https://openreview.net/forum?id=s9efQF3QW1},
 title = {Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient},
 url = {https://openreview.net/forum?id=s9efQF3QW1},
 year = {2023}
}

@article{li2023vt,
 abstract = {Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.},
 author = {Bryan M. Li and Isabel Maria Cornacchia and Nathalie Rochefort and Arno Onken},
 code = {https://github.com/bryanlimy/V1T},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319453765},
 pdf = {https://openreview.net/pdf?id=qHZs2p4ZD4},
 review = {https://openreview.net/forum?id=qHZs2p4ZD4},
 title = {V1T: large-scale mouse V1 response prediction using a Vision Transformer},
 url = {https://openreview.net/forum?id=qHZs2p4ZD4},
 year = {2023}
}

@article{li2024enhancing,
 author = {Keliang Li and Hong Chang and Shiguang Shan and Xilin CHEN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=n2gAD8Fdzk},
 review = {https://openreview.net/forum?id=n2gAD8Fdzk},
 title = {Enhancing Robustness to Class-Conditional Distribution Shift in Long-Tailed Recognition},
 url = {https://openreview.net/forum?id=n2gAD8Fdzk},
 year = {2024}
}

@article{li2024inverse,
 abstract = {The state-of-the-art dimensionality reduction approaches largely rely on complicated optimization procedures. On the other hand, closed-form approaches requiring merely eigen-decomposition do not have enough sophistication and nonlinearity. In this paper, we propose a novel nonlinear dimensionality reduction method -- Inverse Kernel Decomposition (IKD) -- based on an eigen-decomposition of the sample covariance matrix of data. The method is inspired by Gaussian process latent variable models (GPLVMs) and has comparable performance with GPLVMs. To deal with very noisy data with weak correlations, we propose two solutions -- blockwise and geodesic -- to make use of locally correlated data points and provide better and numerically more stable latent estimations. We use synthetic datasets and four real-world datasets to show that IKD is a better dimensionality reduction method than other eigen-decomposition-based methods, and achieves comparable performance against optimization-based methods with faster running speeds. Open-source IKD implementation in Python can be accessed at this \url{https://github.com/JerrySoybean/ikd}.},
 author = {Chengrui Li and Anqi Wu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309044549},
 pdf = {https://openreview.net/pdf?id=H4OE7toXpa},
 review = {https://openreview.net/forum?id=H4OE7toXpa},
 title = {Inverse Kernel Decomposition},
 url = {https://openreview.net/forum?id=H4OE7toXpa},
 year = {2024}
}

@article{li2024visual,
 abstract = {As a popular paradigm of distributed learning, personalized federated learning (PFL) allows personalized models to improve generalization ability and robustness by utilizing knowledge from all distributed clients. Most existing PFL algorithms tackle personalization in a model-centric way, such as personalized layer partition, model regularization, and model interpolation, which all fail to take into account the data characteristics of distributed clients. In this paper, we propose a novel PFL framework for image classification tasks, dubbed pFedPT, that leverages personalized visual prompts to implicitly represent local data distribution information of clients and provides that information to the aggregation model to help with classification tasks. Specifically, in each round of pFedPT training, each client generates a local personalized prompt related to local data distribution. Then, the local model is trained on the input composed of raw data and a visual prompt to learn the distribution information contained in the prompt. During model testing, the aggregated model obtains prior knowledge of the data distributions based on the prompts, which can be seen as an adaptive fine-tuning of the aggregation model to improve model performances on different clients. Furthermore, the visual prompt can be added as an orthogonal method to implement personalization on the client for existing FL methods to boost their performance. Experiments on the CIFAR10 and CIFAR100 datasets show that pFedPT outperforms several state-of-the-art (SOTA) PFL algorithms by a large margin in various settings.},
 author = {Guanghao Li and Wansen Wu and Yan Sun and Li Shen and Baoyuan Wu and Dacheng Tao},
 code = {https://github.com/hkgdifyu/pFedPT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4327673103},
 pdf = {https://openreview.net/pdf?id=dUVejidXO7},
 review = {https://openreview.net/forum?id=dUVejidXO7},
 title = {Visual Prompt Based Personalized Federated Learning},
 url = {https://openreview.net/forum?id=dUVejidXO7},
 year = {2024}
}

@article{lian2024llmgrounded,
 abstract = {Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io},
 author = {Long Lian and Boyi Li and Adam Yala and Trevor Darrell},
 badge = {Featured},
 code = {https://llm-grounded-diffusion.github.io/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4378446358},
 pdf = {https://openreview.net/pdf?id=hFALpTb4fR},
 review = {https://openreview.net/forum?id=hFALpTb4fR},
 title = {LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models},
 url = {https://openreview.net/forum?id=hFALpTb4fR},
 year = {2024}
}

@article{liang2023a,
 abstract = {We study sampling problems associated with potentials that lack smoothness. The potentials can be either convex or non-convex. Departing from the standard smooth setting, the potentials are only assumed to be weakly smooth or non-smooth, or the summation of multiple such functions. We develop a sampling algorithm that resembles proximal algorithms in optimization for this challenging sampling task. Our algorithm is based on a special case of Gibbs sampling known as the alternating sampling framework (ASF). The key contribution of this work is a practical realization of the ASF based on rejection sampling for both non-convex and convex potentials that are not necessarily smooth. In almost all the cases of sampling considered in this work, our proximal sampling algorithm achieves better complexity than all existing methods.},
 author = {Jiaming Liang and Yongxin Chen},
 code = {https://www.dropbox.com/sh/25ku9is6g7dxikv/AADcLBJipPLD30T1XjOf1h1_a?dl=0},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226192527},
 pdf = {https://openreview.net/pdf?id=CkXOwlhf27},
 review = {https://openreview.net/forum?id=CkXOwlhf27},
 title = {A Proximal Algorithm for Sampling},
 url = {https://openreview.net/forum?id=CkXOwlhf27},
 year = {2023}
}

@article{liang2023highmodality,
 abstract = {Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalities {X1,X2} are by measuring how much information can be transferred from X1 to X2, while (2) interaction heterogeneity studies how similarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much information can be transferred from fusing {X1,X2} to {X3,X4}. We show the importance of these 2 proposed metrics as a way to automatically prioritize the fusion of modalities that contain unique information or interactions. The result is a single model, HighMMT, that scales up to 10 modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and 15 tasks from 5 research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning.},
 author = {Paul Pu Liang and Yiwei Lyu and Xiang Fan and Jeffrey Tsaw and Yudong Liu and Shentong Mo and Dani Yogatama and Louis-Philippe Morency and Russ Salakhutdinov},
 code = {https://github.com/pliang279/HighMMT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221150742},
 pdf = {https://openreview.net/pdf?id=ttzypy3kT7},
 review = {https://openreview.net/forum?id=ttzypy3kT7},
 title = {High-Modality Multimodal Transformer: Quantifying Modality &amp; Interaction Heterogeneity for High-Modality Representation Learning},
 url = {https://openreview.net/forum?id=ttzypy3kT7},
 year = {2023}
}

@article{liang2023holistic,
 abstract = {Abstract Language models (LMs) like GPT‐3, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade‐offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top‐level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/ .},
 author = {Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Alexander Cosgrove and Christopher D Manning and Christopher Re and Diana Acosta-Navas and Drew Arad Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue WANG and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Andrew Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification, Expert Certification},
 openalex = {W4378189609},
 pdf = {https://openreview.net/pdf?id=iO4LZibEqW},
 review = {https://openreview.net/forum?id=iO4LZibEqW},
 title = {Holistic Evaluation of Language Models},
 url = {https://openreview.net/forum?id=iO4LZibEqW},
 year = {2023}
}

@article{liang2024exploring,
 abstract = {Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we propose a framework named Unified Instruction Tuning (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we (1) demonstrate the necessity of maintaining format consistency in instruction tuning; (2) improve the generalization performance on unseen instructions on T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the noise of automatic format transfer to make the UIT framework more practical and a smaller offline model based on GPT-J that achieves comparable format transfer capability to OpenAI APIs to reduce costs in practice. Further analysis regarding variations of targeted formats and other effects is intended.},
 author = {Shihao Liang and Runchu Tian and Kunlun Zhu and Yujia Qin and Huadong Wang and Xin Cong and Zhiyuan Liu and Xiaojiang Liu and Maosong Sun},
 code = {https://github.com/thunlp/UnifiedInstructionTuning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385436737},
 pdf = {https://openreview.net/pdf?id=n8fZ6mY6PB},
 review = {https://openreview.net/forum?id=n8fZ6mY6PB},
 title = {Exploring Format Consistency for Instruction Tuning},
 url = {https://openreview.net/forum?id=n8fZ6mY6PB},
 year = {2024}
}

@article{liang2024pullback,
 abstract = {Persistent homology (PH) is a method for generating topology-inspired representations of data. Empirical studies that investigate the properties of PH, such as its sensitivity to perturbations or ability to detect a feature of interest, commonly rely on training and testing an additional model on the basis of the PH representation. To gain more intrinsic insights about PH, independently of the choice of such a model, we propose a novel methodology based on the pull-back geometry that a PH encoding induces on the data manifold. The spectrum and eigenvectors of the induced metric help to identify the most and least significant information captured by PH. Furthermore, the pull-back norm of tangent vectors provides insights about the sensitivity of PH to a given perturbation, or its potential to detect a given feature of interest, and in turn its ability to solve a given classification or regression problem. Experimentally, the insights gained through our methodology align well with the existing knowledge about PH. Moreover, we show that the pull-back norm correlates with the performance on downstream tasks, and can therefore guide the choice of a suitable PH encoding.},
 author = {Shuang Liang and Renata Turkes and Jiayi Li and Nina Otter and Guido Montufar},
 code = {https://github.com/shuangliang15/pullback-geometry-persistent-homology},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387634333},
 pdf = {https://openreview.net/pdf?id=7yswRA8zzw},
 review = {https://openreview.net/forum?id=7yswRA8zzw},
 title = {Pull-back Geometry of Persistent Homology Encodings},
 url = {https://openreview.net/forum?id=7yswRA8zzw},
 year = {2024}
}

@article{liao2022on,
 abstract = {With the motive of training all the parameters of a neural network, we study why and when one can achieve this by iteratively creating, training, and combining randomly selected subnetworks. Such scenarios have either implicitly or explicitly emerged in the recent literature: see e.g., the Dropout family of regularization techniques, or some distributed ML training protocols that reduce communication/computation complexities, such as the Independent Subnet Training protocol. While these methods are studied empirically and utilized in practice, they often enjoy partial or no theoretical support, especially when applied on neural network-based objectives. In this manuscript, our focus is on overparameterized single hidden layer neural networks with ReLU activations in the lazy training regime. By carefully analyzing $i)$ the subnetworks' neural tangent kernel, $ii)$ the surrogate functions' gradient, and $iii)$ how we sample and combine the surrogate functions, we prove linear convergence rate of the training error -- up to a neighborhood around the optimal point -- for an overparameterized single-hidden layer perceptron with a regression loss. Our analysis reveals a dependency of the size of the neighborhood around the optimal point on the number of surrogate models and the number of local training steps for each selected subnetwork. Moreover, the considered framework generalizes and provides new insights on dropout training, multi-sample dropout training, as well as Independent Subnet Training; for each case, we provide convergence results as corollaries of our main theorem.},
 author = {Fangshuo Liao and Anastasios Kyrillidis},
 code = {https://github.com/JLiao980706/MaskedNTK-TMLR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4200631446},
 pdf = {https://openreview.net/pdf?id=e7mYYMSyZH},
 review = {https://openreview.net/forum?id=e7mYYMSyZH},
 title = {On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons},
 url = {https://openreview.net/forum?id=e7mYYMSyZH},
 year = {2022}
}

@article{liao2023learning,
 abstract = {Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse Hessian in noisy loss landscapes and is capable of representing a wide range of inverse Hessians. We experimentally verify that our algorithm can optimize in noisy settings, and show that simpler alternatives for representing the inverse Hessians worsen performance. Lastly, we use our optimizer to train a semi-realistic deep neural network with 95k parameters at speeds comparable to those of standard neural network optimizers.},
 author = {Isaac Liao and Rumen Dangovski and Jakob Nicolaus Foerster and Marin Soljacic},
 code = {https://github.com/iliao2345/l2o_quasi_newton_cleaned},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306178095},
 pdf = {https://openreview.net/pdf?id=Ns2X7Azudy},
 review = {https://openreview.net/forum?id=Ns2X7Azudy},
 title = {Learning to Optimize Quasi-Newton Methods},
 url = {https://openreview.net/forum?id=Ns2X7Azudy},
 year = {2023}
}

@article{likhosherstov2023polyvit,
 abstract = {Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters? We present PolyViT, a model trained on image, audio and video which answers this question. By co-training different tasks on a single modality, we are able to improve the accuracy of each individual task and achieve state-of-the-art results on 5 standard video- and audio-classification datasets. Co-training PolyViT on multiple modalities and tasks leads to a model that is even more parameter-efficient, and learns representations that generalize across multiple domains. Moreover, we show that co-training is simple and practical to implement, as we do not need to tune hyperparameters for each combination of datasets, but can simply adapt those from standard, single-task training.},
 author = {Valerii Likhosherstov and Anurag Arnab and Krzysztof Marcin Choromanski and Mario Lucic and Yi Tay and Mostafa Dehghani},
 code = {https://github.com/google-research/scenic/tree/main/scenic/projects/polyvit},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309435465},
 pdf = {https://openreview.net/pdf?id=zKnqZeUCLO},
 review = {https://openreview.net/forum?id=zKnqZeUCLO},
 title = {PolyViT: Co-training Vision Transformers on Images, Videos and Audio},
 url = {https://openreview.net/forum?id=zKnqZeUCLO},
 year = {2023}
}

@article{lim2023accelerated,
 abstract = {Quality-Diversity (QD) optimization algorithms are a well-known approach to generate large collections of diverse and high-quality solutions. However, derived from evolutionary computation, QD algorithms are population-based methods which are known to be data-inefficient and requires large amounts of computational resources. This makes QD algorithms slow when used in applications where solution evaluations are computationally costly. A common approach to speed up QD algorithms is to evaluate solutions in parallel, for instance by using physical simulators in robotics. Yet, this approach is limited to several dozen of parallel evaluations as most physics simulators can only be parallelized more with a greater number of CPUs. With recent advances in simulators that run on accelerators, thousands of evaluations can now be performed in parallel on single GPU/TPU. In this paper, we present QDax, an accelerated implementation of MAP-Elites which leverages massive parallelism on accelerators to make QD algorithms more accessible. We show that QD algorithms are ideal candidates to take advantage of progress in hardware acceleration. We demonstrate that QD algorithms can scale with massive parallelism to be run at interactive timescales without any significant effect on the performance. Results across standard optimization functions and four neuroevolution benchmark environments shows that experiment runtimes are reduced by two factors of magnitudes, turning days of computation into minutes. More surprising, we observe that reducing the number of generations by two orders of magnitude, and thus having significantly shorter lineage does not impact the performance of QD algorithms. These results show that QD can now benefit from hardware acceleration, which contributed significantly to the bloom of deep learning.},
 author = {Bryan Lim and Maxime Allard and Luca Grillotti and Antoine Cully},
 code = {https://github.com/adaptive-intelligent-robotics/QDax},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4304697723},
 pdf = {https://openreview.net/pdf?id=znNITCJyTI},
 review = {https://openreview.net/forum?id=znNITCJyTI},
 title = {Accelerated Quality-Diversity through Massive Parallelism},
 url = {https://openreview.net/forum?id=znNITCJyTI},
 year = {2023}
}

@article{lim2023unifying,
 author = {Yi Heng Lim and Muhammad Firmansyah Kasim},
 code = {https://github.com/machine-discovery/research/tree/master/constr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ZOAb497iaY},
 review = {https://openreview.net/forum?id=ZOAb497iaY},
 title = {Unifying physical systems{\textquoteright} inductive biases in neural {ODE} using dynamics constraints},
 url = {https://openreview.net/forum?id=ZOAb497iaY},
 year = {2023}
}

@article{lin2022conformal,
 abstract = {Cross-sectional prediction is common in many domains such as healthcare, including forecasting tasks using electronic health records, where different patients form a cross-section. We focus on the task of constructing valid prediction intervals (PIs) in time series regression with a cross-section. A prediction interval is considered valid if it covers the true response with (a pre-specified) high probability. We first distinguish between two notions of validity in such a setting: cross-sectional and longitudinal. Cross-sectional validity is concerned with validity across the cross-section of the time series data, while longitudinal validity accounts for the temporal dimension. Coverage guarantees along both these dimensions are ideally desirable; however, we show that distribution-free longitudinal validity is theoretically impossible. Despite this limitation, we propose Conformal Prediction with Temporal Dependence (CPTD), a procedure that is able to maintain strict cross-sectional validity while improving longitudinal coverage. CPTD is post-hoc and light-weight, and can easily be used in conjunction with any prediction model as long as a calibration set is available. We focus on neural networks due to their ability to model complicated data such as diagnosis codes for time series regression, and perform extensive experimental validation to verify the efficacy of our approach. We find that CPTD outperforms baselines on a variety of datasets by improving longitudinal coverage and often providing more efficient (narrower) PIs.},
 author = {Zhen Lin and Shubhendu Trivedi and Jimeng Sun},
 code = {https://github.com/zlin7/CPTD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281974197},
 pdf = {https://openreview.net/pdf?id=8QoxXTDcsH},
 review = {https://openreview.net/forum?id=8QoxXTDcsH},
 title = {Conformal Prediction Intervals with Temporal Dependence},
 url = {https://openreview.net/forum?id=8QoxXTDcsH},
 year = {2022}
}

@article{lin2022exploring,
 abstract = {Temporal point process (TPP) is commonly used to model the asynchronous event sequence featuring occurrence timestamps and revealed by probabilistic models conditioned on historical impacts. While lots of previous works have focused on `goodness-of-fit' of TPP models by maximizing the likelihood, their predictive performance is unsatisfactory, which means the timestamps generated by models are far apart from true observations. Recently, deep generative models such as denoising diffusion and score matching models have achieved great progress in image generating tasks by demonstrating their capability of generating samples of high quality. However, there are no complete and unified works exploring and studying the potential of generative models in the context of event occurence modeling for TPP. In this work, we try to fill the gap by designing a unified \textbf{g}enerative framework for \textbf{n}eural \textbf{t}emporal \textbf{p}oint \textbf{p}rocess (\textsc{GNTPP}) model to explore their feasibility and effectiveness, and further improve models' predictive performance. Besides, in terms of measuring the historical impacts, we revise the attentive models which summarize influence from historical events with an adaptive reweighting term considering events' type relation and time intervals. Extensive experiments have been conducted to illustrate the improved predictive capability of \textsc{GNTPP} with a line of generative probabilistic decoders, and performance gain from the revised attention. To the best of our knowledge, this is the first work that adapts generative models in a complete unified framework and studies their effectiveness in the context of TPP. Our codebase including all the methods given in Section.5.1.1 is open in \url{https://github.com/BIRD-TAO/GNTPP}. We hope the code framework can facilitate future research in Neural TPPs.},
 author = {Haitao Lin and Lirong Wu and Guojiang Zhao and Liu Pai and Stan Z. Li},
 code = {https://github.com/BIRD-TAO/GNTPP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4290064599},
 pdf = {https://openreview.net/pdf?id=NPfS5N3jbL},
 review = {https://openreview.net/forum?id=NPfS5N3jbL},
 title = {Exploring Generative Neural Temporal Point Process},
 url = {https://openreview.net/forum?id=NPfS5N3jbL},
 year = {2022}
}

@article{lin2022reasonable,
 abstract = {Multi-Task Learning (MTL) has achieved success in various fields. However, how to balance different tasks to achieve good performance is a key problem. To achieve the task balancing, there are many works to carefully design dynamical loss/gradient weighting strategies but the basic random experiments are ignored to examine their effectiveness. In this paper, we propose the Random Weighting (RW) methods, including Random Loss Weighting (RLW) and Random Gradient Weighting (RGW), where an MTL model is trained with random loss/gradient weights sampled from a distribution. To show the effectiveness and necessity of RW methods, theoretically we analyze the convergence of RW and reveal that RW has a higher probability to escape local minima, resulting in better generalization ability. Empirically, we extensively evaluate the proposed RW methods to compare with twelve state-of-the-art methods on five image datasets and two multilingual problems from the XTREME benchmark to show RW methods can achieve comparable performance with state-of-the-art baselines. Therefore, we think that the RW methods are important baselines for MTL and should attract more attentions.},
 author = {Baijiong Lin and Feiyang YE and Yu Zhang and Ivor Tsang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4288791681},
 pdf = {https://openreview.net/pdf?id=jjtFD8A1Wx},
 review = {https://openreview.net/forum?id=jjtFD8A1Wx},
 title = {Reasonable Effectiveness of Random Weighting: A Litmus Test for Multi-Task Learning},
 url = {https://openreview.net/forum?id=jjtFD8A1Wx},
 year = {2022}
}

@article{lin2022teaching,
 abstract = {We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. "90% confidence" or "high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.},
 author = {Stephanie Lin and Jacob Hilton and Owain Evans},
 code = {https://github.com/sylinrl/CalibratedMath},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281748205},
 pdf = {https://openreview.net/pdf?id=8s8K2UZGTZ},
 review = {https://openreview.net/forum?id=8s8K2UZGTZ},
 title = {Teaching Models to Express Their Uncertainty in Words},
 url = {https://openreview.net/forum?id=8s8K2UZGTZ},
 year = {2022}
}

@article{lin2023mixture,
 abstract = {In this paper, we propose a latent-variable generative model called mixture of dynamical variational autoencoders (MixDVAE) to model the dynamics of a system composed of multiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated into a multi-source mixture model with a discrete observation-to-source assignment latent variable. The posterior distributions of both the discrete observation-to-source assignment variable and the continuous DVAE variables representing the sources content/position are estimated using a variational expectation-maximization algorithm, leading to multi-source trajectories estimation. We illustrate the versatility of the proposed MixDVAE model on two tasks: a computer vision task, namely multi-object tracking, and an audio processing task, namely single-channel audio source separation. Experimental results show that the proposed method works well on these two tasks, and outperforms several baseline methods.},
 author = {Xiaoyu Lin and Laurent Girin and Xavier Alameda-Pineda},
 code = {https://github.com/linxiaoyu1/MixDVAE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4381273917},
 pdf = {https://openreview.net/pdf?id=sbkZKBVC31},
 review = {https://openreview.net/forum?id=sbkZKBVC31},
 title = {Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation},
 url = {https://openreview.net/forum?id=sbkZKBVC31},
 year = {2023}
}

@article{lin2024a,
 abstract = {Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.},
 author = {Jiahe Lin and Huitian Lei and George Michailidis},
 code = {https://github.com/georgemichailidis/vae-multi-level-neural-GC-official},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4392224310},
 pdf = {https://openreview.net/pdf?id=kNCZ95mw7N},
 review = {https://openreview.net/forum?id=kNCZ95mw7N},
 title = {A VAE-based Framework for Learning Multi-Level Neural Granger-Causal
  Connectivity},
 url = {https://openreview.net/forum?id=kNCZ95mw7N},
 year = {2024}
}

@article{lioutas2022titrated,
 author = {Vasileios Lioutas and Adam Scibior and Frank Wood},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=M8D5iZsnrO},
 review = {https://openreview.net/forum?id=M8D5iZsnrO},
 title = {{TITRATED}: Learned Human Driving Behavior without Infractions via Amortized Inference},
 url = {https://openreview.net/forum?id=M8D5iZsnrO},
 year = {2022}
}

@article{liu2022autolambda,
 abstract = {Understanding the structure of multiple related tasks allows for multi-task learning to improve the generalisation ability of one or all of them. However, it usually requires training each pairwise combination of tasks together in order to capture task relationships, at an extremely high computational cost. In this work, we learn task relationships via an automated weighting framework, named Auto-Lambda. Unlike previous methods where task relationships are assumed to be fixed, Auto-Lambda is a gradient-based meta learning framework which explores continuous, dynamic task relationships via task-specific weightings, and can optimise any choice of combination of tasks through the formulation of a meta-loss; where the validation loss automatically influences task weightings throughout training. We apply the proposed framework to both multi-task and auxiliary learning problems in computer vision and robotics, and show that Auto-Lambda achieves state-of-the-art performance, even when compared to optimisation strategies designed specifically for each problem and data domain. Finally, we observe that Auto-Lambda can discover interesting learning behaviors, leading to new insights in multi-task learning. Code is available at https://github.com/lorenmt/auto-lambda.},
 author = {Shikun Liu and Stephen James and Andrew Davison and Edward Johns},
 code = {https://github.com/lorenmt/auto-lambda},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4225452291},
 pdf = {https://openreview.net/pdf?id=KKeCMim5VN},
 review = {https://openreview.net/forum?id=KKeCMim5VN},
 title = {Auto-Lambda: Disentangling Dynamic Task Relationships},
 url = {https://openreview.net/forum?id=KKeCMim5VN},
 year = {2022}
}

@article{liu2022distribution,
 abstract = {We propose Distribution Embedding Networks (DEN) for classification with small data. In the same spirit of meta-learning, DEN learns from a diverse set of training tasks with the goal to generalize to unseen target tasks. Unlike existing approaches which require the inputs of training and target tasks to have the same dimension with possibly similar distributions, DEN allows training and target tasks to live in heterogeneous input spaces. This is especially useful for tabular-data tasks where labeled data from related tasks are scarce. DEN uses a three-block architecture: a covariate transformation block followed by a distribution embedding block and then a classification block. We provide theoretical insights to show that this architecture allows the embedding and classification blocks to be fixed after pre-training on a diverse set of tasks; only the covariate transformation block with relatively few parameters needs to be fine-tuned for each new task. To facilitate training, we also propose an approach to synthesize binary classification tasks, and demonstrate that DEN outperforms existing methods in a number of synthetic and real tasks in numerical studies.},
 author = {Lang Liu and Mahdi Milani Fard and Sen Zhao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221146387},
 pdf = {https://openreview.net/pdf?id=F2rG2CXsgO},
 review = {https://openreview.net/forum?id=F2rG2CXsgO},
 title = {Distribution Embedding Networks for Generalization from a Diverse Set of Classification Tasks},
 url = {https://openreview.net/forum?id=F2rG2CXsgO},
 year = {2022}
}

@article{liu2022from,
 author = {Fusheng Liu and Haizhao Yang and Soufiane Hayou and Qianxiao Li},
 code = {https://github.com/mathematicallfs/Uniform-LGI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=mW6nD3567x},
 review = {https://openreview.net/forum?id=mW6nD3567x},
 title = {From Optimization Dynamics to Generalization Bounds via {\L}ojasiewicz Gradient Inequality},
 url = {https://openreview.net/forum?id=mW6nD3567x},
 year = {2022}
}

@article{liu2022integrating,
 abstract = {In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a significant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workflows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error by approximately 30% as compared to the best performing baseline on the ICLR 2017 data.},
 author = {Yusha Liu and Yichong Xu and Nihar B Shah and Aarti Singh},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4223577623},
 pdf = {https://openreview.net/pdf?id=Kb1lb0vSLa},
 review = {https://openreview.net/forum?id=Kb1lb0vSLa},
 title = {Integrating Rankings into Quantized Scores in Peer Review},
 url = {https://openreview.net/forum?id=Kb1lb0vSLa},
 year = {2022}
}

@article{liu2022towards,
 abstract = {Subgraph similarity search, one of the core problems in graph search, concerns whether a target graph approximately contains a query graph. The problem is recently touched by neural methods. However, current neural methods do not consider pruning the target graph, though pruning is critically important in traditional calculations of subgraph similarities. One obstacle to applying pruning in neural methods is {the discrete property of pruning}. In this work, we convert graph pruning to a problem of node relabeling and then relax it to a differentiable problem. Based on this idea, we further design a novel neural network to approximate a type of subgraph distance: the subgraph edit distance (SED). {In particular, we construct the pruning component using a neural structure, and the entire model can be optimized end-to-end.} In the design of the model, we propose an attention mechanism to leverage the information about the query graph and guide the pruning of the target graph. Moreover, we develop a multi-head pruning strategy such that the model can better explore multiple ways of pruning the target graph. The proposed model establishes new state-of-the-art results across seven benchmark datasets. Extensive analysis of the model indicates that the proposed model can reasonably prune the target graph for SED computation. The implementation of our algorithm is released at our Github repo: https://github.com/tufts-ml/Prune4SED.},
 author = {Linfeng Liu and XU HAN and Dawei Zhou and Liping Liu},
 code = {https://github.com/tufts-ml/Prune4SED},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307011540},
 pdf = {https://openreview.net/pdf?id=CfzIsWWBlo},
 review = {https://openreview.net/forum?id=CfzIsWWBlo},
 title = {Towards Accurate Subgraph Similarity Computation via Neural Graph Pruning},
 url = {https://openreview.net/forum?id=CfzIsWWBlo},
 year = {2022}
}

@article{liu2023action,
 abstract = {Contextual bandit algorithms have many applicants in a variety of scenarios. In order to develop trustworthy contextual bandit systems, understanding the impacts of various adversarial attacks on contextual bandit algorithms is essential. In this paper, we propose a new class of attacks: action poisoning attacks, where an adversary can change the action signal selected by the agent. We design action poisoning attack schemes against linear contextual bandit algorithms in both white-box and black-box settings. We further analyze the cost of the proposed attack strategies for a very popular and widely used bandit algorithm: LinUCB. We show that, in both white-box and black-box settings, the proposed attack schemes can force the LinUCB agent to pull a target arm very frequently by spending only logarithm cost.},
 author = {Guanlin Liu and Lifeng Lai},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4200632678},
 pdf = {https://openreview.net/pdf?id=yhGCKUsKJS},
 review = {https://openreview.net/forum?id=yhGCKUsKJS},
 title = {Efficient Action Poisoning Attacks on Linear Contextual Bandits},
 url = {https://openreview.net/forum?id=yhGCKUsKJS},
 year = {2023}
}

@article{liu2023ap,
 abstract = {The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes during optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activating-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using popular networks (e.g., ResNet, VGG) via two classical and three state-of-the-art pruning methods. The experimental results on public datasets (e.g., CIFAR-10/100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and state-of-the-art networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study to examine the effectiveness of the components comprising AP.},
 author = {Shiyu Liu and Rohan Ghosh and Mehul Motani},
 badge = {Featured},
 code = {https://github.com/Martin1937/Activate-While-Pruning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4311551346},
 pdf = {https://openreview.net/pdf?id=EGQSpkUDdD},
 review = {https://openreview.net/forum?id=EGQSpkUDdD},
 title = {AP: Selective Activation for De-sparsifying Pruned Neural Networks},
 url = {https://openreview.net/forum?id=EGQSpkUDdD},
 year = {2023}
}

@article{liu2023automated,
 author = {Tony Liu and Patrick Lawlor and Lyle Ungar and Konrad Kording and Rahul Ladhania},
 code = {https://github.com/tliu526/rdsgd},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=cdRYoTyHZh},
 review = {https://openreview.net/forum?id=cdRYoTyHZh},
 title = {Automated Detection of Causal Inference Opportunities: Regression Discontinuity Subgroup Discovery},
 url = {https://openreview.net/forum?id=cdRYoTyHZh},
 year = {2023}
}

@article{liu2023dspar,
 author = {Zirui Liu and Kaixiong Zhou and Zhimeng Jiang and Li Li and Rui Chen and Soo-Hyun Choi and Xia Hu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=SaVEXFuozg},
 review = {https://openreview.net/forum?id=SaVEXFuozg},
 title = {{DS}par: An Embarrassingly Simple Strategy for Efficient {GNN} training and inference via Degree-based Sparsification},
 url = {https://openreview.net/forum?id=SaVEXFuozg},
 year = {2023}
}

@article{liu2023dynamics,
 author = {Zixuan Liu and Liu Liu and Bingzhe Wu and Lanqing Li and Xueqian Wang and Bo Yuan and Peilin Zhao},
 code = {https://github.com/Panda-Shawn/DYNAIL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=w36pqfaJ4t},
 review = {https://openreview.net/forum?id=w36pqfaJ4t},
 title = {Dynamics Adapted Imitation Learning},
 url = {https://openreview.net/forum?id=w36pqfaJ4t},
 year = {2023}
}

@article{liu2023graphbased,
 abstract = {There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however, remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-ODE) which is designed with multiple connective ODE-GNN modules to learn better representations by capturing different views of complex local and global dynamic spatio-temporal dependencies. We also add some techniques like shared weights and divergence constraints into the intermediate layers of distinct ODE-GNN modules to further improve their communication towards the forecasting task. Our extensive set of experiments conducted on six real-world datasets demonstrate the superior performance of GRAM-ODE compared with state-of-the-art baselines as well as the contribution of different components to the overall performance. The code is available at https://github.com/zbliu98/GRAM-ODE},
 author = {Zibo Liu and Parshin Shojaee and Chandan K. Reddy},
 code = {https://github.com/zbliu98/GRAM-ODE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4378942797},
 pdf = {https://openreview.net/pdf?id=Oq5XKRVYpQ},
 review = {https://openreview.net/forum?id=Oq5XKRVYpQ},
 title = {Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting},
 url = {https://openreview.net/forum?id=Oq5XKRVYpQ},
 year = {2023}
}

@article{liu2023optimizing,
 abstract = {The importance of learning rate (LR) schedules on network pruning has been observed in a few recent works. As an example, Frankle and Carbin (2019) highlighted that winning tickets (i.e., accuracy preserving subnetworks) can not be found without applying a LR warmup schedule and Renda, Frankle and Carbin (2020) demonstrated that rewinding the LR to its initial state at the end of each pruning cycle improves performance. In this paper, we go one step further by first providing a theoretical justification for the surprising effect of LR schedules. Next, we propose a LR schedule for network pruning called SILO, which stands for S-shaped Improved Learning rate Optimization. The advantages of SILO over existing state-of-the-art (SOTA) LR schedules are two-fold: (i) SILO has a strong theoretical motivation and dynamically adjusts the LR during pruning to improve generalization. Specifically, SILO increases the LR upper bound (max_lr) in an S-shape. This leads to an improvement of 2% - 4% in extensive experiments with various types of networks (e.g., Vision Transformers, ResNet) on popular datasets such as ImageNet, CIFAR-10/100. (ii) In addition to the strong theoretical motivation, SILO is empirically optimal in the sense of matching an Oracle, which exhaustively searches for the optimal value of max_lr via grid search. We find that SILO is able to precisely adjust the value of max_lr to be within the Oracle optimized interval, resulting in performance competitive with the Oracle with significantly lower complexity.},
 author = {Shiyu Liu and Rohan Ghosh and John Chong Min Tan and Mehul Motani},
 code = {https://github.com/Martin1937/SILO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4311551486},
 pdf = {https://openreview.net/pdf?id=nGW2Hotpq3},
 review = {https://openreview.net/forum?id=nGW2Hotpq3},
 title = {Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks},
 url = {https://openreview.net/forum?id=nGW2Hotpq3},
 year = {2023}
}

@article{liu2023rltf,
 abstract = {The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, current representative works either rely solely on offline frameworks, limiting the exploration of new sample spaces, or fall short in the utilization of unit test signals, not accounting for specific error locations within the code. To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: https://github.com/Zyq-scut/RLTF.},
 author = {Jiate Liu and Yiqin Zhu and Kaiwen Xiao and QIANG FU and Xiao Han and Yang Wei and Deheng Ye},
 code = {https://github.com/Zyq-scut/RLTF},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4383989076},
 pdf = {https://openreview.net/pdf?id=hjYmsV6nXZ},
 review = {https://openreview.net/forum?id=hjYmsV6nXZ},
 title = {RLTF: Reinforcement Learning from Unit Test Feedback},
 url = {https://openreview.net/forum?id=hjYmsV6nXZ},
 year = {2023}
}

@article{liu2023smoothed,
 abstract = {Differential privacy (DP) is a widely-accepted and widely-applied notion of privacy based on worst-case analysis. Often, DP classifies most mechanisms without additive noise as non-private (Dwork et al., 2014). Thus, additive noises are added to improve privacy (to achieve DP). However, in many real-world applications, adding additive noise is undesirable (Bagdasaryan et al., 2019) and sometimes prohibited (Liu et al., 2020). In this paper, we propose a natural extension of DP following the worst average-case idea behind the celebrated smoothed analysis (Spielman & Teng, May 2004). Our notion, smoothed DP, can effectively measure the privacy leakage of mechanisms without additive noises under realistic settings. We prove that any discrete mechanism with sampling procedures is more private than what DP predicts, while many continuous mechanisms with sampling procedures are still non-private under smoothed DP. In addition, we prove several desirable properties of smoothed DP, including composition, robustness to post-processing, and distribution reduction. Based on those properties, we propose an efficient algorithm to calculate the privacy parameters for smoothed DP. Experimentally, we verify that, according to smoothed DP, the discrete sampling mechanisms are private in real-world elections, and some discrete neural networks can be private without adding any additive noise. We believe that these results contribute to the theoretical foundation of realistic privacy measures beyond worst-case analysis.},
 author = {Ao Liu and Yu-Xiang Wang and Lirong Xia},
 code = {https://github.com/AoLiu-CS/SmoothedDP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3181016858},
 pdf = {https://openreview.net/pdf?id=CviCLt44Em},
 review = {https://openreview.net/forum?id=CviCLt44Em},
 title = {Smoothed Differential Privacy},
 url = {https://openreview.net/forum?id=CviCLt44Em},
 year = {2023}
}

@article{liu2024candidate,
 abstract = {Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage model. Our first stage adopts the conventional vector distancing metric and performs a fast pruning among candidates. Meanwhile, our second stage employs a dual-encoder architecture, which effectively attends to the input triplet of reference-text-candidate and re-ranks the candidates. Both stages utilize a vision-and-language pre-trained network, which has proven beneficial for various downstream tasks. Our method consistently outperforms state-of-the-art approaches on standard benchmarks for the task. Our implementation is available at https://github.com/Cuberick-Orion/Candidate-Reranking-CIR.},
 author = {Zheyuan Liu and Weixuan Sun and Damien Teney and Stephen Gould},
 code = {https://github.com/Cuberick-Orion/Candidate-Reranking-CIR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4378505291},
 pdf = {https://openreview.net/pdf?id=fJAwemcvpL},
 review = {https://openreview.net/forum?id=fJAwemcvpL},
 title = {Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder},
 url = {https://openreview.net/forum?id=fJAwemcvpL},
 year = {2024}
}

@article{liu2024dynaconf,
 abstract = {Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions.},
 author = {Siqi Liu and Andreas Lehrmann},
 code = {https://github.com/BorealisAI/dcf},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296548925},
 pdf = {https://openreview.net/pdf?id=48pHFcg0YO},
 review = {https://openreview.net/forum?id=48pHFcg0YO},
 title = {DynaConF: Dynamic Forecasting of Non-Stationary Time-Series},
 url = {https://openreview.net/forum?id=48pHFcg0YO},
 year = {2024}
}

@article{liu2024empowering,
 author = {Meng Liu and Haiyang Yu and Shuiwang Ji},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=VDy6LgErFM},
 review = {https://openreview.net/forum?id=VDy6LgErFM},
 title = {Empowering {GNN}s via Edge-Aware Weisfeiler-Leman Algorithm},
 url = {https://openreview.net/forum?id=VDy6LgErFM},
 year = {2024}
}

@article{liu2024pixmim,
 author = {Yuan Liu and Songyang Zhang and Jiacheng Chen and Kai Chen and Dahua Lin},
 code = {https://github.com/open-mmlab/mmselfsup/tree/main/configs/selfsup/pixmim},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=qyfz0QrkqP},
 review = {https://openreview.net/forum?id=qyfz0QrkqP},
 title = {Pix{MIM}: Rethinking Pixel Reconstruction in Masked Image Modeling},
 url = {https://openreview.net/forum?id=qyfz0QrkqP},
 year = {2024}
}

@article{liu2024prismer,
 abstract = {Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.},
 author = {Shikun Liu and Linxi Fan and Edward Johns and Zhiding Yu and Chaowei Xiao and Anima Anandkumar},
 code = {https://github.com/NVlabs/prismer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323557204},
 pdf = {https://openreview.net/pdf?id=R7H43YD6Lo},
 review = {https://openreview.net/forum?id=R7H43YD6Lo},
 title = {Prismer: A Vision-Language Model with An Ensemble of Experts},
 url = {https://openreview.net/forum?id=R7H43YD6Lo},
 year = {2024}
}

@article{liu2024unsupervised,
 abstract = {Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive baselines on two graph DGMs pretrained on two molecule datasets. Additionally, we qualitatively illustrate seven steerable factors learned by GraphCG on five pretrained DGMs over five graph datasets, including two for molecules and three for point clouds.},
 author = {Shengchao Liu and Chengpeng Wang and Jiarui Lu and Weili Nie and Hanchen Wang and Zhuoxinran Li and Bolei Zhou and Jian Tang},
 code = {https://github.com/chao1224/GraphCG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391421423},
 pdf = {https://openreview.net/pdf?id=wyU3Q4gahM},
 review = {https://openreview.net/forum?id=wyU3Q4gahM},
 title = {Unsupervised Discovery of Steerable Factors When Graph Deep Generative
  Models Are Entangled},
 url = {https://openreview.net/forum?id=wyU3Q4gahM},
 year = {2024}
}

@article{liu2024wavebench,
 author = {Tianlin Liu and Jose Antonio Lara Benitez and Florian Faucher and AmirEhsan Khorashadizadeh and Maarten V. de Hoop and Ivan Dokmani{\'c}},
 code = {https://github.com/wavebench/wavebench},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=6wpInwnzs8},
 review = {https://openreview.net/forum?id=6wpInwnzs8},
 title = {WaveBench: Benchmarking Data-driven Solvers for Linear Wave Propagation {PDE}s},
 url = {https://openreview.net/forum?id=6wpInwnzs8},
 year = {2024}
}

@article{liznerski2022exposing,
 abstract = {Due to the intractability of characterizing everything that looks unlike the normal data, anomaly detection (AD) is traditionally treated as an unsupervised problem utilizing only normal samples. However, it has recently been found that unsupervised image AD can be drastically improved through the utilization of huge corpora of random images to represent anomalousness; a technique which is known as Outlier Exposure. In this paper we show that specialized AD learning methods seem unnecessary for state-of-the-art performance, and furthermore one can achieve strong performance with just a small collection of Outlier Exposure data, contradicting common assumptions in the field of AD. We find that standard classifiers and semi-supervised one-class methods trained to discern between normal samples and relatively few random natural images are able to outperform the current state of the art on an established AD benchmark with ImageNet. Further experiments reveal that even one well-chosen outlier sample is sufficient to achieve decent performance on this benchmark (79.3% AUC). We investigate this phenomenon and find that one-class methods are more robust to the choice of training outliers, indicating that there are scenarios where these are still more useful than standard classifiers. Additionally, we include experiments that delineate the scenarios where our results hold. Lastly, no training samples are necessary when one uses the representations learned by CLIP, a recent foundation model, which achieves state-of-the-art AD results on CIFAR-10 and ImageNet in a zero-shot setting.},
 author = {Philipp Liznerski and Lukas Ruff and Robert A. Vandermeulen and Billy Joe Franks and Klaus Robert Muller and Marius Kloft},
 code = {https://github.com/liznerski/eoe},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281481864},
 pdf = {https://openreview.net/pdf?id=3v78awEzyB},
 review = {https://openreview.net/forum?id=3v78awEzyB},
 title = {Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images},
 url = {https://openreview.net/forum?id=3v78awEzyB},
 year = {2022}
}

@article{loaiza-ganem2022diagnosing,
 abstract = {Likelihood-based, or explicit, deep generative models use neural networks to construct flexible high-dimensional densities. This formulation directly contradicts the manifold hypothesis, which states that observed data lies on a low-dimensional manifold embedded in high-dimensional ambient space. In this paper we investigate the pathologies of maximum-likelihood training in the presence of this dimensionality mismatch. We formally prove that degenerate optima are achieved wherein the manifold itself is learned but not the distribution on it, a phenomenon we call manifold overfitting. We propose a class of two-step procedures consisting of a dimensionality reduction step followed by maximum-likelihood density estimation, and prove that they recover the data-generating distribution in the nonparametric regime, thus avoiding manifold overfitting. We also show that these procedures enable density estimation on the manifolds learned by implicit models, such as generative adversarial networks, hence addressing a major shortcoming of these models. Several recently proposed methods are instances of our two-step procedures; we thus unify, extend, and theoretically justify a large class of models.},
 author = {Gabriel Loaiza-Ganem and Brendan Leigh Ross and Jesse C Cresswell and Anthony L. Caterini},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/layer6ai-labs/two_step_zoo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4224105959},
 pdf = {https://openreview.net/pdf?id=0nEZCVshxS},
 review = {https://openreview.net/forum?id=0nEZCVshxS},
 title = {Diagnosing and Fixing Manifold Overfitting in Deep Generative Models},
 url = {https://openreview.net/forum?id=0nEZCVshxS},
 year = {2022}
}

@article{loh2023mitigating,
 author = {Charlotte Loh and Rumen Dangovski and Shivchander Sudalairaj and Seungwook Han and Ligong Han and Leonid Karlinsky and Marin Soljacic and Akash Srivastava},
 code = {https://github.com/clott3/BaM-SSL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=PRrKOaDQtQ},
 review = {https://openreview.net/forum?id=PRrKOaDQtQ},
 title = {Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging},
 url = {https://openreview.net/forum?id=PRrKOaDQtQ},
 year = {2023}
}

@article{longa2023graph,
 abstract = {Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.},
 author = {Antonio Longa and Veronica Lachi and Gabriele Santin and Monica Bianchini and Bruno Lepri and Pietro Lio and franco scarselli and Andrea Passerini},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320340942},
 pdf = {https://openreview.net/pdf?id=pHCdMat0gI},
 review = {https://openreview.net/forum?id=pHCdMat0gI},
 title = {Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities},
 url = {https://openreview.net/forum?id=pHCdMat0gI},
 year = {2023}
}

@article{look2022cheap,
 abstract = {Graph neural networks are often used to model interacting dynamical systems since they gracefully scale to systems with a varying and high number of agents. While there has been much progress made for deterministic interacting systems, modeling is much more challenging for stochastic systems in which one is interested in obtaining a predictive distribution over future trajectories. Existing methods are either computationally slow since they rely on Monte Carlo sampling or make simplifying assumptions such that the predictive distribution is unimodal. In this work, we present a deep state-space model which employs graph neural networks in order to model the underlying interacting dynamical system. The predictive distribution is multimodal and has the form of a Gaussian mixture model, where the moments of the Gaussian components can be computed via deterministic moment matching rules. Our moment matching scheme can be exploited for sample-free inference, leading to more efficient and stable training compared to Monte Carlo alternatives. Furthermore, we propose structured approximations to the covariance matrices of the Gaussian components in order to scale up to systems with many agents. We benchmark our novel framework on two challenging autonomous driving datasets. Both confirm the benefits of our method compared to state-of-the-art methods. We further demonstrate the usefulness of our individual contributions in a carefully designed ablation study and provide a detailed runtime analysis of our proposed covariance approximations. Finally, we empirically demonstrate the generalization ability of our method by evaluating its performance on unseen scenarios.},
 author = {Andreas Look and Barbara Rakitsch and Melih Kandemir and Jan Peters},
 code = {https://github.com/boschresearch/Deterministic-Graph-Deep-State-Space-Models},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4368754778},
 pdf = {https://openreview.net/pdf?id=dqgdBy4Uv5},
 review = {https://openreview.net/forum?id=dqgdBy4Uv5},
 title = {Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems},
 url = {https://openreview.net/forum?id=dqgdBy4Uv5},
 year = {2023}
}

@article{lopez,
 code = {https://github.com/AI4HealthUOL/SSSD},
 pdf = {https://openreview.net/pdf?id=hHiIbk7ApW},
 review = {https://openreview.net/forum?id=hHiIbk7ApW}
}

@article{lowy2022a,
 abstract = {Despite the success of large-scale empirical risk minimization (ERM) at achieving high accuracy across a variety of machine learning tasks, fair ERM is hindered by the incompatibility of fairness constraints with stochastic optimization. We consider the problem of fair classification with discrete sensitive attributes and potentially large models and data sets, requiring stochastic solvers. Existing in-processing fairness algorithms are either impractical in the large-scale setting because they require large batches of data at each iteration or they are not guaranteed to converge. In this paper, we develop the first stochastic in-processing fairness algorithm with guaranteed convergence. For demographic parity, equalized odds, and equal opportunity notions of fairness, we provide slight variations of our algorithm--called FERMI--and prove that each of these variations converges in stochastic optimization with any batch size. Empirically, we show that FERMI is amenable to stochastic solvers with multiple (non-binary) sensitive attributes and non-binary targets, performing well even with minibatch size as small as one. Extensive experiments show that FERMI achieves the most favorable tradeoffs between fairness violation and test accuracy across all tested setups compared with state-of-the-art baselines for demographic parity, equalized odds, equal opportunity. These benefits are especially significant with small batch sizes and for non-binary classification with large number of sensitive attributes, making FERMI a practical, scalable fairness algorithm. The code for all of the experiments in this paper is available at: https://github.com/optimization-for-data-driven-science/FERMI.},
 author = {Andrew Lowy and Sina Baharlouei and Rakesh Pavan and Meisam Razaviyayn and Ahmad Beirami},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/optimization-for-data-driven-science/FERMI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4296157137},
 pdf = {https://openreview.net/pdf?id=P9Cj6RJmN2},
 review = {https://openreview.net/forum?id=P9Cj6RJmN2},
 title = {A Stochastic Optimization Framework for Fair Risk Minimization},
 url = {https://openreview.net/forum?id=P9Cj6RJmN2},
 year = {2022}
}

@article{lu2022domaininvariant,
 abstract = {Deep learning has achieved great success in the past few years. However, the performance of deep learning is likely to impede in face of non-IID situations. Domain generalization (DG) enables a model to generalize to an unseen test distribution, i.e., to learn domain-invariant representations. In this paper, we argue that domain-invariant features should be originating from both internal and mutual sides. Internal invariance means that the features can be learned with a single domain and the features capture intrinsic semantics of data, i.e., the property within a domain, which is agnostic to other domains. Mutual invariance means that the features can be learned with multiple domains (cross-domain) and the features contain common information, i.e., the transferable features w.r.t. other domains. We then propose DIFEX for Domain-Invariant Feature EXploration. DIFEX employs a knowledge distillation framework to capture the high-level Fourier phase as the internally-invariant features and learn cross-domain correlation alignment as the mutually-invariant features. We further design an exploration loss to increase the feature diversity for better generalization. Extensive experiments on both time-series and visual benchmarks demonstrate that the proposed DIFEX achieves state-of-the-art performance.},
 author = {Wang Lu and Jindong Wang and Haoliang Li and Yiqiang Chen and Xing Xie},
 code = {https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4288054906},
 pdf = {https://openreview.net/pdf?id=0xENE7HiYm},
 review = {https://openreview.net/forum?id=0xENE7HiYm},
 title = {Domain-invariant Feature Exploration for Domain Generalization},
 url = {https://openreview.net/forum?id=0xENE7HiYm},
 year = {2022}
}

@article{lu2022indiscriminate,
 abstract = {Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting "poisoned" data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.},
 author = {Yiwei Lu and Gautam Kamath and Yaoliang Yu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4224281092},
 pdf = {https://openreview.net/pdf?id=x4hmIsWu7e},
 review = {https://openreview.net/forum?id=x4hmIsWu7e},
 title = {Indiscriminate Data Poisoning Attacks on Neural Networks},
 url = {https://openreview.net/forum?id=x4hmIsWu7e},
 year = {2022}
}

@article{lu2023challenges,
 abstract = {Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform existing offline RL methods and establish competitive baselines for continuous control in the visual domain. We rigorously evaluate these algorithms and perform an empirical evaluation of the differences between state-of-the-art model-based and model-free offline RL methods for continuous control from visual observations. All code and data used in this evaluation are open-sourced to facilitate progress in this domain.},
 author = {Cong Lu and Philip J. Ball and Tim G. J. Rudner and Jack Parker-Holder and Michael A Osborne and Yee Whye Teh},
 code = {https://github.com/conglu1997/v-d4rl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282813522},
 pdf = {https://openreview.net/pdf?id=1QqIfGZOWu},
 review = {https://openreview.net/forum?id=1QqIfGZOWu},
 title = {Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations},
 url = {https://openreview.net/forum?id=1QqIfGZOWu},
 year = {2023}
}

@article{lu2023fmicl,
 author = {Yiwei Lu and Guojun Zhang and Sun Sun and Hongyu Guo and Yaoliang Yu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ZD03VUZmRx},
 review = {https://openreview.net/forum?id=ZD03VUZmRx},
 title = {\$f\$-{MICL}: Understanding and Generalizing Info{NCE}-based Contrastive Learning},
 url = {https://openreview.net/forum?id=ZD03VUZmRx},
 year = {2023}
}

@article{lu2023using,
 abstract = {We address the problem of evaluating the quality of self-supervised learning (SSL) models without access to supervised labels, while being agnostic to the architecture, learning algorithm or data manipulation used during training. We argue that representations can be evaluated through the lens of expressiveness and learnability. We propose to use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to assess learnability. CL is measured in terms of the performance of a KNN classifier trained to predict labels obtained by clustering the representations with K-means. We thus combine CL and ID into a single predictor -- CLID. Through a large-scale empirical study with a diverse family of SSL algorithms, we find that CLID better correlates with in-distribution model performance than other competing recent evaluation schemes. We also benchmark CLID on out-of-domain generalization, where CLID serves as a predictor of the transfer performance of SSL models on several visual classification tasks, yielding improvements with respect to the competing baselines.},
 author = {Yuchen Lu and Zhen Liu and Aristide Baratin and Romain Laroche and Aaron Courville and Alessandro Sordoni},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281967464},
 pdf = {https://openreview.net/pdf?id=BxdrpnRHNh},
 review = {https://openreview.net/forum?id=BxdrpnRHNh},
 title = {Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods},
 url = {https://openreview.net/forum?id=BxdrpnRHNh},
 year = {2023}
}

@article{lu2024hyperspherical,
 author = {Jitao Lu and Danyang Wu and Feiping Nie and Rong Wang and Xuelong Li},
 code = {https://github.com/MoetaYuko/HPNC},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=z3ZlnaOM0d},
 review = {https://openreview.net/forum?id=z3ZlnaOM0d},
 title = {Hyperspherical Prototype Node Clustering},
 url = {https://openreview.net/forum?id=z3ZlnaOM0d},
 year = {2024}
}

@article{lukasik2022teachers,
 author = {Michal Lukasik and Srinadh Bhojanapalli and Aditya Krishna Menon and Sanjiv Kumar},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ph3AYXpwEb},
 review = {https://openreview.net/forum?id=ph3AYXpwEb},
 title = {Teacher{\textquoteright}s pet: understanding and mitigating biases in distillation},
 url = {https://openreview.net/forum?id=ph3AYXpwEb},
 year = {2022}
}

@article{lukasik2023improving,
 author = {Jovita Lukasik and Paul Gavrikov and Janis Keuper and Margret Keuper},
 code = {https://github.com/jovitalukasik/filter_freq_reg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=2wecNCpZ7Y},
 review = {https://openreview.net/forum?id=2wecNCpZ7Y},
 title = {Improving Native {CNN} Robustness with Filter Frequency Regularization},
 url = {https://openreview.net/forum?id=2wecNCpZ7Y},
 year = {2023}
}

@article{luo2023finitetime,
 abstract = {Decentralized Actor-Critic (AC) algorithms have been widely utilized for multi-agent reinforcement learning (MARL) and have achieved remarkable success. Apart from its empirical success, the theoretical convergence property of decentralized AC algorithms is largely unexplored. Most of the existing finite-time convergence results are derived based on either double-loop update or two-timescale step sizes rule, and this is the case even for centralized AC algorithm under a single-agent setting. In practice, the \emph{single-timescale} update is widely utilized, where actor and critic are updated in an alternating manner with step sizes being of the same order. In this work, we study a decentralized \emph{single-timescale} AC algorithm.Theoretically, using linear approximation for value and reward estimation, we show that the algorithm has sample complexity of $\tilde{\mathcal{O}}(\varepsilon^{-2})$ under Markovian sampling, which matches the optimal complexity with a double-loop implementation (here, $\tilde{\mathcal{O}}$ hides a logarithmic term). When we reduce to the single-agent setting, our result yields new sample complexity for centralized AC using a single-timescale update scheme. The central to establishing our complexity results is \emph{the hidden smoothness of the optimal critic variable} we revealed. We also provide a local action privacy-preserving version of our algorithm and its analysis. Finally, we conduct experiments to show the superiority of our algorithm over the existing decentralized AC algorithms.},
 author = {qijun luo and Xiao Li},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282946380},
 pdf = {https://openreview.net/pdf?id=KQRv0O8iW4},
 review = {https://openreview.net/forum?id=KQRv0O8iW4},
 title = {Finite-Time Analysis of Fully Decentralized Single-Timescale Actor-Critic},
 url = {https://openreview.net/forum?id=KQRv0O8iW4},
 year = {2023}
}

@article{luo2023neural,
 abstract = {3D shapes have complementary abstractions from low-level geometry to part-based hierarchies to languages, which convey different levels of information. This paper presents a unified framework to translate between pairs of shape abstractions: $\textit{Text}$ $\Longleftrightarrow$ $\textit{Point Cloud}$ $\Longleftrightarrow$ $\textit{Program}$. We propose $\textbf{Neural Shape Compiler}$ to model the abstraction transformation as a conditional generation process. It converts 3D shapes of three abstract types into unified discrete shape code, transforms each shape code into code of other abstract types through the proposed $\textit{ShapeCode Transformer}$, and decodes them to output the target shape abstraction. Point Cloud code is obtained in a class-agnostic way by the proposed $\textit{Point}$VQVAE. On Text2Shape, ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compiler shows strengths in $\textit{Text}$ $\Longrightarrow$ $\textit{Point Cloud}$, $\textit{Point Cloud}$ $\Longrightarrow$ $\textit{Text}$, $\textit{Point Cloud}$ $\Longrightarrow$ $\textit{Program}$, and Point Cloud Completion tasks. Additionally, Neural Shape Compiler benefits from jointly training on all heterogeneous data and tasks.},
 author = {Tiange Luo and Honglak Lee and Justin Johnson},
 code = {https://github.com/tiangeluo/ShapeCompiler},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4313197332},
 pdf = {https://openreview.net/pdf?id=gR9UVgH8PZ},
 review = {https://openreview.net/forum?id=gR9UVgH8PZ},
 title = {Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program},
 url = {https://openreview.net/forum?id=gR9UVgH8PZ},
 year = {2023}
}

@article{luo2023rignn,
 author = {Xiao Luo and Yusheng Zhao and Zhengyang Mao and Yifang Qin and Wei Ju and Ming Zhang and Yizhou Sun},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=qcCE4mC2jI},
 review = {https://openreview.net/forum?id=qcCE4mC2jI},
 title = {{RIGNN}: A Rationale Perspective for Semi-supervised Open-world Graph Classification},
 url = {https://openreview.net/forum?id=qcCE4mC2jI},
 year = {2023}
}

@article{luzi2024boomerang,
 abstract = {Diffusion models can be viewed as mapping points in a high-dimensional latent space onto a low-dimensional learned manifold, typically an image manifold. The intermediate values between the latent space and image manifold can be interpreted as noisy images which are determined by the noise scheduling scheme employed during pre-training. We exploit this interpretation to introduce Boomerang, a local image manifold sampling approach using the dynamics of diffusion models. We call it Boomerang because we first add noise to an input image, moving it closer to the latent space, then bring it back to the image space through diffusion dynamics. We use this method to generate images which are similar, but nonidentical, to the original input images on the image manifold. We are able to set how close the generated image is to the original based on how much noise we add. Additionally, the generated images have a degree of stochasticity, allowing us to locally sample as many times as we want without repetition. We show three applications for which Boomerang can be used. First, we provide a framework for constructing privacy-preserving datasets having controllable degrees of anonymity. Second, we show how to use Boomerang for data augmentation while staying on the image manifold. Third, we introduce a framework for image super-resolution with 8x upsampling. Boomerang does not require any modification to the training of diffusion models and can be used with pretrained models on a single, inexpensive GPU.},
 author = {Lorenzo Luzi and Paul M Mayer and Josue Casco-Rodriguez and Ali Siahkoohi and Richard Baraniuk},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307206816},
 pdf = {https://openreview.net/pdf?id=NYdThkjNW1},
 review = {https://openreview.net/forum?id=NYdThkjNW1},
 title = {Boomerang: Local sampling on image manifolds using diffusion models},
 url = {https://openreview.net/forum?id=NYdThkjNW1},
 year = {2024}
}

@article{m,
 code = {https://github.com/luis-mueller/probing-graph-transformers},
 pdf = {https://openreview.net/pdf?id=HhbqHBBrfZ},
 review = {https://openreview.net/forum?id=HhbqHBBrfZ}
}

@article{m,
 code = {https://github.com/tamaramueller/population_graphs},
 pdf = {https://openreview.net/pdf?id=TTRDCVnbjI},
 review = {https://openreview.net/forum?id=TTRDCVnbjI}
}

@article{m,
 badge = {Survey},
 pdf = {https://openreview.net/pdf?id=sWlHhfijcS},
 review = {https://openreview.net/forum?id=sWlHhfijcS}
}

@article{m,
 code = {https://github.com/XarwinM/competence_estimation},
 pdf = {https://openreview.net/pdf?id=TSy0vuwQFN},
 review = {https://openreview.net/forum?id=TSy0vuwQFN}
}

@article{m,
 code = {https://github.com/balintmate/boltzmann-interpolations},
 pdf = {https://openreview.net/pdf?id=TH6YrEcbth},
 review = {https://openreview.net/forum?id=TH6YrEcbth}
}

@article{ma2023partitionbased,
 abstract = {We study the problem of semi-supervised learning with Graph Neural Networks (GNNs) in an active learning setup. We propose GraphPart, a novel partition-based active learning approach for GNNs. GraphPart first splits the graph into disjoint partitions and then selects representative nodes within each partition to query. The proposed method is motivated by a novel analysis of the classification error under realistic smoothness assumptions over the graph and the node features. Extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing active learning methods for GNNs under a wide range of annotation budget constraints. In addition, the proposed method does not introduce additional hyperparameters, which is crucial for model training, especially in the active learning setting where a labeled validation set may not be available.},
 author = {Jiaqi Ma and Ziqiao Ma and Joyce Chai and Qiaozhu Mei},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4226077070},
 pdf = {https://openreview.net/pdf?id=e0xaRylNuT},
 review = {https://openreview.net/forum?id=e0xaRylNuT},
 title = {Partition-Based Active Learning for Graph Neural Networks},
 url = {https://openreview.net/forum?id=e0xaRylNuT},
 year = {2023}
}

@article{ma2023understanding,
 abstract = {Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks have smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.},
 author = {Avery Ma and Yangchen Pan and Amir-massoud Farahmand},
 badge = {Featured},
 code = {https://github.com/averyma/opt-robust},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4385848802},
 pdf = {https://openreview.net/pdf?id=ed8SkMdYFT},
 review = {https://openreview.net/forum?id=ed8SkMdYFT},
 title = {Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods},
 url = {https://openreview.net/forum?id=ed8SkMdYFT},
 year = {2023}
}

@article{magner2022fast,
 author = {Abram Magner and Carolyn S Kaminski and Petko Bogdanov},
 code = {https://github.com/carolynk/fastclock},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=k4iWTEdUSF},
 review = {https://openreview.net/forum?id=k4iWTEdUSF},
 title = {Fast and Accurate Spreading Process Temporal Scale Estimation},
 url = {https://openreview.net/forum?id=k4iWTEdUSF},
 year = {2022}
}

@article{maharana2024exposing,
 abstract = {As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, CocoCon, where we create contrast sets by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art vision-language models suffer from a surprisingly high degree of inconsistent behavior across tasks, especially for more heterogeneous tasks. To alleviate this issue, we propose a rank correlation-based auxiliary training objective, computed over large automatically created cross-task contrast sets, that improves the multi-task consistency of large unified models while retaining their original accuracy on downstream tasks.},
 author = {Adyasha Maharana and Amita Kamath and Christopher Clark and Mohit Bansal and Aniruddha Kembhavi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4361229338},
 pdf = {https://openreview.net/pdf?id=ue9igTDLN2},
 review = {https://openreview.net/forum?id=ue9igTDLN2},
 title = {Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models},
 url = {https://openreview.net/forum?id=ue9igTDLN2},
 year = {2024}
}

@article{mahdavi2023towards,
 abstract = {In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data generation. Finally, we propose an attention-based 2WL-graph neural network (GNN) processor which complements message-passing GNNs so their combination outperforms the state-of-the-art model by a 3% margin averaged over all algorithms. Our code is available at: \url{https://github.com/smahdavi4/clrs}.},
 author = {Sadegh Mahdavi and Kevin Swersky and Thomas Kipf and Milad Hashemi and Christos Thrampoulidis and Renjie Liao},
 code = {https://github.com/smahdavi4/clrs},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308164970},
 pdf = {https://openreview.net/pdf?id=xkrtvHlp3P},
 review = {https://openreview.net/forum?id=xkrtvHlp3P},
 title = {Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks},
 url = {https://openreview.net/forum?id=xkrtvHlp3P},
 year = {2023}
}

@article{maheshwari2023fairgrad,
 abstract = {We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision. FairGrad is available as a PyPI package at - https://pypi.org/project/fairgrad},
 author = {Gaurav Maheshwari and Micha{\"e}l Perrot},
 code = {https://pypi.org/project/fairgrad},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283398163},
 pdf = {https://openreview.net/pdf?id=0f8tU3QwWD},
 review = {https://openreview.net/forum?id=0f8tU3QwWD},
 title = {FairGrad: Fairness Aware Gradient Descent},
 url = {https://openreview.net/forum?id=0f8tU3QwWD},
 year = {2023}
}

@article{maile2022structural,
 author = {Kaitlin Maile and Luga Herv{\'e} and Dennis George Wilson},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 pdf = {https://openreview.net/pdf?id=gzhEGhcsnN},
 review = {https://openreview.net/forum?id=gzhEGhcsnN},
 title = {Structural Learning in Artificial Neural Networks: A Neural Operator Perspective},
 url = {https://openreview.net/forum?id=gzhEGhcsnN},
 year = {2022}
}

@article{makar2023fairness,
 abstract = {Robustness to distribution shift and fairness have independently emerged as two important desiderata required of modern machine learning models. While these two desiderata seem related, the connection between them is often unclear in practice. Here, we discuss these connections through a causal lens, focusing on anti-causal prediction tasks, where the input to a classifier (e.g., an image) is assumed to be generated as a function of the target label and the protected attribute. By taking this perspective, we draw explicit connections between a common fairness criterion - separation - and a common notion of robustness - risk invariance. These connections provide new motivation for applying the separation criterion in anticausal settings, and inform old discussions regarding fairness-performance tradeoffs. In addition, our findings suggest that robustness-motivated approaches can be used to enforce separation, and that they often work better in practice than methods designed to directly enforce separation. Using a medical dataset, we empirically validate our findings on the task of detecting pneumonia from X-rays, in a setting where differences in prevalence across sex groups motivates a fairness mitigation. Our findings highlight the importance of considering causal structure when choosing and enforcing fairness criteria.},
 author = {Maggie Makar and Alexander D'Amour},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297901640},
 pdf = {https://openreview.net/pdf?id=mrTXGDZns2},
 review = {https://openreview.net/forum?id=mrTXGDZns2},
 title = {Fairness and robustness in anti-causal prediction},
 url = {https://openreview.net/forum?id=mrTXGDZns2},
 year = {2023}
}

@article{makarenko2023adaptive,
 abstract = {We propose Adaptive Compressed Gradient Descent (AdaCGD) - a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022), which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level, or adapt between two extremes only, our proposal is to perform a much finer adaptation. In particular, we allow the user to choose any number of arbitrarily chosen contractive compression mechanisms, such as Top-K sparsification with a user-defined selection of sparsification levels K, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., we allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods.},
 author = {Maksim Makarenko and Elnur Gasanov and Abdurakhmon Sadiev and Rustem Islamov and Peter Richt{\'a}rik},
 code = {https://github.com/makamoa/aclag/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308015852},
 pdf = {https://openreview.net/pdf?id=Rb6VDOHebB},
 review = {https://openreview.net/forum?id=Rb6VDOHebB},
 title = {Adaptive Compression for Communication-Efficient Distributed Training},
 url = {https://openreview.net/forum?id=Rb6VDOHebB},
 year = {2023}
}

@article{makino2023detecting,
 author = {Taro Makino and Yixin Wang and Krzysztof J. Geras and Kyunghyun Cho},
 code = {https://github.com/nyukat/incidental-correlation},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=QoRo9QmOAr},
 review = {https://openreview.net/forum?id=QoRo9QmOAr},
 title = {Detecting incidental correlation in multimodal learning via latent variable modeling},
 url = {https://openreview.net/forum?id=QoRo9QmOAr},
 year = {2023}
}

@article{malhotra2023dropped,
 author = {Aakarsh Malhotra and Mayank Vatsa and Richa Singh},
 code = {https://github.com/aakarshmalhotra/DST.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=myjAVQrRxS},
 review = {https://openreview.net/forum?id=myjAVQrRxS},
 title = {Dropped Scheduled Task: Mitigating Negative Transfer in Multi-task Learning using Dynamic Task Dropping},
 url = {https://openreview.net/forum?id=myjAVQrRxS},
 year = {2023}
}

@article{mandal2023a,
 abstract = {Comparison-based learning addresses the problem of learning when, instead of explicit features or pairwise similarities, one only has access to comparisons of the form: \emph{Object $A$ is more similar to $B$ than to $C$.} Recently, it has been shown that, in Hierarchical Clustering, single and complete linkage can be directly implemented using only such comparisons while several algorithms have been proposed to emulate the behaviour of average linkage. Hence, finding hierarchies (or dendrograms) using only comparisons is a well understood problem. However, evaluating their meaningfulness when no ground-truth nor explicit similarities are available remains an open question. In this paper, we bridge this gap by proposing a new revenue function that allows one to measure the goodness of dendrograms using only comparisons. We show that this function is closely related to Dasgupta's cost for hierarchical clustering that uses pairwise similarities. On the theoretical side, we use the proposed revenue function to resolve the open problem of whether one can approximately recover a latent hierarchy using few triplet comparisons. On the practical side, we present principled algorithms for comparison-based hierarchical clustering based on the maximisation of the revenue and we empirically compare them with existing methods.},
 author = {Aishik Mandal and Micha{\"e}l Perrot and Debarghya Ghoshdastidar},
 code = {https://github.com/jitaishik/Revenue_ComparisonHC},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4311607068},
 pdf = {https://openreview.net/pdf?id=QzWr4w8PXx},
 review = {https://openreview.net/forum?id=QzWr4w8PXx},
 title = {A Revenue Function for Comparison-Based Hierarchical Clustering},
 url = {https://openreview.net/forum?id=QzWr4w8PXx},
 year = {2023}
}

@article{mansilla2024demographicallyinformed,
 author = {Lucas Mansilla and Estanislao Claucich and Rodrigo Echeveste and Diego H Milone and Enzo Ferrante},
 code = {https://github.com/lamansilla/DIPDI-Biases},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=8W6IDyFZgC},
 review = {https://openreview.net/forum?id=8W6IDyFZgC},
 title = {Demographically-Informed Prediction Discrepancy Index: Early Warnings of Demographic Biases for Unlabeled Populations},
 url = {https://openreview.net/forum?id=8W6IDyFZgC},
 year = {2024}
}

@article{manupriya2024mmdregularized,
 abstract = {We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our work is motivated by the observation that the literature on UOT is focused on regularization based on $\phi$-divergence (e.g., KL divergence). Despite the popularity of MMD, its role as a regularizer in the context of UOT seems less understood. We begin by deriving a specific dual of MMD-regularized UOT (MMD-UOT), which helps us prove several useful properties. One interesting outcome of this duality result is that MMD-UOT induces novel metrics, which not only lift the ground metric like the Wasserstein but are also sample-wise efficient to estimate like the MMD. Further, for real-world applications involving non-discrete measures, we present an estimator for the transport plan that is supported only on the given ($m$) samples. Under certain conditions, we prove that the estimation error with this finitely-supported transport plan is also $\mathcal{O}(1/\sqrt{m})$. As far as we know, such error bounds that are free from the curse of dimensionality are not known for $\phi$-divergence regularized UOT. Finally, we discuss how the proposed estimator can be computed efficiently using accelerated gradient descent. Our experiments show that MMD-UOT consistently outperforms popular baselines, including KL-regularized UOT and MMD, in diverse machine learning applications.},
 author = {Piyushi Manupriya and SakethaNath Jagarlapudi and Pratik Jawanpuria},
 code = {https://github.com/Piyushi-0/MMD-reg-OT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308609496},
 pdf = {https://openreview.net/pdf?id=eN9CjU3h1b},
 review = {https://openreview.net/forum?id=eN9CjU3h1b},
 title = {MMD-Regularized Unbalanced Optimal Transport},
 url = {https://openreview.net/forum?id=eN9CjU3h1b},
 year = {2024}
}

@article{mao2023offline,
 abstract = {We study learning optimal policies from a logged dataset, i.e., offline RL, with function approximation. Despite the efforts devoted, existing algorithms with theoretic finite-sample guarantees typically assume exploratory data coverage or strong realizable function classes, which is hard to be satisfied in reality. While there are recent works that successfully tackle these strong assumptions, they either require the gap assumptions that only could be satisfied by part of MDPs or use the behavior regularization that makes the optimality of learned policy even intractable. To solve this challenge, we provide finite-sample guarantees for a simple algorithm based on marginalized importance sampling (MIS), showing that sample-efficient offline RL for general MDPs is possible with only a partial coverage dataset and weak realizable function classes given additional side information of a covering distribution. Furthermore, we demonstrate that the covering distribution trades off prior knowledge of the optimal trajectories against the coverage requirement of the dataset, revealing the effect of this inductive bias in the learning processes.},
 author = {Chenjie Mao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4377864600},
 pdf = {https://openreview.net/pdf?id=AfXq3x3X16},
 review = {https://openreview.net/forum?id=AfXq3x3X16},
 title = {Offline Reinforcement Learning with Additional Covering Distributions},
 url = {https://openreview.net/forum?id=AfXq3x3X16},
 year = {2023}
}

@article{marcos2022attribute,
 author = {Diego Marcos and Aike Potze and Wenjia Xu and Devis Tuia and Zeynep Akata},
 code = {https://github.com/aikepotze/AMIL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=nmFczdJtc2},
 review = {https://openreview.net/forum?id=nmFczdJtc2},
 title = {Attribute Prediction as Multiple Instance Learning},
 url = {https://openreview.net/forum?id=nmFczdJtc2},
 year = {2022}
}

@article{markovich2024qdc,
 abstract = {Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods.},
 author = {Thomas Markovich},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385210474},
 pdf = {https://openreview.net/pdf?id=uXGUSX8GoY},
 review = {https://openreview.net/forum?id=uXGUSX8GoY},
 title = {QDC: Quantum Diffusion Convolution Kernels on Graphs},
 url = {https://openreview.net/forum?id=uXGUSX8GoY},
 year = {2024}
}

@article{masters2023gps,
 abstract = {We present GPS++, a hybrid Message Passing Neural Network / Graph Transformer model for molecular property prediction. Our model integrates a well-tuned local message passing component and biased global attention with other key ideas from prior literature to achieve state-of-the-art results on large-scale molecular dataset PCQM4Mv2. Through a thorough ablation study we highlight the impact of individual components and find that nearly all of the model's performance can be maintained without any use of global self-attention, showing that message passing is still a competitive approach for 3D molecular property prediction despite the recent dominance of graph transformers. We also find that our approach is significantly more accurate than prior art when 3D positional information is not available.},
 author = {Dominic Masters and Josef Dean and Kerstin Klaeser and Zhiyi Li and Samuel Maddrell-Mander and Adam Sanders and Hatem Helal and Deniz Beker and Andrew William Fitzgibbon and Shenyang Huang and Ladislav Ramp{\'a}{\v{s}}ek and Dominique Beaini},
 code = {https://github.com/graphcore/ogb-lsc-pcqm4mv2},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319453706},
 pdf = {https://openreview.net/pdf?id=moVEUgJaHO},
 review = {https://openreview.net/forum?id=moVEUgJaHO},
 title = {GPS++: Reviving the Art of Message Passing for Molecular Property Prediction},
 url = {https://openreview.net/forum?id=moVEUgJaHO},
 year = {2023}
}

@article{mathieu2024bandits,
 abstract = {We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit problem with $k$ unknown reward distributions, which are heavy-tailed and corrupted by a history-independent adversary or Nature. To be specific, the reward obtained by playing an arm comes from corresponding heavy-tailed reward distribution with probability $1-\varepsilon \in (0.5,1]$ and an arbitrary corruption distribution of unbounded support with probability $\varepsilon \in [0,0.5)$. First, we provide $\textit{a problem-dependent lower bound on the regret}$ of any corrupted bandit algorithm. The lower bounds indicate that the corrupted bandit problem is harder than the classical stochastic bandit problem with sub-Gaussian or heavy-tail rewards. Following that, we propose a novel UCB-type algorithm for corrupted bandits, namely HubUCB, that builds on Huber's estimator for robust mean estimation. Leveraging a novel concentration inequality of Huber's estimator, we prove that HubUCB achieves a near-optimal regret upper bound. Since computing Huber's estimator has quadratic complexity, we further introduce a sequential version of Huber's estimator that exhibits linear complexity. We leverage this sequential estimator to design SeqHubUCB that enjoys similar regret guarantees while reducing the computational burden. Finally, we experimentally illustrate the efficiency of HubUCB and SeqHubUCB in solving corrupted bandits for different reward distributions and different levels of corruptions.},
 author = {Timoth{\'e}e Mathieu and Debabrota Basu and Odalric-Ambrym Maillard},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221142009},
 pdf = {https://openreview.net/pdf?id=oGIR0ic3jU},
 review = {https://openreview.net/forum?id=oGIR0ic3jU},
 title = {Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithm},
 url = {https://openreview.net/forum?id=oGIR0ic3jU},
 year = {2024}
}

@article{matoba2023benefits,
 author = {Kyle Matoba and Nikolaos Dimitriadis and Fran{\c{c}}ois Fleuret},
 code = {https://github.com/idiap/benefits-of-max-pooling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=YgeXqrH7gA},
 review = {https://openreview.net/forum?id=YgeXqrH7gA},
 title = {Benefits of Max Pooling in Neural Networks: Theoretical and Experimental Evidence},
 url = {https://openreview.net/forum?id=YgeXqrH7gA},
 year = {2023}
}

@article{matsubara2023sc,
 author = {Yoshitomo Matsubara and Ruihan Yang and Marco Levorato and Stephan Mandt},
 code = {https://github.com/yoshitomo-matsubara/sc2-benchmark},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=p28wv4G65d},
 review = {https://openreview.net/forum?id=p28wv4G65d},
 title = {{SC}2 Benchmark: Supervised Compression for Split Computing},
 url = {https://openreview.net/forum?id=p28wv4G65d},
 year = {2023}
}

@article{mccabe2023towards,
 abstract = {Neural operators have proven to be a promising approach for modeling spatiotemporal systems in the physical sciences. However, training these models for large systems can be quite challenging as they incur significant computational and memory expense -- these systems are often forced to rely on autoregressive time-stepping of the neural network to predict future temporal states. While this is effective in managing costs, it can lead to uncontrolled error growth over time and eventual instability. We analyze the sources of this autoregressive error growth using prototypical neural operator models for physical systems and explore ways to mitigate it. We introduce architectural and application-specific improvements that allow for careful control of instability-inducing operations within these models without inflating the compute/memory expense. We present results on several scientific systems that include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution global weather forecasting system. We demonstrate that applying our design principles to neural operators leads to significantly lower errors for long-term forecasts as well as longer time horizons without qualitative signs of divergence compared to the original models for these systems. We open-source our \href{https://github.com/mikemccabe210/stabilizing_neural_operators}{code} for reproducibility.},
 author = {Michael McCabe and Peter Harrington and Shashank Subramanian and Jed Brown},
 code = {https://github.com/mikemccabe210/stabilizing_neural_operators},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4381564169},
 pdf = {https://openreview.net/pdf?id=RFfUUtKYOG},
 review = {https://openreview.net/forum?id=RFfUUtKYOG},
 title = {Towards Stability of Autoregressive Neural Operators},
 url = {https://openreview.net/forum?id=RFfUUtKYOG},
 year = {2023}
}

@article{mccarter2023the,
 abstract = {Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering protection from the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.},
 author = {Calvin McCarter},
 code = {https://github.com/calvinmccarter/kditransform},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386907212},
 pdf = {https://openreview.net/pdf?id=6OEcDKZj5j},
 review = {https://openreview.net/forum?id=6OEcDKZj5j},
 title = {The Kernel Density Integral Transformation},
 url = {https://openreview.net/forum?id=6OEcDKZj5j},
 year = {2023}
}

@article{mcgrath2023when,
 abstract = {As machine learning (ML) models are increasingly being employed to assist human decision makers, it becomes critical to provide these decision makers with relevant inputs which can help them decide if and how to incorporate model predictions into their decision making. For instance, communicating the uncertainty associated with model predictions could potentially be helpful in this regard. However, there is little to no research that systematically explores if and how conveying predictive uncertainty impacts decision making. In this work, we carry out user studies to systematically assess how people respond to different types of predictive uncertainty i.e., posterior predictive distributions with different shapes and variances, in the context of ML assisted decision making. To the best of our knowledge, this work marks one of the first attempts at studying this question. Our results demonstrate that people are more likely to agree with a model prediction when they observe the corresponding uncertainty associated with the prediction. This finding holds regardless of the properties (shape or variance) of predictive uncertainty (posterior predictive distribution), suggesting that uncertainty is an effective tool for persuading humans to agree with model predictions. Furthermore, we also find that other factors such as domain expertise and familiarity with ML also play a role in determining how someone interprets and incorporates predictive uncertainty into their decision making.},
 author = {Sean McGrath and Parth Mehta and Alexandra Zytek and Isaac Lage and Himabindu Lakkaraju},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3098251333},
 pdf = {https://openreview.net/pdf?id=pbs22kJmEO},
 review = {https://openreview.net/forum?id=pbs22kJmEO},
 title = {When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making.},
 url = {https://openreview.net/forum?id=pbs22kJmEO},
 year = {2023}
}

@article{mcinroe2024multihorizon,
 author = {Trevor McInroe and Lukas Sch{\"a}fer and Stefano V Albrecht},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=fovUNTilp9},
 review = {https://openreview.net/forum?id=fovUNTilp9},
 title = {Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning},
 url = {https://openreview.net/forum?id=fovUNTilp9},
 year = {2024}
}

@article{mckenzie2023inverse,
 abstract = {Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.},
 author = {Ian R. McKenzie and Alexander Lyzhov and Michael Martin Pieler and Alicia Parrish and Aaron Mueller and Ameya Prabhu and Euan McLean and Xudong Shen and Joe Cavanagh and Andrew George Gritsevskiy and Derik Kauffman and Aaron T. Kirtland and Zhengping Zhou and Yuhui Zhang and Sicong Huang and Daniel Wurgaft and Max Weiss and Alexis Ross and Gabriel Recchia and Alisa Liu and Jiacheng Liu and Tom Tseng and Tomasz Korbak and Najoung Kim and Samuel R. Bowman and Ethan Perez},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4381245716},
 pdf = {https://openreview.net/pdf?id=DwgRm72GQF},
 review = {https://openreview.net/forum?id=DwgRm72GQF},
 title = {Inverse Scaling: When Bigger Isn't Better},
 url = {https://openreview.net/forum?id=DwgRm72GQF},
 year = {2023}
}

@article{mehta2023differentially,
 abstract = {Leveraging transfer learning has recently been shown to be an effective strategy for training large models with Differential Privacy (DP). Moreover, somewhat surprisingly, recent works have found that privately training just the last layer of a pre-trained model provides the best utility with DP. While past studies largely rely on algorithms like DP-SGD for training large models, in the specific case of privately learning from features, we observe that computational burden is low enough to allow for more sophisticated optimization schemes, including second-order methods. To that end, we systematically explore the effect of design parameters such as loss function and optimization algorithm. We find that, while commonly used logistic regression performs better than linear regression in the non-private setting, the situation is reversed in the private setting. We find that linear regression is much more effective than logistic regression from both privacy and computational aspects, especially at stricter epsilon values ($\epsilon < 1$). On the optimization side, we also explore using Newton's method, and find that second-order information is quite helpful even with privacy, although the benefit significantly diminishes with stricter privacy guarantees. While both methods use second-order information, least squares is effective at lower epsilons while Newton's method is effective at larger epsilon values. To combine the benefits of both, we propose a novel algorithm called DP-FC, which leverages feature covariance instead of the Hessian of the logistic regression loss and performs well across all $\epsilon$ values we tried. With this, we obtain new SOTA results on ImageNet-1k, CIFAR-100 and CIFAR-10 across all values of $\epsilon$ typically considered. Most remarkably, on ImageNet-1K, we obtain top-1 accuracy of 88\% under (8, $8 * 10^{-7}$)-DP and 84.3\% under (0.1, $8 * 10^{-7}$)-DP.},
 author = {Harsh Mehta and Walid Krichene and Abhradeep Guha Thakurta and Alexey Kurakin and Ashok Cutkosky},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4310271776},
 pdf = {https://openreview.net/pdf?id=Cj6pLclmwT},
 review = {https://openreview.net/forum?id=Cj6pLclmwT},
 title = {Differentially Private Image Classification from Features},
 url = {https://openreview.net/forum?id=Cj6pLclmwT},
 year = {2023}
}

@article{mehta2023separable,
 abstract = {Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires $O(k^2)$ time complexity with respect to the number of tokens (or patches) $k$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. $O(k)$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running $3.2\times$ faster on a mobile device. Our source code is available at: \url{https://github.com/apple/ml-cvnets}},
 author = {Sachin Mehta and Mohammad Rastegari},
 code = {https://github.com/apple/ml-cvnets},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320459518},
 pdf = {https://openreview.net/pdf?id=tBl4yBEjKi},
 review = {https://openreview.net/forum?id=tBl4yBEjKi},
 title = {Separable Self-attention for Mobile Vision Transformers},
 url = {https://openreview.net/forum?id=tBl4yBEjKi},
 year = {2023}
}

@article{mehta2023towards,
 author = {Harsh Mehta and Abhradeep Guha Thakurta and Alexey Kurakin and Ashok Cutkosky},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/google-research/google-research/tree/master/dp_transfer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=Uu8WwCFpQv},
 review = {https://openreview.net/forum?id=Uu8WwCFpQv},
 title = {Towards Large Scale Transfer Learning for Differentially Private Image Classification},
 url = {https://openreview.net/forum?id=Uu8WwCFpQv},
 year = {2023}
}

@article{melia2023rotationinvariant,
 abstract = {Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.},
 author = {Owen Melia and Eric M Jonas and Rebecca Willett},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385848247},
 pdf = {https://openreview.net/pdf?id=nYzhlFyjjd},
 review = {https://openreview.net/forum?id=nYzhlFyjjd},
 title = {Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds},
 url = {https://openreview.net/forum?id=nYzhlFyjjd},
 year = {2023}
}

@article{melnik2023benchmarks,
 abstract = {Physical reasoning is a crucial aspect in the development of general AI systems, given that human learning starts with interacting with the physical world before progressing to more complex concepts. Although researchers have studied and assessed the physical reasoning of AI approaches through various specific benchmarks, there is no comprehensive approach to evaluating and measuring progress. Therefore, we aim to offer an overview of existing benchmarks and their solution approaches and propose a unified perspective for measuring the physical reasoning capacity of AI systems. We select benchmarks that are designed to test algorithmic performance in physical reasoning tasks. While each of the selected benchmarks poses a unique challenge, their ensemble provides a comprehensive proving ground for an AI generalist agent with a measurable skill level for various physical reasoning concepts. This gives an advantage to such an ensemble of benchmarks over other holistic benchmarks that aim to simulate the real world by intertwining its complexity and many concepts. We group the presented set of physical reasoning benchmarks into subcategories so that more narrow generalist AI agents can be tested first on these groups.},
 author = {Andrew Melnik and Robin Schiewer and Moritz Lange and Andrei Ioan Muresanu and mozhgan saeidi and Animesh Garg and Helge Ritter},
 badge = {Survey},
 code = {https://github.com/ndrwmlnk/Awesome-Benchmarks-for-Physical-Reasoning-AI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4389983566},
 pdf = {https://openreview.net/pdf?id=cHroS8VIyN},
 review = {https://openreview.net/forum?id=cHroS8VIyN},
 title = {Benchmarks for Physical Reasoning AI},
 url = {https://openreview.net/forum?id=cHroS8VIyN},
 year = {2023}
}

@article{mendez2023how,
 abstract = {A major goal of artificial intelligence (AI) is to create an agent capable of acquiring a general understanding of the world. Such an agent would require the ability to continually accumulate and build upon its knowledge as it encounters new experiences. Lifelong or continual learning addresses this setting, whereby an agent faces a continual stream of problems and must strive to capture the knowledge necessary for solving each new task it encounters. If the agent is capable of accumulating knowledge in some form of compositional representation, it could then selectively reuse and combine relevant pieces of knowledge to construct novel solutions. Despite the intuitive appeal of this simple idea, the literatures on lifelong learning and compositional learning have proceeded largely separately. In an effort to promote developments that bridge between the two fields, this article surveys their respective research landscapes and discusses existing and future connections between them.},
 author = {Jorge A Mendez and ERIC EATON},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4286235998},
 pdf = {https://openreview.net/pdf?id=VynY6Bk03b},
 review = {https://openreview.net/forum?id=VynY6Bk03b},
 title = {How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition},
 url = {https://openreview.net/forum?id=VynY6Bk03b},
 year = {2023}
}

@article{menegaux2023selfattention,
 abstract = {We introduce a novel self-attention mechanism, which we call CSA (Chromatic Self-Attention), which extends the notion of attention scores to attention _filters_, independently modulating the feature channels. We showcase CSA in a fully-attentional graph Transformer CGT (Chromatic Graph Transformer) which integrates both graph structural information and edge features, completely bypassing the need for local message-passing components. Our method flexibly encodes graph structure through node-node interactions, by enriching the original edge features with a relative positional encoding scheme. We propose a new scheme based on random walks that encodes both structural and positional information, and show how to incorporate higher-order topological information, such as rings in molecular graphs. Our approach achieves state-of-the-art results on the ZINC benchmark dataset, while providing a flexible framework for encoding graph structure and incorporating higher-order topology.},
 author = {Romain Menegaux and Emmanuel Jehanno and Margot Selosse and Julien Mairal},
 code = {https://github.com/inria-thoth/csa},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4366835585},
 pdf = {https://openreview.net/pdf?id=3dQCNqqv2d},
 review = {https://openreview.net/forum?id=3dQCNqqv2d},
 title = {Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers},
 url = {https://openreview.net/forum?id=3dQCNqqv2d},
 year = {2023}
}

@article{miahi2023resmax,
 author = {Erfan Miahi and Revan MacQueen and Alex Ayoub and Abbas Masoumzadeh and Martha White},
 code = {https://github.com/RevanMacQueen/RL-Exploration},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=wzzrs5QH5k},
 review = {https://openreview.net/forum?id=wzzrs5QH5k},
 title = {Resmax: An Alternative Soft-Greedy Operator for Reinforcement Learning},
 url = {https://openreview.net/forum?id=wzzrs5QH5k},
 year = {2023}
}

@article{mialon2023augmented,
 abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
 author = {Gr{\'e}goire Mialon and Roberto Dessi and Maria Lomeli and Christoforos Nalmpantis and Ramakanth Pasunuru and Roberta Raileanu and Baptiste Roziere and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4321177655},
 pdf = {https://openreview.net/pdf?id=jh7wH2AzKK},
 review = {https://openreview.net/forum?id=jh7wH2AzKK},
 title = {Augmented Language Models: a Survey},
 url = {https://openreview.net/forum?id=jh7wH2AzKK},
 year = {2023}
}

@article{michel2023regret,
 author = {Thomas Michel and Hossein Hajiabolhassan and Ronald Ortner},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=QnT41ZGNh9},
 review = {https://openreview.net/forum?id=QnT41ZGNh9},
 title = {Regret Bounds for Satisficing in Multi-Armed Bandit Problems},
 url = {https://openreview.net/forum?id=QnT41ZGNh9},
 year = {2023}
}

@article{miret2023the,
 abstract = {We present the Open MatSci ML Toolkit: a flexible, self-contained, and scalable Python-based framework to apply deep learning models and methods on scientific data with a specific focus on materials science and the OpenCatalyst Dataset. Our toolkit provides: 1. A scalable machine learning workflow for materials science leveraging PyTorch Lightning, which enables seamless scaling across different computation capabilities (laptop, server, cluster) and hardware platforms (CPU, GPU, XPU). 2. Deep Graph Library (DGL) support for rapid graph neural network prototyping and development. By publishing and sharing this toolkit with the research community via open-source release, we hope to: 1. Lower the entry barrier for new machine learning researchers and practitioners that want to get started with the OpenCatalyst dataset, which presently comprises the largest computational materials science dataset. 2. Enable the scientific community to apply advanced machine learning tools to high-impact scientific challenges, such as modeling of materials behavior for clean energy applications. We demonstrate the capabilities of our framework by enabling three new equivariant neural network models for multiple OpenCatalyst tasks and arrive at promising results for compute scaling and model performance.},
 author = {Santiago Miret and Kin Long Kelvin Lee and Carmelo Gonzales and Marcel Nassar and Matthew Spellings},
 code = {https://github.com/IntelLabs/matsciml},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307936700},
 pdf = {https://openreview.net/pdf?id=QBMyDZsPMd},
 review = {https://openreview.net/forum?id=QBMyDZsPMd},
 title = {The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science},
 url = {https://openreview.net/forum?id=QBMyDZsPMd},
 year = {2023}
}

@article{mirman2022the,
 author = {Matthew B Mirman and Maximilian Baader and Martin Vechev},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=fsacLLU35V},
 review = {https://openreview.net/forum?id=fsacLLU35V},
 title = {The Fundamental Limits of Neural Networks for Interval Certified Robustness},
 url = {https://openreview.net/forum?id=fsacLLU35V},
 year = {2022}
}

@article{miscouridou2023coxhawkes,
 abstract = {Hawkes processes are point process models that have been used to capture self-excitatory behavior in social interactions, neural activity, earthquakes and viral epidemics. They can model the occurrence of the times and locations of events. Here we develop a new class of spatiotemporal Hawkes processes that can capture both triggering and clustering behavior and we provide an efficient method for performing inference. We use a log-Gaussian Cox process (LGCP) as prior for the background rate of the Hawkes process which gives arbitrary flexibility to capture a wide range of underlying background effects (for infectious diseases these are called endemic effects). The Hawkes process and LGCP are computationally expensive due to the former having a likelihood with quadratic complexity in the number of observations and the latter involving inversion of the precision matrix which is cubic in observations. Here we propose a novel approach to perform MCMC sampling for our Hawkes process with LGCP background, using pre-trained Gaussian Process generators which provide direct and cheap access to samples during inference. We show the efficacy and flexibility of our approach in experiments on simulated data and use our methods to uncover the trends in a dataset of reported crimes in the US.},
 author = {Xenia Miscouridou and Samir Bhatt and George Mohler and Seth Flaxman and Swapnil Mishra},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320341744},
 pdf = {https://openreview.net/pdf?id=xzCDD9i4IZ},
 review = {https://openreview.net/forum?id=xzCDD9i4IZ},
 title = {Cox-Hawkes: doubly stochastic spatiotemporal Poisson processes},
 url = {https://openreview.net/forum?id=xzCDD9i4IZ},
 year = {2023}
}

@article{mishra2022objectaware,
 abstract = {A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both object and scene level semantic representations. Using this approach, which we call object-aware cropping, results in significant improvements over scene cropping on classification and object detection benchmarks. For example, on OpenImages, our approach achieves an improvement of 8.8% mAP over random scene-level cropping using MoCo-v2 based pre-training. We also show significant improvements on COCO and PASCAL-VOC object detection and segmentation tasks over the state-of-the-art self-supervised learning approaches. Our approach is efficient, simple and general, and can be used in most existing contrastive and non-contrastive self-supervised learning frameworks.},
 author = {Shlok Kumar Mishra and Anshul Shah and Ankan Bansal and Janit K Anjaria and Abhyuday Narayan Jagannatha and Abhishek Sharma and David Jacobs and Dilip Krishnan},
 badge = {Event: CoLLAs 2023},
 code = {https://github.com/shlokk/object-cropping-ssl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226549701},
 pdf = {https://openreview.net/pdf?id=WXgJN7A69g},
 review = {https://openreview.net/forum?id=WXgJN7A69g},
 title = {Object-Aware Cropping for Self-Supervised Learning},
 url = {https://openreview.net/forum?id=WXgJN7A69g},
 year = {2022}
}

@article{mistry2022exploring,
 author = {Bhumika Mistry and Katayoun Farrahi and Jonathon Hare},
 code = {https://github.com/bmistry4/nalm-division},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=HjelcW6wio},
 review = {https://openreview.net/forum?id=HjelcW6wio},
 title = {Exploring the Learning Mechanisms of Neural Division Modules},
 url = {https://openreview.net/forum?id=HjelcW6wio},
 year = {2022}
}

@article{mitton2023subgraph,
 abstract = {In this work we develop a new method, named Sub-graph Permutation Equivariant Networks (SPEN), which provides a framework for building graph neural networks that operate on sub-graphs, while using a base update function that is permutation equivariant, that are equivariant to a novel choice of automorphism group. Message passing neural networks have been shown to be limited in their expressive power and recent approaches to over come this either lack scalability or require structural information to be encoded into the feature space. The general framework presented here overcomes the scalability issues associated with global permutation equivariance by operating more locally on sub-graphs. In addition, through operating on sub-graphs the expressive power of higher-dimensional global permutation equivariant networks is improved; this is due to fact that two non-distinguishable graphs often contain distinguishable sub-graphs. Furthermore, the proposed framework only requires a choice of $k$-hops for creating ego-network sub-graphs and a choice of representation space to be used for each layer, which makes the method easily applicable across a range of graph based domains. We experimentally validate the method on a range of graph benchmark classification tasks, demonstrating statistically indistinguishable results from the state-of-the-art on six out of seven benchmarks. Further, we demonstrate that the use of local update functions offers a significant improvement in GPU memory over global methods.},
 author = {Joshua Mitton and Roderick Murray-Smith},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226135176},
 pdf = {https://openreview.net/pdf?id=3agxS3aDUs},
 review = {https://openreview.net/forum?id=3agxS3aDUs},
 title = {Subgraph Permutation Equivariant Networks},
 url = {https://openreview.net/forum?id=3agxS3aDUs},
 year = {2023}
}

@article{miyazawa2023estimating,
 author = {Shuichi Miyazawa and Daichi Mochihashi},
 code = {https://github.com/shu13830/ODEPoissonProcesses/tree/main},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=cJgHzw8Qhq},
 review = {https://openreview.net/forum?id=cJgHzw8Qhq},
 title = {Estimating Differential Equations from Temporal Point Processes},
 url = {https://openreview.net/forum?id=cJgHzw8Qhq},
 year = {2023}
}

@article{mondal2022on,
 abstract = {We show that in a cooperative $N$-agent network, one can design locally executable policies for the agents such that the resulting discounted sum of average rewards (value) well approximates the optimal value computed over all (including non-local) policies. Specifically, we prove that, if $|\mathcal{X}|, |\mathcal{U}|$ denote the size of state, and action spaces of individual agents, then for sufficiently small discount factor, the approximation error is given by $\mathcal{O}(e)$ where $e\triangleq \frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|}+\sqrt{|\mathcal{U}|}\right]$. Moreover, in a special case where the reward and state transition functions are independent of the action distribution of the population, the error improves to $\mathcal{O}(e)$ where $e\triangleq \frac{1}{\sqrt{N}}\sqrt{|\mathcal{X}|}$. Finally, we also devise an algorithm to explicitly construct a local policy. With the help of our approximation results, we further establish that the constructed local policy is within $\mathcal{O}(\max\{e,\epsilon\})$ distance of the optimal policy, and the sample complexity to achieve such a local policy is $\mathcal{O}(\epsilon^{-3})$, for any $\epsilon>0$.},
 author = {Washim Uddin Mondal and Vaneet Aggarwal and Satish Ukkusuri},
 code = {https://github.itap.purdue.edu/Clan-labs/NearOptimalLocalPolicy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4295105844},
 pdf = {https://openreview.net/pdf?id=t5HkgbxZp1},
 review = {https://openreview.net/forum?id=t5HkgbxZp1},
 title = {On the Near-Optimality of Local Policies in Large Cooperative Multi-Agent Reinforcement Learning},
 url = {https://openreview.net/forum?id=t5HkgbxZp1},
 year = {2022}
}

@article{mondal2023meanfield,
 abstract = {Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\mathcal{O}(e)$ where $e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$. The size of the agent population is denoted by the term $N$, and $|\mathcal{X}|, |\mathcal{U}|$ respectively indicate the sizes of (local) state and action spaces of individual agents. The approximation error is found to be independent of the size of the shared global state space. We further demonstrate that in a special case if the reward and state transition functions are independent of the action distribution of the population, then the error can be improved to $e=\frac{\sqrt{|\mathcal{X}|}}{\sqrt{N}}$. Finally, we devise a Natural Policy Gradient based algorithm that solves the MFC problem with $\mathcal{O}(\epsilon^{-3})$ sample complexity and obtains a policy that is within $\mathcal{O}(\max\{e,\epsilon\})$ error of the optimal MARL policy for any $\epsilon>0$.},
 author = {Washim Uddin Mondal and Vaneet Aggarwal and Satish Ukkusuri},
 code = {https://github.itap.purdue.edu/Clan-labs/MeanFieldwithGlobalState},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4317464262},
 pdf = {https://openreview.net/pdf?id=ZME2nZMTvY},
 review = {https://openreview.net/forum?id=ZME2nZMTvY},
 title = {Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State},
 url = {https://openreview.net/forum?id=ZME2nZMTvY},
 year = {2023}
}

@article{mondal2023reinforcement,
 abstract = {We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon. This demonstrates the optimality of the bound in the order of $T$, and an additive impact of the delay.},
 author = {Washim Uddin Mondal and Vaneet Aggarwal},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4372272845},
 pdf = {https://openreview.net/pdf?id=ubCoTAynPp},
 review = {https://openreview.net/forum?id=ubCoTAynPp},
 title = {Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward},
 url = {https://openreview.net/forum?id=ubCoTAynPp},
 year = {2023}
}

@article{moran2022identifiable,
 abstract = {We develop the sparse VAE for unsupervised representation learning on high-dimensional data. The sparse VAE learns a set of latent factors (representations) which summarize the associations in the observed data features. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes. We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned. (In contrast, most deep generative models are not identifiable.) We empirically study the sparse VAE with both simulated and real data. We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.},
 author = {Gemma Elyse Moran and Dhanya Sridhar and Yixin Wang and David Blei},
 code = {https://github.com/gemoran/sparse-vae-code},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4300947078},
 pdf = {https://openreview.net/pdf?id=vd0onGWZbE},
 review = {https://openreview.net/forum?id=vd0onGWZbE},
 title = {Identifiable Deep Generative Models via Sparse Decoding},
 url = {https://openreview.net/forum?id=vd0onGWZbE},
 year = {2022}
}

@article{morel2023turning,
 abstract = {Normalizing Flows (NF) are powerful likelihood-based generative models that are able to trade off between expressivity and tractability to model complex densities. A now well established research avenue leverages optimal transport (OT) and looks for Monge maps, i.e. models with minimal effort between the source and target distributions. This paper introduces a method based on Brenier's polar factorization theorem to transform any trained NF into a more OT-efficient version without changing the final density. We do so by learning a rearrangement of the source (Gaussian) distribution that minimizes the OT cost between the source and the final density. We further constrain the path leading to the estimated Monge map to lie on a geodesic in the space of volume-preserving diffeomorphisms thanks to Euler's equations. The proposed method leads to smooth flows with reduced OT cost for several existing models without affecting the model performance.},
 author = {Guillaume Morel and Lucas Drumetz and Simon Bena{\"\i}chouche and Nicolas Courty and Fran{\c{c}}ois Rousseau},
 code = {https://github.com/morel-g/GPFlow},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296972792},
 pdf = {https://openreview.net/pdf?id=2UQv8L1Cv9},
 review = {https://openreview.net/forum?id=2UQv8L1Cv9},
 title = {Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows},
 url = {https://openreview.net/forum?id=2UQv8L1Cv9},
 year = {2023}
}

@article{morrill2022on,
 author = {James Morrill and Patrick Kidger and Lingyi Yang and Terry Lyons},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=caRBFhxXIG},
 review = {https://openreview.net/forum?id=caRBFhxXIG},
 title = {On the Choice of Interpolation Scheme for Neural {CDE}s},
 url = {https://openreview.net/forum?id=caRBFhxXIG},
 year = {2022}
}

@article{moskvichev2023the,
 author = {Arsenii Kirillovich Moskvichev and Victor Vikram Odouard and Melanie Mitchell},
 code = {https://github.com/victorvikram/ConceptARC},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=8ykyGbtt2q},
 review = {https://openreview.net/forum?id=8ykyGbtt2q},
 title = {The Concept{ARC} Benchmark: Evaluating Understanding and Generalization in the {ARC} Domain},
 url = {https://openreview.net/forum?id=8ykyGbtt2q},
 year = {2023}
}

@article{mouton2023integrating,
 author = {Jacobie Mouton and Rodney Stephen Kroon},
 code = {https://gitlab.com/pleased/grf-and-siren-vae},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=OsKXlWamTQ},
 review = {https://openreview.net/forum?id=OsKXlWamTQ},
 title = {Integrating Bayesian Network Structure into Residual Flows and Variational Autoencoders},
 url = {https://openreview.net/forum?id=OsKXlWamTQ},
 year = {2023}
}

@article{mu2022learning,
 abstract = {We present a two-step hybrid reinforcement learning (RL) policy that is designed to generate interpretable and robust hierarchical policies on the RL problem with graph-based input. Unlike prior deep reinforcement learning policies parameterized by an end-to-end black-box graph neural network, our approach disentangles the decision-making process into two steps. The first step is a simplified classification problem that maps the graph input to an action group where all actions share a similar semantic meaning. The second step implements a sophisticated rule-miner that conducts explicit one-hop reasoning over the graph and identifies decisive edges in the graph input without the necessity of heavy domain knowledge. This two-step hybrid policy presents human-friendly interpretations and achieves better performance in terms of generalization and robustness. Extensive experimental studies on four levels of complex text-based games have demonstrated the superiority of the proposed method compared to the state-of-the-art.},
 author = {Tongzhou Mu and Kaixiang Lin and Feiyang Niu and Govind Thattai},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221153266},
 pdf = {https://openreview.net/pdf?id=Ox5tmhFBrc},
 review = {https://openreview.net/forum?id=Ox5tmhFBrc},
 title = {Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning},
 url = {https://openreview.net/forum?id=Ox5tmhFBrc},
 year = {2022}
}

@article{mu2022modeling,
 author = {Tong Mu and Stephan Zheng and Alexander R Trott},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=DY1pMrmDkm},
 review = {https://openreview.net/forum?id=DY1pMrmDkm},
 title = {Modeling Bounded Rationality in Multi-Agent Simulations Using Rationally Inattentive Reinforcement Learning},
 url = {https://openreview.net/forum?id=DY1pMrmDkm},
 year = {2022}
}

@article{mukherjee2024size,
 abstract = {Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( \sqrt[\leftroot{-1}\uproot{-1}4]{n} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale at least quadratically with it.},
 author = {Anirbit Mukherjee and Amartya Roy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385848465},
 pdf = {https://openreview.net/pdf?id=RwmWODTNFE},
 review = {https://openreview.net/forum?id=RwmWODTNFE},
 title = {Size Lowerbounds for Deep Operator Networks},
 url = {https://openreview.net/forum?id=RwmWODTNFE},
 year = {2024}
}

@article{mukuta2023invariant,
 abstract = {In this study, a novel feature coding method that exploits invariance for transformations represented by a finite group of orthogonal matrices is proposed. We prove that the group-invariant feature vector contains sufficient discriminative information when learning a linear classifier using convex loss minimization. Based on this result, a novel feature model that explicitly consider group action is proposed for principal component analysis and k-means clustering, which are commonly used in most feature coding methods, and global feature functions. Although the global feature functions are in general complex nonlinear functions, the group action on this space can be easily calculated by constructing these functions as tensor-product representations of basic representations, resulting in an explicit form of invariant feature functions. The effectiveness of our method is demonstrated on several image datasets.},
 author = {YUSUKE Mukuta and Tatsuya Harada},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323714695},
 pdf = {https://openreview.net/pdf?id=uv32JOdQuh},
 review = {https://openreview.net/forum?id=uv32JOdQuh},
 title = {Invariant Feature Coding using Tensor Product Representation},
 url = {https://openreview.net/forum?id=uv32JOdQuh},
 year = {2023}
}

@article{murali2023beyond,
 abstract = {Deep Neural Networks (DNNs) are prone to learning spurious features that correlate with the label during training but are irrelevant to the learning problem. This hurts model generalization and poses problems when deploying them in safety-critical applications. This paper aims to better understand the effects of spurious features through the lens of the learning dynamics of the internal neurons during the training process. We make the following observations: (1) While previous works highlight the harmful effects of spurious features on the generalization ability of DNNs, we emphasize that not all spurious features are harmful. Spurious features can be "benign" or "harmful" depending on whether they are "harder" or "easier" to learn than the core features for a given model. This definition is model and dataset dependent. (2) We build upon this premise and use instance difficulty methods (like Prediction Depth (Baldock et al., 2021)) to quantify "easiness" for a given model and to identify this behavior during the training phase. (3) We empirically show that the harmful spurious features can be detected by observing the learning dynamics of the DNN's early layers. In other words, easy features learned by the initial layers of a DNN early during the training can (potentially) hurt model generalization. We verify our claims on medical and vision datasets, both simulated and real, and justify the empirical success of our hypothesis by showing the theoretical connections between Prediction Depth and information-theoretic concepts like 풱-usable information (Ethayarajh et al., 2021). Lastly, our experiments show that monitoring only accuracy during training (as is common in machine learning pipelines) is insufficient to detect spurious features. We, therefore, highlight the need for monitoring early training dynamics using suitable instance difficulty metrics.},
 author = {Nihal Murali and Aahlad Manas Puli and Ke Yu and Rajesh Ranganath and kayhan Batmanghelich},
 code = {https://github.com/batmanlab/TMLR23_Dynamics_of_Spurious_Features},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321472312},
 pdf = {https://openreview.net/pdf?id=Tkvmt9nDmB},
 review = {https://openreview.net/forum?id=Tkvmt9nDmB},
 title = {Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics},
 url = {https://openreview.net/forum?id=Tkvmt9nDmB},
 year = {2023}
}

@article{mushtaq2023distributed,
 author = {Erum Mushtaq and Chaoyang He and Jie Ding and Salman Avestimehr},
 code = {https://github.com/ErumMushtaq/SPIDER.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=sY75NqDRk1},
 review = {https://openreview.net/forum?id=sY75NqDRk1},
 title = {Distributed Architecture Search Over Heterogeneous Distributions},
 url = {https://openreview.net/forum?id=sY75NqDRk1},
 year = {2023}
}

@article{muslimani2023reinforcement,
 author = {Calarina Muslimani and Alex Lewandowski and Dale Schuurmans and Matthew E. Taylor and Jun Luo},
 code = {https://www.dropbox.com/sh/hjkzzgctnqf6d8w/AAAYEycaDvPOeifz8FZbR3kLa?dl=0},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=G2GKiicaJI},
 review = {https://openreview.net/forum?id=G2GKiicaJI},
 title = {Reinforcement Teaching},
 url = {https://openreview.net/forum?id=G2GKiicaJI},
 year = {2023}
}

@article{mwai2023fast,
 author = {Newton Mwai Kinyanjui and Emil Carlsson and Fredrik D. Johansson},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/newtonmwai/fast_treatment_personalization/tree/main/healthy_gym},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=NNRIGE8bvF},
 review = {https://openreview.net/forum?id=NNRIGE8bvF},
 title = {Fast Treatment Personalization with Latent Bandits in Fixed-Confidence Pure Exploration},
 url = {https://openreview.net/forum?id=NNRIGE8bvF},
 year = {2023}
}

@article{n,
 badge = {Survey},
 pdf = {https://openreview.net/pdf?id=vwOKBldzFu},
 review = {https://openreview.net/forum?id=vwOKBldzFu}
}

@article{naganuma2023empirical,
 abstract = {Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts -- namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., Adam) perform worse than non-adaptive optimizers (e.g., SGD, momentum SGD) on out-of-distribution performance. In particular, even though there is no significant difference in in-distribution performance, we show a measurable difference in out-of-distribution performance. ii) in-distribution performance and out-of-distribution performance exhibit three types of behavior depending on the dataset -- linear returns, increasing returns, and diminishing returns. For example, in the training of natural language data using Adam, fine-tuning the performance of in-distribution performance does not significantly contribute to the out-of-distribution generalization performance.},
 author = {Hiroki Naganuma and Kartik Ahuja and Shiro Takagi and Tetsuya Motokawa and Rio Yokota and Kohta Ishikawa and Ikuro Sato and Ioannis Mitliagkas},
 code = {https://github.com/Hiroki11x/Optimizer_Comparison_OOD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309302944},
 pdf = {https://openreview.net/pdf?id=ipe0IMglFF},
 review = {https://openreview.net/forum?id=ipe0IMglFF},
 title = {Empirical Study on Optimizer Selection for Out-of-Distribution Generalization},
 url = {https://openreview.net/forum?id=ipe0IMglFF},
 year = {2023}
}

@article{nagarajan2023fastraingnn,
 author = {Amrit Nagarajan and Anand Raghunathan},
 code = {https://github.com/amrnag/FASTRAIN-GNN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=1IYJfwJtjQ},
 review = {https://openreview.net/forum?id=1IYJfwJtjQ},
 title = {{FASTRAIN}-{GNN}: Fast and Accurate Self-Training for Graph Neural Networks},
 url = {https://openreview.net/forum?id=1IYJfwJtjQ},
 year = {2023}
}

@article{nam2023breaking,
 abstract = {To capture the relationship between samples and labels, conditional generative models often inherit spurious correlations from the training dataset. This can result in label-conditional distributions that are imbalanced with respect to another latent attribute. To mitigate this issue, which we call spurious causality of conditional generation, we propose a general two-step strategy. (a) Fairness Intervention (FI): emphasize the minority samples that are hard to generate due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): explicitly filter the generated samples and ensure that they follow the desired latent attribute distribution. We have designed the fairness intervention to work for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results demonstrate that FICS can effectively resolve spurious causality of conditional generation across various datasets.},
 author = {Junhyun Nam and Sangwoo Mo and Jaeho Lee and Jinwoo Shin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310827539},
 pdf = {https://openreview.net/pdf?id=VV4zJwLwI7},
 review = {https://openreview.net/forum?id=VV4zJwLwI7},
 title = {Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling},
 url = {https://openreview.net/forum?id=VV4zJwLwI7},
 year = {2023}
}

@article{narayan2024expected,
 author = {Taman Narayan and Serena Lutong Wang and Kevin Robert Canini and Maya Gupta},
 code = {https://github.com/google-research/google-research/tree/master/quantile_regression},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Eg8Rnb0Hdd},
 review = {https://openreview.net/forum?id=Eg8Rnb0Hdd},
 title = {Expected Pinball Loss For Quantile Regression And Inverse {CDF} Estimation},
 url = {https://openreview.net/forum?id=Eg8Rnb0Hdd},
 year = {2024}
}

@article{nash2023transframer,
 abstract = {We present a general-purpose framework for image modelling and vision tasks based on probabilistic frame prediction. Our approach unifies a broad range of tasks, from image segmentation, to novel view synthesis and video interpolation. We pair this framework with an architecture we term Transframer, which uses U-Net and Transformer components to condition on annotated context frames, and outputs sequences of sparse, compressed image features. Transframer is the state-of-the-art on a variety of video generation benchmarks, is competitive with the strongest models on few-shot view synthesis, and can generate coherent 30 second videos from a single image without any explicit geometric information. A single generalist Transframer simultaneously produces promising results on 8 tasks, including semantic segmentation, image classification and optical flow prediction with no task-specific architectural components, demonstrating that multi-task computer vision can be tackled using probabilistic image models. Our approach can in principle be applied to a wide range of applications that require learning the conditional structure of annotated image-formatted data.},
 author = {Charlie Nash and Joao Carreira and Jacob C Walker and Iain Barr and Andrew Jaegle and Mateusz Malinowski and Peter Battaglia},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221166209},
 pdf = {https://openreview.net/pdf?id=OJtYpdiHNo},
 review = {https://openreview.net/forum?id=OJtYpdiHNo},
 title = {Transframer: Arbitrary Frame Prediction with Generative Models},
 url = {https://openreview.net/forum?id=OJtYpdiHNo},
 year = {2023}
}

@article{nasirigerdeh2024kernel,
 abstract = {Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private training. Given that, KernelNorm combines the batch-independence property of layer and group normalization with the performance advantage of BatchNorm.},
 author = {Reza Nasirigerdeh and Reihaneh Torkzadehmahani and Daniel Rueckert and Georgios Kaissis},
 code = {https://github.com/reza-nasirigerdeh/norm-torch},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281401546},
 pdf = {https://openreview.net/pdf?id=Uv3XVAEgG6},
 review = {https://openreview.net/forum?id=Uv3XVAEgG6},
 title = {Kernel Normalized Convolutional Networks},
 url = {https://openreview.net/forum?id=Uv3XVAEgG6},
 year = {2024}
}

@article{nava2023metalearning,
 abstract = {We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second "guidance" model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance: "HyperCLIP"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model ("HyperLDM"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.},
 author = {Elvis Nava and Seijin Kobayashi and Yifei Yin and Robert K. Katzschmann and Benjamin F Grewe},
 code = {https://github.com/elvisnava/hyperclip},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306803296},
 pdf = {https://openreview.net/pdf?id=1irVjE7A3w},
 review = {https://openreview.net/forum?id=1irVjE7A3w},
 title = {Meta-Learning via Classifier(-free) Diffusion Guidance},
 url = {https://openreview.net/forum?id=1irVjE7A3w},
 year = {2023}
}

@article{nayak2022zeroshot,
 abstract = {Zero-shot learning relies on semantic class representations such as hand-engineered attributes or learned embeddings to predict classes without any labeled examples. We propose to learn class representations by embedding nodes from common sense knowledge graphs in a vector space. Common sense knowledge graphs are an untapped source of explicit high-level knowledge that requires little human effort to apply to a range of tasks. To capture the knowledge in the graph, we introduce ZSL-KG, a general-purpose framework with a novel transformer graph convolutional network (TrGCN) for generating class representations. Our proposed TrGCN architecture computes non-linear combinations of node neighbourhoods. Our results show that ZSL-KG improves over existing WordNet-based methods on five out of six zero-shot benchmark datasets in language and vision.},
 author = {Nihal V. Nayak and Stephen Bach},
 code = {https://github.com/BatsResearch/zsl-kg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4293566103},
 pdf = {https://openreview.net/pdf?id=h1zuM6cXpH},
 review = {https://openreview.net/forum?id=h1zuM6cXpH},
 title = {Zero-Shot Learning with Common Sense Knowledge Graphs},
 url = {https://openreview.net/forum?id=h1zuM6cXpH},
 year = {2022}
}

@article{ng2023predicting,
 abstract = {Developing and deploying machine learning models safely depends on the ability to characterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) settings where existing methods cannot, requiring only selecting a set of appropriate data transformations. In experiments on robustness benchmarks in image classification, sentiment analysis, and natural language inference, we demonstrate a strong and robust correlation between our neighborhood invariance measure and actual OOD generalization on over 4,600 models evaluated on over 100 unique train/test domain pairs.},
 author = {Nathan Hoyen Ng and Neha Hulkund and Kyunghyun Cho and Marzyeh Ghassemi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4284681971},
 pdf = {https://openreview.net/pdf?id=jYkWdJzTwn},
 review = {https://openreview.net/forum?id=jYkWdJzTwn},
 title = {Predicting Out-of-Domain Generalization with Neighborhood Invariance},
 url = {https://openreview.net/forum?id=jYkWdJzTwn},
 year = {2023}
}

@article{ng2024blind,
 abstract = {Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy observations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the midpoint of the subreads in both the latent and sequence spaces. This set embedding represents the "average" of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of $\leq 6$ subreads with 17% fewer errors and large reads of $>6$ subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications.},
 author = {Nathan Hoyen Ng and Ji Won Park and Jae Hyeon Lee and Ryan Lewis Kelly and Stephen Ra and Kyunghyun Cho},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386557019},
 pdf = {https://openreview.net/pdf?id=3s7ior0WZ5},
 review = {https://openreview.net/forum?id=3s7ior0WZ5},
 title = {Blind Biological Sequence Denoising with Self-Supervised Set Learning},
 url = {https://openreview.net/forum?id=3s7ior0WZ5},
 year = {2024}
}

@article{ngo2024highdimensional,
 abstract = {Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions. We evaluate our proposed method on various benchmark synthetic and real-world problems. The results demonstrate that our method outperforms existing state-of-the-art techniques.},
 author = {Lam Ngo and Huong Ha and Jeffrey Chan and Vu Nguyen and Hongyu Zhang},
 code = {https://github.com/LamNgo1/cma-meta-algorithm},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391622593},
 pdf = {https://openreview.net/pdf?id=eTgxr7gPuU},
 review = {https://openreview.net/forum?id=eTgxr7gPuU},
 title = {High-dimensional Bayesian Optimization via Covariance Matrix Adaptation
  Strategy},
 url = {https://openreview.net/forum?id=eTgxr7gPuU},
 year = {2024}
}

@article{nguyen-tang2022on,
 author = {Thanh Nguyen-Tang and Sunil Gupta and Hung Tran-The and Svetha Venkatesh},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LdEm0umNcv},
 review = {https://openreview.net/forum?id=LdEm0umNcv},
 title = {On Sample Complexity of Offline Reinforcement Learning with Deep Re{LU} Networks in Besov Spaces},
 url = {https://openreview.net/forum?id=LdEm0umNcv},
 year = {2022}
}

@article{nguyen2022on,
 abstract = {Recent work has uncovered a striking phenomenon in large-capacity neural networks: they contain blocks of contiguous hidden layers with highly similar representations. This block structure has two seemingly contradictory properties: on the one hand, its constituent layers exhibit highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds. Our work seeks to reconcile these discrepant properties by investigating the origin of the block structure in relation to the data and training methods. By analyzing properties of the dominant PCs, we find that the block structure arises from dominant datapoints - a small group of examples that share similar image statistics (e.g. background color). However, the set of dominant datapoints, and the precise shared image statistic, can vary across random seeds. Thus, the block structure reflects meaningful dataset statistics, but is simultaneously unique to each model. Through studying hidden layer activations and creating synthetic datapoints, we demonstrate that these simple image statistics dominate the representational geometry of the layers inside the block structure. We explore how the phenomenon evolves through training, finding that the block structure takes shape early in training, but the underlying representations and the corresponding dominant datapoints continue to change substantially. Finally, we study the interplay between the block structure and different training mechanisms, introducing a targeted intervention to eliminate the block structure, as well as examining the effects of pretraining and Shake-Shake regularization.},
 author = {Thao Nguyen and Maithra Raghu and Simon Kornblith},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221157427},
 pdf = {https://openreview.net/pdf?id=9tl6zjLYVS},
 review = {https://openreview.net/forum?id=9tl6zjLYVS},
 title = {On the Origins of the Block Structure Phenomenon in Neural Network Representations},
 url = {https://openreview.net/forum?id=9tl6zjLYVS},
 year = {2022}
}

@article{nguyen2023improved,
 abstract = {Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent splits of the training data. We find that using a novel sample-splitting procedure achieves robust worst-group performance in the fine-tuning step. When evaluated on benchmark image and text classification tasks, our approach consistently performs favorably to group DRO, JTT, and other strong baselines when either group labels are available during training or are only given in validation sets. Importantly, our method only relies on a single hyperparameter, which adjusts the fraction of labels used for training feature extractors vs. training classification layers. We justify the rationale of our splitting scheme with a generalization-bound analysis of the worst-group loss.},
 author = {Thien Hang Nguyen and Hongyang R. Zhang and Huy Nguyen},
 code = {https://github.com/timmytonga/crois},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4300973223},
 pdf = {https://openreview.net/pdf?id=Qlvgq9eC63},
 review = {https://openreview.net/forum?id=Qlvgq9eC63},
 title = {Improved Group Robustness via Classifier Retraining on Independent Splits},
 url = {https://openreview.net/forum?id=Qlvgq9eC63},
 year = {2023}
}

@article{nguyen2023task,
 abstract = {Developing meta-learning algorithms that are un-biased toward a subset of training tasks often requires hand-designed criteria to weight tasks, potentially resulting in sub-optimal solutions. In this paper, we introduce a new principled and fully-automated task-weighting algorithm for meta-learning methods. By considering the weights of tasks within the same mini-batch as an action, and the meta-parameter of interest as the system state, we cast the task-weighting meta-learning problem to a trajectory optimisation and employ the iterative linear quadratic regulator to determine the optimal action or weights of tasks. We theoretically show that the proposed algorithm converges to an $\epsilon_{0}$-stationary point, and empirically demonstrate that the proposed approach out-performs common hand-engineering weighting methods in two few-shot learning benchmarks.},
 author = {Cuong C. Nguyen and Thanh-Toan Do and Gustavo Carneiro},
 code = {https://github.com/cnguyen10/tow},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4313598236},
 pdf = {https://openreview.net/pdf?id=SSkTBUyJip},
 review = {https://openreview.net/forum?id=SSkTBUyJip},
 title = {Task Weighting in Meta-learning with Trajectory Optimisation},
 url = {https://openreview.net/forum?id=SSkTBUyJip},
 year = {2023}
}

@article{nickelsen2023improved,
 author = {Daniel Nickelsen and Bubacarr Bah},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=0ck7hJ8EVC},
 review = {https://openreview.net/forum?id=0ck7hJ8EVC},
 title = {Improved identification accuracy in equation learning via comprehensive \${\textbackslash}boldsymbol\{R{\textasciicircum}2\}\$-elimination and Bayesian model selection},
 url = {https://openreview.net/forum?id=0ck7hJ8EVC},
 year = {2023}
}

@article{nie2022ghostsr,
 author = {Ying Nie and Kai Han and Zhenhua Liu and Chuanjian Liu and Yunhe Wang},
 code = {https://gitee.com/mindspore/models/tree/master/research/cv/GhostSR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=tbd9f3HwPy},
 review = {https://openreview.net/forum?id=tbd9f3HwPy},
 title = {Ghost{SR}: Learning Ghost Features for Efficient Image Super-Resolution},
 url = {https://openreview.net/forum?id=tbd9f3HwPy},
 year = {2022}
}

@article{niknejad2024online,
 author = {Nariman Niknejad and Farnaz Adib Yaghmaie and Hamidreza Modares},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=pfbVayaUMc},
 review = {https://openreview.net/forum?id=pfbVayaUMc},
 title = {Online Reference Tracking For Linear Systems with Unknown Dynamics and Unknown Disturbances},
 url = {https://openreview.net/forum?id=pfbVayaUMc},
 year = {2024}
}

@article{niu2023mlbfgs,
 abstract = {Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes. We provide a supporting convergence analysis for mL-BFGS in stochastic settings. To investigate mL-BFGS potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). Results show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup.},
 author = {Yue Niu and Zalan Fabian and Sunwoo Lee and Mahdi Soltanolkotabi and Salman Avestimehr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385327591},
 pdf = {https://openreview.net/pdf?id=9jnsPp8DP3},
 review = {https://openreview.net/forum?id=9jnsPp8DP3},
 title = {mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization},
 url = {https://openreview.net/forum?id=9jnsPp8DP3},
 year = {2023}
}

@article{niu2023overcoming,
 author = {Yue Niu and Saurav Prakash and Souvik Kundu and Sunwoo Lee and Salman Avestimehr},
 code = {https://github.com/yuehniu/modeldecomp-fl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=lx1WnkL9fk},
 review = {https://openreview.net/forum?id=lx1WnkL9fk},
 title = {Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients},
 url = {https://openreview.net/forum?id=lx1WnkL9fk},
 year = {2023}
}

@article{norcliffe2023benchmarking,
 abstract = {Multiple sclerosis is a disease that affects the brain and spinal cord, it can lead to severe disability and has no known cure. The majority of prior work in machine learning for multiple sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory tests; these modalities are both expensive to acquire and can be unreliable. In a recent paper it was shown that disease progression can be predicted effectively using performance outcome measures and demographic data. In our work we build on this to investigate the modeling side, using continuous time models to predict progression. We benchmark four continuous time models using a publicly available multiple sclerosis dataset. We find that the best continuous model is often able to outperform the best benchmarked discrete time model. We also carry out an extensive ablation to discover the sources of performance gains, we find that standardizing existing features leads to a larger performance increase than interpolating missing features.},
 author = {Alexander Luke Ian Norcliffe and Lev Proleev and Diana Mincu and F Lee Hartsell and Katherine A Heller and Subhrajit Roy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321177563},
 pdf = {https://openreview.net/pdf?id=2uMnAwWnRy},
 review = {https://openreview.net/forum?id=2uMnAwWnRy},
 title = {Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression},
 url = {https://openreview.net/forum?id=2uMnAwWnRy},
 year = {2023}
}

@article{norcliffe2023faster,
 author = {Alexander Luke Ian Norcliffe and Marc Peter Deisenroth},
 code = {https://github.com/a-norcliffe/torch_gq_adjoint},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=f0FSDAy1bU},
 review = {https://openreview.net/forum?id=f0FSDAy1bU},
 title = {Faster Training of Neural {ODE}s Using Gau{\ss}{\textendash}Legendre Quadrature},
 url = {https://openreview.net/forum?id=f0FSDAy1bU},
 year = {2023}
}

@article{oala2023data,
 abstract = {This dataset accompanies the paper titled <em>Data Models for Dataset Drift Controls in Machine Learning with Images</em><br> <br> that appeared in the Transactions on Machine Learning Research<br> <br> https://openreview.net/forum?id=I4IkGmgFJz<br> <pre><code>@article{ oala2023data, title={Data Models for Dataset Drift Controls in Machine Learning With Optical Images}, author={Luis Oala and Marco Aversa and Gabriel Nobis and Kurt Willis and Yoan Neuenschwander and Mich{\`e}le Buck and Christian Matek and Jerome Extermann and Enrico Pomarico and Wojciech Samek and Roderick Murray-Smith and Christoph Clausen and Bruno Sanguinetti}, journal={Transactions on Machine Learning Research}, issn={2835-8856}, year={2023}, url={https://openreview.net/forum?id=I4IkGmgFJz}, note={} }</code></pre> We make available two datasets. <strong>Raw-Microscopy:</strong> <strong>940 raw bright-field microscopy images</strong> of human blood smear slides for leukocyte classification (microscopy/images/raw_scale100) with corresponding labels (microscopy/labels). <strong>5,640 variations measured at six additional different intensities </strong>(microscopy/images/raw_scale001-raw_scale0075) <strong>11,280 images of the raw sensor data processed through twelve different pipelines</strong> (microscopy/images/processed_views) <strong>Raw-Drone:</strong> <strong>548 raw drone camera images for car segmentation</strong> (drone/images_tiles_256/raw_scale100) with corresponding binary segmentation mask (drone/masks_tiles_256). The images and the masks are cropped from 12 raw drone camera images (drone/images_full/raw_scale100) and 12 masks (drone/masks_full) of size 3648 by 5472. <strong>3,288 variations measured at six additional different intensities</strong> (drone/images_tiles_256/raw_scale001-raw_scale075). <strong>6,576 images of the raw sensor data processed through twelve different pipelines</strong> (drone/images_tiles_256/processed_views). Detailed datasheets for the two datasets can be found in the appendices of the TMLR paper. The code repository for this project can be found at https://github.com/aiaudit-org/raw2logit},
 author = {Luis Oala and Marco Aversa and Gabriel Nobis and Kurt Willis and Yoan Neuenschwander and Mich{\`e}le Buck and Christian Matek and Jerome Extermann and Enrico Pomarico and Wojciech Samek and Roderick Murray-Smith and Christoph Clausen and Bruno Sanguinetti},
 code = {https://github.com/aiaudit-org/raw2logit},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4393669810},
 pdf = {https://openreview.net/pdf?id=I4IkGmgFJz},
 review = {https://openreview.net/forum?id=I4IkGmgFJz},
 title = {Data Models for Dataset Drift Controls in Machine Learning With Optical Images - Datasets},
 url = {https://openreview.net/forum?id=I4IkGmgFJz},
 year = {2023}
}

@article{ohib2022explicit,
 abstract = {We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure (an affine function of the ratio of the $\ell_1$ and $\ell_2$ norms). Existing approaches either project each vector individually or require the use of a regularization parameter which implicitly maps to the average $\ell_0$-measure of sparsity. Instead, in our approach we set the sparsity level for the whole set explicitly and simultaneously project a group of vectors with the sparsity level of each vector tuned automatically. We show that the computational complexity of our projection operator is linear in the size of the problem. Additionally, we propose a generalization of this projection by replacing the $\ell_1$ norm by its weighted version. We showcase the efficacy of our approach in both supervised and unsupervised learning tasks on image datasets including CIFAR10 and ImageNet. In deep neural network pruning, the sparse models produced by our method on ResNet50 have significantly higher accuracies at corresponding sparsity values compared to existing competitors. In nonnegative matrix factorization, our approach yields competitive reconstruction errors against state-of-the-art algorithms.},
 author = {Riyasat Ohib and Nicolas Gillis and Niccolo Dalmasso and Sameena Shah and Vamsi K. Potluru and Sergey Plis},
 code = {https://github.com/riohib/gsp-for-deeplearning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287997123},
 pdf = {https://openreview.net/pdf?id=jIrOeWjdpc},
 review = {https://openreview.net/forum?id=jIrOeWjdpc},
 title = {Explicit Group Sparse Projection with Applications to Deep Learning and NMF},
 url = {https://openreview.net/forum?id=jIrOeWjdpc},
 year = {2022}
}

@article{olatunji2023releasing,
 abstract = {With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PrivGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PrivGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PrivGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Renyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to several baselines adapted for graph-structured data. Our code is available at https://github.com/iyempissy/privGnn.},
 author = {Iyiola Emmanuel Olatunji and Thorben Funke and Megha Khosla},
 code = {https://github.com/iyempissy/privGnn},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4388327286},
 pdf = {https://openreview.net/pdf?id=wk8oXR0kFA},
 review = {https://openreview.net/forum?id=wk8oXR0kFA},
 title = {Releasing Graph Neural Networks with Differential Privacy Guarantees},
 url = {https://openreview.net/forum?id=wk8oXR0kFA},
 year = {2023}
}

@article{oldewage2024series,
 abstract = {Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact inverse Hessian with absolute-value eigenvalues. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performance. We demonstrate this in a variety of settings, including a ResNet-18 trained on CIFAR-10.},
 author = {Elre Talea Oldewage and Ross M Clarke and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
 code = {https://github.com/rmclarke/SeriesOfHessianVectorProducts},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387946345},
 pdf = {https://openreview.net/pdf?id=qBZeQBEDIW},
 review = {https://openreview.net/forum?id=qBZeQBEDIW},
 title = {Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks},
 url = {https://openreview.net/forum?id=qBZeQBEDIW},
 year = {2024}
}

@article{oquab2024dinov,
 author = {Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
 code = {https://github.com/facebookresearch/dinov2},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=a68SUt6zFt},
 review = {https://openreview.net/forum?id=a68SUt6zFt},
 title = {{DINO}v2: Learning Robust Visual Features without Supervision},
 url = {https://openreview.net/forum?id=a68SUt6zFt},
 year = {2024}
}

@article{ortiz-jimenez2023catastrophic,
 abstract = {Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of these easy features induces a learning shortcut that leads to CO. Our findings provide new insights into the mechanisms of CO and improve our understanding of the dynamics of AT. The code to reproduce our experiments can be found at https://github.com/gortizji/co_features.},
 author = {Guillermo Ortiz-Jimenez and Pau de Jorge and Amartya Sanyal and Adel Bibi and Puneet K. Dokania and Pascal Frossard and Gr{\'e}gory Rogez and Philip Torr},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/gortizji/co_features},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4283070725},
 pdf = {https://openreview.net/pdf?id=10hCbu70Sr},
 review = {https://openreview.net/forum?id=10hCbu70Sr},
 title = {Catastrophic overfitting can be induced with discriminative non-robust features},
 url = {https://openreview.net/forum?id=10hCbu70Sr},
 year = {2023}
}

@article{osa2023offline,
 author = {Takayuki Osa and Akinobu Hayashi and Pranav Deo and Naoki Morihira and Takahide Yoshiike},
 code = {https://github.com/TakaOsa/DMPO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=zkRCp4RmAF},
 review = {https://openreview.net/forum?id=zkRCp4RmAF},
 title = {Offline Reinforcement Learning with Mixture of Deterministic Policies},
 url = {https://openreview.net/forum?id=zkRCp4RmAF},
 year = {2023}
}

@article{osama2023online,
 author = {Muhammad Osama and Dave Zachariah and Peter Stoica and Thomas B. Sch{\"o}n},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=nAr9PhyEbQ},
 review = {https://openreview.net/forum?id=nAr9PhyEbQ},
 title = {Online Learning for Prediction via Covariance Fitting: Computation, Performance and Robustness},
 url = {https://openreview.net/forum?id=nAr9PhyEbQ},
 year = {2023}
}

@article{ozbulak2023know,
 abstract = {Although supervised learning has been highly successful in improving the state-of-the-art in the domain of image-based computer vision in the past, the margin of improvement has diminished significantly in recent years, indicating that a plateau is in sight. Meanwhile, the use of self-supervised learning (SSL) for the purpose of natural language processing (NLP) has seen tremendous successes during the past couple of years, with this new learning paradigm yielding powerful language models. Inspired by the excellent results obtained in the field of NLP, self-supervised methods that rely on clustering, contrastive learning, distillation, and information-maximization, which all fall under the banner of discriminative SSL, have experienced a swift uptake in the area of computer vision. Shortly afterwards, generative SSL frameworks that are mostly based on masked image modeling, complemented and surpassed the results obtained with discriminative SSL. Consequently, within a span of three years, over $100$ unique general-purpose frameworks for generative and discriminative SSL, with a focus on imaging, were proposed. In this survey, we review a plethora of research efforts conducted on image-oriented SSL, providing a historic view and paying attention to best practices as well as useful software packages. While doing so, we discuss pretext tasks for image-based SSL, as well as techniques that are commonly used in image-based SSL. Lastly, to aid researchers who aim at contributing to image-focused SSL, we outline a number of promising research directions.},
 author = {Utku Ozbulak and Hyun Jung Lee and Beril Boga and Esla Timothy Anzaku and Ho-min Park and Arnout Van Messem and Wesley De Neve and Joris Vankerschaver},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4378464916},
 pdf = {https://openreview.net/pdf?id=Ma25S4ludQ},
 review = {https://openreview.net/forum?id=Ma25S4ludQ},
 title = {Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training},
 url = {https://openreview.net/forum?id=Ma25S4ludQ},
 year = {2023}
}

@article{ozdemir2023oadat,
 abstract = {Optoacoustic (OA) imaging is based on excitation of biological tissues with nanosecond-duration laser pulses followed by subsequent detection of ultrasound waves generated via light-absorption-mediated thermoelastic expansion. OA imaging features a powerful combination between rich optical contrast and high resolution in deep tissues. This enabled the exploration of a number of attractive new applications both in clinical and laboratory settings. However, no standardized datasets generated with different types of experimental set-up and associated processing methods are available to facilitate advances in broader applications of OA in clinical settings. This complicates an objective comparison between new and established data processing methods, often leading to qualitative results and arbitrary interpretations of the data. In this paper, we provide both experimental and synthetic OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries. We further provide trained neural networks to tackle three important challenges related to OA image processing, namely accurate reconstruction under limited view tomographic conditions, removal of spatial undersampling artifacts and anatomical segmentation for improved image reconstruction. Specifically, we define 44 experiments corresponding to the aforementioned challenges as benchmarks to be used as a reference for the development of more advanced processing methods.},
 author = {Firat Ozdemir and Berkan Lafci and Xose Luis Dean-Ben and Daniel Razansky and Fernando Perez-Cruz},
 code = {https://github.com/berkanlafci/oadat},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283218099},
 pdf = {https://openreview.net/pdf?id=BVi6MhKO0G},
 review = {https://openreview.net/forum?id=BVi6MhKO0G},
 title = {OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing},
 url = {https://openreview.net/forum?id=BVi6MhKO0G},
 year = {2023}
}

@article{p,
 code = {https://github.com/machine-teaching-group/tmlr2024_neurtasksyn},
 pdf = {https://openreview.net/pdf?id=aYkYajcJDN},
 review = {https://openreview.net/forum?id=aYkYajcJDN}
}

@article{p,
 pdf = {https://openreview.net/pdf?id=gvcDSDYUZx},
 review = {https://openreview.net/forum?id=gvcDSDYUZx}
}

@article{pal,
 code = {https://github.com/cs1160701/OnLearningTheKernel},
 pdf = {https://openreview.net/pdf?id=tLIBAEYjcv},
 review = {https://openreview.net/forum?id=tLIBAEYjcv}
}

@article{pal2023understanding,
 abstract = {Randomized smoothing is a technique for providing provable robustness guarantees against adversarial attacks while making minimal assumptions about a classifier. This method relies on taking a majority vote of any base classifier over multiple noise-perturbed inputs to obtain a smoothed classifier, and it remains the tool of choice to certify deep and complex neural network models. Nonetheless, non-trivial performance of such smoothed classifier crucially depends on the base model being trained on noise-augmented data, i.e., on a smoothed input distribution. While widely adopted in practice, it is still unclear how this noisy training of the base classifier precisely affects the risk of the robust smoothed classifier, leading to heuristics and tricks that are poorly understood. In this work we analyze these trade-offs theoretically in a binary classification setting, proving that these common observations are not universal. We show that, without making stronger distributional assumptions, no benefit can be expected from predictors trained with noise-augmentation, and we further characterize distributions where such benefit is obtained. Our analysis has direct implications to the practical deployment of randomized smoothing, and we illustrate some of these via experiments on CIFAR-10 and MNIST, as well as on synthetic datasets.},
 author = {Ambar Pal and Jeremias Sulam},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/ambarpal/randomized-smoothing},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4375959363},
 pdf = {https://openreview.net/pdf?id=fvyh6mDWFr},
 review = {https://openreview.net/forum?id=fvyh6mDWFr},
 title = {Understanding Noise-Augmented Training for Randomized Smoothing},
 url = {https://openreview.net/forum?id=fvyh6mDWFr},
 year = {2023}
}

@article{pandey2022diffusevae,
 author = {Kushagra Pandey and Avideep Mukherjee and Piyush Rai and Abhishek Kumar},
 code = {https://github.com/kpandey008/DiffuseVAE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ygoNPRiLxw},
 review = {https://openreview.net/forum?id=ygoNPRiLxw},
 title = {Diffuse{VAE}: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents},
 url = {https://openreview.net/forum?id=ygoNPRiLxw},
 year = {2022}
}

@article{pandey2024datadependent,
 author = {Harsh Pandey and Amitabha Bagchi and Srikanta J. Bedathur and Arindam Bhattacharya},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=mH6TelHVKD},
 review = {https://openreview.net/forum?id=mH6TelHVKD},
 title = {Data-Dependent Generalization Bounds for Neural Networks with Re{LU}},
 url = {https://openreview.net/forum?id=mH6TelHVKD},
 year = {2024}
}

@article{panknin2023local,
 abstract = {Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess the effectiveness of our LFC estimate in an AL application on a prototypical low-dimensional synthetic dataset, before taking on the challenging real-world task of reconstructing a quantum chemical force field for a small organic molecule and demonstrating state-of-the-art performance with a significantly reduced training demand.},
 author = {Danny Panknin and Stefan Chmiela and Klaus Robert Muller and Shinichi Nakajima},
 code = {https://github.com/DPanknin/modelagnostic_superior_training},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4304978482},
 pdf = {https://openreview.net/pdf?id=w4MoQ39zmc},
 review = {https://openreview.net/forum?id=w4MoQ39zmc},
 title = {Local Function Complexity for Active Learning via Mixture of Gaussian Processes},
 url = {https://openreview.net/forum?id=w4MoQ39zmc},
 year = {2023}
}

@article{papadimitriou2022bayesian,
 author = {Dimitris Papadimitriou and Usman Anwar and Daniel S. Brown},
 code = {https://github.com/gitcal/BICRL.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=oRjk5V9eDp},
 review = {https://openreview.net/forum?id=oRjk5V9eDp},
 title = {Bayesian Methods for Constraint Inference in Reinforcement Learning},
 url = {https://openreview.net/forum?id=oRjk5V9eDp},
 year = {2022}
}

@article{paren2022faking,
 author = {Alasdair Paren and Rudra P. K. Poudel and M. Pawan Kumar},
 code = {https://github.com/Alasdair-P/alig_plus},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=OslAMMF4ZP},
 review = {https://openreview.net/forum?id=OslAMMF4ZP},
 title = {Faking Interpolation Until You Make It},
 url = {https://openreview.net/forum?id=OslAMMF4ZP},
 year = {2022}
}

@article{peer2022improving,
 abstract = {Training deep neural networks is a very demanding task, especially challenging is how to adapt architectures to improve the performance of trained models. We can find that sometimes, shallow networks generalize better than deep networks, and the addition of more layers results in higher training and test errors. The deep residual learning framework addresses this degradation problem by adding skip connections to several neural network layers. It would at first seem counter-intuitive that such skip connections are needed to train deep networks successfully as the expressivity of a network would grow exponentially with depth. In this paper, we first analyze the flow of information through neural networks. We introduce and evaluate the batch-entropy which quantifies the flow of information through each layer of a neural network. We prove empirically and theoretically that a positive batch-entropy is required for gradient descent-based training approaches to optimize a given loss function successfully. Based on those insights, we introduce batch-entropy regularization to enable gradient descent-based training algorithms to optimize the flow of information through each hidden layer individually. With batch-entropy regularization, gradient descent optimizers can transform untrainable networks into trainable networks. We show empirically that we can therefore train a "vanilla" fully connected network and convolutional neural network -- no skip connections, batch normalization, dropout, or any other architectural tweak -- with 500 layers by simply adding the batch-entropy regularization term to the loss function. The effect of batch-entropy regularization is not only evaluated on vanilla neural networks, but also on residual networks, autoencoders, and also transformer models over a wide range of computer vision as well as natural language processing tasks.},
 author = {David Peer and Bart Keulen and Sebastian Stabinger and Justus Piater and Antonio Rodriguez-sanchez},
 code = {https://github.com/peerdavid/layerwise-batch-entropy/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320517113},
 pdf = {https://openreview.net/pdf?id=LJohl5DnZf},
 review = {https://openreview.net/forum?id=LJohl5DnZf},
 title = {Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization},
 url = {https://openreview.net/forum?id=LJohl5DnZf},
 year = {2022}
}

@article{pendurkar2023the,
 author = {Sumedh Pendurkar and Taoan Huang and Brendan Juba and Jiapeng Zhang and Sven Koenig and Guni Sharon},
 code = {https://github.com/Pi-Star-Lab/unscalable-heuristic-approximator},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=JllRdycmLk},
 review = {https://openreview.net/forum?id=JllRdycmLk},
 title = {The (Un)Scalability of Informed Heuristic Function Estimation in {NP}-Hard Search Problems},
 url = {https://openreview.net/forum?id=JllRdycmLk},
 year = {2023}
}

@article{peng2022understanding,
 abstract = {The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role in tackling Natural Language Processing challenges for low-resource languages. Its dominant approaches assumed that the relationship between embeddings could be represented by a linear mapping, but there has been no exploration of the conditions under which this assumption holds. Such a research gap becomes very critical recently, as it has been evidenced that relaxing mappings to be non-linear can lead to better performance in some cases. We, for the first time, present a theoretical analysis that identifies the preservation of analogies encoded in monolingual word embeddings as a necessary and sufficient condition for the ground-truth CLWE mapping between those embeddings to be linear. On a novel cross-lingual analogy dataset that covers five representative analogy categories for twelve distinct languages, we carry out experiments which provide direct empirical support for our theoretical claim. These results offer additional insight into the observations of other researchers and contribute inspiration for the development of more effective cross-lingual representation learning strategies.},
 author = {Xutan Peng and Mark Stevenson and Chenghua Lin and Chen Li},
 code = {https://github.com/Pzoom522/xANLG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287818033},
 pdf = {https://openreview.net/pdf?id=8HuyXvbvqX},
 review = {https://openreview.net/forum?id=8HuyXvbvqX},
 title = {Understanding Linearity of Cross-Lingual Word Embedding Mappings},
 url = {https://openreview.net/forum?id=8HuyXvbvqX},
 year = {2022}
}

@article{peng2023a,
 abstract = {Masked image modeling has demonstrated great potential to eliminate the label-hungry problem of training large-scale vision Transformers, achieving impressive performance on various downstream tasks. In this work, we propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images. Experimental results on image classification and semantic segmentation show that MaskDistill achieves comparable or superior performance than state-of-the-art methods. When using the huge vision Transformer and pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on ImageNet-1k (224 size) and 58.8% semantic segmentation mIoU metric on ADE20k (512 size). The code and pretrained models will be available at https://aka.ms/unimim.},
 author = {Zhiliang Peng and Li Dong and Hangbo Bao and Furu Wei and Qixiang Ye},
 code = {https://aka.ms/unimim},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307011239},
 pdf = {https://openreview.net/pdf?id=wmGlMhaBe0},
 review = {https://openreview.net/forum?id=wmGlMhaBe0},
 title = {A Unified View of Masked Image Modeling},
 url = {https://openreview.net/forum?id=wmGlMhaBe0},
 year = {2023}
}

@article{perez-suay2023fair,
 author = {Adrian Perez-Suay and Paula Gordaliza and Jean-Michel Loubes and Dino Sejdinovic and Gustau Camps-Valls},
 code = {https://www.uv.es/pesuaya/data/code/2023_FACIL.zip},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=MyQ1e1VQQ3},
 review = {https://openreview.net/forum?id=MyQ1e1VQQ3},
 title = {Fair Kernel Regression through Cross-Covariance Operators},
 url = {https://openreview.net/forum?id=MyQ1e1VQQ3},
 year = {2023}
}

@article{pethick2023revisiting,
 abstract = {Despite progress in adversarial training (AT), there is a substantial gap between the top-performing and worst-performing classes in many datasets. For example, on CIFAR10, the accuracies for the best and worst classes are 74% and 23%, respectively. We argue that this gap can be reduced by explicitly optimizing for the worst-performing class, resulting in a min-max-max optimization formulation. Our method, called class focused online learning (CFOL), includes high probability convergence guarantees for the worst class loss and can be easily integrated into existing training setups with minimal computational overhead. We demonstrate an improvement to 32% in the worst class accuracy on CIFAR10, and we observe consistent behavior across CIFAR100 and STL10. Our study highlights the importance of moving beyond average accuracy, which is particularly important in safety-critical applications.},
 author = {Thomas Pethick and Grigorios Chrysos and Volkan Cevher},
 code = {https://github.com/LIONS-EPFL/class-focused-online-learning-code},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321392741},
 pdf = {https://openreview.net/pdf?id=wkecshlYxI},
 review = {https://openreview.net/forum?id=wkecshlYxI},
 title = {Revisiting adversarial training for the worst-performing class},
 url = {https://openreview.net/forum?id=wkecshlYxI},
 year = {2023}
}

@article{pfeiffer2023cocofl,
 author = {Kilian Pfeiffer and Martin Rapp and Ramin Khalili and Joerg Henkel},
 code = {https://github.com/k1l1/CoCoFL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=XJIg4kQbkv},
 review = {https://openreview.net/forum?id=XJIg4kQbkv},
 title = {CoCo{FL}: Communication- and Computation-Aware Federated Learning via Partial {NN} Freezing and Quantization},
 url = {https://openreview.net/forum?id=XJIg4kQbkv},
 year = {2023}
}

@article{pfeiffer2023modular,
 abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
 author = {Jonas Pfeiffer and Sebastian Ruder and Ivan Vuli{\'c} and Edoardo Ponti},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4321650187},
 pdf = {https://openreview.net/pdf?id=z9EkXfvxta},
 review = {https://openreview.net/forum?id=z9EkXfvxta},
 title = {Modular Deep Learning},
 url = {https://openreview.net/forum?id=z9EkXfvxta},
 year = {2023}
}

@article{pfrommer2023projected,
 abstract = {Randomized smoothing is the current state-of-the-art method for producing provably robust classifiers. While randomized smoothing typically yields robust $\ell_2$-ball certificates, recent research has generalized provable robustness to different norm balls as well as anisotropic regions. This work considers a classifier architecture that first projects onto a low-dimensional approximation of the data manifold and then applies a standard classifier. By performing randomized smoothing in the low-dimensional projected space, we characterize the certified region of our smoothed composite classifier back in the high-dimensional input space and prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and SVHN that classifiers without the initial projection are vulnerable to perturbations that are normal to the data manifold and yet are captured by the certified regions of our method. We compare the volume of our certified regions against various baselines and show that our method improves on the state-of-the-art by many orders of magnitude.},
 author = {Samuel Pfrommer and Brendon G. Anderson and Somayeh Sojoudi},
 code = {https://github.com/spfrommer/projected_randomized_smoothing},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387075729},
 pdf = {https://openreview.net/pdf?id=FObkvLwNSo},
 review = {https://openreview.net/forum?id=FObkvLwNSo},
 title = {Projected Randomized Smoothing for Certified Adversarial Robustness},
 url = {https://openreview.net/forum?id=FObkvLwNSo},
 year = {2023}
}

@article{pich,
 code = {https://github.com/AlexPiche/fr-tmlr},
 pdf = {https://openreview.net/pdf?id=BFvoemrmqX},
 review = {https://openreview.net/forum?id=BFvoemrmqX}
}

@article{picot2023a,
 author = {Marine Picot and Federica Granese and Guillaume Staerman and Marco Romanelli and Francisco Messina and Pablo Piantanida and Pierre Colombo},
 code = {https://github.com/MarinePICOT/HAMPER},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=YtU0nDb5e8},
 review = {https://openreview.net/forum?id=YtU0nDb5e8},
 title = {A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection},
 url = {https://openreview.net/forum?id=YtU0nDb5e8},
 year = {2023}
}

@article{pilar2022incorporating,
 abstract = {Machine learning models can be improved by adapting them to respect existing background knowledge. In this paper we consider multitask Gaussian processes, with background knowledge in the form of constraints that require a specific sum of the outputs to be constant. This is achieved by conditioning the prior distribution on the constraint fulfillment. The approach allows for both linear and nonlinear constraints. We demonstrate that the constraints are fulfilled with high precision and that the construction can improve the overall prediction accuracy as compared to the standard Gaussian process.},
 author = {Philipp Pilar and Carl Jidling and Thomas B. Sch{\"o}n and Niklas Wahlstr{\"o}m},
 code = {https://github.com/ppilar/SumConstraint},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221151679},
 pdf = {https://openreview.net/pdf?id=gzu4ZbBY7S},
 review = {https://openreview.net/forum?id=gzu4ZbBY7S},
 title = {Incorporating Sum Constraints into Multitask Gaussian Processes},
 url = {https://openreview.net/forum?id=gzu4ZbBY7S},
 year = {2022}
}

@article{plumb2022finding,
 abstract = {Image classifiers often use spurious patterns, such as "relying on the presence of a person to detect a tennis racket, which do not generalize. In this work, we present an end-to-end pipeline for identifying and mitigating spurious patterns for such models, under the assumption that we have access to pixel-wise object-annotations. We start by identifying patterns such as "the model's prediction for tennis racket changes 63% of the time if we hide the people." Then, if a pattern is spurious, we mitigate it via a novel form of data augmentation. We demonstrate that our method identifies a diverse set of spurious patterns and that it mitigates them by producing a model that is both more accurate on a distribution where the spurious pattern is not helpful and more robust to distribution shift.},
 author = {Gregory Plumb and Marco Tulio Ribeiro and Ameet Talwalkar},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/GDPlumb/SPIRE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4287126366},
 pdf = {https://openreview.net/pdf?id=whJPugmP5I},
 review = {https://openreview.net/forum?id=whJPugmP5I},
 title = {Finding and Fixing Spurious Patterns with Explanations},
 url = {https://openreview.net/forum?id=whJPugmP5I},
 year = {2022}
}

@article{plumb2023towards,
 abstract = {A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM design and evaluation. Overall, we hope that the methodology and analyses presented in this work will help facilitate a more rigorous science of blindspot discovery.},
 author = {Gregory Plumb and Nari Johnson and Angel Cabrera and Ameet Talwalkar},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/njohnson99/spotcheck},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4285070034},
 pdf = {https://openreview.net/pdf?id=MaDvbLaBiF},
 review = {https://openreview.net/forum?id=MaDvbLaBiF},
 title = {Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models},
 url = {https://openreview.net/forum?id=MaDvbLaBiF},
 year = {2023}
}

@article{polaczyk2023improved,
 author = {Bart{\l}omiej Polaczyk and Jacek Cyranka},
 code = {https://github.com/MIMUW-RL/improved_overparametrization_tmlr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=RjZq6W6FoE},
 review = {https://openreview.net/forum?id=RjZq6W6FoE},
 title = {Improved Overparametrization Bounds for Global Convergence of {SGD} for Shallow Neural Networks},
 url = {https://openreview.net/forum?id=RjZq6W6FoE},
 year = {2023}
}

@article{pomponi2022centroids,
 abstract = {Catastrophic forgetting (CF) occurs when a neural network loses the information previously learned while training on a set of samples from a different distribution, i.e., a new task. Existing approaches have achieved remarkable results in mitigating CF, especially in a scenario called task incremental learning. However, this scenario is not realistic, and limited work has been done to achieve good results on more realistic scenarios. In this paper, we propose a novel regularization method called Centroids Matching, that, inspired by meta-learning approaches, fights CF by operating in the feature space produced by the neural network, achieving good results while requiring a small memory footprint. Specifically, the approach classifies the samples directly using the feature vectors produced by the neural network, by matching those vectors with the centroids representing the classes from the current task, or all the tasks up to that point. Centroids Matching is faster than competing baselines, and it can be exploited to efficiently mitigate CF, by preserving the distances between the embedding space produced by the model when past tasks were over, and the one currently produced, leading to a method that achieves high accuracy on all the tasks, without using an external memory when operating on easy scenarios, or using a small one for more realistic ones. Extensive experiments demonstrate that Centroids Matching achieves accuracy gains on multiple datasets and scenarios.},
 author = {Jary Pomponi and Simone Scardapane and Aurelio Uncini},
 code = {https://github.com/jaryP/CentroidsMatching},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4300201182},
 pdf = {https://openreview.net/pdf?id=7gzQltQSwr},
 review = {https://openreview.net/forum?id=7gzQltQSwr},
 title = {Centroids Matching: an efficient Continual Learning approach operating in the embedding space},
 url = {https://openreview.net/forum?id=7gzQltQSwr},
 year = {2022}
}

@article{ponnoprat2023dirichlet,
 abstract = {Given an empirical distribution $f(x)$ of sensitive data $x$, we consider the task of minimizing $F(y) = D_{\text{KL}} (f(x)\Vert y)$ over a probability simplex, while protecting the privacy of $x$. We observe that, if we take the exponential mechanism and use the KL divergence as the loss function, then the resulting algorithm is the Dirichlet mechanism that outputs a single draw from a Dirichlet distribution. Motivated by this, we propose a R\'enyi differentially private (RDP) algorithm that employs the Dirichlet mechanism to solve the KL divergence minimization task. In addition, given $f(x)$ as above and $\hat{y}$ an output of the Dirichlet mechanism, we prove a probability tail bound on $D_{\text{KL}} (f(x)\Vert \hat{y})$, which is then used to derive a lower bound for the sample complexity of our RDP algorithm. Experiments on real-world datasets demonstrate advantages of our algorithm over Gaussian and Laplace mechanisms in supervised classification and maximum likelihood estimation.},
 author = {Donlapark Ponnoprat},
 code = {https://github.com/donlapark/Dirichlet-Mechanism},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286693812},
 pdf = {https://openreview.net/pdf?id=lmr2WwlaFc},
 review = {https://openreview.net/forum?id=lmr2WwlaFc},
 title = {Dirichlet Mechanism for Differentially Private KL Divergence Minimization},
 url = {https://openreview.net/forum?id=lmr2WwlaFc},
 year = {2023}
}

@article{pouplin2023identifying,
 abstract = {Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points. Generative models are often stochastic, causing the data space, the Riemannian metric, and the geodesics, to be stochastic as well. Stochastic objects are at best impractical, and at worst impossible, to manipulate. A common solution is to approximate the stochastic pullback metric by its expectation. But the geodesics derived from this expected Riemannian metric do not correspond to the expected length-minimising curves. In this work, we propose another metric whose geodesics explicitly minimise the expected length of the pullback metric. We show this metric defines a Finsler metric, and we compare it with the expected Riemannian metric. In high dimensions, we prove that both metrics converge to each other at a rate of $O\left(\frac{1}{D}\right)$. This convergence implies that the established expected Riemannian metric is an accurate approximation of the theoretically more grounded Finsler metric. This provides justification for using the expected Riemannian metric for practical implementations.},
 author = {Alison Pouplin and David Eklund and Carl Henrik Ek and S{\o}ren Hauberg},
 code = {https://github.com/a-pouplin/latent_distances_finsler},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4312090883},
 pdf = {https://openreview.net/pdf?id=Q2Gi0TUAdS},
 review = {https://openreview.net/forum?id=Q2Gi0TUAdS},
 title = {Identifying latent distances with Finslerian geometry},
 url = {https://openreview.net/forum?id=Q2Gi0TUAdS},
 year = {2023}
}

@article{prabhu2023bridging,
 abstract = {Sim2Real domain adaptation (DA) research focuses on the constrained setting of adapting from a labeled synthetic source domain to an unlabeled or sparsely labeled real target domain. However, for high-stakes applications (e.g. autonomous driving), it is common to have a modest amount of human-labeled real data in addition to plentiful auto-labeled source data (e.g. from a driving simulator). We study this setting of supervised sim2real DA applied to 2D object detection. We propose Domain Translation via Conditional Alignment and Reweighting (CARE) a novel algorithm that systematically exploits target labels to explicitly close the sim2real appearance and content gaps. We present an analytical justification of our algorithm and demonstrate strong gains over competing methods on standard benchmarks.},
 author = {Viraj Uday Prabhu and David Acuna and Rafid Mahmood and Marc T. Law and Yuan-Hong Liao and Judy Hoffman and Sanja Fidler and James Lucas},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320343031},
 pdf = {https://openreview.net/pdf?id=lAQQx7hlku},
 review = {https://openreview.net/forum?id=lAQQx7hlku},
 title = {Bridging the Sim2Real gap with CARE: Supervised Detection Adaptation with Conditional Alignment and Reweighting},
 url = {https://openreview.net/forum?id=lAQQx7hlku},
 year = {2023}
}

@article{prakash2024federated,
 abstract = {Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit communication cost for the hulls, we propose a new quantization method for the Poincar\'e disc coupled with Reed-Solomon-like encoding. Fourth, at the server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\sim 11\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces.},
 author = {Saurav Prakash and Jin Sima and Chao Pan and Eli Chien and Olgica Milenkovic},
 code = {https://github.com/sauravpr/hyperbolic_federated_classification},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385848988},
 pdf = {https://openreview.net/pdf?id=umggDfMHha},
 review = {https://openreview.net/forum?id=umggDfMHha},
 title = {Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls},
 url = {https://openreview.net/forum?id=umggDfMHha},
 year = {2024}
}

@article{pramanick2023volta,
 author = {Shraman Pramanick and Li Jing and Sayan Nag and Jiachen Zhu and Hardik J Shah and Yann LeCun and Rama Chellappa},
 code = {https://github.com/ShramanPramanick/VoLTA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Kt2VJrCKo4},
 review = {https://openreview.net/forum?id=Kt2VJrCKo4},
 title = {Vo{LTA}: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment},
 url = {https://openreview.net/forum?id=Kt2VJrCKo4},
 year = {2023}
}

@article{prieto2023parameter,
 author = {Lucas Prieto and Jeroen Den Boef and Paul Groth and Joran Cornelisse},
 code = {https://github.com/LucasPrietoAl/GNPD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LIT8tjs6rJ},
 review = {https://openreview.net/forum?id=LIT8tjs6rJ},
 title = {Parameter Efficient Node Classification on Homophilic Graphs},
 url = {https://openreview.net/forum?id=LIT8tjs6rJ},
 year = {2023}
}

@article{qi2023attentionalbiased,
 abstract = {In this paper, we present a simple yet effective provable method (named ABSGD) for addressing the data imbalance or label noise problem in deep learning. Our method is a simple modification to momentum SGD where we assign an individual importance weight to each sample in the mini-batch. The individual-level weight of sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of distributionally robust optimization (DRO). Depending on whether the scaling factor is positive or negative, ABSGD is guaranteed to converge to a stationary point of an information-regularized min-max or min-min DRO problem, respectively. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propagations for computing mini-batch stochastic gradients, our method is more efficient with only one backward propagation at each iteration as in standard deep learning methods. ABSGD is flexible enough to combine with other robust losses without any additional cost. Our empirical studies on several benchmark datasets demonstrate the effectiveness of the proposed method.\footnote{Code is available at:\url{https://github.com/qiqi-helloworld/ABSGD/}}},
 author = {Qi Qi and Yi Xu and Wotao Yin and Rong Jin and Tianbao Yang},
 code = {https://github.com/qiqi-helloworld/ABSGD/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4312224476},
 pdf = {https://openreview.net/pdf?id=B0WYWvVA2r},
 review = {https://openreview.net/forum?id=B0WYWvVA2r},
 title = {Attentional-Biased Stochastic Gradient Descent},
 url = {https://openreview.net/forum?id=B0WYWvVA2r},
 year = {2023}
}

@article{qi2023stochastic,
 abstract = {Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.},
 author = {Qi Qi and Jiameng Lyu and Kung-Sik Chan and Er-Wei Bai and Tianbao Yang},
 code = {https://github.com/qiqi-helloworld/SCDRO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306176138},
 pdf = {https://openreview.net/pdf?id=VpaXrBFYZ9},
 review = {https://openreview.net/forum?id=VpaXrBFYZ9},
 title = {Stochastic Constrained DRO with a Complexity Independent of Sample Size},
 url = {https://openreview.net/forum?id=VpaXrBFYZ9},
 year = {2023}
}

@article{qiao2024transfer,
 author = {Sheng Qiao and Yong He and Wenxin Zhou},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=d3xwrfAG4V},
 review = {https://openreview.net/forum?id=d3xwrfAG4V},
 title = {Transfer Learning for High-dimensional Quantile Regression with Statistical Guarantee},
 url = {https://openreview.net/forum?id=d3xwrfAG4V},
 year = {2024}
}

@article{qin2023linearized,
 abstract = {Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications. Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification. Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers. The code is available at https://github.com/OpenNLPLab/Lrpe.},
 author = {Zhen Qin and Weixuan Sun and Kaiyue Lu and Hui Deng and Dongxu Li and Xiaodong Han and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
 code = {https://github.com/OpenNLPLab/Lrpe},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384811648},
 pdf = {https://openreview.net/pdf?id=xoLyps2qWc},
 review = {https://openreview.net/forum?id=xoLyps2qWc},
 title = {Linearized Relative Positional Encoding},
 url = {https://openreview.net/forum?id=xoLyps2qWc},
 year = {2023}
}

@article{qiu2022gfnet,
 author = {Haibo Qiu and Baosheng Yu and Dacheng Tao},
 code = {https://github.com/haibo-qiu/GFNet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LSAAlS7Yts},
 review = {https://openreview.net/forum?id=LSAAlS7Yts},
 title = {{GFN}et: Geometric Flow Network for 3D Point Cloud Semantic Segmentation},
 url = {https://openreview.net/forum?id=LSAAlS7Yts},
 year = {2022}
}

@article{rahman2022generative,
 abstract = {We propose the generative adversarial neural operator (GANO), a generative model paradigm for learning probabilities on infinite-dimensional function spaces. The natural sciences and engineering are known to have many types of data that are sampled from infinite-dimensional function spaces, where classical finite-dimensional deep generative adversarial networks (GANs) may not be directly applicable. GANO generalizes the GAN framework and allows for the sampling of functions by learning push-forward operator maps in infinite-dimensional spaces. GANO consists of two main components, a generator neural operator and a discriminator neural functional. The inputs to the generator are samples of functions from a user-specified probability measure, e.g., Gaussian random field (GRF), and the generator outputs are synthetic data functions. The input to the discriminator is either a real or synthetic data function. In this work, we instantiate GANO using the Wasserstein criterion and show how the Wasserstein loss can be computed in infinite-dimensional spaces. We empirically study GANO in controlled cases where both input and output functions are samples from GRFs and compare its performance to the finite-dimensional counterpart GAN. We empirically study the efficacy of GANO on real-world function data of volcanic activities and show its superior performance over GAN.},
 author = {Md Ashiqur Rahman and Manuel A Florez and Anima Anandkumar and Zachary E Ross and Kamyar Azizzadenesheli},
 code = {https://github.com/kazizzad/GANO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4229459060},
 pdf = {https://openreview.net/pdf?id=X1VzbBU6xZ},
 review = {https://openreview.net/forum?id=X1VzbBU6xZ},
 title = {Generative Adversarial Neural Operators},
 url = {https://openreview.net/forum?id=X1VzbBU6xZ},
 year = {2022}
}

@article{rahman2023generating,
 abstract = {Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.},
 author = {Arrasy Rahman and Elliot Fosong and Ignacio Carlucho and Stefano V Albrecht},
 code = {https://github.com/uoe-agents/BRDiv},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4288803944},
 pdf = {https://openreview.net/pdf?id=l5BzfQhROl},
 review = {https://openreview.net/forum?id=l5BzfQhROl},
 title = {Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity},
 url = {https://openreview.net/forum?id=l5BzfQhROl},
 year = {2023}
}

@article{rahman2023uno,
 abstract = {Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g., function spaces. Prior works on neural operators proposed a series of novel methods to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-NO on PDE benchmarks, namely, Darcy's flow law and the Navier-Stokes equations. We show that U-NO results in an average of 26% and 44% prediction improvement on Darcy's flow and turbulent Navier-Stokes equations, respectively, over the state of the art. On Navier-Stokes 3D spatiotemporal operator learning task, we show U-NO provides 37% improvement over the state of art methods.},
 author = {Md Ashiqur Rahman and Zachary E Ross and Kamyar Azizzadenesheli},
 code = {https://github.com/ashiq24/UNO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4224952693},
 pdf = {https://openreview.net/pdf?id=j3oQF9coJd},
 review = {https://openreview.net/forum?id=j3oQF9coJd},
 title = {U-NO: U-shaped Neural Operators},
 url = {https://openreview.net/forum?id=j3oQF9coJd},
 year = {2023}
}

@article{raja2022distributed,
 abstract = {This paper considers the problem of estimating the principal eigenvector of a covariance matrix from independent and identically distributed data samples in streaming settings. The streaming rate of data in many contemporary applications can be high enough that a single processor cannot finish an iteration of existing methods for eigenvector estimation before a new sample arrives. This paper formulates and analyzes a distributed variant of the classical Krasulina's method (D-Krasulina) that can keep up with the high streaming rate of data by distributing the computational load across multiple processing nodes. The analysis shows that---under appropriate conditions---D-Krasulina converges to the principal eigenvector in an order-wise optimal manner; i.e., after receiving $M$ samples across all nodes, its estimation error can be $O(1/M)$. In order to reduce the network communication overhead, the paper also develops and analyzes a mini-batch extension of D-Krasulina, which is termed DM-Krasulina. The analysis of DM-Krasulina shows that it can also achieve order-optimal estimation error rates under appropriate conditions, even when some samples have to be discarded within the network due to communication latency. Finally, experiments are performed over synthetic and real-world data to validate the convergence behaviors of D-Krasulina and DM-Krasulina in high-rate streaming settings.},
 author = {Haroon Raja and Waheed Bajwa},
 code = {https://github.com/INSPIRE-Lab-US/Distributed-streaming-PCA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W2997657780},
 pdf = {https://openreview.net/pdf?id=CExeD0jpB6},
 review = {https://openreview.net/forum?id=CExeD0jpB6},
 title = {Distributed Stochastic Algorithms for High-rate Streaming Principal Component Analysis},
 url = {https://openreview.net/forum?id=CExeD0jpB6},
 year = {2022}
}

@article{ramezani-kebrya2022mixtailor,
 abstract = {Implementations of SGD on distributed systems create new vulnerabilities, which can be identified and misused by one or more adversarial agents. Recently, it has been shown that well-known Byzantine-resilient gradient aggregation schemes are indeed vulnerable to informed attackers that can tailor the attacks (Fang et al., 2020; Xie et al., 2020b). We introduce MixTailor, a scheme based on randomization of the aggregation strategies that makes it impossible for the attacker to be fully informed. Deterministic schemes can be integrated into MixTailor on the fly without introducing any additional hyperparameters. Randomization decreases the capability of a powerful adversary to tailor its attacks, while the resulting randomized aggregation scheme is still competitive in terms of performance. For both iid and non-iid settings, we establish almost sure convergence guarantees that are both stronger and more general than those available in the literature. Our empirical studies across various datasets, attacks, and settings, validate our hypothesis and show that MixTailor successfully defends when well-known Byzantine-tolerant schemes fail.},
 author = {Ali Ramezani-Kebrya and Iman Tabrizian and Fartash Faghri and Petar Popovski},
 code = {https://github.com/Tabrizian/mix-tailor},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286233900},
 pdf = {https://openreview.net/pdf?id=tqDhrbKJLS},
 review = {https://openreview.net/forum?id=tqDhrbKJLS},
 title = {MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks},
 url = {https://openreview.net/forum?id=tqDhrbKJLS},
 year = {2022}
}

@article{ramezani-kebrya2023federated,
 abstract = {This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients.},
 author = {Ali Ramezani-Kebrya and Fanghui Liu and Thomas Pethick and Grigorios Chrysos and Volkan Cevher},
 code = {https://github.com/LIONS-EPFL/Federated_Learning_Covariate_Shift_Code},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380136785},
 pdf = {https://openreview.net/pdf?id=N7lCDaeNiS},
 review = {https://openreview.net/forum?id=N7lCDaeNiS},
 title = {Federated Learning under Covariate Shifts with Generalization Guarantees},
 url = {https://openreview.net/forum?id=N7lCDaeNiS},
 year = {2023}
}

@article{ramkumar2023learn,
 abstract = {Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalizable features. We show that our training paradigm provides consistent performance gains across datasets in both classification and few-shot settings. We further show that it leads to more robust and well-calibrated models.},
 author = {Vijaya Raghavan T Ramkumar and Elahe Arani and Bahram Zonooz},
 code = {https://github.com/NeurAI-Lab/LURE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4330337906},
 pdf = {https://openreview.net/pdf?id=WN1O2MJDST},
 review = {https://openreview.net/forum?id=WN1O2MJDST},
 title = {Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks},
 url = {https://openreview.net/forum?id=WN1O2MJDST},
 year = {2023}
}

@article{ranadive2023on,
 abstract = {It is commonly observed that deep networks trained for classification exhibit class-selective neurons in their early and intermediate layers. Intriguingly, recent studies have shown that these class-selective neurons can be ablated without deteriorating network function. But if class-selective neurons are not necessary, why do they exist? We attempt to answer this question in a series of experiments on ResNet-50s trained on ImageNet. We first show that class-selective neurons emerge during the first few epochs of training, before receding rapidly but not completely; this suggests that class-selective neurons found in trained networks are in fact vestigial remains of early training. With single-neuron ablation experiments, we then show that class-selective neurons are important for network function in this early phase of training. We also observe that the network is close to a linear regime in this early phase; we thus speculate that class-selective neurons appear early in training as quasi-linear shortcut solutions to the classification task. Finally, in causal experiments where we regularize against class selectivity at different points in training, we show that the presence of class-selective neurons early in training is critical to the successful training of the network; in contrast, class-selective neurons can be suppressed later in training with little effect on final accuracy. It remains to be understood by which mechanism the presence of class-selective neurons in the early phase of training contributes to the successful training of networks.},
 author = {Omkar Ranadive and Nikhil Thakurdesai and Ari S. Morcos and Matthew L Leavitt and Stephane Deny},
 code = {https://github.com/Omkar-Ranadive/Class-Selective-Neurons},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4378770583},
 pdf = {https://openreview.net/pdf?id=JaNlH6dZYk},
 review = {https://openreview.net/forum?id=JaNlH6dZYk},
 title = {On the special role of class-selective neurons in early training},
 url = {https://openreview.net/forum?id=JaNlH6dZYk},
 year = {2023}
}

@article{rath2024discovering,
 author = {Lucas Rath and Alexander von Rohr and Andreas Schultze and Sebastian Trimpe and Burkhard Corves},
 code = {https://github.com/lucasrm25/Model-Structure-Selection-CBOSS},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=2iOOvQmJBK},
 review = {https://openreview.net/forum?id=2iOOvQmJBK},
 title = {Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian Optimization},
 url = {https://openreview.net/forum?id=2iOOvQmJBK},
 year = {2024}
}

@article{rebain2023attention,
 abstract = {Neural fields model signals by mapping coordinate inputs to sampled values. They are becoming an increasingly important backbone architecture across many fields from vision and graphics to biology and astronomy. In this paper, we explore the differences between common conditioning mechanisms within these networks, an essential ingredient in shifting neural fields from memorization of signals to generalization, where the set of signals lying on a manifold is modelled jointly. In particular, we are interested in the scaling behaviour of these mechanisms to increasingly high-dimensional conditioning variables. As we show in our experiments, high-dimensional conditioning is key to modelling complex data distributions, thus it is important to determine what architecture choices best enable this when working on such problems. To this end, we run experiments modelling 2D, 3D, and 4D signals with neural fields, employing concatenation, hyper-network, and attention-based conditioning strategies -- a necessary but laborious effort that has not been performed in the literature. We find that attention-based conditioning outperforms other approaches in a variety of settings.},
 author = {Daniel Rebain and Mark J. Matthews and Kwang Moo Yi and Gopal Sharma and Dmitry Lagun and Andrea Tagliasacchi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296974352},
 pdf = {https://openreview.net/pdf?id=GzqdMrFQsE},
 review = {https://openreview.net/forum?id=GzqdMrFQsE},
 title = {Attention Beats Concatenation for Conditioning Neural Fields},
 url = {https://openreview.net/forum?id=GzqdMrFQsE},
 year = {2023}
}

@article{reed2022a,
 abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
 author = {Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio G{\'o}mez Colmenarejo and Alexander Novikov and Gabriel Barth-maron and Mai Gim{\'e}nez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification, Outstanding Certification},
 openalex = {W4307868887},
 pdf = {https://openreview.net/pdf?id=1ikK0kHjvj},
 review = {https://openreview.net/forum?id=1ikK0kHjvj},
 title = {A Generalist Agent},
 url = {https://openreview.net/forum?id=1ikK0kHjvj},
 year = {2022}
}

@article{reinke2023successor,
 abstract = {Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies for general reward functions. We introduce different SFR variations, prove its convergence, and provide a guarantee on its transfer performance. Experimental evaluations based on SFR with function approximation demonstrate its advantage over SF not only for general reward functions, but also in the case of linearly decomposable reward functions.},
 author = {Chris Reinke and Xavier Alameda-Pineda},
 code = {https://gitlab.inria.fr/robotlearn/sfr_learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286890519},
 pdf = {https://openreview.net/pdf?id=MTFf1rDDEI},
 review = {https://openreview.net/forum?id=MTFf1rDDEI},
 title = {Successor Feature Representations},
 url = {https://openreview.net/forum?id=MTFf1rDDEI},
 year = {2023}
}

@article{reizinger2023jacobianbased,
 author = {Patrik Reizinger and Yash Sharma and Matthias Bethge and Bernhard Sch{\"o}lkopf and Ferenc Husz{\'a}r and Wieland Brendel},
 code = {https://github.com/rpatrik96/nl-causal-representations},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=2Yo9xqR6Ab},
 review = {https://openreview.net/forum?id=2Yo9xqR6Ab},
 title = {Jacobian-based Causal Discovery with Nonlinear {ICA}},
 url = {https://openreview.net/forum?id=2Yo9xqR6Ab},
 year = {2023}
}

@article{rhodes2022enhanced,
 abstract = {The recent introduction of gradient-based MCMC for discrete spaces holds great promise, and comes with the tantalising possibility of new discrete counterparts to celebrated continuous methods such as MALA and HMC. Towards this goal, we introduce several discrete Metropolis-Hastings samplers that are conceptually-inspired by MALA, and demonstrate their strong empirical performance across a range of challenging sampling problems in Bayesian inference and energy-based modelling. Methodologically, we identify why discrete analogues to preconditioned MALA are generally intractable, motivating us to introduce a new kind of preconditioning based on auxiliary variables and the `Gaussian integral trick'.},
 author = {Benjamin Rhodes and Michael U. Gutmann},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4289645243},
 pdf = {https://openreview.net/pdf?id=j2Mid5hFUJ},
 review = {https://openreview.net/forum?id=j2Mid5hFUJ},
 title = {Enhanced gradient-based MCMC in discrete spaces},
 url = {https://openreview.net/forum?id=j2Mid5hFUJ},
 year = {2022}
}

@article{riaz2023partial,
 author = {Bilal Riaz and Yuksel Karahan and Austin J. Brockmeier},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=75CcopPxIr},
 review = {https://openreview.net/forum?id=75CcopPxIr},
 title = {Partial Optimal Transport for Support Subset Selection},
 url = {https://openreview.net/forum?id=75CcopPxIr},
 year = {2023}
}

@article{rigter2024world,
 abstract = {World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD, score-based generative models, and classifier-guided diffusion models. Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion. For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements. For long trajectories, PolyGRAD obtains comparable performance to baselines. Our experiments demonstrate that PolyGRAD enables performant policies to be trained via on-policy RL in imagination for MuJoCo continuous control domains. Thus, PolyGRAD introduces a new paradigm for accurate on-policy world modelling without autoregressive sampling.},
 author = {Marc Rigter and Jun Yamada and Ingmar Posner},
 code = {https://github.com/marc-rigter/polygrad-world-models},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4389814346},
 pdf = {https://openreview.net/pdf?id=9CcgO0LhKG},
 review = {https://openreview.net/forum?id=9CcgO0LhKG},
 title = {World Models via Policy-Guided Trajectory Diffusion},
 url = {https://openreview.net/forum?id=9CcgO0LhKG},
 year = {2024}
}

@article{roh2023drfairness,
 author = {Yuji Roh and Weili Nie and De-An Huang and Steven Euijong Whang and Arash Vahdat and Anima Anandkumar},
 code = {https://github.com/NVlabs/Dr-Fairness},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=TyBd56VK7z},
 review = {https://openreview.net/forum?id=TyBd56VK7z},
 title = {Dr-Fairness: Dynamic Data Ratio Adjustment for Fair Training on Real and Generated Data},
 url = {https://openreview.net/forum?id=TyBd56VK7z},
 year = {2023}
}

@article{rohrhofer2023on,
 abstract = {This paper empirically studies commonly observed training difficulties of Physics-Informed Neural Networks (PINNs) on dynamical systems. Our results indicate that fixed points which are inherent to these systems play a key role in the optimization of the in PINNs embedded physics loss function. We observe that the loss landscape exhibits local optima that are shaped by the presence of fixed points. We find that these local optima contribute to the complexity of the physics loss optimization which can explain common training difficulties and resulting nonphysical predictions. Under certain settings, e.g., initial conditions close to fixed points or long simulations times, we show that those optima can even become better than that of the desired solution.},
 author = {Franz M. Rohrhofer and Stefan Posch and Clemens G{\"o}{\ss}nitzer and Bernhard C Geiger},
 code = {https://github.com/frohrhofer/PINNs_fixed_points},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221166763},
 pdf = {https://openreview.net/pdf?id=56cTmVrg5w},
 review = {https://openreview.net/forum?id=56cTmVrg5w},
 title = {On the Role of Fixed Points of Dynamical Systems in Training Physics-Informed Neural Networks},
 url = {https://openreview.net/forum?id=56cTmVrg5w},
 year = {2023}
}

@article{romero2024wavelet,
 abstract = {Leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. However, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: time-series. In this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. We identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. Intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the wavelet transform. Inspired by this resemblance, we term our networks Wavelet Networks, and show that they perform nested non-linear wavelet-like time-frequency transforms. Empirical results show that Wavelet Networks outperform conventional CNNs on raw waveforms, and match strongly engineered spectrogram techniques across several tasks and time-series types, including audio, environmental sounds, and electrical signals. Our code is publicly available at https://github.com/dwromero/wavelet_networks.},
 author = {David W. Romero and Erik J Bekkers and Jakub M. Tomczak and Mark Hoogendoorn},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/dwromero/wavelet_networks},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4391156722},
 pdf = {https://openreview.net/pdf?id=ga5SNulYet},
 review = {https://openreview.net/forum?id=ga5SNulYet},
 title = {Wavelet Networks: Scale Equivariant Learning From Raw Waveforms},
 url = {https://openreview.net/forum?id=ga5SNulYet},
 year = {2024}
}

@article{rosca2023on,
 abstract = {The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.},
 author = {Mihaela Rosca and Yan Wu and Chongli Qin and Benoit Dherin},
 code = {https://github.com/deepmind/discretisation_drift},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319451598},
 pdf = {https://openreview.net/pdf?id=EYrRzKPinA},
 review = {https://openreview.net/forum?id=EYrRzKPinA},
 title = {On a continuous time model of gradient descent dynamics and instability in deep learning},
 url = {https://openreview.net/forum?id=EYrRzKPinA},
 year = {2023}
}

@article{ross2023learning,
 abstract = {Hamiltonian mechanics is one of the cornerstones of natural sciences. Recently there has been significant interest in learning Hamiltonian systems in a free-form way directly from trajectory data. Previous methods have tackled the problem of learning from many short, low-noise trajectories, but learning from a small number of long, noisy trajectories, whilst accounting for model uncertainty has not been addressed. In this work, we present a Gaussian process model for Hamiltonian systems with efficient decoupled parameterisation, and introduce an energy-conserving shooting method that allows robust inference from both short and long trajectories. We demonstrate the method's success in learning Hamiltonian systems in various data settings.},
 author = {Magnus Ross and Markus Heinonen},
 code = {https://github.com/magnusross/hgp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323323208},
 pdf = {https://openreview.net/pdf?id=DHEZuKStzH},
 review = {https://openreview.net/forum?id=DHEZuKStzH},
 title = {Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processes},
 url = {https://openreview.net/forum?id=DHEZuKStzH},
 year = {2023}
}

@article{ross2024neural,
 abstract = {Natural data observed in $\mathbb{R}^n$ is often constrained to an $m$-dimensional manifold $\mathcal{M}$, where $m < n$. This work focuses on the task of building theoretically principled generative models for such data. Current generative models learn $\mathcal{M}$ by mapping an $m$-dimensional latent variable through a neural network $f_\theta: \mathbb{R}^m \to \mathbb{R}^n$. These procedures, which we call pushforward models, incur a straightforward limitation: manifolds cannot in general be represented with a single parameterization, meaning that attempts to do so will incur either computational instability or the inability to learn probability densities within the manifold. To remedy this problem, we propose to model $\mathcal{M}$ as a neural implicit manifold: the set of zeros of a neural network. We then learn the probability density within $\mathcal{M}$ with a constrained energy-based model, which employs a constrained variant of Langevin dynamics to train and sample from the learned manifold. In experiments on synthetic and natural data, we show that our model can learn manifold-supported distributions with complex topologies more accurately than pushforward models.},
 author = {Brendan Leigh Ross and Gabriel Loaiza-Ganem and Anthony L. Caterini and Jesse C. Cresswell},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/layer6ai-labs/implicit-manifolds},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4283445668},
 pdf = {https://openreview.net/pdf?id=lTOku838Zv},
 review = {https://openreview.net/forum?id=lTOku838Zv},
 title = {Neural Implicit Manifold Learning for Topology-Aware Density Estimation},
 url = {https://openreview.net/forum?id=lTOku838Zv},
 year = {2024}
}

@article{rouillard2023pavi,
 abstract = {Given observed data and a probabilistic generative model, Bayesian inference searches for the distribution of the model's parameters that could have yielded the data. Inference is challenging for large population studies where millions of measurements are performed over a cohort of hundreds of subjects, resulting in a massive parameter space. This large cardinality renders off-the-shelf Variational Inference (VI) computationally impractical. In this work, we design structured VI families that efficiently tackle large population studies. Our main idea is to share the parameterization and learning across the different i.i.d. variables in a generative model, symbolized by the model's \textit{plates}. We name this concept \textit{plate amortization}. Contrary to off-the-shelf stochastic VI, which slows down inference, plate amortization results in orders of magnitude faster to train variational distributions. Applied to large-scale hierarchical problems, PAVI yields expressive, parsimoniously parameterized VI with an affordable training time. This faster convergence effectively unlocks inference in those large regimes. We illustrate the practical utility of PAVI through a challenging Neuroimaging example featuring 400 million latent parameters, demonstrating a significant step towards scalable and expressive Variational Inference.},
 author = {Louis Rouillard and Alexandre Le Bris and Thomas Moreau and Demian Wassermann},
 badge = {Reproducibility},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4386360318},
 pdf = {https://openreview.net/pdf?id=vlY9GDCCA6},
 review = {https://openreview.net/forum?id=vlY9GDCCA6},
 title = {PAVI: Plate-Amortized Variational Inference},
 url = {https://openreview.net/forum?id=vlY9GDCCA6},
 year = {2023}
}

@article{roulet2023target,
 author = {Vincent Roulet and Zaid Harchaoui},
 code = {https://github.com/vroulet/tpri},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=vxyjTUPV24},
 review = {https://openreview.net/forum?id=vxyjTUPV24},
 title = {Target Propagation via Regularized Inversion for Recurrent Neural Networks},
 url = {https://openreview.net/forum?id=vxyjTUPV24},
 year = {2023}
}

@article{rozner2024domaingeneralizable,
 abstract = {This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.},
 author = {Amit Rozner and Barak Battash and Lior Wolf and Ofir Lindenbaum},
 code = {https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318906780},
 pdf = {https://openreview.net/pdf?id=O9RUANpPmb},
 review = {https://openreview.net/forum?id=O9RUANpPmb},
 title = {Domain-Generalizable Multiple-Domain Clustering},
 url = {https://openreview.net/forum?id=O9RUANpPmb},
 year = {2024}
}

@article{ruhkopf2023masif,
 author = {Tim Ruhkopf and Aditya Mohan and Difan Deng and Alexander Tornede and Frank Hutter and Marius Lindauer},
 badge = {Event: AutoML 2023},
 code = {https://github.com/automl/masif},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=5aYGXxByI6},
 review = {https://openreview.net/forum?id=5aYGXxByI6},
 title = {{MASIF}: Meta-learned Algorithm Selection using Implicit Fidelity Information},
 url = {https://openreview.net/forum?id=5aYGXxByI6},
 year = {2023}
}

@article{ruiz2022sequentially,
 abstract = {Causal discovery, the learning of causality in a data mining scenario, has been of strong scientific and theoretical interest as a starting point to identify "what causes what?" Contingent on assumptions and a proper learning algorithm, it is sometimes possible to identify and accurately estimate a causal directed acyclic graph (DAG), as opposed to a Markov equivalence class of graphs that gives ambiguity of causal directions. The focus of this paper is in highlighting the identifiability and estimation of DAGs with general error distributions through a general sequential sorting procedure that orders variables one at a time, starting at root nodes, followed by children of the root nodes, and so on until completion. We demonstrate a novel application of this general approach to estimate the topological ordering of a DAG. At each step of the procedure, only simple likelihood ratio scores are calculated on regression residuals to decide the next node to append to the current partial ordering. The computational complexity of our algorithm on a p-node problem is O(pd), where d is the maximum neighborhood size. Under mild assumptions, the population version of our procedure provably identifies a true ordering of the underlying DAG. We provide extensive numerical evidence to demonstrate that this sequential procedure scales to possibly thousands of nodes and works well for high-dimensional data. We accompany these numerical experiments with an application to a single-cell gene expression dataset.},
 author = {Gabriel Ruiz and OSCAR HERNAN MADRID PADILLA and Qing Zhou},
 code = {https://gabriel-ruiz.github.io/scorelingam/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281399847},
 pdf = {https://openreview.net/pdf?id=4pCjIGIjrt},
 review = {https://openreview.net/forum?id=4pCjIGIjrt},
 title = {Sequentially learning the topological ordering of causal directed acyclic graphs with likelihood ratio scores},
 url = {https://openreview.net/forum?id=4pCjIGIjrt},
 year = {2022}
}

@article{rusak2022if,
 abstract = {We demonstrate that self-learning techniques like entropy minimization and pseudo-labeling are simple and effective at improving performance of a deployed computer vision model under systematic domain shifts. We conduct a wide range of large-scale experiments and show consistent improvements irrespective of the model architecture, the pre-training technique or the type of distribution shift. At the same time, self-learning is simple to use in practice because it does not require knowledge or access to the original training data or scheme, is robust to hyperparameter choices, is straight-forward to implement and requires only a few adaptation epochs. This makes self-learning techniques highly attractive for any practitioner who applies machine learning algorithms in the real world. We present state-of-the-art adaptation results on CIFAR10-C (8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error), theoretically study the dynamics of self-supervised adaptation methods and propose a new classification dataset (ImageNet-D) which is challenging even with adaptation.},
 author = {Evgenia Rusak and Steffen Schneider and George Pachitariu and Luisa Eck and Peter Vincent Gehler and Oliver Bringmann and Wieland Brendel and Matthias Bethge},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/bethgelab/robustness},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4287200447},
 pdf = {https://openreview.net/pdf?id=vqRzLv6POg},
 review = {https://openreview.net/forum?id=vqRzLv6POg},
 title = {If your data distribution shifts, use self-learning},
 url = {https://openreview.net/forum?id=vqRzLv6POg},
 year = {2022}
}

@article{sabanayagam2023analysis,
 abstract = {The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory. In this work, we theoretically analyze the influence of different aspects of the GNN architecture using the Graph Neural Tangent Kernel in a semi-supervised node classification setting. Under the population Degree Corrected Stochastic Block Model, we prove that: (i) linear networks capture the class information as good as ReLU networks; (ii) row normalization preserves the underlying class structure better than other convolutions; (iii) performance degrades with network depth due to over-smoothing, but the loss in class information is the slowest in row normalization; (iv) skip connections retain the class information even at infinite depth, thereby eliminating over-smoothing. We finally validate our theoretical findings numerically and on real datasets such as Cora and Citeseer.},
 author = {Mahalakshmi Sabanayagam and Pascal Esser and Debarghya Ghoshdastidar},
 code = {https://github.com/mahalakshmi-sabanayagam/NTK_GCN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306890372},
 pdf = {https://openreview.net/pdf?id=xgYgDEof29},
 review = {https://openreview.net/forum?id=xgYgDEof29},
 title = {Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel},
 url = {https://openreview.net/forum?id=xgYgDEof29},
 year = {2023}
}

@article{sachdeva2023data,
 abstract = {The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.},
 author = {Noveen Sachdeva and Julian McAuley},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4315881290},
 pdf = {https://openreview.net/pdf?id=lmXMXP74TO},
 review = {https://openreview.net/forum?id=lmXMXP74TO},
 title = {Data Distillation: A Survey},
 url = {https://openreview.net/forum?id=lmXMXP74TO},
 year = {2023}
}

@article{sadashivaiah2024to,
 author = {Vijay Sadashivaiah and Keerthiram Murugesan and Ronny Luss and Pin-Yu Chen and Chris Sims and James Hendler and Amit Dhurandhar},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=BNP4MxzDEI},
 review = {https://openreview.net/forum?id=BNP4MxzDEI},
 title = {To Transfer or Not to Transfer: Suppressing Concepts from Source Representations},
 url = {https://openreview.net/forum?id=BNP4MxzDEI},
 year = {2024}
}

@article{sadeghi2022on,
 abstract = {Many applications of representation learning, such as privacy preservation, algorithmic fairness, and domain adaptation, desire explicit control over semantic information being discarded. This goal is formulated as satisfying two objectives: maximizing utility for predicting a target attribute while simultaneously being invariant (independent) to a known semantic attribute. Solutions to invariant representation learning (IRepL) problems lead to a trade-off between utility and invariance when they are competing. While existing works study bounds on this trade-off, two questions remain outstanding: 1) What is the exact trade-off between utility and invariance? and 2) What are the encoders (mapping the data to a representation) that achieve the trade-off, and how can we estimate it from training data? This paper addresses these questions for IRepLs in reproducing kernel Hilbert spaces (RKHS)s. Under the assumption that the distribution of a low-dimensional projection of high-dimensional data is approximately normal, we derive a closed-form solution for the global optima of the underlying optimization problem for encoders in RKHSs. This yields closed formulae for a near-optimal trade-off, corresponding optimal representation dimensionality, and the corresponding encoder(s). We also numerically quantify the trade-off on representative problems and compare them to those achieved by baseline IRepL algorithms.},
 author = {Bashir Sadeghi and Sepehr Dehdashtian and Vishnu Boddeti},
 badge = {Featured},
 code = {https://github.com/human-analysis/tradeoff-invariant-representation-learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4286984326},
 pdf = {https://openreview.net/pdf?id=3gfpBR1ncr},
 review = {https://openreview.net/forum?id=3gfpBR1ncr},
 title = {On Characterizing the Trade-off in Invariant Representation Learning},
 url = {https://openreview.net/forum?id=3gfpBR1ncr},
 year = {2022}
}

@article{sadrizadeh2023transfool,
 abstract = {Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to improvement in terms of success rate, semantic similarity, and fluency compared to the existing attacks both in white-box and black-box settings. Thus, TransFool permits us to better characterize the vulnerability of NMT models and outlines the necessity to design strong defense mechanisms and more robust NMT systems for real-life applications.},
 author = {Sahar Sadrizadeh and Ljiljana Dolamic and Pascal Frossard},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320340787},
 pdf = {https://openreview.net/pdf?id=sFk3aBNb81},
 review = {https://openreview.net/forum?id=sFk3aBNb81},
 title = {TransFool: An Adversarial Attack against Neural Machine Translation Models},
 url = {https://openreview.net/forum?id=sFk3aBNb81},
 year = {2023}
}

@article{saglam2024mitigating,
 abstract = {Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an adequate single-step off-policy correction that is applicable to deterministic policy networks. Theoretical and empirical studies demonstrate that it can achieve a "safe" off-policy learning and substantially improve the state-of-the-art by attaining higher returns in fewer steps than the competing methods through an effective schedule of the learning rate in Q-learning and policy optimization.},
 author = {Baturay Saglam and Do{\u{g}}an Can {\c{C}}i{\c{c}}ek and Furkan Burak Mutlu and Suleyman Kozat},
 code = {https://github.com/baturaysaglam/AC-Off-POC},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4289596367},
 pdf = {https://openreview.net/pdf?id=CyjG4ZKCtE},
 review = {https://openreview.net/forum?id=CyjG4ZKCtE},
 title = {Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach},
 url = {https://openreview.net/forum?id=CyjG4ZKCtE},
 year = {2024}
}

@article{saha2022on,
 author = {Saptarshi Saha and Utpal Garain},
 code = {https://github.com/Saptarshi-Saha-1996/Noise-Abduction-for-Counterfactuals},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4FU8Jz1Oyj},
 review = {https://openreview.net/forum?id=4FU8Jz1Oyj},
 title = {On Noise Abduction for Answering Counterfactual Queries: A Practical Outlook},
 url = {https://openreview.net/forum?id=4FU8Jz1Oyj},
 year = {2022}
}

@article{saha2023revisiting,
 abstract = {Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser arXiv:2108.09135 [cs.CV], the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout arXiv:1708.04552v2 [cs.CV] augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1\% to 62.3\% against a 3\% square patch applied anywhere on the image.},
 author = {Aniruddha Saha and Shuhua Yu and Mohammad Sadegh Norouzzadeh and Wan-Yi Lin and Chaithanya Kumar Mummadi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4381826952},
 pdf = {https://openreview.net/pdf?id=2tdhQMLg36},
 review = {https://openreview.net/forum?id=2tdhQMLg36},
 title = {Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches},
 url = {https://openreview.net/forum?id=2tdhQMLg36},
 year = {2023}
}

@article{saha2024learning,
 author = {Akash Saha and Balamurugan Palaniappan},
 code = {https://github.com/akashsaha06/graph-inducedOVK},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=f9l4eiPKpV},
 review = {https://openreview.net/forum?id=f9l4eiPKpV},
 title = {Learning Sparse Graphs for Functional Regression using Graph-induced Operator-valued Kernels},
 url = {https://openreview.net/forum?id=f9l4eiPKpV},
 year = {2024}
}

@article{sakhi2023fast,
 abstract = {An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.},
 author = {Otmane Sakhi and David Rohde and Nicolas Chopin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385934114},
 pdf = {https://openreview.net/pdf?id=f7a8XCRtUu},
 review = {https://openreview.net/forum?id=f7a8XCRtUu},
 title = {Fast Slate Policy Optimization: Going Beyond Plackett-Luce},
 url = {https://openreview.net/forum?id=f7a8XCRtUu},
 year = {2023}
}

@article{salehi2022a,
 abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which our survey covers extensively. Finally, having a unified cross-domain perspective, we discuss and shed light on future lines of research, intending to bring these fields closer together.},
 author = {Mohammadreza Salehi and Hossein Mirzaei and Dan Hendrycks and Yixuan Li and Mohammad Hossein Rohban and Mohammad Sabokrou},
 code = {https://github.com/taslimisina/osr-ood-ad-methods},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320460883},
 pdf = {https://openreview.net/pdf?id=aRtjVZvbpK},
 review = {https://openreview.net/forum?id=aRtjVZvbpK},
 title = {A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges},
 url = {https://openreview.net/forum?id=aRtjVZvbpK},
 year = {2022}
}

@article{salvador2023a,
 abstract = {Unsupervised Domain Adaptation (UDA) aims at classifying unlabeled target images leveraging source labeled ones. In this work, we consider the Partial Domain Adaptation (PDA) variant, where we have extra source classes not present in the target domain. Most successful algorithms use model selection strategies that rely on target labels to find the best hyper-parameters and/or models along training. However, these strategies violate the main assumption in PDA: only unlabeled target domain samples are available. Moreover, there are also inconsistencies in the experimental settings - architecture, hyper-parameter tuning, number of runs - yielding unfair comparisons. The main goal of this work is to provide a realistic evaluation of PDA methods with the different model selection strategies under a consistent evaluation protocol. We evaluate 7 representative PDA algorithms on 2 different real-world datasets using 7 different model selection strategies. Our two main findings are: (i) without target labels for model selection, the accuracy of the methods decreases up to 30 percentage points; (ii) only one method and model selection pair performs well on both datasets. Experiments were performed with our PyTorch framework, BenchmarkPDA, which we open source.},
 author = {Tiago Salvador and Kilian FATRAS and Ioannis Mitliagkas and Adam M Oberman},
 badge = {Event: CoLLAs 2023},
 code = {https://github.com/oberman-lab/BenchmarkPDA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4302305705},
 pdf = {https://openreview.net/pdf?id=XcVzIBXeRn},
 review = {https://openreview.net/forum?id=XcVzIBXeRn},
 title = {A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods},
 url = {https://openreview.net/forum?id=XcVzIBXeRn},
 year = {2023}
}

@article{sarfraz2023a,
 abstract = {Humans excel at continually acquiring, consolidating, and retaining information from an ever-changing environment, whereas artificial neural networks (ANNs) exhibit catastrophic forgetting. There are considerable differences in the complexity of synapses, the processing of information, and the learning mechanisms in biological neural networks and their artificial counterparts, which may explain the mismatch in performance. We consider a biologically plausible framework that constitutes separate populations of exclusively excitatory and inhibitory neurons that adhere to Dale's principle, and the excitatory pyramidal neurons are augmented with dendritic-like structures for context-dependent processing of stimuli. We then conduct a comprehensive study on the role and interactions of different mechanisms inspired by the brain, including sparse non-overlapping representations, Hebbian learning, synaptic consolidation, and replay of past activations that accompanied the learning event. Our study suggests that the employing of multiple complementary mechanisms in a biologically plausible architecture, similar to the brain, may be effective in enabling continual learning in ANNs.},
 author = {Fahad Sarfraz and Elahe Arani and Bahram Zonooz},
 badge = {Event: CoLLAs 2023},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4366233292},
 pdf = {https://openreview.net/pdf?id=DJr6zorJM2},
 review = {https://openreview.net/forum?id=DJr6zorJM2},
 title = {A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning},
 url = {https://openreview.net/forum?id=DJr6zorJM2},
 year = {2023}
}

@article{sarkans2023modelling,
 author = {Elvijs Sarkans and Sumon Ahmed and Magnus Rattray and Alexis Boukouvalas},
 code = {https://github.com/ManchesterBioinference/BranchedGP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=9KoBOlstTq},
 review = {https://openreview.net/forum?id=9KoBOlstTq},
 title = {Modelling sequential branching dynamics with a multivariate branching Gaussian process},
 url = {https://openreview.net/forum?id=9KoBOlstTq},
 year = {2023}
}

@article{sasso2023multisource,
 abstract = {A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.},
 author = {Remo Sasso and Matthia Sabatelli and Marco A. Wiering},
 code = {https://github.com/remosasso/multi-source-TL-for-deep-MBRL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281713815},
 pdf = {https://openreview.net/pdf?id=1nhTDzxxMA},
 review = {https://openreview.net/forum?id=1nhTDzxxMA},
 title = {Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning},
 url = {https://openreview.net/forum?id=1nhTDzxxMA},
 year = {2023}
}

@article{satija2023group,
 author = {Harsh Satija and Alessandro Lazaric and Matteo Pirotta and Joelle Pineau},
 code = {https://github.com/hercky/group-fairness-in-RL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=JkIH4MeOc3},
 review = {https://openreview.net/forum?id=JkIH4MeOc3},
 title = {Group Fairness in Reinforcement Learning},
 url = {https://openreview.net/forum?id=JkIH4MeOc3},
 year = {2023}
}

@article{saul2022a,
 author = {Lawrence K. Saul},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=p8gncJbMit},
 review = {https://openreview.net/forum?id=p8gncJbMit},
 title = {A geometrical connection between sparse and low-rank matrices and its application to manifold learning},
 url = {https://openreview.net/forum?id=p8gncJbMit},
 year = {2022}
}

@article{saul2023weightbalancing,
 author = {Lawrence K. Saul},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=uaHyXxyp2r},
 review = {https://openreview.net/forum?id=uaHyXxyp2r},
 title = {Weight-balancing fixes and flows for deep learning},
 url = {https://openreview.net/forum?id=uaHyXxyp2r},
 year = {2023}
}

@article{scemama2024on,
 abstract = {Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how this combination effects out-of-distribution coverage; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets. Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. We evaluate prediction sets as a result of combining split conformal methods and neural networks trained with (i) stochastic gradient descent, (ii) deep ensembles, and (iii) mean-field variational inference. Our results suggest that combining Bayesian deep learning models with split conformal prediction can, in some cases, cause unintended consequences such as reducing out-of-distribution coverage.},
 author = {Paul Scemama and Ariel Kapusta},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4388962970},
 pdf = {https://openreview.net/pdf?id=TySx8fsSSU},
 review = {https://openreview.net/forum?id=TySx8fsSSU},
 title = {On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning},
 url = {https://openreview.net/forum?id=TySx8fsSSU},
 year = {2024}
}

@article{schaipp2023a,
 abstract = {Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.},
 author = {Fabian Schaipp and Robert M. Gower and Michael Ulbrich},
 badge = {Reproducibility},
 code = {https://github.com/fabian-sp/ProxSPS},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4316126733},
 pdf = {https://openreview.net/pdf?id=jWr41htaB3},
 review = {https://openreview.net/forum?id=jWr41htaB3},
 title = {A Stochastic Proximal Polyak Step Size},
 url = {https://openreview.net/forum?id=jWr41htaB3},
 year = {2023}
}

@article{schiele2024disciplined,
 abstract = {We consider convex-concave saddle point problems, and more generally convex optimization problems we refer to as $\textit{saddle problems}$, which include the partial supremum or infimum of convex-concave saddle functions. Saddle problems arise in a wide range of applications, including game theory, machine learning, and finance. It is well known that a saddle problem can be reduced to a single convex optimization problem by dualizing either the convex (min) or concave (max) objectives, reducing a min-max problem into a min-min (or max-max) problem. Carrying out this conversion by hand can be tedious and error prone. In this paper we introduce $\textit{disciplined saddle programming}$ (DSP), a domain specific language (DSL) for specifying saddle problems, for which the dualizing trick can be automated. The language and methods are based on recent work by Juditsky and Nemirovski arXiv:2102.01002 [math.OC], who developed the idea of conic-representable saddle point programs, and showed how to carry out the required dualization automatically using conic duality. Juditsky and Nemirovski's conic representation of saddle problems extends Nesterov and Nemirovski's earlier development of conic representable convex problems; DSP can be thought of as extending disciplined convex programming (DCP) to saddle problems. Just as DCP makes it easy for users to formulate and solve complex convex problems, DSP allows users to easily formulate and solve saddle problems. Our method is implemented in an open-source package, also called DSP.},
 author = {Philipp Schiele and Eric Sager Luxenberg and Stephen P. Boyd},
 code = {https://github.com/cvxgrp/dsp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318906019},
 pdf = {https://openreview.net/pdf?id=KhMLfEIoUm},
 review = {https://openreview.net/forum?id=KhMLfEIoUm},
 title = {Disciplined Saddle Programming},
 url = {https://openreview.net/forum?id=KhMLfEIoUm},
 year = {2024}
}

@article{schioppa2023stacking,
 author = {Andrea Schioppa and Nal Kalchbrenner},
 code = {https://github.com/google-research/google-research/tree/master/lasagna_mt},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=mNEqiC924B},
 review = {https://openreview.net/forum?id=mNEqiC924B},
 title = {Stacking Diverse Architectures to Improve Machine Translation},
 url = {https://openreview.net/forum?id=mNEqiC924B},
 year = {2023}
}

@article{schlegel2023investigating,
 author = {Matthew Kyle Schlegel and Volodymyr Tkachuk and Adam M White and Martha White},
 badge = {Event: CoLLAs 2023},
 code = {https://github.com/mkschleg/ActionRNNs.jl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=K6g4MbAC1r},
 review = {https://openreview.net/forum?id=K6g4MbAC1r},
 title = {Investigating Action Encodings in Recurrent Neural Networks in Reinforcement Learning},
 url = {https://openreview.net/forum?id=K6g4MbAC1r},
 year = {2023}
}

@article{schmier2023positive,
 abstract = {Detecting test data deviating from training data is a central problem for safe and robust machine learning. Likelihoods learned by a generative model, e.g., a normalizing flow via standard log-likelihood training, perform poorly as an outlier score. We propose to use an unlabelled auxiliary dataset and a probabilistic outlier score for outlier detection. We use a self-supervised feature extractor trained on the auxiliary dataset and train a normalizing flow on the extracted features by maximizing the likelihood on in-distribution data and minimizing the likelihood on the contrastive dataset. We show that this is equivalent to learning the normalized positive difference between the in-distribution and the contrastive feature density. We conduct experiments on benchmark datasets and compare to the likelihood, the likelihood ratio and state-of-the-art anomaly detection methods.},
 author = {Robert Schmier and Ullrich Koethe and Christoph-Nikolas Straehle},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4294006717},
 pdf = {https://openreview.net/pdf?id=B4J40x7NjA},
 review = {https://openreview.net/forum?id=B4J40x7NjA},
 title = {Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data},
 url = {https://openreview.net/forum?id=B4J40x7NjA},
 year = {2023}
}

@article{schubert2023polter,
 abstract = {The goal of Unsupervised Reinforcement Learning (URL) is to find a reward-agnostic prior policy on a task domain, such that the sample-efficiency on supervised downstream tasks is improved. Although agents initialized with such a prior policy can achieve a significantly higher reward with fewer samples when finetuned on the downstream task, it is still an open question how an optimal pretrained prior policy can be achieved in practice. In this work, we present POLTER (Policy Trajectory Ensemble Regularization) - a general method to regularize the pretraining that can be applied to any URL algorithm and is especially useful on data- and knowledge-based URL algorithms. It utilizes an ensemble of policies that are discovered during pretraining and moves the policy of the URL algorithm closer to its optimal prior. Our method is based on a theoretical framework, and we analyze its practical effects on a white-box benchmark, allowing us to study POLTER with full control. In our main experiments, we evaluate POLTER on the Unsupervised Reinforcement Learning Benchmark (URLB), which consists of 12 tasks in 3 domains. We demonstrate the generality of our approach by improving the performance of a diverse set of data- and knowledge-based URL algorithms by 19% on average and up to 40% in the best case. Under a fair comparison with tuned baselines and tuned POLTER, we establish a new state-of-the-art for model-free methods on the URLB.},
 author = {Frederik Schubert and Carolin Benjamins and Sebastian D{\"o}hler and Bodo Rosenhahn and Marius Lindauer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281478003},
 pdf = {https://openreview.net/pdf?id=Hnr23knZfY},
 review = {https://openreview.net/forum?id=Hnr23knZfY},
 title = {POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning},
 url = {https://openreview.net/forum?id=Hnr23knZfY},
 year = {2023}
}

@article{schwarz2022metalearning,
 abstract = {Recent work in Deep Learning has re-imagined the representation of data as functions mapping from a coordinate space to an underlying continuous signal. When such functions are approximated by neural networks this introduces a compelling alternative to the more common multi-dimensional array representation. Recent work on such Implicit Neural Representations (INRs) has shown that - following careful architecture search - INRs can outperform established compression methods such as JPEG (e.g. Dupont et al., 2021). In this paper, we propose crucial steps towards making such ideas scalable: Firstly, we employ state-of-the-art network sparsification techniques to drastically improve compression. Secondly, introduce the first method allowing for sparsification to be employed in the inner-loop of commonly used Meta-Learning algorithms, drastically improving both compression and the computational cost of learning INRs. The generality of this formalism allows us to present results on diverse data modalities such as images, manifolds, signed distance functions, 3D shapes and scenes, several of which establish new state-of-the-art results.},
 author = {Jonathan Schwarz and Yee Whye Teh},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4280532700},
 pdf = {https://openreview.net/pdf?id=Cct7kqbHK6},
 review = {https://openreview.net/forum?id=Cct7kqbHK6},
 title = {Meta-Learning Sparse Compression Networks},
 url = {https://openreview.net/forum?id=Cct7kqbHK6},
 year = {2022}
}

@article{schwinn2022behind,
 author = {Leo Schwinn and Doina Precup and Bjoern Eskofier and Dario Zanca},
 code = {https://github.com/SchwinnL/NeVA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=7iSYW1FRWA},
 review = {https://openreview.net/forum?id=7iSYW1FRWA},
 title = {Behind the Machine{\textquoteright}s Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention},
 url = {https://openreview.net/forum?id=7iSYW1FRWA},
 year = {2022}
}

@article{scott2023crossclient,
 abstract = {We present Cross-Client Label Propagation(XCLP), a new method for transductive federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches.},
 author = {Jonathan Scott and Michelle Yeo and Christoph H Lampert},
 code = {https://github.com/jonnyascott/xclp},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306179353},
 pdf = {https://openreview.net/pdf?id=gY04GX8R5k},
 review = {https://openreview.net/forum?id=gY04GX8R5k},
 title = {Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning},
 url = {https://openreview.net/forum?id=gY04GX8R5k},
 year = {2023}
}

@article{segert2022a,
 abstract = {Understanding how agents learn to generalize -- and, in particular, to extrapolate -- in high-dimensional, naturalistic environments remains a challenge for both machine learning and the study of biological agents. One approach to this has been the use of function learning paradigms, which allow peoples' empirical patterns of generalization for smooth scalar functions to be described precisely. However, to date, such work has not succeeded in identifying mechanisms that acquire the kinds of general purpose representations over which function learning can operate to exhibit the patterns of generalization observed in human empirical studies. Here, we present a framework for how a learner may acquire such representations, that then support generalization -- and extrapolation in particular -- in a few-shot fashion. Taking inspiration from a classic theory of visual processing, we construct a self-supervised encoder that implements the basic inductive bias of invariance under topological distortions. We show the resulting representations outperform those from other models for unsupervised time series learning in several downstream function learning tasks, including extrapolation.},
 author = {Simon Segert and Jonathan Cohen},
 code = {https://github.com/SimonSegert/functionlearning-contrastive-tmlr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287119467},
 pdf = {https://openreview.net/pdf?id=ILPFasEaHA},
 review = {https://openreview.net/forum?id=ILPFasEaHA},
 title = {A Self-Supervised Framework for Function Learning and Extrapolation},
 url = {https://openreview.net/forum?id=ILPFasEaHA},
 year = {2022}
}

@article{selega2023multiobjective,
 abstract = {Many practical applications require optimization of multiple, computationally expensive, and possibly competing objectives that are well-suited for multi-objective Bayesian optimization (MOBO) procedures. However, for many types of biomedical data, measures of data analysis workflow success are often heuristic and therefore it is not known a priori which objectives are useful. Thus, MOBO methods that return the full Pareto front may be suboptimal in these cases. Here we propose a novel MOBO method that adaptively updates the scalarization function using properties of the posterior of a multi-output Gaussian process surrogate function. This approach selects useful objectives based on a flexible set of desirable criteria, allowing the functional form of each objective to guide optimization. We demonstrate the qualitative behaviour of our method on toy data and perform proof-of-concept analyses of single-cell RNA sequencing and highly multiplexed imaging datasets.},
 author = {Alina Selega and Kieran R. Campbell},
 badge = {Event: AutoML 2023},
 code = {https://github.com/camlab-bioml/2022_manatee_paper_analyses},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282563525},
 pdf = {https://openreview.net/pdf?id=QspAcsAyis},
 review = {https://openreview.net/forum?id=QspAcsAyis},
 title = {Multi-objective Bayesian Optimization with Heuristic Objectives for Biomedical and Molecular Data Analysis Workflows},
 url = {https://openreview.net/forum?id=QspAcsAyis},
 year = {2023}
}

@article{sen2022inrv,
 abstract = {Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showcasing the potential of the proposed representation space.},
 author = {Bipasha Sen and Aditya Agarwal and Vinay P Namboodiri and C.V. Jawahar},
 code = {https://skymanaditya1.github.io/INRV/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307928875},
 pdf = {https://openreview.net/pdf?id=aIoEkwc2oB},
 review = {https://openreview.net/forum?id=aIoEkwc2oB},
 title = {INR-V: A Continuous Representation Space for Video-based Generative Tasks},
 url = {https://openreview.net/forum?id=aIoEkwc2oB},
 year = {2022}
}

@article{shama,
 pdf = {https://openreview.net/pdf?id=LnjclqBl8R},
 review = {https://openreview.net/forum?id=LnjclqBl8R}
}

@article{shao2022on,
 abstract = {Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness.},
 author = {Rulin Shao and Zhouxing Shi and Jinfeng Yi and Pin-Yu Chen and Cho-Jui Hsieh},
 code = {https://github.com/RulinShao/on-the-adversarial-robustness-of-visual-transformer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308245819},
 pdf = {https://openreview.net/pdf?id=lE7K4n1Esk},
 review = {https://openreview.net/forum?id=lE7K4n1Esk},
 title = {On the Adversarial Robustness of Vision Transformers},
 url = {https://openreview.net/forum?id=lE7K4n1Esk},
 year = {2022}
}

@article{sharma2023federated,
 abstract = {Minimax optimization has seen a surge in interest with the advent of modern applications such as GANs, and it is inherently more challenging than simple minimization. The difficulty is exacerbated by the training data residing at multiple edge devices or \textit{clients}, especially when these clients can have heterogeneous datasets and local computation capabilities. We propose a general federated minimax optimization framework that subsumes such settings and several existing methods like Local SGDA. We show that naive aggregation of heterogeneous local progress results in optimizing a mismatched objective function -- a phenomenon previously observed in standard federated minimization. To fix this problem, we propose normalizing the client updates by the number of local steps undertaken between successive communication rounds. We analyze the convergence of the proposed algorithm for classes of nonconvex-concave and nonconvex-nonconcave functions and characterize the impact of heterogeneous client data, partial client participation, and heterogeneous local computations. Our analysis works under more general assumptions on the intra-client noise and inter-client heterogeneity than so far considered in the literature. For all the function classes considered, we significantly improve the existing computation and communication complexity results. Experimental results support our theoretical claims.},
 author = {Pranay Sharma and Rohan Panda and Gauri Joshi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320342275},
 pdf = {https://openreview.net/pdf?id=NnUmg1chLL},
 review = {https://openreview.net/forum?id=NnUmg1chLL},
 title = {Federated Minimax Optimization with Client Heterogeneity},
 url = {https://openreview.net/forum?id=NnUmg1chLL},
 year = {2023}
}

@article{shen2023implicit,
 author = {Macheng Shen and JONATHAN P HOW},
 code = {https://github.com/MachengShen/ImplicitEnsembleTraining},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LfTukxzxTj},
 review = {https://openreview.net/forum?id=LfTukxzxTj},
 title = {Implicit Ensemble Training for Efficient and Robust Multiagent Reinforcement Learning},
 url = {https://openreview.net/forum?id=LfTukxzxTj},
 year = {2023}
}

@article{shen2024wasserstein,
 abstract = {Off-policy evaluation and learning are concerned with assessing a given policy and learning an optimal policy from offline data without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. While Wasserstein DRO is generally computationally more expensive compared to KL DRO, we present a regularized method and a practical (biased) stochastic gradient descent method to optimize the policy efficiently. We also provide a theoretical analysis of the finite sample complexity and iteration complexity for our proposed method. We further validate our approach using a public dataset that was recorded in a randomized stoke trial.},
 author = {Yi Shen and Pan Xu and Michael Zavlanos},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4386907945},
 pdf = {https://openreview.net/pdf?id=NmpjDHWIvg},
 review = {https://openreview.net/forum?id=NmpjDHWIvg},
 title = {Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits},
 url = {https://openreview.net/forum?id=NmpjDHWIvg},
 year = {2024}
}

@article{shi2023aisarah,
 abstract = {We present AI-SARAH, a practical variant of SARAH. As a variant of SARAH, this algorithm employs the stochastic recursive gradient yet adjusts step-size based on local geometry. AI-SARAH implicitly computes step-size and efficiently estimates local Lipschitz smoothness of stochastic functions. It is fully adaptive, tune-free, straightforward to implement, and computationally efficient. We provide technical insight and intuitive illustrations on its design and convergence. We conduct extensive empirical analysis and demonstrate its strong performance compared with its classical counterparts and other state-of-the-art first-order methods in solving convex machine learning problems.},
 author = {Zheng Shi and Abdurakhmon Sadiev and Nicolas Loizou and Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}},
 code = {https://github.com/shizheng-rlfresh/ai_sarah},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3131494425},
 pdf = {https://openreview.net/pdf?id=WoXJFsJ6Zw},
 review = {https://openreview.net/forum?id=WoXJFsJ6Zw},
 title = {AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods},
 url = {https://openreview.net/forum?id=WoXJFsJ6Zw},
 year = {2023}
}

@article{shi2024revisiting,
 author = {Dai Shi and Zhiqi Shao and Yi Guo and Qibin Zhao and Junbin Gao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=q4iSLPoFe7},
 review = {https://openreview.net/forum?id=q4iSLPoFe7},
 title = {Revisiting Generalized p-Laplacian Regularized Framelet {GCN}s: Convergence, Energy Dynamic and as Non-Linear Diffusion},
 url = {https://openreview.net/forum?id=q4iSLPoFe7},
 year = {2024}
}

@article{shin2023benchmarks,
 abstract = {Learning a reward function from human preferences is challenging as it typically requires having a high-fidelity simulator or using expensive and potentially unsafe actual physical rollouts in the environment. However, in many tasks the agent might have access to offline data from related tasks in the same target environment. While offline data is increasingly being used to aid policy optimization via offline RL, our observation is that it can be a surprisingly rich source of information for preference learning as well. We propose an approach that uses an offline dataset to craft preference queries via pool-based active learning, learns a distribution over reward functions, and optimizes a corresponding policy via offline RL. Crucially, our proposed approach does not require actual physical rollouts or an accurate simulator for either the reward learning or policy optimization steps. To test our approach, we first evaluate existing offline RL benchmarks for their suitability for offline reward learning. Surprisingly, for many offline RL domains, we find that simply using a trivial reward function results good policy performance, making these domains ill-suited for evaluating learned rewards. To address this, we identify a subset of existing offline RL benchmarks that are well suited for offline reward learning and also propose new offline apprenticeship learning benchmarks which allow for more open-ended behaviors. When evaluated on this curated set of domains, our empirical results suggest that combining offline RL with learned human preferences can enable an agent to learn to perform novel tasks that were not explicitly shown in the offline data.},
 author = {Daniel Shin and Anca Dragan and Daniel S. Brown},
 code = {https://github.com/danielshin1/oprl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4313598235},
 pdf = {https://openreview.net/pdf?id=TGuXXlbKsn},
 review = {https://openreview.net/forum?id=TGuXXlbKsn},
 title = {Benchmarks and Algorithms for Offline Preference-Based Reward Learning},
 url = {https://openreview.net/forum?id=TGuXXlbKsn},
 year = {2023}
}

@article{shin2023online,
 author = {Eura Shin and Predrag Klasnja and Susan Murphy and Finale Doshi-Velez},
 code = {https://github.com/dtak/kernel-evolutions-public},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=23WZFQBUh5},
 review = {https://openreview.net/forum?id=23WZFQBUh5},
 title = {Online model selection by learning how compositional kernels evolve},
 url = {https://openreview.net/forum?id=23WZFQBUh5},
 year = {2023}
}

@article{shin2024pseudodifferential,
 abstract = {Learning the mapping between two function spaces has garnered considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) was recently proposed to learn solution operators, and it achieved an excellent performance. In this study, we propose a novel \textit{pseudo-differential integral operator} (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalized differential operator characterized by a certain symbol. We parameterize this symbol using a neural network and demonstrate that the neural network-based symbol is contained in a smooth symbol class. Subsequently, we verify that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a \textit{pseudo-differential neural operator} (PDNO) and learn the nonlinear solution operator of PDEs. We experimentally validate the effectiveness of the proposed model by utilizing Darcy flow and the Navier-Stokes equation. The obtained results indicate that the proposed PDNO outperforms the existing neural operator approaches in most experiments.},
 author = {Jin Young Shin and Jae Yong Lee and Hyung Ju Hwang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221159534},
 pdf = {https://openreview.net/pdf?id=805jKZ0Gqf},
 review = {https://openreview.net/forum?id=805jKZ0Gqf},
 title = {Pseudo-Differential Integral Operator for Learning Solution Operators of Partial Differential Equations},
 url = {https://openreview.net/forum?id=805jKZ0Gqf},
 year = {2024}
}

@article{shit2022online,
 author = {Supratim Shit and Anirban Dasgupta and Rachit Chhaya and Jayesh Choudhari},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=lAv8fShACA},
 review = {https://openreview.net/forum?id=lAv8fShACA},
 title = {Online Coresets for Parameteric and Non-Parametric Bregman Clustering},
 url = {https://openreview.net/forum?id=lAv8fShACA},
 year = {2022}
}

@article{shojaee2023executionbased,
 abstract = {The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.},
 author = {Parshin Shojaee and Aneesh Jain and Sindhu Tipirneni and Chandan K. Reddy},
 code = {https://github.com/reddy-lab-code-research/PPOCoder},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320858785},
 pdf = {https://openreview.net/pdf?id=0XBuaxqEcG},
 review = {https://openreview.net/forum?id=0XBuaxqEcG},
 title = {Execution-based Code Generation using Deep Reinforcement Learning},
 url = {https://openreview.net/forum?id=0XBuaxqEcG},
 year = {2023}
}

@article{shukor2023unival,
 author = {Mustafa Shukor and Corentin Dancette and Alexandre Rame and Matthieu Cord},
 code = {https://unival-model.github.io/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4uflhObpcp},
 review = {https://openreview.net/forum?id=4uflhObpcp},
 title = {Un{IVAL}: Unified Model for Image, Video, Audio and Language Tasks},
 url = {https://openreview.net/forum?id=4uflhObpcp},
 year = {2023}
}

@article{siddiqui2024blockwise,
 abstract = {Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.},
 author = {Shoaib Siddiqui and David Krueger and Yann LeCun and Stephane Deny},
 code = {https://github.com/shoaibahmed/blockwise_ssl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319323548},
 pdf = {https://openreview.net/pdf?id=M2m618iIPk},
 review = {https://openreview.net/forum?id=M2m618iIPk},
 title = {Blockwise Self-Supervised Learning at Scale},
 url = {https://openreview.net/forum?id=M2m618iIPk},
 year = {2024}
}

@article{sikchi2023a,
 abstract = {We propose a new framework for imitation learning -- treating imitation as a two-player ranking-based game between a policy and a reward. In this game, the reward agent learns to satisfy pairwise performance rankings between behaviors, while the policy agent learns to maximize this reward. In imitation learning, near-optimal expert data can be difficult to obtain, and even in the limit of infinite data cannot imply a total ordering over trajectories as preferences can. On the other hand, learning from preferences alone is challenging as a large number of preferences are required to infer a high-dimensional reward function, though preference data is typically much easier to collect than expert demonstrations. The classical inverse reinforcement learning (IRL) formulation learns from expert demonstrations but provides no mechanism to incorporate learning from offline preferences and vice versa. We instantiate the proposed ranking-game framework with a novel ranking loss giving an algorithm that can simultaneously learn from expert demonstrations and preferences, gaining the advantages of both modalities. Our experiments show that the proposed method achieves state-of-the-art sample efficiency and can solve previously unsolvable tasks in the Learning from Observation (LfO) setting. Project video and code can be found at https://hari-sikchi.github.io/rank-game/},
 author = {Harshit Sikchi and Akanksha Saran and Wonjoon Goo and Scott Niekum},
 code = {https://github.com/hari-sikchi/rank-game},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226326927},
 pdf = {https://openreview.net/pdf?id=d3rHk4VAf0},
 review = {https://openreview.net/forum?id=d3rHk4VAf0},
 title = {A Ranking Game for Imitation Learning},
 url = {https://openreview.net/forum?id=d3rHk4VAf0},
 year = {2023}
}

@article{simkus2023conditional,
 abstract = {Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get "stuck" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.},
 author = {Vaidotas Simkus and Michael U. Gutmann},
 code = {https://github.com/vsimkus/vae-conditional-sampling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386043261},
 pdf = {https://openreview.net/pdf?id=I5sJ6PU6JN},
 review = {https://openreview.net/forum?id=I5sJ6PU6JN},
 title = {Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling},
 url = {https://openreview.net/forum?id=I5sJ6PU6JN},
 year = {2023}
}

@article{simon2023the,
 author = {James B Simon and Madeline Dickens and Dhruva Karkada and Michael Deweese},
 code = {https://github.com/james-simon/eigenlearning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=FDbQGCAViI},
 review = {https://openreview.net/forum?id=FDbQGCAViI},
 title = {The Eigenlearning Framework: A Conservation Law Perspective on Kernel Ridge Regression and Wide Neural Networks},
 url = {https://openreview.net/forum?id=FDbQGCAViI},
 year = {2023}
}

@article{singh2023signed,
 abstract = {Graph convolutional networks (GCNs) and its variants are designed for unsigned graphs containing only positive links. Many existing GCNs have been derived from the spectral domain analysis of signals lying over (unsigned) graphs and in each convolution layer they perform low-pass filtering of the input features followed by a learnable linear transformation. Their extension to signed graphs with positive as well as negative links imposes multiple issues including computational irregularities and ambiguous frequency interpretation, making the design of computationally efficient low pass filters challenging. In this paper, we address these issues via spectral analysis of signed graphs and propose two different signed graph neural networks, one keeps only low-frequency information and one also retains high-frequency information. We further introduce magnetic signed Laplacian and use its eigendecomposition for spectral analysis of directed signed graphs. We test our methods for node classification and link sign prediction tasks on signed graphs and achieve state-of-the-art performances.},
 author = {Rahul Singh and Yongxin Chen},
 code = {https://github.com/rahulsinghchandraul/Spectral_Signed_GNN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4292109051},
 pdf = {https://openreview.net/pdf?id=RZveYHgZbu},
 review = {https://openreview.net/forum?id=RZveYHgZbu},
 title = {Signed Graph Neural Networks: A Frequency Perspective},
 url = {https://openreview.net/forum?id=RZveYHgZbu},
 year = {2023}
}

@article{singh2023transductive,
 abstract = {The versatility to learn from a handful of samples is the hallmark of human intelligence. Few-shot learning is an endeavour to transcend this capability down to machines. Inspired by the promise and power of probabilistic deep learning, we propose a novel variational inference network for few-shot classification (coined as TRIDENT) to decouple the representation of an image into semantic and label latent variables, and simultaneously infer them in an intertwined fashion. To induce task-awareness, as part of the inference mechanics of TRIDENT, we exploit information across both query and support images of a few-shot task using a novel built-in attention-based transductive feature extraction module (we call AttFEX). Our extensive experimental results corroborate the efficacy of TRIDENT and demonstrate that, using the simplest of backbones, it sets a new state-of-the-art in the most commonly adopted datasets miniImageNet and tieredImageNet (offering up to 4% and 5% improvements, respectively), as well as for the recent challenging cross-domain miniImagenet --> CUB scenario offering a significant margin (up to 20% improvement) beyond the best existing cross-domain baselines. Code and experimentation can be found in our GitHub repository: https://github.com/anujinho/trident},
 author = {Anuj Rajeeva Singh and Hadi Jamali-Rad},
 code = {https://github.com/anujinho/trident},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320518464},
 pdf = {https://openreview.net/pdf?id=bomdTc9HyL},
 review = {https://openreview.net/forum?id=bomdTc9HyL},
 title = {Transductive Decoupled Variational Inference for Few-Shot Classification},
 url = {https://openreview.net/forum?id=bomdTc9HyL},
 year = {2023}
}

@article{sinha2024break,
 abstract = {Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the distribution of existing human adversaries, meanwhile, degrade robustness.},
 author = {Aradhana Sinha and Ananth Balashankar and Ahmad Beirami and Thi Avrahami and Jilin Chen and Alex Beutel},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4387994950},
 pdf = {https://openreview.net/pdf?id=UAT4j3Y7HP},
 review = {https://openreview.net/forum?id=UAT4j3Y7HP},
 title = {Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks},
 url = {https://openreview.net/forum?id=UAT4j3Y7HP},
 year = {2024}
}

@article{sixt2022a,
 abstract = {Saliency methods attempt to explain deep neural networks by highlighting the most salient features of a sample. Some widely used methods are based on a theoretical framework called Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Taylor Theorem to the network's layers. However, recent work has found these methods to be independent of the network's deeper layers and appear to respond only to lower-level image structure. Here, we investigate the DTD theory to better understand this perplexing behavior and found that the Deep Taylor Decomposition is equivalent to the basic gradient$\times$input method when the Taylor root points (an important parameter of the algorithm chosen by the user) are locally constant. If the root points are locally input-dependent, then one can justify any explanation. In this case, the theory is under-constrained. In an empirical evaluation, we find that DTD roots do not lie in the same linear regions as the input - contrary to a fundamental assumption of the Taylor theorem. The theoretical foundations of DTD were cited as a source of reliability for the explanations. However, our findings urge caution in making such claims.},
 author = {Leon Sixt and Tim Landgraf},
 code = {https://github.com/berleon/A-Rigorous-Study-Of-The-Deep-Taylor-Decomposition},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309299565},
 pdf = {https://openreview.net/pdf?id=Y4mgmw9OgV},
 review = {https://openreview.net/forum?id=Y4mgmw9OgV},
 title = {A Rigorous Study Of The Deep Taylor Decomposition},
 url = {https://openreview.net/forum?id=Y4mgmw9OgV},
 year = {2022}
}

@article{smith2023unsupervised,
 abstract = {Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard datasets, and rendering and training speeds at orders of magnitude faster than existing 3D approaches.},
 author = {Cameron Omid Smith and Hong-Xing Yu and Sergey Zakharov and Fredo Durand and Joshua B. Tenenbaum and Jiajun Wu and Vincent Sitzmann},
 code = {https://github.com/cameronosmith/COLF},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4280629166},
 pdf = {https://openreview.net/pdf?id=B7PFZtm8DA},
 review = {https://openreview.net/forum?id=B7PFZtm8DA},
 title = {Unsupervised Discovery and Composition of Object Light Fields},
 url = {https://openreview.net/forum?id=B7PFZtm8DA},
 year = {2023}
}

@article{sodhani2023improving,
 author = {Shagun Sodhani and Sergey Levine and Amy Zhang},
 badge = {Event: CoLLAs 2023},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LwEWrrKyja},
 review = {https://openreview.net/forum?id=LwEWrrKyja},
 title = {Improving Generalization with Approximate Factored Value Functions},
 url = {https://openreview.net/forum?id=LwEWrrKyja},
 year = {2023}
}

@article{soemers2023towards,
 author = {Dennis J. N. J. Soemers and Vegard Mella and Eric Piette and Matthew Stephenson and Cameron Browne and Olivier Teytaud},
 code = {https://github.com/DennisSoemers/Transfer-DNNs-Ludii-Polygames},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=vJcTm2v9Ku},
 review = {https://openreview.net/forum?id=vJcTm2v9Ku},
 title = {Towards a General Transfer Approach for Policy-Value Networks},
 url = {https://openreview.net/forum?id=vJcTm2v9Ku},
 year = {2023}
}

@article{soga2023bridging,
 abstract = {A current goal in the graph neural network literature is to enable transformers to operate on graph-structured data, given their success on language and vision tasks. Since the transformer's original sinusoidal positional encodings (PEs) are not applicable to graphs, recent work has focused on developing graph PEs, rooted in spectral graph theory or various spatial features of a graph. In this work, we introduce a new graph PE, Graph Automaton PE (GAPE), based on weighted graph-walking automata (a novel extension of graph-walking automata). We compare the performance of GAPE with other PE schemes on both machine translation and graph-structured tasks, and we show that it generalizes several other PEs. An additional contribution of this study is a theoretical and controlled experimental comparison of many recent PEs in graph transformers, independent of the use of edge features.},
 author = {Patrick Soga and David Chiang},
 code = {https://github.com/AJB117/gape},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4311640659},
 pdf = {https://openreview.net/pdf?id=tE2NiMGd07},
 review = {https://openreview.net/forum?id=tE2NiMGd07},
 title = {Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata},
 url = {https://openreview.net/forum?id=tE2NiMGd07},
 year = {2023}
}

@article{soleymani2022causal,
 abstract = {The problem of inferring the direct causal parents of a response variable among a large set of explanatory variables is of high practical importance in many disciplines. Recent work in the field of causal discovery exploits invariance properties of models across different experimental conditions for detecting direct causal links. However, these approaches generally do not scale well with the number of explanatory variables, are difficult to extend to nonlinear relationships, and require data across different experiments. Inspired by {\em Debiased} machine learning methods, we study a one-vs.-the-rest feature selection approach to discover the direct causal parent of the response. We propose an algorithm that works for purely observational data, while also offering theoretical guarantees, including the case of partially nonlinear relationships. Requiring only one estimation for each variable, we can apply our approach even to large graphs, demonstrating significant improvements compared to established approaches.},
 author = {Ashkan Soleymani and Anant Raj and Stefan Bauer and Bernhard Sch{\"o}lkopf and Michel Besserve},
 code = {https://github.com/Ashkan-Soleymani98/CORTH-Features-Causal-Feature-Selection-via-Orthogonal-Search},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3038219390},
 pdf = {https://openreview.net/pdf?id=Q54jBjc896},
 review = {https://openreview.net/forum?id=Q54jBjc896},
 title = {Causal Feature Selection via Orthogonal Search.},
 url = {https://openreview.net/forum?id=Q54jBjc896},
 year = {2022}
}

@article{song2023provably,
 abstract = {Trust-region methods based on Kullback-Leibler divergence are pervasively used to stabilize policy optimization in reinforcement learning. In this paper, we exploit more flexible metrics and examine two natural extensions of policy optimization with Wasserstein and Sinkhorn trust regions, namely Wasserstein policy optimization (WPO) and Sinkhorn policy optimization (SPO). Instead of restricting the policy to a parametric distribution class, we directly optimize the policy distribution and derive their closed-form policy updates based on the Lagrangian duality. Theoretically, we show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Moreover, we prove that with a decaying Lagrangian multiplier to the trust region constraint, both methods converge to global optimality. Experiments across tabular domains, robotic locomotion, and continuous control tasks further demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO, over state-of-art policy gradient methods.},
 author = {Jun Song and Niao He and Lijun Ding and Chaoyue Zhao},
 code = {https://github.com/efficientwpo/EfficientWPO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4382322374},
 pdf = {https://openreview.net/pdf?id=jkTqJJOGMS},
 review = {https://openreview.net/forum?id=jkTqJJOGMS},
 title = {Provably Convergent Policy Optimization via Metric-aware Trust Region Methods},
 url = {https://openreview.net/forum?id=jkTqJJOGMS},
 year = {2023}
}

@article{sonthalia2023training,
 author = {Rishi Sonthalia and Raj Rao Nadakuditi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=FdMWtpVT1I},
 review = {https://openreview.net/forum?id=FdMWtpVT1I},
 title = {Training Data Size Induced Double Descent For Denoising Feedforward Neural Networks and the Role of Training Noise},
 url = {https://openreview.net/forum?id=FdMWtpVT1I},
 year = {2023}
}

@article{sordello2024robust,
 abstract = {This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be very robust with a set of default parameters for a wide range of problems and, moreover, can yield better generalization performance than other adaptive gradient methods such as Adam.},
 author = {Matteo Sordello and Niccolo Dalmasso and Hangfeng He and Weijie J Su},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W2980680952},
 pdf = {https://openreview.net/pdf?id=3PbxuMNQkp},
 review = {https://openreview.net/forum?id=3PbxuMNQkp},
 title = {Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic},
 url = {https://openreview.net/forum?id=3PbxuMNQkp},
 year = {2024}
}

@article{spencer2022deconstructing,
 abstract = {This paper presents an open and comprehensive framework to systematically evaluate state-of-the-art contributions to self-supervised monocular depth estimation. This includes pretraining, backbone, architectural design choices and loss functions. Many papers in this field claim novelty in either architecture design or loss formulation. However, simply updating the backbone of historical systems results in relative improvements of 25%, allowing them to outperform the majority of existing systems. A systematic evaluation of papers in this field was not straightforward. The need to compare like-with-like in previous papers means that longstanding errors in the evaluation protocol are ubiquitous in the field. It is likely that many papers were not only optimized for particular datasets, but also for errors in the data and evaluation criteria. To aid future research in this area, we release a modular codebase (https://github.com/jspenmar/monodepth_benchmark), allowing for easy evaluation of alternate design decisions against corrected data and evaluation criteria. We re-implement, validate and re-evaluate 16 state-of-the-art contributions and introduce a new dataset (SYNS-Patches) containing dense outdoor depth maps in a variety of both natural and urban scenes. This allows for the computation of informative metrics in complex regions such as depth boundaries.},
 author = {Jaime Spencer and Chris Russell and Simon Hadfield and Richard Bowden},
 badge = {Reproducibility},
 code = {https://github.com/jspenmar/monodepth_benchmark},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4289785280},
 pdf = {https://openreview.net/pdf?id=GFK1FheE7F},
 review = {https://openreview.net/forum?id=GFK1FheE7F},
 title = {Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter},
 url = {https://openreview.net/forum?id=GFK1FheE7F},
 year = {2022}
}

@article{srivastava2023beyond,
 abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
 author = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri{\`a} Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Johan Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlm{\"u}ller and Andrew M. Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka{\c{s}} and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart{\l}omiej Bojanowski and Batuhan {\"O}zyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and Cesar Ferri and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Christopher Waites and Christian Voigt and Christopher D Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and C. Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Mosegu{\'\i} Gonz{\'a}lez and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodol{\`a} and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart{\'\i}nez-Plumed and Francesca Happ{\'e} and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ{\'a}n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Xinyue Wang and Gonzalo Jaimovitch-Lopez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Francis Anthony Shevlin and Hinrich Schuetze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern{\'a}ndez Fisac and James B Simon and James Koppel and James Zheng and James Zou and Jan Kocon and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and J{\"o}rg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh Dhole and Kevin Gimpel and Kevin Omondi and Kory Wallace Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros-Col{\'o}n and Luke Metz and L{\"u}tfi Kerem Senel and Maarten Bosma and Maarten Sap and Maartje Ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramirez-Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L Leavitt and Matthias Hagen and M{\'a}ty{\'a}s Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael Andrew Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Micha{\l} Sw{\k{e}}drowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan Andrew Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter W Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Mi{\l}kowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Rapha{\"e}l Milli{\`e}re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Russ Salakhutdinov and Ryan Andrew Chi and Seungjae Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel Stern Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Shammie Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven Piantadosi and Stuart Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsunori Hashimoto and Te-Lin Wu and Th{\'e}o Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Venkatesh Ramasesh and vinay uday prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
 code = {https://github.com/google/BIG-bench},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281690148},
 pdf = {https://openreview.net/pdf?id=uyTL5Bvosj},
 review = {https://openreview.net/forum?id=uyTL5Bvosj},
 title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
 url = {https://openreview.net/forum?id=uyTL5Bvosj},
 year = {2023}
}

@article{srivastava2023estimating,
 abstract = {Functions of the ratio of the densities $p/q$ are widely used in machine learning to quantify the discrepancy between the two distributions $p$ and $q$. For high-dimensional distributions, binary classification-based density ratio estimators have shown great promise. However, when densities are well separated, estimating the density ratio with a binary classifier is challenging. In this work, we show that the state-of-the-art density ratio estimators perform poorly on well-separated cases and demonstrate that this is due to distribution shifts between training and evaluation time. We present an alternative method that leverages multi-class classification for density ratio estimation and does not suffer from distribution shift issues. The method uses a set of auxiliary densities $\{m_k\}_{k=1}^K$ and trains a multi-class logistic regression to classify the samples from $p, q$, and $\{m_k\}_{k=1}^K$ into $K+2$ classes. We show that if these auxiliary densities are constructed such that they overlap with $p$ and $q$, then a multi-class logistic regression allows for estimating $\log p/q$ on the domain of any of the $K+2$ distributions and resolves the distribution shift problems of the current state-of-the-art methods. We compare our method to state-of-the-art density ratio estimators on both synthetic and real datasets and demonstrate its superior performance on the tasks of density ratio estimation, mutual information estimation, and representation learning. Code: https://www.blackswhan.com/mdre/},
 author = {Akash Srivastava and Seungwook Han and Kai Xu and Benjamin Rhodes and Michael U. Gutmann},
 code = {https://blackswhan.com/mdre/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4367701197},
 pdf = {https://openreview.net/pdf?id=jM8nzUzBWr},
 review = {https://openreview.net/forum?id=jM8nzUzBWr},
 title = {Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression},
 url = {https://openreview.net/forum?id=jM8nzUzBWr},
 year = {2023}
}

@article{staerman2024a,
 abstract = {The design of a metric between probability distributions is a longstanding problem motivated by numerous applications in Machine Learning. Focusing on continuous probability distributions on the Euclidean space $\mathbb{R}^d$, we introduce a novel pseudo-metric between probability distributions by leveraging the extension of univariate quantiles to multivariate spaces. Data depth is a nonparametric statistical tool that measures the centrality of any element $x\in\mathbb{R}^d$ with respect to (w.r.t.) a probability distribution or a data set. It is a natural median-oriented extension of the cumulative distribution function (cdf) to the multivariate case. Thus, its upper-level sets -- the depth-trimmed regions -- give rise to a definition of multivariate quantiles. The new pseudo-metric relies on the average of the Hausdorff distance between the depth-based quantile regions w.r.t. each distribution. Its good behavior w.r.t. major transformation groups, as well as its ability to factor out translations, are depicted. Robustness, an appealing feature of this pseudo-metric, is studied through the finite sample breakdown point. Moreover, we propose an efficient approximation method with linear time complexity w.r.t. the size of the data set and its dimension. The quality of this approximation as well as the performance of the proposed approach are illustrated in numerical experiments.},
 author = {Guillaume Staerman and Pavlo Mozharovskyi and Pierre Colombo and Stephan Cl{\'e}men{\c{c}}on and Florence d'Alch{\'e}-Buc},
 code = {https://github.com/GuillaumeStaermanML/DRPM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4301372337},
 pdf = {https://openreview.net/pdf?id=QySD5r7PeE},
 review = {https://openreview.net/forum?id=QySD5r7PeE},
 title = {A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions},
 url = {https://openreview.net/forum?id=QySD5r7PeE},
 year = {2024}
}

@article{stan2022secure,
 abstract = {Multi-source unsupervised domain adaptation (MUDA) is a framework to address the challenge of annotated data scarcity in a target domain via transferring knowledge from multiple annotated source domains. When the source domains are distributed, data privacy and security can become significant concerns and protocols may limit data sharing, yet existing MUDA methods overlook these constraints. We develop an algorithm to address MUDA when source domain data cannot be shared with the target or across the source domains. Our method is based on aligning the distributions of source and target domains indirectly via estimating the source feature embeddings and predicting over a confidence based combination of domain specific model predictions. We provide theoretical analysis to support our approach and conduct empirical experiments to demonstrate that our algorithm is effective.},
 author = {Serban Stan and Mohammad Rostami},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4295307174},
 pdf = {https://openreview.net/pdf?id=UDmH3HxxPp},
 review = {https://openreview.net/forum?id=UDmH3HxxPp},
 title = {Secure Domain Adaptation with Multiple Sources},
 url = {https://openreview.net/forum?id=UDmH3HxxPp},
 year = {2022}
}

@article{starre2023an,
 abstract = {Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate this by combining R-MAX, a prototypical MBRL algorithm, with abstraction, thus producing the first performance guarantees for model-based 'RL from Abstracted Observations': model-based reinforcement learning with an abstract model.},
 author = {Rolf A. N. Starre and Marco Loog and Elena Congeduti and Frans A Oliehoek},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4294057479},
 pdf = {https://openreview.net/pdf?id=YQWOzzSMPp},
 review = {https://openreview.net/forum?id=YQWOzzSMPp},
 title = {An Analysis of Model-Based Reinforcement Learning From Abstracted Observations},
 url = {https://openreview.net/forum?id=YQWOzzSMPp},
 year = {2023}
}

@article{steiner2022how,
 abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation ("AugReg" for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
 author = {Andreas Peter Steiner and Alexander Kolesnikov and Xiaohua Zhai and Ross Wightman and Jakob Uszkoreit and Lucas Beyer},
 code = {https://github.com/google-research/vision_transformer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287116734},
 pdf = {https://openreview.net/pdf?id=4nPswr1KcP},
 review = {https://openreview.net/forum?id=4nPswr1KcP},
 title = {How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
 url = {https://openreview.net/forum?id=4nPswr1KcP},
 year = {2022}
}

@article{stone2023spectral,
 author = {Iris R Stone and Yotam Sagiv and Il Memming Park and Jonathan W. Pillow},
 code = {https://github.com/irisstone/bestLDS},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=giw2vcAhiH},
 review = {https://openreview.net/forum?id=giw2vcAhiH},
 title = {Spectral learning of Bernoulli linear dynamical systems models for decision-making},
 url = {https://openreview.net/forum?id=giw2vcAhiH},
 year = {2023}
}

@article{struckmeier2023ilpomp,
 author = {Oliver Struckmeier and Ville Kyrki},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=f3JLnnZsAm},
 review = {https://openreview.net/forum?id=f3JLnnZsAm},
 title = {{ILPO}-{MP}: Mode Priors Prevent Mode Collapse when Imitating Latent Policies from Observations},
 url = {https://openreview.net/forum?id=f3JLnnZsAm},
 year = {2023}
}

@article{struckmeier2023learning,
 abstract = {Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Despite its widespread use in machine learning (ML), OT problem still bears its computational burden, while at the same time suffering from the curse of dimensionality for measures supported on general high-dimensional spaces. In this paper, we propose to tackle these challenges using representation learning. In particular, we seek to learn an embedding space such that the samples of the two input measures become alignable in it with a simple affine mapping that can be calculated efficiently in closed-form. We then show that such approach leads to results that are comparable to solving the original OT problem when applied to the transfer learning task on which many OT baselines where previously evaluated in both homogeneous and heterogeneous DA settings. The code for our contribution is available at \url{https://github.com/Oleffa/LaOT}.},
 author = {Oliver Struckmeier and Ievgen Redko and Anton Mallasto and Karol Arndt and Markus Heinonen and Ville Kyrki},
 code = {https://github.com/Oleffa/LaOT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4376632879},
 pdf = {https://openreview.net/pdf?id=nOIGfQnFZm},
 review = {https://openreview.net/forum?id=nOIGfQnFZm},
 title = {Learning representations that are closed-form Monge mapping optimal with application to domain adaptation},
 url = {https://openreview.net/forum?id=nOIGfQnFZm},
 year = {2023}
}

@article{stubbemann2023intrinsic,
 abstract = {The concept of dimension is essential to grasp the complexity of data. A naive approach to determine the dimension of a dataset is based on the number of attributes. More sophisticated methods derive a notion of intrinsic dimension (ID) that employs more complex feature functions, e.g., distances between data points. Yet, many of these approaches are based on empirical observations, cannot cope with the geometric character of contemporary datasets, and do lack an axiomatic foundation. A different approach was proposed by V. Pestov, who links the intrinsic dimension axiomatically to the mathematical concentration of measure phenomenon. First methods to compute this and related notions for ID were computationally intractable for large-scale real-world datasets. In the present work, we derive a computationally feasible method for determining said axiomatic ID functions. Moreover, we demonstrate how the geometric properties of complex data are accounted for in our modeling. In particular, we propose a principle way to incorporate neighborhood information, as in graph data, into the ID. This allows for new insights into common graph learning procedures, which we illustrate by experiments on the Open Graph Benchmark.},
 author = {Maximilian Stubbemann and Tom Hanika and Friedrich Martin Schneider},
 code = {https://github.com/mstubbemann/ID4GeoL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4305026100},
 pdf = {https://openreview.net/pdf?id=85BfDdYMBY},
 review = {https://openreview.net/forum?id=85BfDdYMBY},
 title = {Intrinsic Dimension for Large-Scale Geometric Learning},
 url = {https://openreview.net/forum?id=85BfDdYMBY},
 year = {2023}
}

@article{stutz2023conformal,
 abstract = {Conformal Prediction (CP) allows to perform rigorous uncertainty quantification by constructing a prediction set $C(X)$ satisfying $\mathbb{P}(Y \in C(X))\geq 1-\alpha$ for a user-chosen $\alpha \in [0,1]$ by relying on calibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\mathbb{P}=\mathbb{P}^{X} \otimes \mathbb{P}^{Y|X}$. It is typically implicitly assumed that $\mathbb{P}^{Y|X}$ is the "true" posterior label distribution. However, in many real-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating expert opinions using a voting procedure, resulting in a one-hot distribution $\mathbb{P}_{vote}^{Y|X}$. For such ``voted'' labels, CP guarantees are thus w.r.t. $\mathbb{P}_{vote}=\mathbb{P}^X \otimes \mathbb{P}_{vote}^{Y|X}$ rather than the true distribution $\mathbb{P}$. In cases with unambiguous ground truth labels, the distinction between $\mathbb{P}_{vote}$ and $\mathbb{P}$ is irrelevant. However, when experts do not agree because of ambiguous labels, approximating $\mathbb{P}^{Y|X}$ with a one-hot distribution $\mathbb{P}_{vote}^{Y|X}$ ignores this uncertainty. In this paper, we propose to leverage expert opinions to approximate $\mathbb{P}^{Y|X}$ using a non-degenerate distribution $\mathbb{P}_{agg}^{Y|X}$. We develop Monte Carlo CP procedures which provide guarantees w.r.t. $\mathbb{P}_{agg}=\mathbb{P}^X \otimes \mathbb{P}_{agg}^{Y|X}$ by sampling multiple synthetic pseudo-labels from $\mathbb{P}_{agg}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a case study of skin condition classification with significant disagreement among expert annotators, we show that applying CP w.r.t. $\mathbb{P}_{vote}$ under-covers expert annotations: calibrated for $72\%$ coverage, it falls short by on average $10\%$; our Monte Carlo CP closes this gap both empirically and theoretically.},
 author = {David Stutz and Abhijit Guha Roy and Tatiana Matejovicova and Patricia Strachan and Ali Taylan Cemgil and Arnaud Doucet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384811655},
 pdf = {https://openreview.net/pdf?id=CAd6V2qXxc},
 review = {https://openreview.net/forum?id=CAd6V2qXxc},
 title = {Conformal prediction under ambiguous ground truth},
 url = {https://openreview.net/forum?id=CAd6V2qXxc},
 year = {2023}
}

@article{su2023contrastive,
 abstract = {Generating text with autoregressive language models (LMs) is of great importance to many natural language processing (NLP) applications. Previous solutions for this task often produce text that contains degenerative expressions or lacks semantic consistency. Recently, Su et al. introduced a new decoding method, contrastive search, based on the isotropic representation space of the language model and obtained new state of the art on various benchmarks. Additionally, Su et al. argued that the representations of autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also shared by previous studies. Therefore, to ensure the language model follows an isotropic distribution, Su et al. proposed a contrastive learning scheme, SimCTG, which calibrates the language model's representations through additional training. In this study, we first answer the question: "Are autoregressive LMs really anisotropic?". To this end, we extensively evaluate the isotropy of LMs across 16 major languages. Surprisingly, we find that the anisotropic problem only exists in the two specific English GPT-2-small/medium models. On the other hand, all other evaluated LMs are naturally isotropic which is in contrast to the conclusion drawn by previous studies. Based on our findings, we further assess the contrastive search decoding method using off-the-shelf LMs on four generation tasks across 16 languages. Our experimental results demonstrate that contrastive search significantly outperforms previous decoding methods without any additional training. More notably, on 12 out of the 16 evaluated languages, contrastive search performs comparably with human-level performances as judged by human evaluations. Our code and other related resources are publicly available at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.},
 author = {Yixuan Su and Nigel Collier},
 code = {https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307418160},
 pdf = {https://openreview.net/pdf?id=GbkWw3jwL9},
 review = {https://openreview.net/forum?id=GbkWw3jwL9},
 title = {Contrastive Search Is What You Need For Neural Text Generation},
 url = {https://openreview.net/forum?id=GbkWw3jwL9},
 year = {2023}
}

@article{su2024a,
 author = {Kefan Su and Zongqing Lu},
 code = {https://github.com/PKU-RL/DPO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=MppUW90uU2},
 review = {https://openreview.net/forum?id=MppUW90uU2},
 title = {A Fully Decentralized Surrogate for Multi-Agent Policy Optimization},
 url = {https://openreview.net/forum?id=MppUW90uU2},
 year = {2024}
}

@article{such2023bridging,
 author = {Ondrej Such and Ren{\'e} Fabricius},
 code = {https://github.com/ondrej-such/svm3},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=SM1BkjGePI},
 review = {https://openreview.net/forum?id=SM1BkjGePI},
 title = {Bridging performance gap between minimal and maximal {SVM} models},
 url = {https://openreview.net/forum?id=SM1BkjGePI},
 year = {2023}
}

@article{suetake2024synaptic,
 author = {Kazuma Suetake and Takuya Ushimaru and Ryuji Saiin and Yoshihide Sawada},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=42BKnT2qW3},
 review = {https://openreview.net/forum?id=42BKnT2qW3},
 title = {Synaptic Interaction Penalty: Appropriate Penalty Term for Energy-Efficient Spiking Neural Networks},
 url = {https://openreview.net/forum?id=42BKnT2qW3},
 year = {2024}
}

@article{sujit2023bridging,
 abstract = {Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals. A crucial challenge for current deep RL algorithms is that they require a tremendous amount of environment interactions for learning. This can be infeasible in situations where such interactions are expensive; such as in robotics. Offline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning. While online RL algorithms are typically evaluated as a function of the number of environment interactions, there exists no single established protocol for evaluating offline RL methods.In this paper, we propose a sequential approach to evaluate offline RL algorithms as a function of the training set size and thus by their data efficiency. Sequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases. Our approach is generally applicable and easy to implement. We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets.},
 author = {Shivakanth Sujit and Pedro Braga and Jorg Bornschein and Samira Ebrahimi Kahou},
 code = {https://github.com/shivakanthsujit/seq_eval},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4311991644},
 pdf = {https://openreview.net/pdf?id=J3veZdVpts},
 review = {https://openreview.net/forum?id=J3veZdVpts},
 title = {Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies},
 url = {https://openreview.net/forum?id=J3veZdVpts},
 year = {2023}
}

@article{sumers2024cognitive,
 abstract = {Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.},
 author = {Theodore Sumers and Shunyu Yao and Karthik Narasimhan and Thomas Griffiths},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4386528753},
 pdf = {https://openreview.net/pdf?id=1i6ZCvflQJ},
 review = {https://openreview.net/forum?id=1i6ZCvflQJ},
 title = {Cognitive Architectures for Language Agents},
 url = {https://openreview.net/forum?id=1i6ZCvflQJ},
 year = {2024}
}

@article{sun2023opencon,
 abstract = {Machine learning models deployed in the wild naturally encounter unlabeled samples from both known and novel classes. Challenges arise in learning from both the labeled and unlabeled data, in an open-world semi-supervised manner. In this paper, we introduce a new learning framework, open-world contrastive learning (OpenCon). OpenCon tackles the challenges of learning compact representations for both known and novel classes and facilitates novelty discovery along the way. We demonstrate the effectiveness of OpenCon on challenging benchmark datasets and establish competitive performance. On the ImageNet dataset, OpenCon significantly outperforms the current best method by 11.9% and 7.4% on novel and overall classification accuracy, respectively. Theoretically, OpenCon can be rigorously interpreted from an EM algorithm perspective--minimizing our contrastive loss partially maximizes the likelihood by clustering similar samples in the embedding space. The code is available at https://github.com/deeplearning-wisc/opencon.},
 author = {Yiyou Sun and Yixuan Li},
 code = {https://github.com/deeplearning-wisc/opencon},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4290056061},
 pdf = {https://openreview.net/pdf?id=2wWJxtpFer},
 review = {https://openreview.net/forum?id=2wWJxtpFer},
 title = {OpenCon: Open-world Contrastive Learning},
 url = {https://openreview.net/forum?id=2wWJxtpFer},
 year = {2023}
}

@article{sun2023pairwise,
 author = {Tao Sun and Qingsong Wang and Yunwen Lei and Dongsheng Li and Bao Wang},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/normalbasis/adaptive_pairwise_learning.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=rq1SaHQg2k},
 review = {https://openreview.net/forum?id=rq1SaHQg2k},
 title = {Pairwise Learning with Adaptive Online Gradient Descent},
 url = {https://openreview.net/forum?id=rq1SaHQg2k},
 year = {2023}
}

@article{sun2023prudexcompass,
 abstract = {The financial markets, which involve more than $90 trillion market capitals, attract the attention of innumerable investors around the world. Recently, reinforcement learning in financial markets (FinRL) has emerged as a promising direction to train agents for making profitable investment decisions. However, the evaluation of most FinRL methods only focuses on profit-related measures and ignores many critical axes, which are far from satisfactory for financial practitioners to deploy these methods into real-world financial markets. Therefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability, Risk-control, Universality, Diversity, rEliability, and eXplainability, with a total of 17 measures for a systematic evaluation. Specifically, i) we propose AlphaMix+ as a strong FinRL baseline, which leverages mixture-of-experts (MoE) and risk-sensitive approaches to make diversified risk-aware investment decisions, ii) we evaluate 8 FinRL methods in 4 long-term real-world datasets of influential financial markets to demonstrate the usage of our PRUDEX-Compass, iii) PRUDEX-Compass together with 4 real-world datasets, standard implementation of 8 FinRL methods and a portfolio management environment is released as public resources to facilitate the design and comparison of new FinRL methods. We hope that PRUDEX-Compass can not only shed light on future FinRL research to prevent untrustworthy results from stagnating FinRL into successful industry deployment but also provide a new challenging algorithm evaluation scenario for the reinforcement learning (RL) community.},
 author = {Shuo Sun and Molei Qin and Xinrun Wang and Bo An},
 badge = {Reproducibility},
 code = {https://github.com/TradeMaster-NTU/PRUDEX-Compass},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 openalex = {W4320351218},
 pdf = {https://openreview.net/pdf?id=JjbsIYOuNi},
 review = {https://openreview.net/forum?id=JjbsIYOuNi},
 title = {PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets},
 url = {https://openreview.net/forum?id=JjbsIYOuNi},
 year = {2023}
}

@article{sun2024federated,
 author = {Lukang Sun and Adil Salim and Peter Richt{\'a}rik},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Sj7bFPeR6W},
 review = {https://openreview.net/forum?id=Sj7bFPeR6W},
 title = {Federated Sampling with Langevin Algorithm under Isoperimetry},
 url = {https://openreview.net/forum?id=Sj7bFPeR6W},
 year = {2024}
}

@article{szyller2023on,
 author = {Sebastian Szyller and Rui Zhang and Jian Liu and N Asokan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LKz5SqIXPJ},
 review = {https://openreview.net/forum?id=LKz5SqIXPJ},
 title = {On the Robustness of Dataset Inference},
 url = {https://openreview.net/forum?id=LKz5SqIXPJ},
 year = {2023}
}

@article{t,
 code = {https://github.com/toenshoff/CRaWl},
 pdf = {https://openreview.net/pdf?id=vgXnEyeWVY},
 review = {https://openreview.net/forum?id=vgXnEyeWVY}
}

@article{tachella2023learning,
 author = {Juli{\'a}n Tachella and Laurent Jacques},
 badge = {Featured},
 code = {https://github.com/tachella/ssbm},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 pdf = {https://openreview.net/pdf?id=ioFIAQOBOS},
 review = {https://openreview.net/forum?id=ioFIAQOBOS},
 title = {Learning to reconstruct signals from binary measurements alone},
 url = {https://openreview.net/forum?id=ioFIAQOBOS},
 year = {2023}
}

@article{takano2023selfsupervision,
 author = {Kyo Takano},
 code = {https://github.com/kyo-takano/efficientcube},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=bnBeNFB27b},
 review = {https://openreview.net/forum?id=bnBeNFB27b},
 title = {Self-Supervision is All You Need for Solving Rubik{\textquoteright}s Cube},
 url = {https://openreview.net/forum?id=bnBeNFB27b},
 year = {2023}
}

@article{takeishi2023a,
 abstract = {In anomaly detection, the degree of irregularity is often summarized as a real-valued anomaly score. We address the problem of attributing such anomaly scores to input features for interpreting the results of anomaly detection. We particularly investigate the use of the Shapley value for attributing anomaly scores of semi-supervised detection methods. We propose a characteristic function specifically designed for attributing anomaly scores. The idea is to approximate the absence of some features by locally minimizing the anomaly score with regard to the to-be-absent features. We examine the applicability of the proposed characteristic function and other general approaches for interpreting anomaly scores on multiple datasets and multiple anomaly detection methods. The results indicate the potential utility of the attribution methods including the proposed one.},
 author = {Naoya Takeishi and Yoshinobu Kawahara},
 code = {https://github.com/n-takeishi/anomaly-attribution/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319451646},
 pdf = {https://openreview.net/pdf?id=eLX5XrajXh},
 review = {https://openreview.net/forum?id=eLX5XrajXh},
 title = {A Characteristic Function for Shapley-Value-Based Attribution of Anomaly Scores},
 url = {https://openreview.net/forum?id=eLX5XrajXh},
 year = {2023}
}

@article{takezawa2023momentum,
 abstract = {SGD with momentum is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum is Distributed SGD (DSGD) with momentum (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and deteriorate when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the setting where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coefficient $\beta \in [0, 1)$. Through experiments, we demonstrate that Momentum Tracking is more robust to data heterogeneity than the existing decentralized learning methods with momentum and can consistently outperform these existing methods when the data distributions are heterogeneous.},
 author = {Yuki Takezawa and Han Bao and Kenta Niwa and Ryoma Sato and Makoto Yamada},
 code = {https://github.com/yukiTakezawa/MomentumTracking},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4300980492},
 pdf = {https://openreview.net/pdf?id=8koy8QuTZD},
 review = {https://openreview.net/forum?id=8koy8QuTZD},
 title = {Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data},
 url = {https://openreview.net/forum?id=8koy8QuTZD},
 year = {2023}
}

@article{taki2023structured,
 abstract = {Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.},
 author = {Batoul Ahmad Taki and Anand Sarwate and Waheed U. Bajwa},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385681335},
 pdf = {https://openreview.net/pdf?id=qUxBs3Ln41},
 review = {https://openreview.net/forum?id=qUxBs3Ln41},
 title = {Structured Low-Rank Tensors for Generalized Linear Models},
 url = {https://openreview.net/forum?id=qUxBs3Ln41},
 year = {2023}
}

@article{tamir2023transport,
 abstract = {The dynamic Schr\"odinger bridge problem provides an appealing setting for solving constrained time-series data generation tasks posed as optimal transport problems. It consists of learning non-linear diffusion processes using efficient iterative solvers. Recent works have demonstrated state-of-the-art results (eg. in modelling single-cell embryo RNA sequences or sampling from complex posteriors) but are limited to learning bridges with only initial and terminal constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge (ISB). We integrate Bayesian filtering and optimal control into learning the diffusion process, enabling the generation of constrained stochastic processes governed by sparse observations at intermediate stages and terminal constraints. We assess the effectiveness of our method on synthetic and real-world data generation tasks and we show that the ISB generalises well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times.},
 author = {Ella Tamir and Martin Trapp and Arno Solin},
 code = {https://github.com/AaltoML/iterative-smoothing-bridge},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318907262},
 pdf = {https://openreview.net/pdf?id=Mbc58EzF5q},
 review = {https://openreview.net/forum?id=Mbc58EzF5q},
 title = {Transport with Support: Data-Conditional Diffusion Bridges},
 url = {https://openreview.net/forum?id=Mbc58EzF5q},
 year = {2023}
}

@article{tanguy2023convergence,
 abstract = {Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function.},
 author = {Eloi Tanguy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385293078},
 pdf = {https://openreview.net/pdf?id=aqqfB3p9ZA},
 review = {https://openreview.net/forum?id=aqqfB3p9ZA},
 title = {Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses},
 url = {https://openreview.net/forum?id=aqqfB3p9ZA},
 year = {2023}
}

@article{tao2023agentstate,
 abstract = {In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, are rarely emphasized, and it is not clear what their role or impact is. In this work we explore this idea further, and relate these auxiliary inputs to prior classic approaches to state construction. We present a series of examples illustrating the different ways of using auxiliary inputs for reinforcement learning. We show that these auxiliary inputs can be used to discriminate between observations that would otherwise be aliased, leading to more expressive features that smoothly interpolate between different states. Finally, we show that this approach is complementary to state-of-the-art methods such as recurrent neural networks and truncated back-propagation through time, and acts as a heuristic that facilitates longer temporal credit assignment, leading to better performance.},
 author = {Ruo Yu Tao and Adam White and Marlos C. Machado},
 code = {https://github.com/taodav/aux-inputs},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309299253},
 pdf = {https://openreview.net/pdf?id=RLYkyucU6k},
 review = {https://openreview.net/forum?id=RLYkyucU6k},
 title = {Agent-State Construction with Auxiliary Inputs},
 url = {https://openreview.net/forum?id=RLYkyucU6k},
 year = {2023}
}

@article{telepov2023freed,
 abstract = {A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.},
 author = {Alexander Telepov and Artem Tsypin and Kuzma Khrabrov and Sergey Yakukhnov and Pavel Strashnov and Petr Zhilyaev and Egor Rumiantsev and Daniel Ezhov and Manvel Avetisian and Olga Popova and Artur Kadurin},
 code = {https://github.com/AIRI-Institute/FFREED},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391046098},
 pdf = {https://openreview.net/pdf?id=YVPb6tyRJu},
 review = {https://openreview.net/forum?id=YVPb6tyRJu},
 title = {FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction},
 url = {https://openreview.net/forum?id=YVPb6tyRJu},
 year = {2023}
}

@article{teneggi2023shapxrt,
 abstract = {The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value -- a solution concept from game theory -- is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the SHAPley EXplanation Randomization Test (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley value provide lower and upper bounds to the expected $p$-values of their respective tests. Furthermore, we show that the Shapley value itself provides an upper bound to the expected $p$-value of a global (i.e., overall) null hypothesis. As a result, we further our understanding of Shapley-based explanation methods from a novel perspective and characterize the conditions under which one can make statistically valid claims about feature importance via the Shapley value.},
 author = {Jacopo Teneggi and Beepul Bharti and Yaniv Romano and Jeremias Sulam},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/Sulam-Group/SHAP-XRT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4285594678},
 pdf = {https://openreview.net/pdf?id=WFtTpQ47A7},
 review = {https://openreview.net/forum?id=WFtTpQ47A7},
 title = {SHAP-XRT: The Shapley Value Meets Conditional Independence Testing},
 url = {https://openreview.net/forum?id=WFtTpQ47A7},
 year = {2023}
}

@article{tenison2023gradient,
 abstract = {Federated learning (FL) is an emerging paradigm that permits a large number of clients with heterogeneous data to coordinate learning of a unified global model without the need to share data amongst each other. A major challenge in federated learning is the heterogeneity of data across client, which can degrade the performance of standard FL algorithms. Standard FL algorithms involve averaging of model parameters or gradient updates to approximate the global model at the server. However, we argue that in heterogeneous settings, averaging can result in information loss and lead to poor generalization due to the bias induced by dominant client gradients. We hypothesize that to generalize better across non-i.i.d datasets, the algorithms should focus on learning the invariant mechanism that is constant while ignoring spurious mechanisms that differ across clients. Inspired from recent works in Out-of-Distribution generalization, we propose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates. This aggregation technique for client updates can be adapted as a drop-in replacement in most existing federated algorithms. We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements, particularly in the case of heterogeneous clients.},
 author = {Irene Tenison and Sai Aravind Sreeramadas and Vaikkunth Mugunthan and Edouard Oyallon and Irina Rish and Eugene Belilovsky},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226492406},
 pdf = {https://openreview.net/pdf?id=REAyrhRYAo},
 review = {https://openreview.net/forum?id=REAyrhRYAo},
 title = {Gradient Masked Averaging for Federated Learning},
 url = {https://openreview.net/forum?id=REAyrhRYAo},
 year = {2023}
}

@article{teter2024proximal,
 abstract = {We propose a custom learning algorithm for shallow over-parameterized neural networks, i.e., networks with single hidden layer having infinite width. The infinite width of the hidden layer serves as an abstraction for the over-parameterization. Building on the recent mean field interpretations of learning dynamics in shallow neural networks, we realize mean field learning as a computational algorithm, rather than as an analytical tool. Specifically, we design a Sinkhorn regularized proximal algorithm to approximate the distributional flow for the learning dynamics over weighted point clouds. In this setting, a contractive fixed point recursion computes the time-varying weights, numerically realizing the interacting Wasserstein gradient flow of the parameter distribution supported over the neuronal ensemble. An appealing aspect of the proposed algorithm is that the measure-valued recursions allow meshless computation. We demonstrate the proposed computational framework of interacting weighted particle evolution on binary and multi-class classification. Our algorithm performs gradient descent of the free energy associated with the risk functional.},
 author = {Alexis Teter and Iman Nodozi and Abhishek Halder},
 code = {https://github.com/zalexis12/Proximal-Mean-Field-Learning.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307409486},
 pdf = {https://openreview.net/pdf?id=vyRBsqj5iG},
 review = {https://openreview.net/forum?id=vyRBsqj5iG},
 title = {Proximal Mean Field Learning in Shallow Neural Networks},
 url = {https://openreview.net/forum?id=vyRBsqj5iG},
 year = {2024}
}

@article{teutsch2022flipped,
 abstract = {Sequence-to-sequence models based on LSTM and GRU are a most popular choice for forecasting time series data reaching state-of-the-art performance. Training such models can be delicate though. The two most common training strategies within this context are teacher forcing (TF) and free running (FR). TF can be used to help the model to converge faster but may provoke an exposure bias issue due to a discrepancy between training and inference phase. FR helps to avoid this but does not necessarily lead to better results, since it tends to make the training slow and unstable instead. Scheduled sampling was the first approach tackling these issues by picking the best from both worlds and combining it into a curriculum learning (CL) strategy. Although scheduled sampling seems to be a convincing alternative to FR and TF, we found that, even if parametrized carefully, scheduled sampling may lead to premature termination of the training when applied for time series forecasting. To mitigate the problems of the above approaches we formalize CL strategies along the training as well as the training iteration scale. We propose several new curricula, and systematically evaluate their performance in two experimental sets. For our experiments, we utilize six datasets generated from prominent chaotic systems. We found that the newly proposed increasing training scale curricula with a probabilistic iteration scale curriculum consistently outperforms previous training strategies yielding an NRMSE improvement of up to 81% over FR or TF training. For some datasets we additionally observe a reduced number of training iterations. We observed that all models trained with the new curricula yield higher prediction stability allowing for longer prediction horizons.},
 author = {Philipp Teutsch and Patrick M{\"a}der},
 code = {https://github.com/phit3/flipped_classroom},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306803516},
 pdf = {https://openreview.net/pdf?id=w3x20YEcQK},
 review = {https://openreview.net/forum?id=w3x20YEcQK},
 title = {Flipped Classroom: Effective Teaching for Time Series Forecasting},
 url = {https://openreview.net/forum?id=w3x20YEcQK},
 year = {2022}
}

@article{thilak2024the,
 author = {Vimal Thilak and Etai Littwin and Shuangfei Zhai and Omid Saremi and Roni Paiss and Joshua M. Susskind},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=OZbn8ULouY},
 review = {https://openreview.net/forum?id=OZbn8ULouY},
 title = {The Slingshot Effect: A Late-Stage Optimization Anomaly in Adaptive Gradient Methods},
 url = {https://openreview.net/forum?id=OZbn8ULouY},
 year = {2024}
}

@article{thomas2023graph,
 abstract = {Graphs are ubiquitous in nature and can therefore serve as models for many practical but also theoretical problems. For this purpose, they can be defined as many different types which suitably reflect the individual contexts of the represented problem. To address cutting-edge problems based on graph data, the research field of Graph Neural Networks (GNNs) has emerged. Despite the field's youth and the speed at which new models are developed, many recent surveys have been published to keep track of them. Nevertheless, it has not yet been gathered which GNN can process what kind of graph types. In this survey, we give a detailed overview of already existing GNNs and, unlike previous surveys, categorize them according to their ability to handle different graph types and properties. We consider GNNs operating on static and dynamic graphs of different structural constitutions, with or without node or edge attributes. Moreover, we distinguish between GNN models for discrete-time or continuous-time dynamic graphs and group the models according to their architecture. We find that there are still graph types that are not or only rarely covered by existing GNN models. We point out where models are missing and give potential reasons for their absence.},
 author = {Josephine Thomas and Alice Moallemy-Oureh and Silvia Beddar-Wiesing and Clara Holzh{\"u}ter},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226028871},
 pdf = {https://openreview.net/pdf?id=h4BYtZ79uy},
 review = {https://openreview.net/forum?id=h4BYtZ79uy},
 title = {Graph Neural Networks Designed for Different Graph Types: A Survey},
 url = {https://openreview.net/forum?id=h4BYtZ79uy},
 year = {2023}
}

@article{tillman2023privacypreserving,
 author = {Robert E. Tillman and Tucker Balch and Manuela Veloso},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=vTsfup5ll6},
 review = {https://openreview.net/forum?id=vTsfup5ll6},
 title = {Privacy-Preserving Energy-Based Generative Models for Marginal Distribution Protection},
 url = {https://openreview.net/forum?id=vTsfup5ll6},
 year = {2023}
}

@article{timonen2023invertible,
 author = {Heikki Timonen and Miika Aittala and Jaakko Lehtinen},
 code = {https://github.com/timoneh/hflow},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4rkKN4tM63},
 review = {https://openreview.net/forum?id=4rkKN4tM63},
 title = {Invertible Hierarchical Generative Model for Images},
 url = {https://openreview.net/forum?id=4rkKN4tM63},
 year = {2023}
}

@article{tiulpin2022greedy,
 abstract = {Ensembles of independently trained neural networks are a state-of-the-art approach to estimate predictive uncertainty in Deep Learning, and can be interpreted as an approximation of the posterior distribution via a mixture of delta functions. The training of ensembles relies on non-convexity of the loss landscape and random initialization of their individual members, making the resulting posterior approximation uncontrolled. This paper proposes a novel and principled method to tackle this limitation, minimizing an $f$-divergence between the true posterior and a kernel density estimator (KDE) in a function space. We analyze this objective from a combinatorial point of view, and show that it is submodular with respect to mixture components for any $f$. Subsequently, we consider the problem of greedy ensemble construction. From the marginal gain on the negative $f$-divergence, which quantifies an improvement in posterior approximation yielded by adding a new component into the KDE, we derive a novel diversity term for ensemble methods. The performance of our approach is demonstrated on computer vision out-of-distribution detection benchmarks in a range of architectures trained on multiple datasets. The source code of our method is made publicly available at https://github.com/Oulu-IMEDS/greedy_ensembles_training.},
 author = {Aleksei Tiulpin and Matthew B. Blaschko},
 code = {https://github.com/Oulu-IMEDS/greedy_ensembles_training},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4285050312},
 pdf = {https://openreview.net/pdf?id=P1DuPJzVTN},
 review = {https://openreview.net/forum?id=P1DuPJzVTN},
 title = {Greedy Bayesian Posterior Approximation with Deep Ensembles},
 url = {https://openreview.net/forum?id=P1DuPJzVTN},
 year = {2022}
}

@article{tobaben2023on,
 abstract = {There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in the model vary. We show that to achieve DP accuracy on par with non-private models, the shots per class must be increased as the privacy level increases. We also show that learning parameter-efficient FiLM adapters under DP is competitive with learning just the final classifier layer or learning all of the network parameters. Finally, we evaluate DP federated learning systems and establish state-of-the-art performance on the challenging FLAIR benchmark.},
 author = {Marlon Tobaben and Aliaksandra Shysheya and John F Bronskill and Andrew Paverd and Shruti Tople and Santiago Zanella-Beguelin and Richard E Turner and Antti Honkela},
 code = {https://github.com/cambridge-mlg/dp-few-shot},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319166493},
 pdf = {https://openreview.net/pdf?id=hFsr59Imzm},
 review = {https://openreview.net/forum?id=hFsr59Imzm},
 title = {On the Efficacy of Differentially Private Few-shot Image Classification},
 url = {https://openreview.net/forum?id=hFsr59Imzm},
 year = {2023}
}

@article{tobar2023computationallyefficient,
 author = {Felipe Tobar and Elsa Cazelles and Taco de Wolff},
 code = {https://github.com/GAMES-UChile/Generalised-Variogram-Method},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=slsAQHpS7n},
 review = {https://openreview.net/forum?id=slsAQHpS7n},
 title = {Computationally-efficient initialisation of {GP}s: The generalised variogram method},
 url = {https://openreview.net/forum?id=slsAQHpS7n},
 year = {2023}
}

@article{tohme2023gsr,
 abstract = {Identifying the mathematical relationships that best describe a dataset remains a very challenging problem in machine learning, and is known as Symbolic Regression (SR). In contrast to neural networks which are often treated as black boxes, SR attempts to gain insight into the underlying relationships between the independent variables and the target variable of a given dataset by assembling analytical functions. In this paper, we present GSR, a Generalized Symbolic Regression approach, by modifying the conventional SR optimization problem formulation, while keeping the main SR objective intact. In GSR, we infer mathematical relationships between the independent variables and some transformation of the target variable. We constrain our search space to a weighted sum of basis functions, and propose a genetic programming approach with a matrix-based encoding scheme. We show that our GSR method is competitive with strong SR benchmark methods, achieving promising experimental performance on the well-known SR benchmark problem sets. Finally, we highlight the strengths of GSR by introducing SymSet, a new SR benchmark set which is more challenging relative to the existing benchmarks.},
 author = {Tony Tohme and Dehong Liu and KAMAL YOUCEF-TOUMI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281660239},
 pdf = {https://openreview.net/pdf?id=lheUXtDNvP},
 review = {https://openreview.net/forum?id=lheUXtDNvP},
 title = {GSR: A Generalized Symbolic Regression Approach},
 url = {https://openreview.net/forum?id=lheUXtDNvP},
 year = {2023}
}

@article{tohme2024messy,
 abstract = {We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. However, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost.},
 author = {Tony Tohme and Mohsen Sadr and KAMAL YOUCEF-TOUMI and Nicolas Hadjiconstantinou},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380204846},
 pdf = {https://openreview.net/pdf?id=Y2ru0LuQeS},
 review = {https://openreview.net/forum?id=Y2ru0LuQeS},
 title = {MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation},
 url = {https://openreview.net/forum?id=Y2ru0LuQeS},
 year = {2024}
}

@article{tolooshams2022stable,
 abstract = {The dictionary learning problem, representing data as a combination of a few atoms, has long stood as a popular method for learning representations in statistics and signal processing. The most popular dictionary learning algorithm alternates between sparse coding and dictionary update steps, and a rich literature has studied its theoretical convergence. The success of dictionary learning relies on access to a "good" initial estimate of the dictionary and the ability of the sparse coding step to provide an unbiased estimate of the code. The growing popularity of unrolled sparse coding networks has led to the empirical finding that backpropagation through such networks performs dictionary learning. We offer the theoretical analysis of these empirical results through PUDLE, a Provable Unrolled Dictionary LEarning method. We provide conditions on the network initialization and data distribution sufficient to recover and preserve the support of the latent code. Additionally, we address two challenges; first, the vanilla unrolled sparse coding computes a biased code estimate, and second, gradients during backpropagated learning can become unstable. We show approaches to reduce the bias of the code estimate in the forward pass, and that of the dictionary estimate in the backward pass. We propose strategies to resolve the learning instability by tuning network parameters and modifying the loss function. Overall, we highlight the impact of loss, unrolling, and backpropagation on convergence. We complement our findings through synthetic and image denoising experiments. Finally, we demonstrate PUDLE's interpretability, a driving factor in designing deep networks based on iterative optimizations, by building a mathematical relation between network weights, its output, and the training set.},
 author = {Bahareh Tolooshams and Demba E. Ba},
 code = {https://github.com/btolooshams/stable-interpretable-unrolled-dl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287162428},
 pdf = {https://openreview.net/pdf?id=e3S0Bl2RO8},
 review = {https://openreview.net/forum?id=e3S0Bl2RO8},
 title = {Stable and Interpretable Unrolled Dictionary Learning},
 url = {https://openreview.net/forum?id=e3S0Bl2RO8},
 year = {2022}
}

@article{tomar2023learning,
 abstract = {Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks.},
 author = {Manan Tomar and Utkarsh Aashu Mishra and Amy Zhang and Matthew E. Taylor},
 code = {https://github.com/UtkarshMishra04/pixel-representations-RL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306904054},
 pdf = {https://openreview.net/pdf?id=wIXHG8LZ2w},
 review = {https://openreview.net/forum?id=wIXHG8LZ2w},
 title = {Learning Representations for Pixel-based Control: What Matters and Why?},
 url = {https://openreview.net/forum?id=wIXHG8LZ2w},
 year = {2023}
}

@article{tong2024improving,
 abstract = {Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schr\"odinger bridge inference.},
 author = {Alexander Tong and Kilian FATRAS and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio},
 badge = {Written by Expert Reviewer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4319050105},
 pdf = {https://openreview.net/pdf?id=CD9Snc73AW},
 review = {https://openreview.net/forum?id=CD9Snc73AW},
 title = {Improving and generalizing flow-based generative models with minibatch optimal transport},
 url = {https://openreview.net/forum?id=CD9Snc73AW},
 year = {2024}
}

@article{toni2024personalized,
 abstract = {Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Reinforcement Learning agent coupled with Monte Carlo Tree Search to quickly identify promising recourse plans. Our empirical evaluation on real-world datasets highlights how PEAR produces high-quality personalized recourse in only a handful of iterations.},
 author = {Giovanni De Toni and Paolo Viappiani and Stefano Teso and Bruno Lepri and Andrea Passerini},
 code = {https://github.com/unitn-sml/pear-personalized-algorithmic-recourse},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281984045},
 pdf = {https://openreview.net/pdf?id=8sg2I9zXgO},
 review = {https://openreview.net/forum?id=8sg2I9zXgO},
 title = {Personalized Algorithmic Recourse with Preference Elicitation},
 url = {https://openreview.net/forum?id=8sg2I9zXgO},
 year = {2024}
}

@article{torkzadehmahani2023label,
 abstract = {In learning tasks with label noise, improving model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels, including the noisy ones. Identifying the samples with noisy labels and preventing the model from learning them is a promising approach to address this challenge. When training with noisy labels, the per-class confidence scores of the model, represented by the class probabilities, can be reliable criteria for assessing whether the input label is the true label or the corrupted one. In this work, we exploit this observation and propose a novel discriminator metric called confidence error and a sieving strategy called CONFES to differentiate between the clean and noisy samples effectively. We provide theoretical guarantees on the probability of error for our proposed metric. Then, we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings, such as synthetic and real-world label noise. Moreover, we show CONFES can be combined with other state-of-the-art approaches, such as Co-teaching and DivideMix to further improve model performance.},
 author = {Reihaneh Torkzadehmahani and Reza Nasirigerdeh and Daniel Rueckert and Georgios Kaissis},
 code = {https://github.com/reihaneh-torkzadehmahani/confes},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4305028418},
 pdf = {https://openreview.net/pdf?id=3taIQG4C7H},
 review = {https://openreview.net/forum?id=3taIQG4C7H},
 title = {Label Noise-Robust Learning using a Confidence-Based Sieving Strategy},
 url = {https://openreview.net/forum?id=3taIQG4C7H},
 year = {2023}
}

@article{tornede2024automl,
 author = {Alexander Tornede and Difan Deng and Theresa Eimer and Joseph Giovanelli and Aditya Mohan and Tim Ruhkopf and Sarah Segel and Daphne Theodorakopoulos and Tanja Tornede and Henning Wachsmuth and Marius Lindauer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=cAthubStyG},
 review = {https://openreview.net/forum?id=cAthubStyG},
 title = {Auto{ML} in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks},
 url = {https://openreview.net/forum?id=cAthubStyG},
 year = {2024}
}

@article{toyota2024outofdistribution,
 abstract = {Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk.},
 author = {Shoji Toyota and Kenji Fukumizu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385245463},
 pdf = {https://openreview.net/pdf?id=pWsfWDnJDa},
 review = {https://openreview.net/forum?id=pWsfWDnJDa},
 title = {Out-of-Distribution Optimality of Invariant Risk Minimization},
 url = {https://openreview.net/forum?id=pWsfWDnJDa},
 year = {2024}
}

@article{trockman2023patches,
 abstract = {Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.},
 author = {Asher Trockman and J Zico Kolter},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4285661751},
 pdf = {https://openreview.net/pdf?id=rAnB7JSMXL},
 review = {https://openreview.net/forum?id=rAnB7JSMXL},
 title = {Patches Are All You Need?},
 url = {https://openreview.net/forum?id=rAnB7JSMXL},
 year = {2023}
}

@article{troshin2023wrapped,
 author = {Sergey Troshin and Vlad Niculae},
 code = {https://github.com/ltl-uva/wbg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=KrequDpWzt},
 review = {https://openreview.net/forum?id=KrequDpWzt},
 title = {Wrapped \${\textbackslash}beta\$-Gaussians with compact support for exact probabilistic modeling on manifolds},
 url = {https://openreview.net/forum?id=KrequDpWzt},
 year = {2023}
}

@article{tschernutter2024a,
 abstract = {We propose an algorithm for optimizing the parameters of single hidden layer neural networks. Specifically, we derive a blockwise difference-of-convex (DC) functions representation of the objective function. Based on the latter, we propose a block coordinate descent (BCD) approach that we combine with a tailored difference-of-convex functions algorithm (DCA). We prove global convergence of the proposed algorithm. Furthermore, we mathematically analyze the convergence rate of parameters and the convergence rate in value (i.e., the training loss). We give conditions under which our algorithm converges linearly or even faster depending on the local shape of the loss function. We confirm our theoretical derivations numerically and compare our algorithm against state-of-the-art gradient-based solvers in terms of both training loss and test loss.},
 author = {Daniel Tschernutter and Mathias Kraus and Stefan Feuerriegel},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4391009410},
 pdf = {https://openreview.net/pdf?id=EDqCY6ihbr},
 review = {https://openreview.net/forum?id=EDqCY6ihbr},
 title = {A Globally Convergent Algorithm for Neural Network Parameter Optimization Based on Difference-of-Convex Functions},
 url = {https://openreview.net/forum?id=EDqCY6ihbr},
 year = {2024}
}

@article{tsuchida2023stochastic,
 author = {Russell Tsuchida and Cheng Soon Ong},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/RussellTsuchida/dek.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=p7UTv2hWgM},
 review = {https://openreview.net/forum?id=p7UTv2hWgM},
 title = {Stochastic gradient updates yield deep equilibrium kernels},
 url = {https://openreview.net/forum?id=p7UTv2hWgM},
 year = {2023}
}

@article{tyurin2023sharper,
 abstract = {We revisit the classical problem of finding an approximately stationary point of the average of $n$ smooth and possibly nonconvex functions. The optimal complexity of stochastic first-order methods in terms of the number of gradient evaluations of individual functions is $\mathcal{O}\left(n + n^{1/2}\varepsilon^{-1}\right)$, attained by the optimal SGD methods $\small\sf\color{green}{SPIDER}$(arXiv:1807.01695) and $\small\sf\color{green}{PAGE}$(arXiv:2008.10898), for example, where $\varepsilon$ is the error tolerance. However, i) the big-$\mathcal{O}$ notation hides crucial dependencies on the smoothness constants associated with the functions, and ii) the rates and theory in these methods assume simplistic sampling mechanisms that do not offer any flexibility. In this work we remedy the situation. First, we generalize the $\small\sf\color{green}{PAGE}$ algorithm so that it can provably work with virtually any (unbiased) sampling mechanism. This is particularly useful in federated learning, as it allows us to construct and better understand the impact of various combinations of client and data sampling strategies. Second, our analysis is sharper as we make explicit use of certain novel inequalities that capture the intricate interplay between the smoothness constants and the sampling procedure. Indeed, our analysis is better even for the simple sampling procedure analyzed in the $\small\sf\color{green}{PAGE}$ paper. However, this already improved bound can be further sharpened by a different sampling scheme which we propose. In summary, we provide the most general and most accurate analysis of optimal SGD in the smooth nonconvex regime. Finally, our theoretical findings are supposed with carefully designed experiments.},
 author = {Alexander Tyurin and Lukang Sun and Konstantin Pavlovich Burlachenko and Peter Richt{\'a}rik},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281666149},
 pdf = {https://openreview.net/pdf?id=zKgJ6TWAFE},
 review = {https://openreview.net/forum?id=zKgJ6TWAFE},
 title = {Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling},
 url = {https://openreview.net/forum?id=zKgJ6TWAFE},
 year = {2023}
}

@article{tzannetos2023proximal,
 abstract = {We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.},
 author = {Georgios Tzannetos and B{\'a}rbara Gomes Ribeiro and Parameswaran Kamalaruban and Adish Singla},
 code = {https://github.com/machine-teaching-group/tmlr2023_proximal-curriculum-rl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4367060986},
 pdf = {https://openreview.net/pdf?id=8WUyeeMxMH},
 review = {https://openreview.net/forum?id=8WUyeeMxMH},
 title = {Proximal Curriculum for Reinforcement Learning Agents},
 url = {https://openreview.net/forum?id=8WUyeeMxMH},
 year = {2023}
}

@article{tziotis2023stragglerresilient,
 abstract = {Federated Learning is an emerging learning paradigm that allows training models from samples distributed across a large network of clients while respecting privacy and communication restrictions. Despite its success, federated learning faces several challenges related to its decentralized nature. In this work, we develop a novel algorithmic procedure with theoretical speedup guarantees that simultaneously handles two of these hurdles, namely (i) data heterogeneity, i.e., data distributions can vary substantially across clients, and (ii) system heterogeneity, i.e., the computational power of the clients could differ significantly. Our method relies on ideas from representation learning theory to find a global common representation using all clients' data and learn a user-specific set of parameters leading to a personalized solution for each client. Furthermore, our method mitigates the effects of stragglers by adaptively selecting clients based on their computational characteristics and statistical significance, thus achieving, for the first time, near optimal sample complexity and provable logarithmic speedup. Experimental results support our theoretical findings showing the superiority of our method over alternative personalized federated schemes in system and data heterogeneous environments.},
 author = {Isidoros Tziotis and Zebang Shen and Ramtin Pedarsani and Hamed Hassani and Aryan Mokhtari},
 code = {https://github.com/shenzebang/SRPFL                               https://github.com/shenzebang/PyTorch-Sentiment-Analysis},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281708275},
 pdf = {https://openreview.net/pdf?id=gxEpUFxIgz},
 review = {https://openreview.net/forum?id=gxEpUFxIgz},
 title = {Straggler-Resilient Personalized Federated Learning},
 url = {https://openreview.net/forum?id=gxEpUFxIgz},
 year = {2023}
}

@article{uelwer2022optimizing,
 abstract = {Phase retrieval is the problem of reconstructing images from magnitude-only measurements. In many real-world applications the problem is underdetermined. When training data is available, generative models allow optimization in a lower-dimensional latent space, hereby constraining the solution set to those images that can be synthesized by the generative model. However, not all possible solutions are within the range of the generator. Instead, they are represented with some error. To reduce this representation error in the context of phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO) to extend the range of the generator while still producing images consistent with the training data. Second, we introduce new initialization schemes that further improve the quality of the reconstruction. With extensive experiments on the Fourier phase retrieval problem and thorough ablation studies, we can show the benefits of our modified ILO and the new initialization schemes. Additionally, we analyze the performance of our approach on the Gaussian phase retrieval problem.},
 author = {Tobias Uelwer and Sebastian Konietzny and Stefan Harmeling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281791254},
 pdf = {https://openreview.net/pdf?id=YAVE6jfeJb},
 review = {https://openreview.net/forum?id=YAVE6jfeJb},
 title = {Optimizing Intermediate Representations of Generative Models for Phase Retrieval},
 url = {https://openreview.net/forum?id=YAVE6jfeJb},
 year = {2022}
}

@article{ullah2023clustering,
 author = {Enayat Ullah and Harry Lang and Raman Arora and Vladimir Braverman},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=TzRXyO3CzX},
 review = {https://openreview.net/forum?id=TzRXyO3CzX},
 title = {Clustering using Approximate Nearest Neighbour Oracles},
 url = {https://openreview.net/forum?id=TzRXyO3CzX},
 year = {2023}
}

@article{ullah2023generalization,
 author = {Enayat Ullah and Raman Arora},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=KwWKB9Bqam},
 review = {https://openreview.net/forum?id=KwWKB9Bqam},
 title = {Generalization bounds for Kernel Canonical Correlation Analysis},
 url = {https://openreview.net/forum?id=KwWKB9Bqam},
 year = {2023}
}

@article{ulmer2023prior,
 abstract = {Popular approaches for quantifying predictive uncertainty in deep neural networks often involve distributions over weights or multiple models, for instance via Markov Chain sampling, ensembling, or Monte Carlo dropout. These techniques usually incur overhead by having to train multiple model instances or do not produce very diverse predictions. This comprehensive and extensive survey aims to familiarize the reader with an alternative class of models based on the concept of Evidential Deep Learning: For unfamiliar data, they aim to admit "what they don't know", and fall back onto a prior belief. Furthermore, they allow uncertainty estimation in a single model and forward pass by parameterizing distributions over distributions. This survey recapitulates existing works, focusing on the implementation in a classification setting, before surveying the application of the same paradigm to regression. We also reflect on the strengths and weaknesses compared to other existing methods and provide the most fundamental derivations using a unified notation to aid future research.},
 author = {Dennis Thomas Ulmer and Christian Hardmeier and Jes Frellsen},
 code = {https://github.com/Kaleidophon/evidential-deep-learning-survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286909493},
 pdf = {https://openreview.net/pdf?id=xqS8k9E75c},
 review = {https://openreview.net/forum?id=xqS8k9E75c},
 title = {Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation},
 url = {https://openreview.net/forum?id=xqS8k9E75c},
 year = {2023}
}

@article{unal2023meta,
 author = {Altay Unal and Abdullah Akg{\"u}l and Melih Kandemir and Gozde Unal},
 code = {https://github.com/ituvisionlab/MetaCLGraph},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=8tnrh56P5W},
 review = {https://openreview.net/forum?id=8tnrh56P5W},
 title = {Meta Continual Learning on Graphs with Experience Replay},
 url = {https://openreview.net/forum?id=8tnrh56P5W},
 year = {2023}
}

@article{upadhyay2023hypuc,
 author = {Uddeshya Upadhyay and Sairam Bade and Arjun Puranik and Shahir Asfahan and Melwin Babu and Francisco Lopez-Jimenez and Samuel Asirvatham and Ashim Prasad and Ajit Rajasekharan and Samir Awasthi and Rakesh Barve},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=0Xo9giEZWf},
 review = {https://openreview.net/forum?id=0Xo9giEZWf},
 title = {Hyp{UC}: Hyperfine Uncertainty Calibration with Gradient- boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms},
 url = {https://openreview.net/forum?id=0Xo9giEZWf},
 year = {2023}
}

@article{utpala2023differentially,
 abstract = {Differential privacy has become crucial in the real-world deployment of statistical and machine learning algorithms with rigorous privacy guarantees. The earliest statistical queries, for which differential privacy mechanisms have been developed, were for the release of the sample mean. In Geometric Statistics, the sample Fr\'echet mean represents one of the most fundamental statistical summaries, as it generalizes the sample mean for data belonging to nonlinear manifolds. In that spirit, the only geometric statistical query for which a differential privacy mechanism has been developed, so far, is for the release of the sample Fr\'echet mean: the \emph{Riemannian Laplace mechanism} was recently proposed to privatize the Fr\'echet mean on complete Riemannian manifolds. In many fields, the manifold of Symmetric Positive Definite (SPD) matrices is used to model data spaces, including in medical imaging where privacy requirements are key. We propose a novel, simple and fast mechanism - the \emph{tangent Gaussian mechanism} - to compute a differentially private Fr\'echet mean on the SPD manifold endowed with the log-Euclidean Riemannian metric. We show that our new mechanism has significantly better utility and is computationally efficient -- as confirmed by extensive experiments.},
 author = {Saiteja Utpala and Praneeth Vepakomma and Nina Miolane},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4300992997},
 pdf = {https://openreview.net/pdf?id=mAx8QqZ14f},
 review = {https://openreview.net/forum?id=mAx8QqZ14f},
 title = {Differentially Private Fréchet Mean on the Manifold of Symmetric Positive Definite (SPD) Matrices with log-Euclidean Metric},
 url = {https://openreview.net/forum?id=mAx8QqZ14f},
 year = {2023}
}

@article{utpala2023improved,
 author = {Saiteja Utpala and Andi Han and Pratik Jawanpuria and Bamdev Mishra},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=paguBNtqiO},
 review = {https://openreview.net/forum?id=paguBNtqiO},
 title = {Improved Differentially Private Riemannian Optimization: Fast Sampling and Variance Reduction},
 url = {https://openreview.net/forum?id=paguBNtqiO},
 year = {2023}
}

@article{vafa2024career,
 author = {Keyon Vafa and Emil Palikot and Tianyu Du and Ayush Kanodia and Susan Athey and David Blei},
 code = {https://github.com/keyonvafa/career-code},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4i1MXH8Sle},
 review = {https://openreview.net/forum?id=4i1MXH8Sle},
 title = {{CAREER}: A Foundation Model for Labor Sequence Data},
 url = {https://openreview.net/forum?id=4i1MXH8Sle},
 year = {2024}
}

@article{valk2023assuming,
 author = {Kaspar Valk and Meelis Kull},
 code = {https://github.com/kaspar98/lece-calibration},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=na5sHG69rI},
 review = {https://openreview.net/forum?id=na5sHG69rI},
 title = {Assuming Locally Equal Calibration Errors for Non-Parametric Multiclass Calibration},
 url = {https://openreview.net/forum?id=na5sHG69rI},
 year = {2023}
}

@article{van,
 badge = {Written by Expert Reviewer},
 pdf = {https://openreview.net/pdf?id=akg6kdx0Pk},
 review = {https://openreview.net/forum?id=akg6kdx0Pk}
}

@article{vanderschueren2023noflite,
 author = {Toon Vanderschueren and Jeroen Berrevoets and Wouter Verbeke},
 code = {https://github.com/toonvds/NOFLITE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=EjqopDxLbG},
 review = {https://openreview.net/forum?id=EjqopDxLbG},
 title = {{NOFLITE}: Learning to Predict Individual Treatment Effect Distributions},
 url = {https://openreview.net/forum?id=EjqopDxLbG},
 year = {2023}
}

@article{vardhan2024an,
 author = {Harsh Vardhan and Avishek Ghosh and Arya Mazumdar},
 code = {https://github.com/harshv834/sr-fca},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=1ZGA5mSkoB},
 review = {https://openreview.net/forum?id=1ZGA5mSkoB},
 title = {An Improved Federated Clustering Algorithm with Model-based Clustering},
 url = {https://openreview.net/forum?id=1ZGA5mSkoB},
 year = {2024}
}

@article{varici2024separability,
 author = {Burak Varici and Dmitriy Katz and Dennis Wei and Prasanna Sattigeri and Ali Tajer},
 code = {https://github.com/bvarici/TMLR-mixture-DAG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ALRWXT1RLZ},
 review = {https://openreview.net/forum?id=ALRWXT1RLZ},
 title = {Separability Analysis for Causal Discovery in Mixture of {DAG}s},
 url = {https://openreview.net/forum?id=ALRWXT1RLZ},
 year = {2024}
}

@article{vasconcelos2023uncertainr,
 author = {Francisca Vasconcelos and Bobby He and Nalini M Singh and Yee Whye Teh},
 code = {https://github.com/bobby-he/uncertainr},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=jdGMBgYvfX},
 review = {https://openreview.net/forum?id=jdGMBgYvfX},
 title = {Uncerta{INR}: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography},
 url = {https://openreview.net/forum?id=jdGMBgYvfX},
 year = {2023}
}

@article{vayer2022time,
 abstract = {In this work we address the problem of comparing time series while taking into account both feature space transformation and temporal variability. The proposed framework combines a latent global transformation of the feature space with the widely used Dynamic Time Warping (DTW). The latent global transformation captures the feature invariance while the DTW (or its smooth counterpart soft-DTW) deals with the temporal shifts. We cast the problem as a joint optimization over the global transformation and the temporal alignments. The versatility of our framework allows for several variants depending on the invariance class at stake. Among our contributions we define a differentiable loss for time series and present two algorithms for the computation of time series barycenters under our new geometry. We illustrate the interest of our approach on both simulated and real world data.},
 author = {Titouan Vayer and Romain Tavenard and Laetitia Chapel and R{\'e}mi Flamary and Nicolas Courty and Yann Soullard},
 code = {https://github.com/rtavenar/dtw_gi},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3005359691},
 pdf = {https://openreview.net/pdf?id=JXCH5N4Ujy},
 review = {https://openreview.net/forum?id=JXCH5N4Ujy},
 title = {Time Series Alignment with Global Invariances},
 url = {https://openreview.net/forum?id=JXCH5N4Ujy},
 year = {2022}
}

@article{velazquez2023explaining,
 author = {Diego Velazquez and Pau Rodriguez and Alexandre Lacoste and Issam H. Laradji and Xavier Roca and Jordi Gonz{\`a}lez},
 badge = {Reproducibility},
 code = {https://github.com/dvd42/Bex},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 pdf = {https://openreview.net/pdf?id=RYeRNwRjNE},
 review = {https://openreview.net/forum?id=RYeRNwRjNE},
 title = {Explaining Visual Counterfactual Explainers},
 url = {https://openreview.net/forum?id=RYeRNwRjNE},
 year = {2023}
}

@article{veldanda2023fairness,
 author = {Akshaj Kumar Veldanda and Ivan Brugere and Jiahao Chen and Sanghamitra Dutta and Alan Mishler and Siddharth Garg},
 code = {https://github.com/akshajkumarv/MinDiff},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=f4VyYhkRvi},
 review = {https://openreview.net/forum?id=f4VyYhkRvi},
 title = {Fairness via In-Processing in the Over-parameterized Regime: A Cautionary Tale with MinDiff Loss},
 url = {https://openreview.net/forum?id=f4VyYhkRvi},
 year = {2023}
}

@article{veldanda2024hyperparameter,
 abstract = {Fair machine learning methods seek to train models that balance model performance across demographic subgroups defined over sensitive attributes like race and gender. Although sensitive attributes are typically assumed to be known during training, they may not be available in practice due to privacy and other logistical concerns. Recent work has sought to train fair models without sensitive attributes on training data. However, these methods need extensive hyper-parameter tuning to achieve good results, and hence assume that sensitive attributes are known on validation data. However, this assumption too might not be practical. Here, we propose Antigone, a framework to train fair classifiers without access to sensitive attributes on either training or validation data. Instead, we generate pseudo sensitive attributes on the validation data by training a biased classifier and using the classifier's incorrectly (correctly) labeled examples as proxies for minority (majority) groups. Since fairness metrics like demographic parity, equal opportunity and subgroup accuracy can be estimated to within a proportionality constant even with noisy sensitive attribute information, we show theoretically and empirically that these proxy labels can be used to maximize fairness under average accuracy constraints. Key to our results is a principled approach to select the hyper-parameters of the biased classifier in a completely unsupervised fashion (meaning without access to ground truth sensitive attributes) that minimizes the gap between fairness estimated using noisy versus ground-truth sensitive labels.},
 author = {Akshaj Kumar Veldanda and Ivan Brugere and Sanghamitra Dutta and Alan Mishler and Siddharth Garg},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4319323142},
 pdf = {https://openreview.net/pdf?id=ZSWKdRi2cU},
 review = {https://openreview.net/forum?id=ZSWKdRi2cU},
 title = {Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access},
 url = {https://openreview.net/forum?id=ZSWKdRi2cU},
 year = {2024}
}

@article{vezzani2023skills,
 abstract = {The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common approaches, but current methods have considerable limitations.For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. We compare several common approaches for skill transfer on multiple domains including changes in task and system dynamics. We identify how existing methods can fail and introduce an alternative approach to mitigate these problems. Our approach learns to sequence existing temporally-extended skills for exploration but learns the final policy directly from the raw experience. This conceptual split enables rapid adaptation and thus efficient data collection but without constraining the final solution.It significantly outperforms many classical methods across a suite of evaluation tasks and we use a broad set of ablations to highlight the importance of differentc omponents of our method.},
 author = {Giulia Vezzani and Dhruva Tirumala and Markus Wulfmeier and Dushyant Rao and Abbas Abdolmaleki and Ben Moran and Tuomas Haarnoja and Jan Humplik and Roland Hafner and Michael Neunert and Claudio Fantacci and Tim Hertweck and Thomas Lampe and Fereshteh Sadeghi and Nicolas Heess and Martin Riedmiller},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310280900},
 pdf = {https://openreview.net/pdf?id=JwGKVpRfVD},
 review = {https://openreview.net/forum?id=JwGKVpRfVD},
 title = {SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration},
 url = {https://openreview.net/forum?id=JwGKVpRfVD},
 year = {2023}
}

@article{vielhaben2023multidimensional,
 abstract = {The completeness axiom renders the explanation of a post-hoc XAI method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. Recently, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is expressed within a sample, allowing for concept characterization through prototypical samples, and (2) concept relevance heatmaps, that decompose the model decision into concept contributions. Both tools together enable a detailed understanding of the model reasoning, which is guaranteed to relate to the model via a completeness relation. This paves the way towards more trustworthy concept-based XAI. We empirically demonstrate the superiority of MCD against more constrained concept definitions.},
 author = {Johanna Vielhaben and Stefan Bluecher and Nils Strodthoff},
 code = {https://github.com/jvielhaben/MCD-XAI},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318621303},
 pdf = {https://openreview.net/pdf?id=KxBQPz7HKh},
 review = {https://openreview.net/forum?id=KxBQPz7HKh},
 title = {Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees},
 url = {https://openreview.net/forum?id=KxBQPz7HKh},
 year = {2023}
}

@article{villar2024towards,
 abstract = {Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. Our goal is to understand the implications for machine learning of the many passive symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. This paper is conceptual: It translates among the languages of physics, mathematics, and machine-learning. We believe that consideration and implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century.},
 author = {Soledad Villar and David W Hogg and Weichi Yao and George A Kevrekidis and Bernhard Sch{\"o}lkopf},
 code = {https://github.com/weichiyao/TowardsFullyCovariantML},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318908230},
 pdf = {https://openreview.net/pdf?id=gllUnpYuXg},
 review = {https://openreview.net/forum?id=gllUnpYuXg},
 title = {Towards fully covariant machine learning},
 url = {https://openreview.net/forum?id=gllUnpYuXg},
 year = {2024}
}

@article{villi,
 code = {https://gitlab.in2p3.fr/antoine.villie1/seism},
 pdf = {https://openreview.net/pdf?id=nddEHTSnqg},
 review = {https://openreview.net/forum?id=nddEHTSnqg}
}

@article{vinaroz2022differentially,
 abstract = {We are interested in privatizing an approximate posterior inference algorithm called Expectation Propagation (EP). EP approximates the posterior by iteratively refining approximations to the local likelihoods, and is known to provide better posterior uncertainties than those by variational inference (VI). However, EP needs a large memory to maintain all local approximates associated with each datapoint in the training data. To overcome this challenge, stochastic expectation propagation (SEP) considers a single unique local factor that captures the average effect of each likelihood term to the posterior and refines it in a way analogous to EP. In terms of privacy, SEP is more tractable than EP because at each refining step of a factor, the remaining factors are fixed and do not depend on other datapoints as in EP, which makes the sensitivity analysis straightforward. We provide a theoretical analysis of the privacy-accuracy trade-off in the posterior estimates under our method, called differentially private stochastic expectation propagation (DP-SEP). Furthermore, we demonstrate the performance of our DP-SEP algorithm evaluated on both synthetic and real-world datasets in terms of the quality of posterior estimates at different levels of guaranteed privacy.},
 author = {Margarita Vinaroz and Mijung Park},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226076461},
 pdf = {https://openreview.net/pdf?id=e5ILb2Nqst},
 review = {https://openreview.net/forum?id=e5ILb2Nqst},
 title = {Differentially private stochastic expectation propagation (DP-SEP)},
 url = {https://openreview.net/forum?id=e5ILb2Nqst},
 year = {2022}
}

@article{virgolin2022symbolic,
 abstract = {Symbolic regression (SR) is the task of learning a model of data in the form of a mathematical expression. By their nature, SR models have the potential to be accurate and human-interpretable at the same time. Unfortunately, finding such models, i.e., performing SR, appears to be a computationally intensive task. Historically, SR has been tackled with heuristics such as greedy or genetic algorithms and, while some works have hinted at the possible hardness of SR, no proof has yet been given that SR is, in fact, NP-hard. This begs the question: Is there an exact polynomial-time algorithm to compute SR models? We provide evidence suggesting that the answer is probably negative by showing that SR is NP-hard.},
 author = {Marco Virgolin and Solon P Pissis},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4284664089},
 pdf = {https://openreview.net/pdf?id=LTiaPxqe2e},
 review = {https://openreview.net/forum?id=LTiaPxqe2e},
 title = {Symbolic Regression is NP-hard},
 url = {https://openreview.net/forum?id=LTiaPxqe2e},
 year = {2022}
}

@article{vitrack,
 code = {https://github.com/razla/Foiling-Explanations-in-Deep-Neural-Networks},
 pdf = {https://openreview.net/pdf?id=wvLQMHtyLk},
 review = {https://openreview.net/forum?id=wvLQMHtyLk}
}

@article{vonderfecht2022fingerprints,
 author = {Jeremy Vonderfecht and Feng Liu},
 code = {https://github.com/JeremyIV/SISR-fingerprints},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Jj0qSbtwdb},
 review = {https://openreview.net/forum?id=Jj0qSbtwdb},
 title = {Fingerprints of Super Resolution Networks},
 url = {https://openreview.net/forum?id=Jj0qSbtwdb},
 year = {2022}
}

@article{vora2022nesf,
 author = {Suhani Vora and Noha Radwan and Klaus Greff and Henning Meyer and Kyle Genova and Mehdi S. M. Sajjadi and Etienne Pot and Andrea Tagliasacchi and Daniel Duckworth},
 code = {https://github.com/google-research/jax3d/tree/main/jax3d/projects/nesf},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=ggPhsYCsm9},
 review = {https://openreview.net/forum?id=ggPhsYCsm9},
 title = {Ne{SF}: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes},
 url = {https://openreview.net/forum?id=ggPhsYCsm9},
 year = {2022}
}

@article{vowels2023a,
 author = {Matthew James Vowels and Sina Akbari and Necati Cihan Camgoz and Richard Bowden},
 code = {https://github.com/matthewvowels1/FreeLunchSemiParametrics/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=dQxBRqCjLr},
 review = {https://openreview.net/forum?id=dQxBRqCjLr},
 title = {A Free Lunch with Influence Functions? An Empirical Evaluation of Influence Functions for Average Treatment Effect Estimation},
 url = {https://openreview.net/forum?id=dQxBRqCjLr},
 year = {2023}
}

@article{vyas2023empirical,
 author = {Nikhil Vyas and Yamini Bansal and Preetum Nakkiran},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Y3saBb7mCE},
 review = {https://openreview.net/forum?id=Y3saBb7mCE},
 title = {Empirical Limitations of the {NTK} for Understanding Scaling Laws in Deep Learning},
 url = {https://openreview.net/forum?id=Y3saBb7mCE},
 year = {2023}
}

@article{wagner2023cyclophobic,
 abstract = {In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforcement learning is more sample efficient than other state of the art methods in a variety of tasks.},
 author = {Stefan Sylvius Wagner and Peter Arndt and Jan Robine and Stefan Harmeling},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4386351512},
 pdf = {https://openreview.net/pdf?id=83rgSFPpws},
 review = {https://openreview.net/forum?id=83rgSFPpws},
 title = {Cyclophobic Reinforcement Learning},
 url = {https://openreview.net/forum?id=83rgSFPpws},
 year = {2023}
}

@article{wagner2023timesead,
 author = {Dennis Wagner and Tobias Michels and Florian C.F. Schulz and Arjun Nair and Maja Rudolph and Marius Kloft},
 code = {https://github.com/wagner-d/TimeSeAD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=iMmsCI0JsS},
 review = {https://openreview.net/forum?id=iMmsCI0JsS},
 title = {TimeSe{AD}: Benchmarking Deep Multivariate Time-Series Anomaly Detection},
 url = {https://openreview.net/forum?id=iMmsCI0JsS},
 year = {2023}
}

@article{wagstaff2023hidden,
 abstract = {Trustworthy classifiers are essential to the adoption of machine learning predictions in many real-world settings. The predicted probability of possible outcomes can inform high-stakes decision making, particularly when assessing the expected value of alternative decisions or the risk of bad outcomes. These decisions require well-calibrated probabilities, not just the correct prediction of the most likely class. Black-box classifier calibration methods can improve the reliability of a classifier's output without requiring retraining. However, these methods are unable to detect subpopulations where calibration could also improve prediction accuracy. Such subpopulations are said to exhibit "hidden heterogeneity" (HH), because the original classifier did not detect them. This paper proposes a quantitative measure for HH. It also introduces two similarity-weighted calibration methods that can address HH by adapting locally to each test item: SWC weights the calibration set by similarity to the test item, and SWC-HH explicitly incorporates hidden heterogeneity to filter the calibration set. Experiments show that the improvements in calibration achieved by similarity-based calibration methods correlate with the amount of HH present and, given sufficient calibration data, generally exceed calibration achieved by global methods. HH can therefore serve as a useful diagnostic tool for identifying when local calibration methods would be beneficial.},
 author = {Kiri L. Wagstaff and Thomas G Dietterich},
 code = {https://github.com/wkiri/simcalib},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226472019},
 pdf = {https://openreview.net/pdf?id=RA0TDqt3hC},
 review = {https://openreview.net/forum?id=RA0TDqt3hC},
 title = {Hidden Heterogeneity: When to Choose Similarity-Based Calibration},
 url = {https://openreview.net/forum?id=RA0TDqt3hC},
 year = {2023}
}

@article{wallingford2023fluid,
 abstract = {Modern ML methods excel when training data is IID, large-scale, and well labeled. Learning in less ideal conditions remains an open challenge. The sub-fields of few-shot, continual, transfer, and representation learning have made substantial strides in learning under adverse conditions; each affording distinct advantages through methods and insights. These methods address different challenges such as data arriving sequentially or scarce training examples, however often the difficult conditions an ML system will face over its lifetime cannot be anticipated prior to deployment. Therefore, general ML systems which can handle the many challenges of learning in practical settings are needed. To foster research towards the goal of general ML methods, we introduce a new unified evaluation framework - FLUID (Flexible Sequential Data). FLUID integrates the objectives of few-shot, continual, transfer, and representation learning while enabling comparison and integration of techniques across these subfields. In FLUID, a learner faces a stream of data and must make sequential predictions while choosing how to update itself, adapt quickly to novel classes, and deal with changing data distributions; while accounting for the total amount of compute. We conduct experiments on a broad set of methods which shed new insight on the advantages and limitations of current solutions and indicate new research problems to solve. As a starting point towards more general methods, we present two new baselines which outperform other evaluated methods on FLUID. Project page: https://raivn.cs.washington.edu/projects/FLUID/.},
 author = {Matthew Wallingford and Aditya Kusupati and Keivan Alizadeh-Vahid and Aaron Walsman and Aniruddha Kembhavi and Ali Farhadi},
 code = {https://github.com/RAIVNLab/FLUID},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4206893455},
 pdf = {https://openreview.net/pdf?id=UvJBKWaSSH},
 review = {https://openreview.net/forum?id=UvJBKWaSSH},
 title = {FLUID: A Unified Evaluation Framework for Flexible Sequential Data},
 url = {https://openreview.net/forum?id=UvJBKWaSSH},
 year = {2023}
}

@article{wang2022evolving,
 abstract = {Artificial neural networks (ANNs) are typically confined to accomplishing pre-defined tasks by learning a set of static parameters. In contrast, biological neural networks (BNNs) can adapt to various new tasks by continually updating the neural connections based on the inputs, which is aligned with the paradigm of learning effective learning rules in addition to static parameters, e.g., meta-learning. Among various biologically inspired learning rules, Hebbian plasticity updates the neural network weights using local signals without the guide of an explicit target function, thus enabling an agent to learn automatically without human efforts. However, typical plastic ANNs using a large amount of meta-parameters violate the nature of the genomics bottleneck and potentially deteriorate the generalization capacity. This work proposes a new learning paradigm decomposing those connection-dependent plasticity rules into neuron-dependent rules thus accommodating $\Theta(n^2)$ learnable parameters with only $\Theta(n)$ meta-parameters. We also thoroughly study the effect of different neural modulation on plasticity. Our algorithms are tested in challenging random 2D maze environments, where the agents have to use their past experiences to shape the neural connections and improve their performances for the future. The results of our experiment validate the following: 1. Plasticity can be adopted to continually update a randomly initialized RNN to surpass pre-trained, more sophisticated recurrent models, especially when coming to long-term memorization. 2. Following the genomics bottleneck, the proposed decomposed plasticity can be comparable to or even more effective than canonical plasticity rules in some instances.},
 author = {Fan Wang and Hao Tian and Haoyi Xiong and Hua Wu and Jie Fu and Yang Cao and Yu Kang and Haifeng Wang},
 code = {https://github.com/WorldEditors/EvolvingPlasticANN},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286982485},
 pdf = {https://openreview.net/pdf?id=6qMKztPn0n},
 review = {https://openreview.net/forum?id=6qMKztPn0n},
 title = {Evolving Decomposed Plasticity Rules for Information-Bottlenecked Meta-Learning},
 url = {https://openreview.net/forum?id=6qMKztPn0n},
 year = {2022}
}

@article{wang2022git,
 abstract = {In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \url{https://github.com/microsoft/GenerativeImage2Text}.},
 author = {Jianfeng Wang and Zhengyuan Yang and Xiaowei Hu and Linjie Li and Kevin Lin and Zhe Gan and Zicheng Liu and Ce Liu and Lijuan Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320458302},
 pdf = {https://openreview.net/pdf?id=b4tMhpN0JC},
 review = {https://openreview.net/forum?id=b4tMhpN0JC},
 title = {GIT: A Generative Image-to-text Transformer for Vision and Language},
 url = {https://openreview.net/forum?id=b4tMhpN0JC},
 year = {2022}
}

@article{wang2022no,
 abstract = {The performance of reinforcement learning (RL) agents is sensitive to the choice of hyperparameters. In real-world settings like robotics or industrial control systems, however, testing different hyperparameter configurations directly on the environment can be financially prohibitive, dangerous, or time consuming. We propose a new approach to tune hyperparameters from offline logs of data, to fully specify the hyperparameters for an RL agent that learns online in the real world. The approach is conceptually simple: we first learn a model of the environment from the offline data, which we call a calibration model, and then simulate learning in the calibration model to identify promising hyperparameters. We identify several criteria to make this strategy effective, and develop an approach that satisfies these criteria. We empirically investigate the method in a variety of settings to identify when it is effective and when it fails.},
 author = {Han Wang and Archit Sakhadeo and Adam M White and James M Bell and Vincent Liu and Xutong Zhao and Puer Liu and Tadashi Kozuno and Alona Fyshe and Martha White},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4280563792},
 pdf = {https://openreview.net/pdf?id=AiOUi3440V},
 review = {https://openreview.net/forum?id=AiOUi3440V},
 title = {No More Pesky Hyperparameters: Offline Hyperparameter Tuning for RL},
 url = {https://openreview.net/forum?id=AiOUi3440V},
 year = {2022}
}

@article{wang2022nonparametric,
 author = {Zhunxuan Wang and Linyun He and Chunchuan Lyu and Shay B Cohen},
 code = {https://github.com/uuzeeex/relu-resunit-learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=YiOI0vqJ0n},
 review = {https://openreview.net/forum?id=YiOI0vqJ0n},
 title = {Nonparametric Learning of Two-Layer Re{LU} Residual Units},
 url = {https://openreview.net/forum?id=YiOI0vqJ0n},
 year = {2022}
}

@article{wang2022seminll,
 author = {ZHUOWEI WANG and Jing Jiang and Bo Han and Lei Feng and Bo An and Gang Niu and Guodong Long},
 code = {https://github.com/Wongcheukwai/SemiNLL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=qzM1Tw5i7N},
 review = {https://openreview.net/forum?id=qzM1Tw5i7N},
 title = {Semi{NLL}: A Framework of Noisy-Label Learning by Semi-Supervised Learning},
 url = {https://openreview.net/forum?id=qzM1Tw5i7N},
 year = {2022}
}

@article{wang2022uncertaintybased,
 author = {Jing Wang and Jie Shen and Xiaofei Ma and Andrew Arnold},
 code = {https://github.com/JingWang-RU/ALBUS_activelearningmrc},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=QaDevCcmcg},
 review = {https://openreview.net/forum?id=QaDevCcmcg},
 title = {Uncertainty-Based Active Learning for Reading Comprehension},
 url = {https://openreview.net/forum?id=QaDevCcmcg},
 year = {2022}
}

@article{wang2022variational,
 abstract = {Domain generalization aims to learn an invariant model that can generalize well to the unseen target domain. In this paper, we propose to tackle the problem of domain generalization by delivering an effective framework named Variational Disentanglement Network (VDN), which is capable of disentangling the domain-specific features and task-specific features, where the task-specific features are expected to be better generalized to unseen but related test data. We further show the rationale of our proposed method by proving that our proposed framework is equivalent to minimize the evidence upper bound of the divergence between the distribution of task-specific features and its invariant ground truth derived from variational inference. We conduct extensive experiments to verify our method on three benchmarks, and both quantitative and qualitative results illustrate the effectiveness of our method.},
 author = {Yufei Wang and Haoliang Li and Hao Cheng and Bihan Wen and Lap-Pui Chau and Alex Kot},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3201449419},
 pdf = {https://openreview.net/pdf?id=fudOtITMIZ},
 review = {https://openreview.net/forum?id=fudOtITMIZ},
 title = {Variational Disentanglement for Domain Generalization},
 url = {https://openreview.net/forum?id=fudOtITMIZ},
 year = {2022}
}

@article{wang2023a,
 abstract = {In this paper, we empirically analyze a simple, non-learnable, and nonparametric Nadaraya-Watson (NW) prediction head that can be used with any neural network architecture. In the NW head, the prediction is a weighted average of labels from a support set. The weights are computed from distances between the query feature and support features. This is in contrast to the dominant approach of using a learnable classification head (e.g., a fully-connected layer) on the features, which can be challenging to interpret and can yield poorly calibrated predictions. Our empirical results on an array of computer vision tasks demonstrate that the NW head can yield better calibration with comparable accuracy compared to its parametric counterpart, particularly in data-limited settings. To further increase inference-time efficiency, we propose a simple approach that involves a clustering step run on the training set to create a relatively small distilled support set. Furthermore, we explore two means of interpretability/explainability that fall naturally from the NW head. The first is the label weights, and the second is our novel concept of the ``support influence function,'' which is an easy-to-compute metric that quantifies the influence of a support element on the prediction for a given query. As we demonstrate in our experiments, the influence function can allow the user to debug a trained model. We believe that the NW head is a flexible, interpretable, and highly useful building block that can be used in a range of applications.},
 author = {Alan Q. Wang and Mert R. Sabuncu},
 code = {https://github.com/alanqrwang/nwhead},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310922560},
 pdf = {https://openreview.net/pdf?id=iEq6lhG4O3},
 review = {https://openreview.net/forum?id=iEq6lhG4O3},
 title = {A Flexible Nadaraya-Watson Head Can Offer Explainable and Calibrated Classification},
 url = {https://openreview.net/forum?id=iEq6lhG4O3},
 year = {2023}
}

@article{wang2023adjusting,
 author = {Yixin Wang and Dhanya Sridhar and David Blei},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=P6NcRPb13w},
 review = {https://openreview.net/forum?id=P6NcRPb13w},
 title = {Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness},
 url = {https://openreview.net/forum?id=P6NcRPb13w},
 year = {2023}
}

@article{wang2023crossvalidation,
 author = {Jing Wang and Laurel Hopkins and Tyler Hallman and W. Douglas Robinson and Rebecca Hutchinson},
 code = {https://github.com/Hutchinson-Lab/Cross-validation-for-Geospatial-Data},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=VgJhYu7FmQ},
 review = {https://openreview.net/forum?id=VgJhYu7FmQ},
 title = {Cross-validation for Geospatial Data: Estimating Generalization Performance in Geostatistical Problems},
 url = {https://openreview.net/forum?id=VgJhYu7FmQ},
 year = {2023}
}

@article{wang2023datafree,
 abstract = {The emerging availability of trained machine learning models has put forward the novel concept of Machine Learning Model Market in which one can harness the collective intelligence of multiple well-trained models to improve the performance of the resultant model through one-shot federated learning and ensemble learning in a data-free manner. However, picking the models available in the market for ensemble learning is time-consuming, as using all the models is not always the best approach. It is thus crucial to have an effective ensemble selection strategy that can find a good subset of the base models for the ensemble. Conventional ensemble selection techniques are not applicable, as we do not have access to the local datasets of the parties in the federated learning setting. In this paper, we present a novel Data-Free Diversity-Based method called DeDES to address the ensemble selection problem for models generated by one-shot federated learning in practical applications such as model markets. Experiments showed that our method can achieve both better performance and higher efficiency over 5 datasets and 4 different model structures under the different data-partition strategies.},
 author = {Naibo Wang and Wenjie Feng and yuchen deng and Moming Duan and Fusheng Liu and See-Kiong Ng},
 code = {https://github.com/NaiboWang/Data-Free-Ensemble-Selection-For-One-Shot-Federated-Learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321854002},
 pdf = {https://openreview.net/pdf?id=ORMlg4g3mG},
 review = {https://openreview.net/forum?id=ORMlg4g3mG},
 title = {Data-Free Diversity-Based Ensemble Selection For One-Shot Federated Learning in Machine Learning Model Market},
 url = {https://openreview.net/forum?id=ORMlg4g3mG},
 year = {2023}
}

@article{wang2023discretization,
 abstract = {With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable function spaces, and show that discretization invariance also describes backpropagation through such models. Applied to neural fields, convolutional DI-Nets can learn to classify and segment visual data under various discretizations, and sometimes generalize to new types of discretizations at test time. Code: https://github.com/clintonjwang/DI-net.},
 author = {Clinton Wang and Polina Golland},
 code = {https://github.com/clintonjwang/DI-net},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308164556},
 pdf = {https://openreview.net/pdf?id=CpYBAqDgmz},
 review = {https://openreview.net/forum?id=CpYBAqDgmz},
 title = {Discretization Invariant Networks for Learning Maps between Neural Fields},
 url = {https://openreview.net/forum?id=CpYBAqDgmz},
 year = {2023}
}

@article{wang2023dynamic,
 abstract = {Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the number of replications per trial -- in order to overcome the challenges of sparse rewards, expensive interactions, and noise. An experimental evaluation demonstrates that the new approach outperforms existing baselines across a number of problem domains. We also provide a theoretical foundation and prove that the method asymptotically identifies a near-optimal subgoal design.},
 author = {Yijia Wang and Matthias Poloczek and Daniel R. Jiang},
 code = {https://github.com/yjwang0618/subgoal-based-exploration},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4308245670},
 pdf = {https://openreview.net/pdf?id=ThJl4d5JRg},
 review = {https://openreview.net/forum?id=ThJl4d5JRg},
 title = {Dynamic Subgoal-based Exploration via Bayesian Optimization},
 url = {https://openreview.net/forum?id=ThJl4d5JRg},
 year = {2023}
}

@article{wang2023early,
 abstract = {Deep image prior (DIP) and its variants have showed remarkable potential for solving inverse problems in computer vision, without any extra training data. Practical DIP models are often substantially overparameterized. During the fitting process, these models learn mostly the desired visual content first, and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP often depends critically on good early stopping (ES) that captures the transition period. In this regard, the majority of DIP works for vision tasks only demonstrates the potential of the models -- reporting the peak performance against the ground truth, but provides no clue about how to operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy, which consistently detects near-peak performance across several vision tasks and DIP variants. Based on a simple measure of dispersion of consecutive DIP reconstructions, our ES method not only outpaces the existing ones -- which only work in very narrow domains, but also remains effective when combined with a number of methods that try to mitigate the overfitting. The code is available at https://github.com/sun-umn/Early_Stopping_for_DIP.},
 author = {Hengkang Wang and Taihui Li and Zhong Zhuang and Tiancong Chen and Hengyue Liang and Ju Sun},
 code = {https://github.com/sun-umn/Early_Stopping_for_DIP},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226096164},
 pdf = {https://openreview.net/pdf?id=231ZzrLC8X},
 review = {https://openreview.net/forum?id=231ZzrLC8X},
 title = {Early Stopping for Deep Image Prior},
 url = {https://openreview.net/forum?id=231ZzrLC8X},
 year = {2023}
}

@article{wang2023federated,
 author = {Chi-Hua Wang and Wenjie Li and Guang Lin},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=TjaMO63fc9},
 review = {https://openreview.net/forum?id=TjaMO63fc9},
 title = {Federated High-Dimensional Online Decision Making},
 url = {https://openreview.net/forum?id=TjaMO63fc9},
 year = {2023}
}

@article{wang2023gitnet,
 abstract = {This article introduces GIT-Net, a deep neural network architecture for approximating Partial Differential Equation (PDE) operators, inspired by integral transform operators. GIT-NET harnesses the fact that differential operators commonly used for defining PDEs can often be represented parsimoniously when expressed in specialized functional bases (e.g., Fourier basis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive generalized integral transforms with deep neural networks. When compared to several recently proposed alternatives, GIT-Net's computational and memory requirements scale gracefully with mesh discretizations, facilitating its application to PDE problems on complex geometries. Numerical experiments demonstrate that GIT-Net is a competitive neural network operator, exhibiting small test errors and low evaluations across a range of PDE problems. This stands in contrast to existing neural network operators, which typically excel in just one of these areas.},
 author = {Chao Wang and Alexandre H. Thiery},
 code = {https://github.com/chaow-mat/General_Integral_Transform_Neural_Network},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4389470012},
 pdf = {https://openreview.net/pdf?id=0WKTmrVkd2},
 review = {https://openreview.net/forum?id=0WKTmrVkd2},
 title = {GIT-Net: Generalized Integral Transform for Operator Learning},
 url = {https://openreview.net/forum?id=0WKTmrVkd2},
 year = {2023}
}

@article{wang2023how,
 abstract = {Increasing concerns have been raised on deep learning fairness in recent years. Existing fairness-aware machine learning methods mainly focus on the fairness of in-distribution data. However, in real-world applications, it is common to have distribution shift between the training and test data. In this paper, we first show that the fairness achieved by existing methods can be easily broken by slight distribution shifts. To solve this problem, we propose a novel fairness learning method termed CUrvature MAtching (CUMA), which can achieve robust fairness generalizable to unseen domains with unknown distributional shifts. Specifically, CUMA enforces the model to have similar generalization ability on the majority and minority groups, by matching the loss curvature distributions of the two groups. We evaluate our method on three popular fairness datasets. Compared with existing methods, CUMA achieves superior fairness under unseen distribution shifts, without sacrificing either the overall accuracy or the in-distribution fairness.},
 author = {Haotao Wang and Junyuan Hong and Jiayu Zhou and Zhangyang Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283828984},
 pdf = {https://openreview.net/pdf?id=11pGlecTz2},
 review = {https://openreview.net/forum?id=11pGlecTz2},
 title = {How Robust is Your Fairness? Evaluating and Sustaining Fairness under Unseen Distribution Shifts},
 url = {https://openreview.net/forum?id=11pGlecTz2},
 year = {2023}
}

@article{wang2023learning,
 abstract = {Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible, the causality field should consider a supervised approach in which domain-specific CD procedures are learned from extensive datasets with known causal relationships, rather than being designed by human specialists. Our findings promise a new approach toward improving CD in neural and medical data and for the broader machine learning community.},
 author = {Xinyue Wang and Konrad Kording},
 code = {https://github.com/KordingLab/LearningCausalDiscovery},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297821046},
 pdf = {https://openreview.net/pdf?id=JFaZ94tT8M},
 review = {https://openreview.net/forum?id=JFaZ94tT8M},
 title = {Learning domain-specific causal discovery from time series},
 url = {https://openreview.net/forum?id=JFaZ94tT8M},
 year = {2023}
}

@article{wang2023oneround,
 author = {Jiachen T. Wang and Si Chen and Ruoxi Jia},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=8HQCOMRa7g},
 review = {https://openreview.net/forum?id=8HQCOMRa7g},
 title = {One-Round Active Learning through Data Utility Learning and Proxy Models},
 url = {https://openreview.net/forum?id=8HQCOMRa7g},
 year = {2023}
}

@article{wang2023singlepass,
 abstract = {Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass augmentation-free graph contrastive learning loss based on the property, and provide performance guarantees for the minimizer of the loss on downstream tasks. As a direct consequence of our analysis, we implement the Single-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14 benchmark datasets with varying degrees of homophily, the features learned by the SP-GCL can match or outperform existing strong baselines with significantly less computational overhead, which demonstrates the usefulness of our findings in real-world cases.},
 author = {Haonan Wang and Jieyu Zhang and Qi Zhu and Wei Huang and Kenji Kawaguchi and Xiaokui Xiao},
 code = {https://github.com/haonan3/SPGCL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4309799428},
 pdf = {https://openreview.net/pdf?id=244KePn09i},
 review = {https://openreview.net/forum?id=244KePn09i},
 title = {Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph},
 url = {https://openreview.net/forum?id=244KePn09i},
 year = {2023}
}

@article{wang2023towards,
 abstract = {Backdoor attacks are dangerous and difficult to prevent in federated learning (FL), where training data is sourced from untrusted clients over long periods of time. These difficulties arise because: (a) defenders in FL do not have access to raw training data, and (b) a new phenomenon we identify called backdoor leakage causes models trained continuously to eventually suffer from backdoors due to cumulative errors in defense mechanisms. We propose shadow learning, a framework for defending against backdoor attacks in the FL setting under long-range training. Shadow learning trains two models in parallel: a backbone model and a shadow model. The backbone is trained without any defense mechanism to obtain good performance on the main task. The shadow model combines filtering of malicious clients with early-stopping to control the attack success rate even as the data distribution changes. We theoretically motivate our design and show experimentally that our framework significantly improves upon existing defenses against backdoor attacks.},
 author = {Shuaiqi Wang and Jonathan Hayase and Giulia Fanti and Sewoong Oh},
 code = {https://github.com/wsqwsq/Towards-a-Defense-against-Backdoor-Attacks-in-Continual-Federated-Learning},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281569147},
 pdf = {https://openreview.net/pdf?id=HwcB5elyuG},
 review = {https://openreview.net/forum?id=HwcB5elyuG},
 title = {Towards a Defense Against Federated Backdoor Attacks Under Continuous Training},
 url = {https://openreview.net/forum?id=HwcB5elyuG},
 year = {2023}
}

@article{wang2024a,
 abstract = {In variable selection, a selection rule that prescribes the permissible sets of selected variables (called a "selection dictionary") is desirable due to the inherent structural constraints among the candidate variables. Such selection rules can be complex in real-world data analyses, and failing to incorporate such restrictions could not only compromise the interpretability of the model but also lead to decreased prediction accuracy. However, no general framework has been proposed to formalize selection rules and their applications, which poses a significant challenge for practitioners seeking to integrate these rules into their analyses. In this work, we establish a framework for structured variable selection that can incorporate universal structural constraints. We develop a mathematical language for constructing arbitrary selection rules, where the selection dictionary is formally defined. We demonstrate that all selection rules can be expressed as combinations of operations on constructs, facilitating the identification of the corresponding selection dictionary. Once this selection dictionary is derived, practitioners can apply their own user-defined criteria to select the optimal model. Additionally, our framework enhances existing penalized regression methods for variable selection by providing guidance on how to appropriately group variables to achieve the desired selection rule. Furthermore, our innovative framework opens the door to establishing new l0 norm-based penalized regression techniques that can be tailored to respect arbitrary selection rules, thereby expanding the possibilities for more robust and tailored model development.},
 author = {GUANBO WANG and Mireille Schnitzer and Tom Chen and Rui Wang and Robert W Platt},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4286911983},
 pdf = {https://openreview.net/pdf?id=cvOpIhQQMN},
 review = {https://openreview.net/forum?id=cvOpIhQQMN},
 title = {A general framework for formulating structured variable selection},
 url = {https://openreview.net/forum?id=cvOpIhQQMN},
 year = {2024}
}

@article{wang2024personalized,
 author = {Xiaoyang Wang and Han Zhao and Klara Nahrstedt and Sanmi Koyejo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=N2wx9UVHkH},
 review = {https://openreview.net/forum?id=N2wx9UVHkH},
 title = {Personalized Federated Learning with Spurious Features: An Adversarial Approach},
 url = {https://openreview.net/forum?id=N2wx9UVHkH},
 year = {2024}
}

@article{wasserman2023learning,
 abstract = {Machine learning frameworks such as graph neural networks typically rely on a given, fixed graph to exploit relational inductive biases and thus effectively learn from network data. However, when said graphs are (partially) unobserved, noisy, or dynamic, the problem of inferring graph structure from data becomes relevant. In this paper, we postulate a graph convolutional relationship between the observed and latent graphs, and formulate the graph learning task as a network inverse (deconvolution) problem. In lieu of eigendecomposition-based spectral methods or iterative optimization solutions, we unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that we call a Graph Deconvolution Network (GDN). GDNs can learn a distribution of graphs in a supervised fashion, perform link prediction or edge-weight regression tasks by adapting the loss function, and they are inherently inductive. We corroborate GDN's superior graph recovery performance and its generalization to larger graphs using synthetic data in supervised settings. Furthermore, we demonstrate the robustness and representation power of GDNs on real world neuroimaging and social network datasets.},
 author = {Max Wasserman and Saurabh Sihag and Gonzalo Mateos and Alejandro Ribeiro},
 code = {https://github.com/maxwass/pyGSL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281252732},
 pdf = {https://openreview.net/pdf?id=OILbP0WErR},
 review = {https://openreview.net/forum?id=OILbP0WErR},
 title = {Learning Graph Structure from Convolutional Mixtures},
 url = {https://openreview.net/forum?id=OILbP0WErR},
 year = {2023}
}

@article{weber2023the,
 abstract = {Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. In this direction, we present the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schroedinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. We also show that the training of generative adversarial networks includes a hidden data-optimization sub-problem, which induces the SD flow under certain choices of loss function when the discriminator is optimal. As a result, the SD flow provides a theoretical link between model classes that individually address the three challenges of the "generative modeling trilemma" -- high sample quality, mode coverage, and fast sampling -- thereby setting the stage for a unified approach.},
 author = {Romann M. Weber},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4367061026},
 pdf = {https://openreview.net/pdf?id=dpGSNLUCzu},
 review = {https://openreview.net/forum?id=dpGSNLUCzu},
 title = {The Score-Difference Flow for Implicit Generative Modeling},
 url = {https://openreview.net/forum?id=dpGSNLUCzu},
 year = {2023}
}

@article{wehenkel2023robust,
 abstract = {Hybrid modelling reduces the misspecification of expert models by combining them with machine learning (ML) components learned from data. Similarly to many ML algorithms, hybrid model performance guarantees are limited to the training distribution. Leveraging the insight that the expert model is usually valid even outside the training domain, we overcome this limitation by introducing a hybrid data augmentation strategy termed \textit{expert augmentation}. Based on a probabilistic formalization of hybrid modelling, we demonstrate that expert augmentation, which can be incorporated into existing hybrid systems, improves generalization. We empirically validate the expert augmentation on three controlled experiments modelling dynamical systems with ordinary and partial differential equations. Finally, we assess the potential real-world applicability of expert augmentation on a dataset of a real double pendulum.},
 author = {Antoine Wehenkel and Jens Behrmann and Hsiang Hsu and Guillermo Sapiro and Gilles Louppe and Joern-Henrik Jacobsen},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226458488},
 pdf = {https://openreview.net/pdf?id=oe4dl4MCGY},
 review = {https://openreview.net/forum?id=oe4dl4MCGY},
 title = {Robust Hybrid Learning With Expert Augmentation},
 url = {https://openreview.net/forum?id=oe4dl4MCGY},
 year = {2023}
}

@article{wei2022emergent,
 abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
 author = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4283026156},
 pdf = {https://openreview.net/pdf?id=yzkSU5zdwD},
 review = {https://openreview.net/forum?id=yzkSU5zdwD},
 title = {Emergent Abilities of Large Language Models},
 url = {https://openreview.net/forum?id=yzkSU5zdwD},
 year = {2022}
}

@article{wei2022unsupervised,
 abstract = {Content mismatch usually occurs when data from one modality is translated to another, e.g. language learners producing mispronunciations (errors in speech) when reading a sentence (target text) aloud. However, most existing alignment algorithms assume that the content involved in the two modalities is perfectly matched, thus leading to difficulty in locating such mismatch between speech and text. In this work, we develop an unsupervised learning algorithm that can infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences. More specifically, we propose a hierarchical Bayesian deep learning model, dubbed mismatch localization variational autoencoder (ML-VAE), which decomposes the generative process of the speech into hierarchically structured latent variables, indicating the relationship between the two modalities. Training such a model is very challenging due to the discrete latent variables with complex dependencies involved. To address this challenge, we propose a novel and effective training procedure that alternates between estimating the hard assignments of the discrete latent variables over a specifically designed mismatch localization finite-state acceptor (ML-FSA) and updating the parameters of neural networks. In this work, we focus on the mismatch localization problem for speech and text, and our experimental results show that ML-VAE successfully locates the mismatch between text and speech, without the need for human annotations for model training.},
 author = {Wei Wei and Hengguan Huang and Xiangming Gu and Hao Wang and Ye Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4229002094},
 pdf = {https://openreview.net/pdf?id=29V0xo7jKp},
 review = {https://openreview.net/forum?id=29V0xo7jKp},
 title = {Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization},
 url = {https://openreview.net/forum?id=29V0xo7jKp},
 year = {2022}
}

@article{wei2024a,
 abstract = {Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.},
 author = {Ran Wei and Nathan Lambert and Anthony D McDonald and Alfredo Garcia and Roberto Calandra},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4387560880},
 pdf = {https://openreview.net/pdf?id=tQVZgvXhZb},
 review = {https://openreview.net/forum?id=tQVZgvXhZb},
 title = {A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning},
 url = {https://openreview.net/forum?id=tQVZgvXhZb},
 year = {2024}
}

@article{weihs2022benchmarking,
 author = {Luca Weihs and Amanda Yuile and Ren{\'e}e Baillargeon and Cynthia Fisher and Gary Marcus and Roozbeh Mottaghi and Aniruddha Kembhavi},
 code = {https://allenai.org/project/inflevel/home},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=9NjqD9i48M},
 review = {https://openreview.net/forum?id=9NjqD9i48M},
 title = {Benchmarking Progress to Infant-Level Physical Reasoning in {AI}},
 url = {https://openreview.net/forum?id=9NjqD9i48M},
 year = {2022}
}

@article{weis2023selfsupervised,
 abstract = {Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain areas, that this method yields morphological cell type clusterings that are on par with manual feature-based classification by experts, but without using prior knowledge about the structural features of neurons. Moreover, it outperforms previous approaches on quantitative benchmarks predicting expert labels. Our method could potentially enable data-driven discovery of novel morphological features and cell types in large-scale datasets. It is applicable beyond neuroscience in settings where samples in a dataset are graphs and graph-level embeddings are desired.},
 author = {Marissa A. Weis and Laura Pede and Timo L{\"u}ddecke and Alexander S Ecker},
 code = {https://eckerlab.org/code/weis2023/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4225596203},
 pdf = {https://openreview.net/pdf?id=ThhMzfrd6r},
 review = {https://openreview.net/forum?id=ThhMzfrd6r},
 title = {Self-Supervised Graph Representation Learning for Neuronal Morphologies},
 url = {https://openreview.net/forum?id=ThhMzfrd6r},
 year = {2023}
}

@article{werner2023provably,
 abstract = {Identifying clients with similar objectives and learning a model-per-cluster is an intuitive and interpretable approach to personalization in federated learning. However, doing so with provable and optimal guarantees has remained an open challenge. We formalize this problem as a stochastic optimization problem, achieving optimal convergence rates for a large class of loss functions. We propose simple iterative algorithms which identify clusters of similar clients and train a personalized model-per-cluster, using local client gradients and flexible constraints on the clusters. The convergence rates of our algorithms asymptotically match those obtained if we knew the true underlying clustering of the clients and are provably robust in the Byzantine setting where some fraction of the clients are malicious.},
 author = {Mariel Werner and Lie He and Michael Jordan and Martin Jaggi and Sai Praneeth Karimireddy},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380993337},
 pdf = {https://openreview.net/pdf?id=B0uBSSUy0G},
 review = {https://openreview.net/forum?id=B0uBSSUy0G},
 title = {Provably Personalized and Robust Federated Learning},
 url = {https://openreview.net/forum?id=B0uBSSUy0G},
 year = {2023}
}

@article{wiedemann2023quantum,
 author = {Simon Wiedemann and Daniel Hein and Steffen Udluft and Christian B. Mendl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=HG11PAmwQ6},
 review = {https://openreview.net/forum?id=HG11PAmwQ6},
 title = {Quantum Policy Iteration via Amplitude Estimation and Grover Search {\textendash} Towards Quantum Advantage for Reinforcement Learning},
 url = {https://openreview.net/forum?id=HG11PAmwQ6},
 year = {2023}
}

@article{wispinski2023adaptive,
 abstract = {Patch foraging is one of the most heavily studied behavioral optimization challenges in biology. However, despite its importance to biological intelligence, this behavioral optimization problem is understudied in artificial intelligence research. Patch foraging is especially amenable to study given that it has a known optimal solution, which may be difficult to discover given current techniques in deep reinforcement learning. Here, we investigate deep reinforcement learning agents in an ecological patch foraging task. For the first time, we show that machine learning agents can learn to patch forage adaptively in patterns similar to biological foragers, and approach optimal patch foraging behavior when accounting for temporal discounting. Finally, we show emergent internal dynamics in these agents that resemble single-cell recordings from foraging non-human primates, which complements experimental and theoretical work on the neural mechanisms of biological foraging. This work suggests that agents interacting in complex environments with ecologically valid pressures arrive at common solutions, suggesting the emergence of foundational computations behind adaptive, intelligent behavior in both biological and artificial agents.},
 author = {Nathan Wispinski and Andrew Butcher and Kory Wallace Mathewson and Craig S Chapman and Matthew Botvinick and Patrick M. Pilarski},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320477349},
 pdf = {https://openreview.net/pdf?id=a0T3nOP9sB},
 review = {https://openreview.net/forum?id=a0T3nOP9sB},
 title = {Adaptive patch foraging in deep reinforcement learning agents},
 url = {https://openreview.net/forum?id=a0T3nOP9sB},
 year = {2023}
}

@article{wolfe2024how,
 abstract = {Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. Lastly, we empirically validate our theoretical results on a multi-layer perceptron trained on MNIST.},
 author = {Cameron R. Wolfe and Fangshuo Liao and Qihan Wang and Junhyung Lyle Kim and Anastasios Kyrillidis},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4206868115},
 pdf = {https://openreview.net/pdf?id=UVE7LllpXe},
 review = {https://openreview.net/forum?id=UVE7LllpXe},
 title = {How much pre-training is enough to discover a good subnetwork?},
 url = {https://openreview.net/forum?id=UVE7LllpXe},
 year = {2024}
}

@article{wortsman2023lofi,
 abstract = {When fine-tuning large neural networks, it is common to use multiple nodes and to communicate gradients at each optimization step. By contrast, we investigate completely local fine-tuning, which we refer to as lo-fi. During lo-fi, each node is fine-tuned independently without any communication. Then, the weights are averaged across nodes at the conclusion of fine-tuning. When fine-tuning DeiT-base and DeiT-large on ImageNet, this procedure matches accuracy in-distribution and improves accuracy under distribution shift compared to the baseline, which observes the same amount of data but communicates gradients at each step. We also observe that lo-fi matches the baseline's performance when fine-tuning OPT language models (up to 1.3B parameters) on Common Crawl. By removing the communication requirement, lo-fi reduces resource barriers for fine-tuning large models and enables fine-tuning in settings with prohibitive communication cost.},
 author = {Mitchell Wortsman and Suchin Gururangan and Shen Li and Ali Farhadi and Ludwig Schmidt and Michael Rabbat and Ari S. Morcos},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307206164},
 pdf = {https://openreview.net/pdf?id=1U0aPkBVz0},
 review = {https://openreview.net/forum?id=1U0aPkBVz0},
 title = {lo-fi: distributed fine-tuning without communication},
 url = {https://openreview.net/forum?id=1U0aPkBVz0},
 year = {2023}
}

@article{wu2022completeness,
 author = {Zhijie Wu and Chunjin Song and Guanxiong Chen and Sheng Guo and Weilin Huang},
 code = {https://github.com/zhijieW94/CCNet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4N6T6Rop6k},
 review = {https://openreview.net/forum?id=4N6T6Rop6k},
 title = {Completeness and Coherence Learning for Fast Arbitrary Style Transfer},
 url = {https://openreview.net/forum?id=4N6T6Rop6k},
 year = {2022}
}

@article{wu2023causallyguided,
 abstract = {Graph attention networks estimate the relational importance of node neighbors to aggregate relevant information over local neighborhoods for a prediction task. However, the inferred attentions are vulnerable to spurious correlations and connectivity in the training data, hampering the generalizability of the model. We introduce CAR, a general-purpose regularization framework for graph attention networks. Embodying a causal inference approach, CAR aligns the attention mechanism with the causal effects of active interventions on graph connectivity in a scalable manner. CAR is compatible with a variety of graph attention architectures, and we show that it systematically improves generalizability on various node classification tasks. Our ablation studies indicate that CAR hones in on the aspects of graph structure most pertinent to the prediction (e.g., homophily), and does so more effectively than alternative approaches. Finally, we also show that CAR enhances interpretability of attention weights by accentuating node-neighbor relations that point to causal hypotheses. For social media network-sized graphs, a CAR-guided graph rewiring approach could allow us to combine the scalability of graph convolutional methods with the higher performance of graph attention.},
 author = {Alexander P Wu and Thomas Markovich and Bonnie Berger and Nils Yannick Hammerla and Rohit Singh},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307074454},
 pdf = {https://openreview.net/pdf?id=iDNMZgjJuJ},
 review = {https://openreview.net/forum?id=iDNMZgjJuJ},
 title = {Causally-guided Regularization of Graph Attention Improves Generalizability},
 url = {https://openreview.net/forum?id=iDNMZgjJuJ},
 year = {2023}
}

@article{wu2023chasing,
 author = {Qiming Wu and Xiaohan Chen and Yifan Jiang and Zhangyang Wang},
 code = {https://github.com/VITA-Group/Chasing-Better-DIPs},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=EwJJks2cSa},
 review = {https://openreview.net/forum?id=EwJJks2cSa},
 title = {Chasing Better Deep Image Priors between Over- and Under-parameterization},
 url = {https://openreview.net/forum?id=EwJJks2cSa},
 year = {2023}
}

@article{wu2023expected,
 abstract = {We study the problem of sequential prediction and online minimax regret with stochastically generated features under a general loss function. We introduce a notion of expected worst case minimax regret that generalizes and encompasses prior known minimax regrets. For such minimax regrets we establish tight upper bounds via a novel concept of stochastic global sequential covering. We show that for a hypothesis class of VC-dimension $\mathsf{VC}$ and $i.i.d.$ generated features of length $T$, the cardinality of the stochastic global sequential covering can be upper bounded with high probability (whp) by $e^{O(\mathsf{VC} \cdot \log^2 T)}$. We then improve this bound by introducing a new complexity measure called the Star-Littlestone dimension, and show that classes with Star-Littlestone dimension $\mathsf{SL}$ admit a stochastic global sequential covering of order $e^{O(\mathsf{SL} \cdot \log T)}$. We further establish upper bounds for real valued classes with finite fat-shattering numbers. Finally, by applying information-theoretic tools of the fixed design minimax regrets, we provide lower bounds for the expected worst case minimax regret. We demonstrate the effectiveness of our approach by establishing tight bounds on the expected worst case minimax regrets for logarithmic loss and general mixable losses.},
 author = {Changlong Wu and Mohsen Heidari and Ananth Grama and Wojciech Szpankowski},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4297794440},
 pdf = {https://openreview.net/pdf?id=H1SekypXKA},
 review = {https://openreview.net/forum?id=H1SekypXKA},
 title = {Expected Worst Case Regret via Stochastic Sequential Covering},
 url = {https://openreview.net/forum?id=H1SekypXKA},
 year = {2023}
}

@article{wu2023extended,
 abstract = {A key challenge for much of the machine learning work on remote sensing and earth observation data is the difficulty in acquiring large amounts of accurately labeled data. This is particularly true for semantic segmentation tasks, which are much less common in the remote sensing domain because of the incredible difficulty in collecting precise, accurate, pixel-level annotations at scale. Recent efforts have addressed these challenges both through the creation of supervised datasets as well as the application of self-supervised methods. We continue these efforts on both fronts. First, we generate and release an improved version of the Agriculture-Vision dataset (Chiu et al., 2020b) to include raw, full-field imagery for greater experimental flexibility. Second, we extend this dataset with the release of 3600 large, high-resolution (10cm/pixel), full-field, red-green-blue and near-infrared images for pre-training. Third, we incorporate the Pixel-to-Propagation Module Xie et al. (2021b) originally built on the SimCLR framework into the framework of MoCo-V2 Chen et al.(2020b). Finally, we demonstrate the usefulness of this data by benchmarking different contrastive learning approaches on both downstream classification and semantic segmentation tasks. We explore both CNN and Swin Transformer Liu et al. (2021a) architectures within different frameworks based on MoCo-V2. Together, these approaches enable us to better detect key agricultural patterns of interest across a field from aerial imagery so that farmers may be alerted to problematic areas in a timely fashion to inform their management decisions. Furthermore, the release of these datasets will support numerous avenues of research for computer vision in remote sensing for agriculture.},
 author = {Jing Wu and David Pichler and Daniel Marley and Naira Hovakimyan and David A Wilson and Jennifer Hobbs},
 code = {https://github.com/jingwu6/Extended-Agriculture-Vision-Dataset},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323557148},
 pdf = {https://openreview.net/pdf?id=v5jwDLqfQo},
 review = {https://openreview.net/forum?id=v5jwDLqfQo},
 title = {Extended Agriculture-Vision: An Extension of a Large Aerial Image Dataset for Agricultural Pattern Analysis},
 url = {https://openreview.net/forum?id=v5jwDLqfQo},
 year = {2023}
}

@article{wu2023extreme,
 abstract = {The paper presents a scalable approach for learning spatially distributed visual representations over individual tokens and a holistic instance representation simultaneously. We use self-attention blocks to represent spatially distributed tokens, followed by cross-attention blocks to aggregate the holistic image instance. The core of the approach is the use of extremely large token masking (75\%-90\%) as the data augmentation for supervision. Our model, named ExtreMA, follows the plain BYOL approach where the instance representation from the unmasked subset is trained to predict that from the intact input. Instead of encouraging invariance across inputs, the model is required to capture informative variations in an image. The paper makes three contributions: 1) It presents random masking as a strong and computationally efficient data augmentation for siamese representation learning. 2) With multiple sampling per instance, extreme masking greatly speeds up learning and improves performance with more data. 3) ExtreMA obtains stronger linear probing performance than masked modeling methods, and better transfer performance than prior contrastive models.},
 author = {Zhirong Wu and Zihang Lai and Xiao Sun and Stephen Lin},
 code = {https://github.com/microsoft/ExtreMA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281875059},
 pdf = {https://openreview.net/pdf?id=3epEbhdgbv},
 review = {https://openreview.net/forum?id=3epEbhdgbv},
 title = {Extreme Masking for Learning Instance and Distributed Visual Representations},
 url = {https://openreview.net/forum?id=3epEbhdgbv},
 year = {2023}
}

@article{wu2023greedier,
 abstract = {Sparse subspace clustering (SSC) using greedy-based neighbor selection, such as orthogonal matching pursuit (OMP), has been known as a popular computationally-efficient alternative to the popular L1-minimization based methods. This paper proposes a new SSC scheme using generalized OMP (GOMP), a soup-up of OMP whereby multiple neighbors are identified per iteration, along with a new stopping rule requiring nothing more than a knowledge of the ambient signal dimension. Compared to conventional OMP, which identifies one neighbor per iteration, the proposed GOMP method involves fewer iterations, thereby enjoying lower algorithmic complexity; advantageously, the proposed stopping rule is free from off-line estimation of subspace dimension and noise power. Under the semi-random model, analytic performance guarantees, in terms of neighbor recovery rates, are established to justify the advantage of the proposed GOMP. The results show that, with a high probability, GOMP (i) is halted by the proposed stopping rule, and (ii) can retrieve more true neighbors than OMP, consequently yielding higher final data clustering accuracy. Computer simulations using both synthetic data and real human face data are provided to validate our analytic study and evidence the effectiveness of the proposed approach.},
 author = {Jwo-Yuh Wu and Liang-Chi Huang and Wen Hsuan Li and Chun-Hung Liu and Rung-Hung Gau},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226028081},
 pdf = {https://openreview.net/pdf?id=djD8IbSvgm},
 review = {https://openreview.net/forum?id=djD8IbSvgm},
 title = {Greedier is Better: Selecting Multiple Neighbors per Iteration for Sparse Subspace Clustering},
 url = {https://openreview.net/forum?id=djD8IbSvgm},
 year = {2023}
}

@article{wu2023meanfield,
 abstract = {The stochastic heavy ball method (SHB), also known as stochastic gradient descent (SGD) with Polyak's momentum, is widely used in training neural networks. However, despite the remarkable success of such algorithm in practice, its theoretical characterization remains limited. In this paper, we focus on neural networks with two and three layers and provide a rigorous understanding of the properties of the solutions found by SHB: \emph{(i)} stability after dropping out part of the neurons, \emph{(ii)} connectivity along a low-loss path, and \emph{(iii)} convergence to the global optimum. To achieve this goal, we take a mean-field view and relate the SHB dynamics to a certain partial differential equation in the limit of large network widths. This mean-field perspective has inspired a recent line of work focusing on SGD while, in contrast, our paper considers an algorithm with momentum. More specifically, after proving existence and uniqueness of the limit differential equations, we show convergence to the global optimum and give a quantitative bound between the mean-field limit and the SHB dynamics of a finite-width network. Armed with this last bound, we are able to establish the dropout-stability and connectivity of SHB solutions.},
 author = {Diyuan Wu and Vyacheslav Kungurtsev and Marco Mondelli},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4306309307},
 pdf = {https://openreview.net/pdf?id=gZna3IiGfl},
 review = {https://openreview.net/forum?id=gZna3IiGfl},
 title = {Mean-field analysis for heavy ball methods: Dropout-stability, connectivity, and global convergence},
 url = {https://openreview.net/forum?id=gZna3IiGfl},
 year = {2023}
}

@article{xia2023reliable,
 author = {Meng Xia and Ricardo Henao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=dN9YICB6hN},
 review = {https://openreview.net/forum?id=dN9YICB6hN},
 title = {Reliable Active Learning via Influence Functions},
 url = {https://openreview.net/forum?id=dN9YICB6hN},
 year = {2023}
}

@article{xiao2023deep,
 author = {An Xiao and Hanting Chen and Tianyu Guo and QINGHUA ZHANG and Yunhe Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=6rbcq0qacA},
 review = {https://openreview.net/forum?id=6rbcq0qacA},
 title = {Deep Plug-and-Play Clustering with Unknown Number of Clusters},
 url = {https://openreview.net/forum?id=6rbcq0qacA},
 year = {2023}
}

@article{xie2022solving,
 abstract = {Computing the empirical Wasserstein distance in the Wasserstein-distance-based independence test is an optimal transport (OT) problem with a special structure. This observation inspires us to study a special type of OT problem and propose a modified Hungarian algorithm to solve it exactly. For the OT problem involving two marginals with $m$ and $n$ atoms ($m\geq n$), respectively, the computational complexity of the proposed algorithm is $O(m^2n)$. Computing the empirical Wasserstein distance in the independence test requires solving this special type of OT problem, where $m=n^2$. The associated computational complexity of the proposed algorithm is $O(n^5)$, while the order of applying the classic Hungarian algorithm is $O(n^6)$. In addition to the aforementioned special type of OT problem, it is shown that the modified Hungarian algorithm could be adopted to solve a wider range of OT problems. Broader applications of the proposed algorithm are discussed -- solving the one-to-many assignment problem and the many-to-many assignment problem. We conduct numerical experiments to validate our theoretical results. The experiment results demonstrate that the proposed modified Hungarian algorithm compares favorably with the Hungarian algorithm, the well-known Sinkhorn algorithm, and the network simplex algorithm.},
 author = {Yiling Xie and Yiling Luo and Xiaoming Huo},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307937543},
 pdf = {https://openreview.net/pdf?id=k5m8xXTOrC},
 review = {https://openreview.net/forum?id=k5m8xXTOrC},
 title = {Solving a Special Type of Optimal Transport Problem by a Modified Hungarian Algorithm},
 url = {https://openreview.net/forum?id=k5m8xXTOrC},
 year = {2023}
}

@article{xie2024mixed,
 author = {Wanyun Xie and Thomas Pethick and Ali Ramezani-Kebrya and Volkan Cevher},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=mqMzerrVOB},
 review = {https://openreview.net/forum?id=mqMzerrVOB},
 title = {Mixed Nash for Robust Federated Learning},
 url = {https://openreview.net/forum?id=mqMzerrVOB},
 year = {2024}
}

@article{xie2024on,
 abstract = {In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined, especially when considering their deployment in safety-critical domains like autonomous driving. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection approaches under various adversarial conditions. We systematically analyze the resilience of these models under two attack settings: white-box and black-box; focusing on two primary objectives: classification and localization. Additionally, we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our experiments yield four interesting findings: (a) bird's-eye-view-based representations exhibit stronger robustness against localization attacks; (b) depth-estimation-free approaches have the potential to show stronger robustness; (c) accurate depth estimation effectively improves robustness for depth-estimation-based methods; (d) incorporating multi-frame benign inputs can effectively mitigate adversarial attacks. We hope our findings can steer the development of future camera-based object detection models with enhanced adversarial robustness.},
 author = {Shaoyuan Xie and Zichao Li and Zeyu Wang and Cihang Xie},
 code = {https://github.com/Daniel-xsy/BEV-Attack},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318347643},
 pdf = {https://openreview.net/pdf?id=6SofFlwhEv},
 review = {https://openreview.net/forum?id=6SofFlwhEv},
 title = {On the Adversarial Robustness of Camera-based 3D Object Detection},
 url = {https://openreview.net/forum?id=6SofFlwhEv},
 year = {2024}
}

@article{xiong2022birds,
 abstract = {How do we know when the predictions made by a classifier can be trusted? This is a fundamental problem that also has immense practical applicability, especially in safety-critical areas such as medicine and autonomous driving. The de facto approach of using the classifier's softmax outputs as a proxy for trustworthiness suffers from the over-confidence issue; while the most recent works incur problems such as additional retraining cost and accuracy versus trustworthiness trade-off. In this work, we argue that the trustworthiness of a classifier's prediction for a sample is highly associated with two factors: the sample's neighborhood information and the classifier's output. To combine the best of both worlds, we design a model-agnostic post-hoc approach NeighborAgg to leverage the two essential information via an adaptive neighborhood aggregation. Theoretically, we show that NeighborAgg is a generalized version of a one-hop graph convolutional network, inheriting the powerful modeling ability to capture the varying similarity between samples within each class. We also extend our approach to the closely related task of mislabel detection and provide a theoretical coverage guarantee to bound the false negative. Empirically, extensive experiments on image and tabular benchmarks verify our theory and suggest that NeighborAgg outperforms other methods, achieving state-of-the-art trustworthiness performance.},
 author = {Miao Xiong and Shen Li and Wenjie Feng and Ailin Deng and Jihai Zhang and Bryan Hooi},
 code = {https://github.com/MiaoXiong2320/NeighborAgg.git},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4310514969},
 pdf = {https://openreview.net/pdf?id=p5V8P2J61u},
 review = {https://openreview.net/forum?id=p5V8P2J61u},
 title = {Birds of a Feather Trust Together: Knowing When to Trust a Classifier via Adaptive Neighborhood Aggregation},
 url = {https://openreview.net/forum?id=p5V8P2J61u},
 year = {2022}
}

@article{xu2022exploring,
 abstract = {The task of Few-shot Learning (FSL) aims to do the inference on novel categories containing only few labeled examples, with the help of knowledge learned from base categories containing abundant labeled training samples. While there are numerous works into FSL task, Vision Transformers (ViTs) have rarely been taken as the backbone to FSL with few trials focusing on naive finetuning of whole backbone or classification layer.} Essentially, despite ViTs have been shown to enjoy comparable or even better performance on other vision tasks, it is still very nontrivial to efficiently finetune the ViTs in real-world FSL scenarios. To this end, we propose a novel efficient Transformer Tuning (eTT) method that facilitates finetuning ViTs in the FSL tasks. The key novelties come from the newly presented Attentive Prefix Tuning (APT) and Domain Residual Adapter (DRA) for the task and backbone tuning, individually. Specifically, in APT, the prefix is projected to new key and value pairs that are attached to each self-attention layer to provide the model with task-specific information. Moreover, we design the DRA in the form of learnable offset vectors to handle the potential domain gaps between base and novel data. To ensure the APT would not deviate from the initial task-specific information much, we further propose a novel prototypical regularization, which maximizes the similarity between the projected distribution of prefix and initial prototypes, regularizing the update procedure. Our method receives outstanding performance on the challenging Meta-Dataset. We conduct extensive experiments to show the efficacy of our model.},
 author = {Chengming Xu and Siqian Yang and Yabiao Wang and Zhanxiong Wang and Yanwei Fu and Xiangyang Xue},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4313921076},
 pdf = {https://openreview.net/pdf?id=n3qLz4eL1l},
 review = {https://openreview.net/forum?id=n3qLz4eL1l},
 title = {Exploring Efficient Few-shot Adaptation for Vision Transformers},
 url = {https://openreview.net/forum?id=n3qLz4eL1l},
 year = {2022}
}

@article{xu2022mace,
 abstract = {Generative machine learning models are being increasingly viewed as a way to share sensitive data between institutions. While there has been work on developing differentially private generative modeling approaches, these approaches generally lead to sub-par sample quality, limiting their use in real world applications. Another line of work has focused on developing generative models which lead to higher quality samples but currently lack any formal privacy guarantees. In this work, we propose the first formal framework for membership privacy estimation in generative models. We formulate the membership privacy risk as a statistical divergence between training samples and hold-out samples, and propose sample-based methods to estimate this divergence. Compared to previous works, our framework makes more realistic and flexible assumptions. First, we offer a generalizable metric as an alternative to the accuracy metric especially for imbalanced datasets. Second, we loosen the assumption of having full access to the underlying distribution from previous studies , and propose sample-based estimations with theoretical guarantees. Third, along with the population-level membership privacy risk estimation via the optimal membership advantage, we offer the individual-level estimation via the individual privacy risk. Fourth, our framework allows adversaries to access the trained model via a customized query, while prior works require specific attributes.},
 author = {Yixi Xu and Sumit Mukherjee and Xiyang Liu and Shruti Tople and Rahul M Dodhia and Juan M Lavista Ferres},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3085041063},
 pdf = {https://openreview.net/pdf?id=Zxm0kNe3u7},
 review = {https://openreview.net/forum?id=Zxm0kNe3u7},
 title = {MACE: A Flexible Framework for Membership Privacy Estimation in Generative Models},
 url = {https://openreview.net/forum?id=Zxm0kNe3u7},
 year = {2022}
}

@article{xu2023beyond,
 author = {Shusheng Xu and Yancheng Liang and Yunfei Li and Simon Shaolei Du and Yi Wu},
 code = {https://sites.google.com/view/low-switching-cost-rl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=Xq1sTZTQVm},
 review = {https://openreview.net/forum?id=Xq1sTZTQVm},
 title = {Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning},
 url = {https://openreview.net/forum?id=Xq1sTZTQVm},
 year = {2023}
}

@article{xu2023binary,
 author = {Shirong XU and Chendi Wang and Will Wei Sun and Guang Cheng},
 code = {https://github.com/mukai5566/Label_DP_codes},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=uKCGOw9bGG},
 review = {https://openreview.net/forum?id=uKCGOw9bGG},
 title = {Binary Classification under Local Label Differential Privacy Using Randomized Response Mechanisms},
 url = {https://openreview.net/forum?id=uKCGOw9bGG},
 year = {2023}
}

@article{xu2023efficient,
 abstract = {We study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and the dynamics of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct two new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. We provide a theoretical analysis of the efficiency of our attack and perform an extensive empirical evaluation. Our results show that our attacks efficiently poison agents learning in several popular classical control and MuJoCo environments with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.},
 author = {Yinglun Xu and Qi Zeng and Gagandeep Singh},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4281673141},
 pdf = {https://openreview.net/pdf?id=25G63lDHV2},
 review = {https://openreview.net/forum?id=25G63lDHV2},
 title = {Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning},
 url = {https://openreview.net/forum?id=25G63lDHV2},
 year = {2023}
}

@article{xu2023understanding,
 author = {Zhen Xu and quanming yao and Yong Li and Qiang Yang},
 badge = {Event: AutoML 2023},
 code = {https://github.com/LARS-research/SimpleSTG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=4jEuiMPKSF},
 review = {https://openreview.net/forum?id=4jEuiMPKSF},
 title = {Understanding and Simplifying Architecture Search in Spatio-Temporal Graph Neural Networks},
 url = {https://openreview.net/forum?id=4jEuiMPKSF},
 year = {2023}
}

@article{xu2024leveraging,
 abstract = {To enable video models to be applied seamlessly across video tasks in different environments, various Video Unsupervised Domain Adaptation (VUDA) methods have been proposed to improve the robustness and transferability of video models. Despite improvements made in model robustness, these VUDA methods require access to both source data and source model parameters for adaptation, raising serious data privacy and model portability issues. To cope with the above concerns, this paper firstly formulates Black-box Video Domain Adaptation (BVDA) as a more realistic yet challenging scenario where the source video model is provided only as a black-box predictor. While a few methods for Black-box Domain Adaptation (BDA) are proposed in image domain, these methods cannot apply to video domain since video modality has more complicated temporal features that are harder to align. To address BVDA, we propose a novel Endo and eXo-TEmporal Regularized Network (EXTERN) by applying mask-to-mix strategies and video-tailored regularizations: endo-temporal regularization and exo-temporal regularization, performed across both clip and temporal features, while distilling knowledge from the predictions obtained from the black-box predictor. Empirical results demonstrate the state-of-the-art performance of EXTERN across various cross-domain closed-set and partial-set action recognition benchmarks, which even surpassed most existing video domain adaptation methods with source data accessibility.},
 author = {Yuecong Xu and Jianfei Yang and Haozhi Cao and Min Wu and Xiaoli Li and Lihua Xie and Zhenghua Chen},
 code = {https://xuyu0010.github.io/b2vda.html},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4290857546},
 pdf = {https://openreview.net/pdf?id=icoP08mrQJ},
 review = {https://openreview.net/forum?id=icoP08mrQJ},
 title = {Leveraging Endo- and Exo-Temporal Regularization for Black-box Video Domain Adaptation},
 url = {https://openreview.net/forum?id=icoP08mrQJ},
 year = {2024}
}

@article{xu2024llms,
 author = {Yudong Xu and Wenhao Li and Pashootan Vaezipoor and Scott Sanner and Elias Boutros Khalil},
 code = {https://khalil-research.github.io/LLM4ARC/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=E8m8oySvPJ},
 review = {https://openreview.net/forum?id=E8m8oySvPJ},
 title = {{LLM}s and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations},
 url = {https://openreview.net/forum?id=E8m8oySvPJ},
 year = {2024}
}

@article{yamada2022approximating,
 abstract = {Wasserstein distance, which measures the discrepancy between distributions, shows efficacy in various types of natural language processing (NLP) and computer vision (CV) applications. One of the challenges in estimating Wasserstein distance is that it is computationally expensive and does not scale well for many distribution comparison tasks. In this paper, we aim to approximate the 1-Wasserstein distance by the tree-Wasserstein distance (TWD), where TWD is a 1-Wasserstein distance with tree-based embedding and can be computed in linear time with respect to the number of nodes on a tree. More specifically, we propose a simple yet efficient L1-regularized approach to learning the weights of the edges in a tree. To this end, we first show that the 1-Wasserstein approximation problem can be formulated as a distance approximation problem using the shortest path distance on a tree. We then show that the shortest path distance can be represented by a linear model and can be formulated as a Lasso-based regression problem. Owing to the convex formulation, we can obtain a globally optimal solution efficiently. Moreover, we propose a tree-sliced variant of these methods. Through experiments, we demonstrated that the weighted TWD can accurately approximate the original 1-Wasserstein distance.},
 author = {Makoto Yamada and Yuki Takezawa and Ryoma Sato and Han Bao and Zornitsa Kozareva and Sujith Ravi},
 code = {https://github.com/oist/treeOT/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283645726},
 pdf = {https://openreview.net/pdf?id=Ig82l87ZVU},
 review = {https://openreview.net/forum?id=Ig82l87ZVU},
 title = {Approximating 1-Wasserstein Distance with Trees},
 url = {https://openreview.net/forum?id=Ig82l87ZVU},
 year = {2022}
}

@article{yamada2024evaluating,
 abstract = {Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains.},
 author = {Yutaro Yamada and Yihan Bao and Andrew Kyle Lampinen and Jungo Kasai and Ilker Yildirim},
 code = {https://github.com/runopti/SpatialEvalLLM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387929233},
 pdf = {https://openreview.net/pdf?id=xkiflfKCw3},
 review = {https://openreview.net/forum?id=xkiflfKCw3},
 title = {Evaluating Spatial Understanding of Large Language Models},
 url = {https://openreview.net/forum?id=xkiflfKCw3},
 year = {2024}
}

@article{yamasaki2022unimodal,
 author = {Ryoya Yamasaki},
 code = {https://github.com/yamasakiryoya/ULM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=1l0sClLiPc},
 review = {https://openreview.net/forum?id=1l0sClLiPc},
 title = {Unimodal Likelihood Models for Ordinal Data},
 url = {https://openreview.net/forum?id=1l0sClLiPc},
 year = {2022}
}

@article{yamasaki2023optimal,
 author = {Ryoya Yamasaki},
 code = {https://github.com/yamasakiryoya/OTL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=mHSAy1n65Z},
 review = {https://openreview.net/forum?id=mHSAy1n65Z},
 title = {Optimal Threshold Labeling for Ordinal Regression Methods},
 url = {https://openreview.net/forum?id=mHSAy1n65Z},
 year = {2023}
}

@article{yang2022an,
 author = {Kun Yang and Samory Kpotufe and Nick Feamster},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LFkRUCalFt},
 review = {https://openreview.net/forum?id=LFkRUCalFt},
 title = {An Efficient One-Class {SVM} for Novelty Detection in IoT},
 url = {https://openreview.net/forum?id=LFkRUCalFt},
 year = {2022}
}

@article{yang2023learning,
 author = {Shanchao Yang and MA KAILI and Baoxiang Wang and Tianshu Yu and Hongyuan Zha},
 code = {https://github.com/yangysc/ResiNet},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=moZvOx5cxe},
 review = {https://openreview.net/forum?id=moZvOx5cxe},
 title = {Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring},
 url = {https://openreview.net/forum?id=moZvOx5cxe},
 year = {2023}
}

@article{yang2023learning,
 abstract = {Symbolic reasoning, rule-based symbol manipulation, is a hallmark of human intelligence. However, rule-based systems have had limited success competing with learning-based systems outside formalized domains such as automated theorem proving. We hypothesize that this is due to the manual construction of rules in past attempts. In this work, we ask how we can build a rule-based system that can reason with natural language input but without the manual construction of rules. We propose MetaQNL, a "Quasi-Natural" language that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that induces MetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps. Our approach achieves state-of-the-art accuracy on multiple reasoning benchmarks; it learns compact models with much less data and produces not only answers but also checkable proofs. Further, experiments on a real-world morphological analysis benchmark show that it is possible for our method to handle noise and ambiguity. Code will be released at https://github.com/princeton-vl/MetaQNL.},
 author = {Kaiyu Yang and Jia Deng},
 code = {https://github.com/princeton-vl/MetaQNL.jl},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3215840701},
 pdf = {https://openreview.net/pdf?id=gwRwHUZUgz},
 review = {https://openreview.net/forum?id=gwRwHUZUgz},
 title = {Learning Symbolic Rules for Reasoning in Quasi-Natural Language},
 url = {https://openreview.net/forum?id=gwRwHUZUgz},
 year = {2023}
}

@article{yang2023multidomain,
 abstract = {There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Existing long-tailed classification methods focus on the single-domain setting, where all examples are drawn from the same distribution. However, real-world scenarios often involve multiple domains with distinct imbalanced class distributions. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, which produces invariant predictors by balanced augmenting hidden representations over domains and classes. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on four long-tailed variants of classical domain generalization benchmarks and two real-world imbalanced multi-domain datasets. The results indicate that TALLY consistently outperforms other state-of-the-art methods in both subpopulation shift and domain shift.},
 author = {Xinyu Yang and Huaxiu Yao and Allan Zhou and Chelsea Finn},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307473962},
 pdf = {https://openreview.net/pdf?id=4UXJhNSbwd},
 review = {https://openreview.net/forum?id=4UXJhNSbwd},
 title = {Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations},
 url = {https://openreview.net/forum?id=4UXJhNSbwd},
 year = {2023}
}

@article{yang2023probing,
 abstract = {We study out-of-distribution (OOD) prediction behavior of neural networks when they classify images from unseen classes or corrupted images. To probe the OOD behavior, we introduce a new measure, nearest category generalization (NCG), where we compute the fraction of OOD inputs that are classified with the same label as their nearest neighbor in the training set. Our motivation stems from understanding the prediction patterns of adversarially robust networks, since previous work has identified unexpected consequences of training to be robust to norm-bounded perturbations. We find that robust networks have consistently higher NCG accuracy than natural training, even when the OOD data is much farther away than the robustness radius. This implies that the local regularization of robust training has a significant impact on the network's decision regions. We replicate our findings using many datasets, comparing new and existing training methods. Overall, adversarially robust networks resemble a nearest neighbor classifier when it comes to OOD data. Code available at https://github.com/yangarbiter/nearest-category-generalization.},
 author = {Yao-Yuan Yang and Cyrus Rashtchian and Ruslan Salakhutdinov and Kamalika Chaudhuri},
 code = {https://github.com/yangarbiter/nearest-category-generalization},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287595879},
 pdf = {https://openreview.net/pdf?id=fTNorIvVXG},
 review = {https://openreview.net/forum?id=fTNorIvVXG},
 title = {Probing Predictions on OOD Images via Nearest Categories},
 url = {https://openreview.net/forum?id=fTNorIvVXG},
 year = {2023}
}

@article{yang2023the,
 author = {Ruo Yang and Ping Liu and Mustafa Bilgic},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=gvqzvUVPiQ},
 review = {https://openreview.net/forum?id=gvqzvUVPiQ},
 title = {The Analysis of the Expected Change in the Classification Probability of the Predicted Label},
 url = {https://openreview.net/forum?id=gvqzvUVPiQ},
 year = {2023}
}

@article{yang2023when,
 abstract = {Federated Learning has become a widely-used framework which allows learning a global model on decentralized local datasets under the condition of protecting local data privacy. However, federated learning faces severe optimization difficulty when training samples are not independently and identically distributed (non-i.i.d.). In this paper, we point out that the client sampling practice plays a decisive role in the aforementioned optimization difficulty. We find that the negative client sampling will cause the merged data distribution of currently sampled clients heavily inconsistent with that of all available clients, and further make the aggregated gradient unreliable. To address this issue, we propose a novel learning rate adaptation mechanism to adaptively adjust the server learning rate for the aggregated gradient in each round, according to the consistency between the merged data distribution of currently sampled clients and that of all available clients. Specifically, we make theoretical deductions to find a meaningful and robust indicator that is positively related to the optimal server learning rate and can effectively reflect the merged data distribution of sampled clients, and we utilize it for the server learning rate adaptation. Extensive experiments on multiple image and text classification tasks validate the great effectiveness of our method.},
 author = {Wenkai Yang and Yankai Lin and Guangxiang Zhao and Peng Li and Jie Zhou and Xu Sun},
 code = {https://github.com/lancopku/FedGLAD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318239196},
 pdf = {https://openreview.net/pdf?id=v73h3bYE2Z},
 review = {https://openreview.net/forum?id=v73h3bYE2Z},
 title = {When to Trust Aggregated Gradients: Addressing Negative Client Sampling in Federated Learning},
 url = {https://openreview.net/forum?id=v73h3bYE2Z},
 year = {2023}
}

@article{yang2024exploit,
 abstract = {Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has long been suffering from fragmentary object regions led by Class Activation Map (CAM), which is incapable of generating fine-grained masks for semantic segmentation. To guide CAM to find more non-discriminating object patterns, this paper turns to an interesting working mechanism in agent learning named Complementary Learning System (CLS). CLS holds that the neocortex builds a sensation of general knowledge, while the hippocampus specially learns specific details, completing the learned patterns. Motivated by this simple but effective learning pattern, we propose a General-Specific Learning Mechanism (GSLM) to explicitly drive a coarse-grained CAM to a fine-grained pseudo mask. Specifically, GSLM develops a General Learning Module (GLM) and a Specific Learning Module (SLM). The GLM is trained with image-level supervision to extract coarse and general localization representations from CAM. Based on the general knowledge in the GLM, the SLM progressively exploits the specific spatial knowledge from the localization representations, expanding the CAM in an explicit way. To this end, we propose the Seed Reactivation to help SLM reactivate non-discriminating regions by setting a boundary for activation values, which successively identifies more regions of CAM. Without extra refinement processes, our method is able to achieve breakthrough improvements for CAM of over 20.0% mIoU on PASCAL VOC 2012 and 10.0% mIoU on MS COCO 2014 datasets, representing a new state-of-the-art among existing WSSS methods.},
 author = {Wankou Yang and Jiren Mai and Fei Zhang and Tongliang Liu and Bo Han},
 code = {https://github.com/tmlr-group/GSLM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323557114},
 pdf = {https://openreview.net/pdf?id=KutEe24Yai},
 review = {https://openreview.net/forum?id=KutEe24Yai},
 title = {Exploit CAM by itself: Complementary Learning System for Weakly Supervised Semantic Segmentation},
 url = {https://openreview.net/forum?id=KutEe24Yai},
 year = {2024}
}

@article{yang2024predictive,
 abstract = {This paper presents "Predictive Pipelined Decoding (PPD)," an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.},
 author = {Seongjun Yang and Gibbeum Lee and Jaewoong Cho and Dimitris Papailiopoulos and Kangwook Lee},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4384264648},
 pdf = {https://openreview.net/pdf?id=yUmJ483OB0},
 review = {https://openreview.net/forum?id=yUmJ483OB0},
 title = {Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding},
 url = {https://openreview.net/forum?id=yUmJ483OB0},
 year = {2024}
}

@article{yau2023docom,
 abstract = {This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm $\texttt{DoCoM}$ for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that $\texttt{DoCoM}$ finds a near-stationary solution at all participating agents satisfying $\mathbb{E}[ \| \nabla f( \theta ) \|^2 ] = \mathcal{O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of $\texttt{DoCoM}$. As a corollary, our analysis also established the linear convergence of $\texttt{DoCoM}$ to a global optimal solution for objective functions with the Polyak-{\L}ojasiewicz condition. Numerical experiments demonstrate that our algorithm outperforms several state-of-the-art algorithms in practice.},
 author = {Chung-Yiu Yau and Hoi To Wai},
 code = {https://github.com/OscarYau525/docom},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221146430},
 pdf = {https://openreview.net/pdf?id=W0ehjkl9x7},
 review = {https://openreview.net/forum?id=W0ehjkl9x7},
 title = {DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity},
 url = {https://openreview.net/forum?id=W0ehjkl9x7},
 year = {2023}
}

@article{yavas2024fixedbudget,
 abstract = {We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\theta^*$ may be of large dimension $d$, but only a few, say $s \ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed sparsity $s$ and budget $T$, the exponent in the error probability of Lasso-OD depends on $s$ but not on the dimension $d$, yielding a significant performance improvement for sparse and high-dimensional linear bandits. Furthermore, we show that Lasso-OD is almost minimax optimal in the exponent. Finally, we provide numerical examples to demonstrate the significant performance improvement over the existing algorithms for non-sparse linear bandits such as OD-LinBAI, BayesGap, Peace, LinearExploration, and GSE.},
 author = {Recep Can Yavas and Vincent Y. F. Tan},
 code = {https://github.com/recepcyavas/TMLR_sparse_linear_bandit},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4388276084},
 pdf = {https://openreview.net/pdf?id=Igxp7FC8uf},
 review = {https://openreview.net/forum?id=Igxp7FC8uf},
 title = {Fixed-Budget Best-Arm Identification in Sparse Linear Bandits},
 url = {https://openreview.net/forum?id=Igxp7FC8uf},
 year = {2024}
}

@article{yin2024modulora,
 author = {Junjie Yin and Jiahao Dong and Yingheng Wang and Christopher De Sa and Volodymyr Kuleshov},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 pdf = {https://openreview.net/pdf?id=r9p9CV52MV},
 review = {https://openreview.net/forum?id=r9p9CV52MV},
 title = {ModuLo{RA}: Finetuning 2-Bit {LLM}s on Consumer {GPU}s by Integrating with Modular Quantizers},
 url = {https://openreview.net/forum?id=r9p9CV52MV},
 year = {2024}
}

@article{yoo2023data,
 abstract = {Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the first meta-analysis on the role of data augmentation in SSAD.},
 author = {Jaemin Yoo and Tiancheng Zhao and Leman Akoglu},
 code = {https://github.com/jaeminyoo/SSL-AD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4292215431},
 pdf = {https://openreview.net/pdf?id=HyzCuCV1jH},
 review = {https://openreview.net/forum?id=HyzCuCV1jH},
 title = {Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success},
 url = {https://openreview.net/forum?id=HyzCuCV1jH},
 year = {2023}
}

@article{yoon2022limis,
 abstract = {Understanding black-box machine learning models is crucial for their widespread adoption. Learning globally interpretable models is one approach, but achieving high performance with them is challenging. An alternative approach is to explain individual predictions using locally interpretable models. For locally interpretable modeling, various methods have been proposed and indeed commonly used, but they suffer from low fidelity, i.e. their explanations do not approximate the predictions well. In this paper, our goal is to push the state-of-the-art in high-fidelity locally interpretable modeling. We propose a novel framework, Locally Interpretable Modeling using Instance-wise Subsampling (LIMIS). LIMIS utilizes a policy gradient to select a small number of instances and distills the black-box model into a low-capacity locally interpretable model using those selected instances. Training is guided with a reward obtained directly by measuring the fidelity of the locally interpretable models. We show on multiple tabular datasets that LIMIS near-matches the prediction accuracy of black-box models, significantly outperforming state-of-the-art locally interpretable models in terms of fidelity and prediction accuracy.},
 author = {Jinsung Yoon and Sercan O Arik and Tomas Pfister},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4296938603},
 pdf = {https://openreview.net/pdf?id=S8eABAy8P3},
 review = {https://openreview.net/forum?id=S8eABAy8P3},
 title = {LIMIS: Locally Interpretable Modeling using Instance-wise Subsampling},
 url = {https://openreview.net/forum?id=S8eABAy8P3},
 year = {2022}
}

@article{yoon2022selfsupervise,
 abstract = {Anomaly detection (AD), separating anomalies from normal data, has many applications across domains, from security to healthcare. While most previous works were shown to be effective for cases with fully or partially labeled data, that setting is in practice less common due to labeling being particularly tedious for this task. In this paper, we focus on fully unsupervised AD, in which the entire training dataset, containing both normal and anomalous samples, is unlabeled. To tackle this problem effectively, we propose to improve the robustness of one-class classification trained on self-supervised representations using a data refinement process. Our proposed data refinement approach is based on an ensemble of one-class classifiers (OCCs), each of which is trained on a disjoint subset of training data. Representations learned by self-supervised learning on the refined data are iteratively updated as the data refinement improves. We demonstrate our method on various unsupervised AD tasks with image and tabular data. With a 10% anomaly ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the proposed method outperforms the state-of-the-art one-class classifier by 6.3 AUC and 12.5 average precision / 22.9 F1-score.},
 author = {Jinsung Yoon and Kihyuk Sohn and Chun-Liang Li and Sercan O Arik and Chen-Yu Lee and Tomas Pfister},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4298052062},
 pdf = {https://openreview.net/pdf?id=b3v1UrtF6G},
 review = {https://openreview.net/forum?id=b3v1UrtF6G},
 title = {Self-supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection},
 url = {https://openreview.net/forum?id=b3v1UrtF6G},
 year = {2022}
}

@article{yoon2023spade,
 abstract = {Semi-supervised anomaly detection is a common problem, as often the datasets containing anomalies are partially labeled. We propose a canonical framework: Semi-supervised Pseudo-labeler Anomaly Detection with Ensembling (SPADE) that isn't limited by the assumption that labeled and unlabeled data come from the same distribution. Indeed, the assumption is often violated in many applications - for example, the labeled data may contain only anomalies unlike unlabeled data, or unlabeled data may contain different types of anomalies, or labeled data may contain only 'easy-to-label' samples. SPADE utilizes an ensemble of one class classifiers as the pseudo-labeler to improve the robustness of pseudo-labeling with distribution mismatch. Partial matching is proposed to automatically select the critical hyper-parameters for pseudo-labeling without validation data, which is crucial with limited labeled data. SPADE shows state-of-the-art semi-supervised anomaly detection performance across a wide range of scenarios with distribution mismatch in both tabular and image domains. In some common real-world settings such as model facing new types of unlabeled anomalies, SPADE outperforms the state-of-the-art alternatives by 5% AUC in average.},
 author = {Jinsung Yoon and Kihyuk Sohn and Chun-Liang Li and Sercan O Arik and Tomas Pfister},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4310628941},
 pdf = {https://openreview.net/pdf?id=JwDpZSv3yz},
 review = {https://openreview.net/forum?id=JwDpZSv3yz},
 title = {SPADE: Semi-supervised Anomaly Detection under Distribution Mismatch},
 url = {https://openreview.net/forum?id=JwDpZSv3yz},
 year = {2023}
}

@article{you2022maxaffine,
 abstract = {In this paper, we study the importance of pruning in Deep Networks (DNs) and the yin & yang relationship between (1) pruning highly overparametrized DNs that have been trained from random initialization and (2) training small DNs that have been "cleverly" initialized. As in most cases practitioners can only resort to random initialization, there is a strong need to develop a grounded understanding of DN pruning. Current literature remains largely empirical, lacking a theoretical understanding of how pruning affects DNs' decision boundary, how to interpret pruning, and how to design corresponding principled pruning techniques. To tackle those questions, we propose to employ recent advances in the theoretical analysis of Continuous Piecewise Affine (CPA) DNs. From this perspective, we will be able to detect the early-bird (EB) ticket phenomenon, provide interpretability into current pruning techniques, and develop a principled pruning strategy. In each step of our study, we conduct extensive experiments supporting our claims and results; while our main goal is to enhance the current understanding towards DN pruning instead of developing a new pruning method, our spline pruning criteria in terms of layerwise and global pruning is on par with or even outperforms state-of-the-art pruning methods.},
 author = {Haoran You and Randall Balestriero and Zhihan Lu and Yutong Kou and Huihong Shi and Shunyao Zhang and Shang Wu and Yingyan Lin and Richard Baraniuk},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287393457},
 pdf = {https://openreview.net/pdf?id=bMar2OkxVu},
 review = {https://openreview.net/forum?id=bMar2OkxVu},
 title = {Max-Affine Spline Insights Into Deep Network Pruning},
 url = {https://openreview.net/forum?id=bMar2OkxVu},
 year = {2022}
}

@article{yu2022coca,
 abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.},
 author = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4229042118},
 pdf = {https://openreview.net/pdf?id=Ee277P3AYC},
 review = {https://openreview.net/forum?id=Ee277P3AYC},
 title = {CoCa: Contrastive Captioners are Image-Text Foundation Models},
 url = {https://openreview.net/forum?id=Ee277P3AYC},
 year = {2022}
}

@article{yu2022scaling,
 abstract = {We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.},
 author = {Jiahui Yu and Yuanzhong Xu and Jing Yu Koh and Thang Luong and Gunjan Baid and Zirui Wang and Vijay Vasudevan and Alexander Ku and Yinfei Yang and Burcu Karagol Ayan and Ben Hutchinson and Wei Han and Zarana Parekh and Xin Li and Han Zhang and Jason Baldridge and Yonghui Wu},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4283388932},
 pdf = {https://openreview.net/pdf?id=AFDcYJKhND},
 review = {https://openreview.net/forum?id=AFDcYJKhND},
 title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
 url = {https://openreview.net/forum?id=AFDcYJKhND},
 year = {2022}
}

@article{yu2023continual,
 abstract = {It has been observed that neural networks perform poorly when the data or tasks are presented sequentially. Unlike humans, neural networks suffer greatly from catastrophic forgetting, making it impossible to perform life-long learning. To address this issue, memory-based continual learning has been actively studied and stands out as one of the best-performing methods. We examine memory-based continual learning and identify that large variation in the representation space is crucial for avoiding catastrophic forgetting. Motivated by this, we propose to diversify representations by using two types of perturbations: model-agnostic variation (i.e., the variation is generated without the knowledge of the learned neural network) and model-based variation (i.e., the variation is conditioned on the learned neural network). We demonstrate that enlarging representational variation serves as a general principle to improve continual learning. Finally, we perform empirical studies which demonstrate that our method, as a simple plug-and-play component, can consistently improve a number of memory-based continual learning methods by a large margin.},
 author = {Longhui Yu and Tianyang Hu and Lanqing HONG and Zhen Liu and Adrian Weller and Weiyang Liu},
 code = {https://github.com/yulonghui/MOCA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4305029958},
 pdf = {https://openreview.net/pdf?id=iDxfGaMYVr},
 review = {https://openreview.net/forum?id=iDxfGaMYVr},
 title = {Continual Learning by Modeling Intra-Class Variation},
 url = {https://openreview.net/forum?id=iDxfGaMYVr},
 year = {2023}
}

@article{yu2023individual,
 abstract = {Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose output-specific $(\varepsilon,\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\varepsilon$ of the class with the lowest test accuracy is 44.2\% higher than that of the class with the highest accuracy.},
 author = {Da Yu and Gautam Kamath and Janardhan Kulkarni and Tie-Yan Liu and Jian Yin and Huishuai Zhang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4288811803},
 pdf = {https://openreview.net/pdf?id=l4Jcxs0fpC},
 review = {https://openreview.net/forum?id=l4Jcxs0fpC},
 title = {Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent},
 url = {https://openreview.net/forum?id=l4Jcxs0fpC},
 year = {2023}
}

@article{yu2023learning,
 abstract = {Photorealistic object appearance modeling from 2D images is a constant topic in vision and graphics. While neural implicit methods (such as Neural Radiance Fields) have shown high-fidelity view synthesis results, they cannot relight the captured objects. More recent neural inverse rendering approaches have enabled object relighting, but they represent surface properties as simple BRDFs, and therefore cannot handle translucent objects. We propose Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct object appearance from only images. OSFs not only support free-viewpoint object relighting, but also can model both opaque and translucent objects. While accurately modeling subsurface light transport for translucent objects can be highly complex and even intractable for neural methods, OSFs learn to approximate the radiance transfer from a distant light to an outgoing direction at any spatial location. This approximation avoids explicitly modeling complex subsurface scattering, making learning a neural implicit model tractable. Experiments on real and synthetic data show that OSFs accurately reconstruct appearances for both opaque and translucent objects, allowing faithful free-viewpoint relighting as well as scene composition.},
 author = {Hong-Xing Yu and Michelle Guo and Alireza Fathi and Yen-Yu Chang and Eric Ryan Chan and Ruohan Gao and Thomas Funkhouser and Jiajun Wu},
 code = {https://github.com/michguo/osf},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4324109391},
 pdf = {https://openreview.net/pdf?id=NrfSRtTpN5},
 review = {https://openreview.net/forum?id=NrfSRtTpN5},
 title = {Learning Object-Centric Neural Scattering Functions for Free-Viewpoint Relighting and Scene Composition},
 url = {https://openreview.net/forum?id=NrfSRtTpN5},
 year = {2023}
}

@article{yu2023scalable,
 abstract = {Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.},
 author = {Hanlin Yu and Marcelo Hartmann and Bernardo Williams and Arto Klami},
 code = {https://github.com/ksnxr/SSGRLDNDM},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4323924246},
 pdf = {https://openreview.net/pdf?id=dXAuvo6CGI},
 review = {https://openreview.net/forum?id=dXAuvo6CGI},
 title = {Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics},
 url = {https://openreview.net/forum?id=dXAuvo6CGI},
 year = {2023}
}

@article{yu2024a,
 author = {Jun Yu and Zhaoming Kong and Kun Chen and Xin Zhang and Yong Chen and Lifang He},
 code = {https://github.com/junfish/gSTCCA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=zc0Y0cAuTV},
 review = {https://openreview.net/forum?id=zc0Y0cAuTV},
 title = {A Multilinear Least-Squares Formulation for Sparse Tensor Canonical Correlation Analysis},
 url = {https://openreview.net/forum?id=zc0Y0cAuTV},
 year = {2024}
}

@article{yu2024tensorvae,
 author = {Hongyang Yu and Hongjiang Yu},
 code = {https://github.com/yuh8/TensorVAE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=rQqzt4gYcc},
 review = {https://openreview.net/forum?id=rQqzt4gYcc},
 title = {Tensor{VAE}: a simple and efficient generative model for conditional molecular conformation generation},
 url = {https://openreview.net/forum?id=rQqzt4gYcc},
 year = {2024}
}

@article{yuan2022a,
 author = {Zhiri YUAN and Xixu HU and Qi WU and Shumin MA and Cheuk Hang LEUNG and Xin Shen and Yiyan HUANG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=yeT9cBq8Cn},
 review = {https://openreview.net/forum?id=yeT9cBq8Cn},
 title = {A Unified Domain Adaptation Framework with Distinctive Divergence Analysis},
 url = {https://openreview.net/forum?id=yeT9cBq8Cn},
 year = {2022}
}

@article{yun2023do,
 abstract = {Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts--such as colors, shapes, or the attributes of object parts--emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap first asks a VL model to generate primitive concept activations with text prompts, and then learns to construct a composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to composite concepts (e.g. a red-winged blackbird). We show that a composition model can be reliably learn from ground truth primitive concepts. We thus hypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept activations can be used to learn a composition model similar to the one designed by experts. We propose a quantitative metric to measure the degree of similarity, and refer to the metric as the interpretability metric. We also measure the classification accuracy when using the primitive concept activations and the learned composition model to predict the composite concepts, and refer to it as the usefulness metric. Our study reveals that state-of-the-art VL pretrained models learn primitive concepts that are highly useful for fine-grained visual recognition on the CUB dataset, and compositional generalization tasks on the MIT-States dataset. However, we observe that the learned composition models have low interpretability in our qualitative analyses. Our results reveal the limitations of existing VL models, and the necessity of pretraining objectives that encourage the acquisition of primitive concepts.},
 author = {Tian Yun and Usha Bhalla and Ellie Pavlick and Chen Sun},
 code = {https://github.com/tttyuntian/vlm_primitive_concepts},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226271906},
 pdf = {https://openreview.net/pdf?id=YwNrPLjHSL},
 review = {https://openreview.net/forum?id=YwNrPLjHSL},
 title = {Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?},
 url = {https://openreview.net/forum?id=YwNrPLjHSL},
 year = {2023}
}

@article{ze,
 code = {https://github.com/zecevic-matej/Not-All-Causal-Inference-is-the-Same},
 pdf = {https://openreview.net/pdf?id=ySWQ6eXAKp},
 review = {https://openreview.net/forum?id=ySWQ6eXAKp}
}

@article{ze,
 code = {https://github.com/MoritzWillig/causalParrots/},
 pdf = {https://openreview.net/pdf?id=tv46tCzs83},
 review = {https://openreview.net/forum?id=tv46tCzs83}
}

@article{zecchin2023communicationefficient,
 abstract = {Decentralized learning algorithms empower interconnected devices to share data and computational resources to collaboratively train a machine learning model without the aid of a central coordinator. In the case of heterogeneous data distributions at the network nodes, collaboration can yield predictors with unsatisfactory performance for a subset of the devices. For this reason, in this work, we consider the formulation of a distributionally robust decentralized learning task and we propose a decentralized single loop gradient descent/ascent algorithm (AD-GDA) to directly solve the underlying minimax optimization problem. We render our algorithm communication-efficient by employing a compressed consensus scheme and we provide convergence guarantees for smooth convex and non-convex loss functions. Finally, we corroborate the theoretical findings with empirical results that highlight AD-GDA's ability to provide unbiased predictors and to greatly improve communication efficiency compared to existing distributionally robust algorithms.},
 author = {Matteo Zecchin and Marios Kountouris and David Gesbert},
 code = {https://github.com/MatteoEURECOM/AD-GDA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4282039920},
 pdf = {https://openreview.net/pdf?id=tnRRHzZPMq},
 review = {https://openreview.net/forum?id=tnRRHzZPMq},
 title = {Communication-Efficient Distributionally Robust Decentralized Learning},
 url = {https://openreview.net/forum?id=tnRRHzZPMq},
 year = {2023}
}

@article{zee2023a,
 abstract = {While current deep learning algorithms have been successful for a wide variety of artificial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients that are computed by backpropagation of errors (backprop). Gradients are required to obtain synaptic weight adjustments but require knowledge of feed-forward activities in order to conduct backward propagation, a biologically implausible process. This is known as the "weight transport problem". Therefore, in this work, we present a more biologically plausible approach towards solving the weight transport problem for image data. This approach, which we name the error kernel driven activation alignment (EKDAA) algorithm, accomplishes through the introduction of locally derived error transmission kernels and error maps. Like standard deep learning networks, EKDAA performs the standard forward process via weights and activation functions; however, its backward error computation involves adaptive error kernels that propagate local error signals through the network. The efficacy of EKDAA is demonstrated by performing visual-recognition tasks on the Fashion MNIST, CIFAR-10 and SVHN benchmarks, along with demonstrating its ability to extract visual features from natural color images. Furthermore, in order to demonstrate its non-reliance on gradient computations, results are presented for an EKDAA trained CNN that employs a non-differentiable activation function.},
 author = {Timothy Zee and Alex Ororbia and Ankur Mali and Ifeoma Nwogu},
 code = {https://github.com/tzee/EKDAA-Release},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281940211},
 pdf = {https://openreview.net/pdf?id=leqr0vQzeN},
 review = {https://openreview.net/forum?id=leqr0vQzeN},
 title = {A Robust Backpropagation-Free Framework for Images},
 url = {https://openreview.net/forum?id=leqr0vQzeN},
 year = {2023}
}

@article{zerouali2023error,
 author = {Ahmed J Zerouali and Douglas Blair Tweed},
 code = {https://github.com/mathfrak-g-hat/Error_Bounds_Dynamics_Bootstrapping_ACRL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=QCjMJfSnYk},
 review = {https://openreview.net/forum?id=QCjMJfSnYk},
 title = {Error bounds and dynamics of bootstrapping in actor-critic reinforcement learning},
 url = {https://openreview.net/forum?id=QCjMJfSnYk},
 year = {2023}
}

@article{zhan2022unsupervised,
 abstract = {We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. We integrate modern program synthesis techniques with the variational autoencoding (VAE) framework, in order to learn a neurosymbolic encoder in conjunction with a standard decoder. The programmatic descriptions from our encoders can benefit many analysis workflows, such as in behavior modeling where interpreting agent actions and movements is important. We evaluate our method on learning latent representations for real-world trajectory data from animal biology and sports analytics. We show that our approach offers significantly better separation of meaningful categories than standard VAEs and leads to practical gains on downstream analysis tasks, such as for behavior classification.},
 author = {Eric Zhan and Jennifer J. Sun and Ann Kennedy and Yisong Yue and Swarat Chaudhuri},
 code = {https://github.com/ezhan94/neurosymbolic-encoders},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3184744745},
 pdf = {https://openreview.net/pdf?id=eWvBEMTlRq},
 review = {https://openreview.net/forum?id=eWvBEMTlRq},
 title = {Unsupervised Learning of Neurosymbolic Encoders},
 url = {https://openreview.net/forum?id=eWvBEMTlRq},
 year = {2022}
}

@article{zhan2023pareto,
 abstract = {Pool-based Active Learning (AL) has achieved great success in minimizing labeling cost by sequentially selecting informative unlabeled samples from a large unlabeled data pool and querying their labels from oracle/annotators. However, existing AL sampling strategies might not work well in out-of-distribution (OOD) data scenarios, where the unlabeled data pool contains some data samples that do not belong to the classes of the target task. Achieving good AL performance under OOD data scenarios is a challenging task due to the natural conflict between AL sampling strategies and OOD sample detection. AL selects data that are hard to be classified by the current basic classifier (e.g., samples whose predicted class probabilities have high entropy), while OOD samples tend to have more uniform predicted class probabilities (i.e., high entropy) than in-distribution (ID) data. In this paper, we propose a sampling scheme, Monte-Carlo Pareto Optimization for Active Learning (POAL), which selects optimal subsets of unlabeled samples with fixed batch size from the unlabeled data pool. We cast the AL sampling task as a multi-objective optimization problem, and thus we utilize Pareto optimization based on two conflicting objectives: (1) the normal AL data sampling scheme (e.g., maximum entropy), and (2) the confidence of not being an OOD sample. Experimental results show its effectiveness on both classical Machine Learning (ML) and Deep Learning (DL) tasks.},
 author = {Xueying Zhan and Zeyu Dai and Qingzhong Wang and Qing Li and Haoyi Xiong and Dejing Dou and Antoni B. Chan},
 code = {https://github.com/SineZHAN/POAL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283824046},
 pdf = {https://openreview.net/pdf?id=dXnccpSSYF},
 review = {https://openreview.net/forum?id=dXnccpSSYF},
 title = {Pareto Optimization for Active Learning under Out-of-Distribution Data Scenarios},
 url = {https://openreview.net/forum?id=dXnccpSSYF},
 year = {2023}
}

@article{zhang2022noilin,
 abstract = {Adversarial training (AT) formulated as the minimax optimization problem can effectively enhance the model's robustness against adversarial attacks. The existing AT methods mainly focused on manipulating the inner maximization for generating quality adversarial variants or manipulating the outer minimization for designing effective learning objectives. However, empirical results of AT always exhibit the robustness at odds with accuracy and the existence of the cross-over mixture problem, which motivates us to study some label randomness for benefiting the AT. First, we thoroughly investigate noisy labels (NLs) injection into AT's inner maximization and outer minimization, respectively and obtain the observations on when NL injection benefits AT. Second, based on the observations, we propose a simple but effective method -- NoiLIn that randomly injects NLs into training data at each training epoch and dynamically increases the NL injection rate once robust overfitting occurs. Empirically, NoiLIn can significantly mitigate the AT's undesirable issue of robust overfitting and even further improve the generalization of the state-of-the-art AT methods. Philosophically, NoiLIn sheds light on a new perspective of learning with NLs: NLs should not always be deemed detrimental, and even in the absence of NLs in the training set, we may consider injecting them deliberately. Codes are available in https://github.com/zjfheart/NoiLIn.},
 author = {Jingfeng Zhang and Xilie Xu and Bo Han and Tongliang Liu and Lizhen Cui and Gang Niu and Masashi Sugiyama},
 code = {https://github.com/zjfheart/NoiLIn},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4290067191},
 pdf = {https://openreview.net/pdf?id=zlQXV7xtZs},
 review = {https://openreview.net/forum?id=zlQXV7xtZs},
 title = {NoiLIn: Improving Adversarial Training and Correcting Stereotype of Noisy Labels},
 url = {https://openreview.net/forum?id=zlQXV7xtZs},
 year = {2022}
}

@article{zhang2023a,
 abstract = {We make contributions towards improving adaptive-optimizer performance. Our improvements are based on suppression of the range of adaptive stepsizes in the AdaBelief optimizer. Firstly, we show that the particular placement of the parameter epsilon within the update expressions of AdaBelief reduces the range of the adaptive stepsizes, making AdaBelief closer to SGD with momentum. Secondly, we extend AdaBelief by further suppressing the range of the adaptive stepsizes. To achieve the above goal, we perform mutual layerwise vector projections between the gradient g_t and its first momentum m_t before using them to estimate the second momentum. The new optimization method is referred to as Aida. Thirdly, extensive experimental results show that Aida outperforms nine optimizers when training transformers and LSTMs for NLP, and VGG and ResNet for image classification over CIAF10 and CIFAR100 while matching the best performance of the nine methods when training WGAN-GP models for image generation tasks. Furthermore, Aida produces higher validation accuracies than AdaBelief for training ResNet18 over ImageNet. Code is available <a href="https://github.com/guoqiang-x-zhang/AidaOptimizer">at this URL</a>},
 author = {Guoqiang Zhang and Kenta Niwa and W. Bastiaan Kleijn},
 code = {https://github.com/guoqiang-zhang-x/Aida-Optimizer},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318071021},
 pdf = {https://openreview.net/pdf?id=VI2JjIfU37},
 review = {https://openreview.net/forum?id=VI2JjIfU37},
 title = {A DNN Optimizer that Improves over AdaBelief by Suppression of the Adaptive Stepsize Range},
 url = {https://openreview.net/forum?id=VI2JjIfU37},
 year = {2023}
}

@article{zhang2023cae,
 author = {Xinyu Zhang and Jiahui Chen and Junkun Yuan and Qiang Chen and Jian Wang and Xiaodi Wang and Shumin Han and Xiaokang Chen and Jimin Pi and Kun Yao and Junyu Han and Errui Ding and Jingdong Wang},
 code = {https://github.com/Atten4Vis/CAE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=f36LaK7M0F},
 review = {https://openreview.net/forum?id=f36LaK7M0F},
 title = {{CAE} v2: Context Autoencoder with {CLIP} Latent Alignment},
 url = {https://openreview.net/forum?id=f36LaK7M0F},
 year = {2023}
}

@article{zhang2023exploring,
 abstract = {Previous works on Treatment Effect Estimation (TEE) are not in widespread use because they are predominantly theoretical, where strong parametric assumptions are made but untractable for practical application. Recent work uses multilayer perceptron (MLP) for modeling casual relationships, however, MLPs lag far behind recent advances in ML methodology, which limits their applicability and generalizability. To extend beyond the single domain formulation and towards more realistic learning scenarios, we explore model design spaces beyond MLPs, i.e., transformer backbones, which provide flexibility where attention layers govern interactions among treatments and covariates to exploit structural similarities of potential outcomes for confounding control. Through careful model design, Transformers as Treatment Effect Estimators (TransTEE) is proposed. We show empirically that TransTEE can: (1) serve as a general purpose treatment effect estimator that significantly outperforms competitive baselines in a variety of challenging TEE problems (e.g., discrete, continuous, structured, or dosage-associated treatments) and is applicable to both when covariates are tabular and when they consist of structural data (e.g., texts, graphs); (2) yield multiple advantages: compatibility with propensity score modeling, parameter efficiency, robustness to continuous treatment value distribution shifts, explainable in covariate adjustment, and real-world utility in auditing pre-trained language models},
 author = {YiFan Zhang and Hanlin Zhang and Zachary Chase Lipton and Li Erran Li and Eric Xing},
 code = {https://github.com/hlzhang109/TransTEE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281668675},
 pdf = {https://openreview.net/pdf?id=1kl4YM2Q7P},
 review = {https://openreview.net/forum?id=1kl4YM2Q7P},
 title = {Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation},
 url = {https://openreview.net/forum?id=1kl4YM2Q7P},
 year = {2023}
}

@article{zhang2023limitation,
 abstract = {In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future.},
 author = {Leyang Zhang and Zhi-Qin John Xu and Tao Luo and Yaoyu Zhang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221153610},
 pdf = {https://openreview.net/pdf?id=140kSqm0uy},
 review = {https://openreview.net/forum?id=140kSqm0uy},
 title = {Limitation of Characterizing Implicit Regularization by Data-independent Functions},
 url = {https://openreview.net/forum?id=140kSqm0uy},
 year = {2023}
}

@article{zhang2023mind,
 author = {Chunhui Zhang and Hongfu Liu and Jundong Li and Yanfang Ye and Chuxu Zhang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LEVbhNrLEL},
 review = {https://openreview.net/forum?id=LEVbhNrLEL},
 title = {Mind the Gap: Mitigating the Distribution Gap in Graph Few-shot Learning},
 url = {https://openreview.net/forum?id=LEVbhNrLEL},
 year = {2023}
}

@article{zhang2023novel,
 abstract = {While the novel class discovery has recently made great progress, existing methods typically focus on improving algorithms on class-balanced benchmarks. However, in real-world recognition tasks, the class distributions of their corresponding datasets are often imbalanced, which leads to serious performance degeneration of those methods. In this paper, we consider a more realistic setting for novel class discovery where the distributions of novel and known classes are long-tailed. One main challenge of this new problem is to discover imbalanced novel classes with the help of long-tailed known classes. To tackle this problem, we propose an adaptive self-labeling strategy based on an equiangular prototype representation of classes. Our method infers high-quality pseudo-labels for the novel classes by solving a relaxed optimal transport problem and effectively mitigates the class biases in learning the known and novel classes. We perform extensive experiments on CIFAR100, ImageNet100, Herbarium19 and large-scale iNaturalist18 datasets, and the results demonstrate the superiority of our method. Our code is available at https://github.com/kleinzcy/NCDLR.},
 author = {Chuyu Zhang and Ruijie Xu and Xuming He},
 code = {https://github.com/kleinzcy/NCDLR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4385681408},
 pdf = {https://openreview.net/pdf?id=ey5b7kODvK},
 review = {https://openreview.net/forum?id=ey5b7kODvK},
 title = {Novel Class Discovery for Long-tailed Recognition},
 url = {https://openreview.net/forum?id=ey5b7kODvK},
 year = {2023}
}

@article{zhang2023proportional,
 abstract = {This paper studies federated learning (FL) with non-identical data generating distributions and under the notion of proportional fairness. Users are partitioned into disjoint clusters based on their data distributions, and a distinct model is learnt for each cluster. Different than previous work that considered clustering to optimize personalized learning performance, here clustering is inspected under the disparate impact doctrine which requires that protected classes (e.g., those of certain race or gender) have representations in every cluster that are approximately equal to their representation in the overall set of users. A family of iterative algorithms that balance the learning performance and proportional fairness through cluster assignments as randomized functions of the learning losses is proposed. The trade-off induced by our algorithms between accuracy of cluster estimation and the introduced randomization level is characterized. The proposed algorithm is examined on a real dataset to evaluate its performance.},
 author = {Guojun Zhang and Saber Malekmohammadi and Xi Chen and Yaoliang Yu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4289655143},
 pdf = {https://openreview.net/pdf?id=ryUHgEdWCQ},
 review = {https://openreview.net/forum?id=ryUHgEdWCQ},
 title = {Proportional Fair Clustered Federated Learning},
 url = {https://openreview.net/forum?id=ryUHgEdWCQ},
 year = {2023}
}

@article{zhang2023replayenhanced,
 abstract = {Replaying past experiences has proven to be a highly effective approach for averting catastrophic forgetting in supervised continual learning. However, some crucial factors are still largely ignored, making it vulnerable to serious failure, when used as a solution to forgetting in continual reinforcement learning, even in the context of perfect memory where all data of previous tasks are accessible in the current task. On the one hand, since most reinforcement learning algorithms are not invariant to the reward scale, the previously well-learned tasks (with high rewards) may appear to be more salient to the current learning process than the current task (with small initial rewards). This causes the agent to concentrate on those salient tasks at the expense of generality on the current task. On the other hand, offline learning on replayed tasks while learning a new task may induce a distributional shift between the dataset and the learned policy on old tasks, resulting in forgetting. In this paper, we introduce RECALL, a replay-enhanced method that greatly improves the plasticity of existing replay-based methods on new tasks while effectively avoiding the recurrence of catastrophic forgetting in continual reinforcement learning. RECALL leverages adaptive normalization on approximate targets and policy distillation on old tasks to enhance generality and stability, respectively. Extensive experiments on the Continual World benchmark show that RECALL performs significantly better than purely perfect memory replay, and achieves comparable or better overall performance against state-of-the-art continual learning methods.},
 author = {Tiantian Zhang and Kevin Zehua Shen and Zichuan Lin and Bo Yuan and Xueqian Wang and Xiu Li and Deheng Ye},
 code = {https://github.com/Sweety-dm/RECALL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4388926448},
 pdf = {https://openreview.net/pdf?id=91hfMEUukm},
 review = {https://openreview.net/forum?id=91hfMEUukm},
 title = {Replay-enhanced Continual Reinforcement Learning},
 url = {https://openreview.net/forum?id=91hfMEUukm},
 year = {2023}
}

@article{zhang2023scalable,
 abstract = {To more efficiently address image compressed sensing (CS) problems, we present a novel content-aware scalable network dubbed CASNet which collectively achieves adaptive sampling rate allocation, fine granular scalability and high-quality reconstruction. We first adopt a data-driven saliency detector to evaluate the importances of different image regions and propose a saliency-based block ratio aggregation (BRA) strategy for sampling rate allocation. A unified learnable generating matrix is then developed to produce sampling matrix of any CS ratio with an ordered structure. Being equipped with the optimization-inspired recovery subnet guided by saliency information and a multi-block training scheme preventing blocking artifacts, CASNet jointly reconstructs the image blocks sampled at various sampling rates with one single model. To accelerate training convergence and improve network robustness, we propose an SVD-based initialization scheme and a random transformation enhancement (RTE) strategy, which are extensible without introducing extra parameters. All the CASNet components can be combined and learned end-to-end. We further provide a four-stage implementation for evaluation and practical deployments. Experiments demonstrate that CASNet outperforms other CS networks by a large margin, validating the collaboration and mutual supports among its components and strategies.},
 author = {Zhonghao Zhang and Yipeng Liu and Xingyu Cao and Fei Wen and Ce Zhu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4292230951},
 pdf = {https://openreview.net/pdf?id=10JdgrzNOk},
 review = {https://openreview.net/forum?id=10JdgrzNOk},
 title = {Content-Aware Scalable Deep Compressed Sensing},
 url = {https://openreview.net/forum?id=10JdgrzNOk},
 year = {2023}
}

@article{zhang2023transport,
 abstract = {Variational inference often minimizes the "reverse" Kullbeck-Leibler (KL) KL(q||p) from the approximate distribution q to the posterior p. Recent work studies the "forward" KL KL(p||q), which unlike reverse KL does not lead to variational approximations that underestimate uncertainty. This paper introduces Transport Score Climbing (TSC), a method that optimizes KL(p||q) by using Hamiltonian Monte Carlo (HMC) and a novel adaptive transport map. The transport map improves the trajectory of HMC by acting as a change of variable between the latent variable space and a warped space. TSC uses HMC samples to dynamically train the transport map while optimizing KL(p||q). TSC leverages synergies, where better transport maps lead to better HMC sampling, which then leads to better transport maps. We demonstrate TSC on synthetic and real data. We find that TSC achieves competitive performance when training variational autoencoders on large-scale data.},
 author = {Liyi Zhang and David Blei and Christian A Naesseth},
 code = {https://github.com/zhang-liyi/tsc},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226067822},
 pdf = {https://openreview.net/pdf?id=7KW7zvKd7J},
 review = {https://openreview.net/forum?id=7KW7zvKd7J},
 title = {Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport},
 url = {https://openreview.net/forum?id=7KW7zvKd7J},
 year = {2023}
}

@article{zhang2023twolevel,
 author = {Su Zhang and Srijita Das and Sriram Ganapathi Subramanian and Matthew E. Taylor},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=LfQ6uAVAEo},
 review = {https://openreview.net/forum?id=LfQ6uAVAEo},
 title = {Two-Level Actor-Critic Using Multiple Teachers},
 url = {https://openreview.net/forum?id=LfQ6uAVAEo},
 year = {2023}
}

@article{zhang2023unsupervised,
 abstract = {Unsupervised domain adaptation transfers knowledge from learned source domain to a different (but related) target distribution, for which only few or no labeled data is available. Some researchers proposed upper bounds for the target error when transferring the knowledge, i.e.,Ben-David et al. (2010) established a theory based on minimizing the source error and distance between marginal distributions simultaneously. However, in most works the joint error is usually ignored. In this paper, we argue that the joint error is essential for the domain adaptation problem, in particular if the samples from different classes in source/target are closely aligned when matching the marginal distributions. To tackle this problem, we propose a novel upper bound that includes the joint error. Moreover, we utilize a constrained hypothesis space to further tighten up this bound. Furthermore, we propose a novel cross margin discrepancy to measure the dissimilarity between hypotheses. We show in this paper that the new cross margin discrepancy is able to alleviate instability during adversarial learning. In addition, we present extensive empirical evidence that shows that our proposal outperforms related approaches in image classification error rates on standard domain adaptation benchmarks.},
 author = {Dexuan Zhang and Thomas Westfechtel and Tatsuya Harada},
 code = {https://drive.google.com/file/d/1-UTT8TP-3LBVUql109EZS9PwDidMqWLb/view?usp=sharing},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3129649114},
 pdf = {https://openreview.net/pdf?id=kiPsMct7vL},
 review = {https://openreview.net/forum?id=kiPsMct7vL},
 title = {Unsupervised Domain Adaptation via Minimized Joint Error},
 url = {https://openreview.net/forum?id=kiPsMct7vL},
 year = {2023}
}

@article{zhang2024accountable,
 abstract = {The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models.},
 author = {Zhiwei Zhang and Yuliang Liu},
 badge = {Survey},
 code = {https://matrix-alpha.github.io/},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4324106613},
 pdf = {https://openreview.net/pdf?id=kQmz1BMIYi},
 review = {https://openreview.net/forum?id=kQmz1BMIYi},
 title = {Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation},
 url = {https://openreview.net/forum?id=kQmz1BMIYi},
 year = {2024}
}

@article{zhang2024distributional,
 author = {Dinghuai Zhang and Ling Pan and Ricky T. Q. Chen and Aaron Courville and Yoshua Bengio},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/zdhNarsil/Distributional-GFlowNets},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 pdf = {https://openreview.net/pdf?id=vFSsRYGpjW},
 review = {https://openreview.net/forum?id=vFSsRYGpjW},
 title = {Distributional {GF}lowNets with Quantile Flows},
 url = {https://openreview.net/forum?id=vFSsRYGpjW},
 year = {2024}
}

@article{zhang2024functional,
 abstract = {The estimation of cumulative distribution functions (CDF) is an important learning task with a great variety of downstream applications, such as risk assessments in predictions and decision making. In this paper, we study functional regression of contextual CDFs where each data point is sampled from a linear combination of context dependent CDF basis functions. We propose functional ridge-regression-based estimation methods that estimate CDFs accurately everywhere. In particular, given $n$ samples with $d$ basis functions, we show estimation error upper bounds of $\widetilde O(\sqrt{d/n})$ for fixed design, random design, and adversarial context cases. We also derive matching information theoretic lower bounds, establishing minimax optimality for CDF functional regression. Furthermore, we remove the burn-in time in the random design setting using an alternative penalized estimator. Then, we consider agnostic settings where there is a mismatch in the data generation process. We characterize the error of the proposed estimators in terms of the mismatched error, and show that the estimators are well-behaved under model mismatch. Moreover, to complete our study, we formalize infinite dimensional models where the parameter space is an infinite dimensional Hilbert space, and establish a self-normalized estimation error upper bound for this setting. Notably, the upper bound reduces to the $\widetilde O(\sqrt{d/n})$ bound when the parameter space is constrained to be $d$-dimensional. Our comprehensive numerical experiments validate the efficacy of our estimation methods in both synthetic and practical settings.},
 author = {Qian Zhang and Anuran Makur and Kamyar Azizzadenesheli},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281630447},
 pdf = {https://openreview.net/pdf?id=ZOqJCP4eMk},
 review = {https://openreview.net/forum?id=ZOqJCP4eMk},
 title = {Functional Linear Regression of Cumulative Distribution Functions},
 url = {https://openreview.net/forum?id=ZOqJCP4eMk},
 year = {2024}
}

@article{zhang2024learning,
 abstract = {Learning and decision-making in domains with naturally high noise-to-signal ratio, such as Finance or Healthcare, is often challenging, while the stakes are very high. In this paper, we study the problem of learning and acting under a general noisy generative process. In this problem, the data distribution has a significant proportion of uninformative samples with high noise in the label, while part of the data contains useful information represented by low label noise. This dichotomy is present during both training and inference, which requires the proper handling of uninformative data during both training and testing. We propose a novel approach to learning under these conditions via a loss inspired by the selective learning theory. By minimizing this loss, the model is guaranteed to make a near-optimal decision by distinguishing informative data from uninformative data and making predictions. We build upon the strength of our theoretical guarantees by describing an iterative algorithm, which jointly optimizes both a predictor and a selector, and evaluates its empirical performance in a variety of settings.},
 author = {Yikai Zhang and Songzhu Zheng and Mina Dalirrooyfard and Pengxiang Wu and Anderson Schneider and Anant Raj and Yuriy Nevmyvaka and Chao Chen},
 code = {https://github.com/morganstanley/MSML/tree/main/paper/Learn_to_Abstain},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4387076419},
 pdf = {https://openreview.net/pdf?id=KKARKoPcEA},
 review = {https://openreview.net/forum?id=KKARKoPcEA},
 title = {Learning to Abstain From Uninformative Data},
 url = {https://openreview.net/forum?id=KKARKoPcEA},
 year = {2024}
}

@article{zhang2024understanding,
 author = {Guojun Zhang and Mahdi Beitollahi and Alex Bie and Xi Chen},
 code = {https://github.com/huawei-noah/Federated-Learning/tree/main/Layer_Normalization},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=6BDHUkSPna},
 review = {https://openreview.net/forum?id=6BDHUkSPna},
 title = {Understanding the Role of Layer Normalization in Label-Skewed Federated Learning},
 url = {https://openreview.net/forum?id=6BDHUkSPna},
 year = {2024}
}

@article{zhao2022extracting,
 author = {Haiyan Zhao and Tianyi Zhou and Guodong Long and Jing Jiang and Chengqi Zhang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=RP6G787uD8},
 review = {https://openreview.net/forum?id=RP6G787uD8},
 title = {Extracting Local Reasoning Chains of Deep Neural Networks},
 url = {https://openreview.net/forum?id=RP6G787uD8},
 year = {2022}
}

@article{zhao2022zero,
 abstract = {Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training. However, selecting the appropriate variance becomes challenging especially as the number of layers grows. In this work, we replace random weight initialization with a fully deterministic initialization scheme, viz., ZerO, which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. Through both theoretical and empirical studies, we demonstrate that ZerO is able to train networks without damaging their expressivity. Applying ZerO on ResNet achieves state-of-the-art performance on various datasets, including ImageNet, which suggests random weights may be unnecessary for network initialization. In addition, ZerO has many benefits, such as training ultra deep networks (without batch-normalization), exhibiting low-rank learning trajectories that result in low-rank and sparse solutions, and improving training reproducibility.},
 author = {Jiawei Zhao and Florian Tobias Schaefer and Anima Anandkumar},
 code = {https://github.com/jiaweizzhao/ZerO-initialization},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3208461463},
 pdf = {https://openreview.net/pdf?id=1AxQpKmiTc},
 review = {https://openreview.net/forum?id=1AxQpKmiTc},
 title = {ZerO Initialization: Initializing Neural Networks with only Zeros and Ones},
 url = {https://openreview.net/forum?id=1AxQpKmiTc},
 year = {2022}
}

@article{zhao2023complementary,
 author = {Kang Zhao and Yijun Tan and Kai Han and Ting Hu and Hanting Chen and Tao Yuan and Yunhe Wang and Jun Yao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=g1B4qgOw79},
 review = {https://openreview.net/forum?id=g1B4qgOw79},
 title = {Complementary Sparsity: Accelerating Sparse {CNN}s with High Accuracy on General-Purpose Computing Platforms},
 url = {https://openreview.net/forum?id=g1B4qgOw79},
 year = {2023}
}

@article{zhao2023consistent,
 abstract = {Collaborative filtering is the de facto standard for analyzing users' activities and building recommendation systems for items. In this work we develop Sliced Anti-symmetric Decomposition (SAD), a new model for collaborative filtering based on implicit feedback. In contrast to traditional techniques where a latent representation of users (user vectors) and items (item vectors) are estimated, SAD introduces one additional latent vector to each item, using a novel three-way tensor view of user-item interactions. This new vector extends user-item preferences calculated by standard dot products to general inner products, producing interactions between items when evaluating their relative preferences. SAD reduces to state-of-the-art (SOTA) collaborative filtering models when the vector collapses to 1, while in this paper we allow its value to be estimated from data. Allowing the values of the new item vector to be different from 1 has profound implications. It suggests users may have nonlinear mental models when evaluating items, allowing the existence of cycles in pairwise comparisons. We demonstrate the efficiency of SAD in both simulated and real world datasets containing over 1M user-item interactions. By comparing with seven SOTA collaborative filtering models with implicit feedbacks, SAD produces the most consistent personalized preferences, in the meanwhile maintaining top-level of accuracy in personalized recommendations. We release the model and inference algorithms in a Python library https://github.com/apple/ml-sad.},
 author = {Shiwen Zhao and Guillermo Sapiro},
 code = {https://github.com/apple/ml-sad},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221144218},
 pdf = {https://openreview.net/pdf?id=HqIuAzBxbh},
 review = {https://openreview.net/forum?id=HqIuAzBxbh},
 title = {Consistent Collaborative Filtering via Tensor Decomposition},
 url = {https://openreview.net/forum?id=HqIuAzBxbh},
 year = {2023}
}

@article{zhao2023costs,
 abstract = {Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy with a real-valued target is not entirely clear. In this paper, we characterize the inherent tradeoff between statistical parity and accuracy in the regression setting by providing a lower bound on the error of any fair regressor. Our lower bound is sharp, algorithm-independent, and admits a simple interpretation: when the moments of the target differ between groups, any fair algorithm has to make an error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair algorithm, using the Wasserstein distance to measure the quality of the approximation. With our novel lower bound, we also show that the price paid by a fair regressor that does not take the protected attribute as input is less than that of a fair regressor with explicit access to the protected attribute. On the upside, we establish the first connection between individual fairness, accuracy parity, and the Wasserstein distance by showing that if a regressor is individually fair, it also approximately verifies the accuracy parity, where the gap is given by the Wasserstein distance between the two groups. Inspired by our theoretical results, we develop a practical algorithm for fair regression through the lens of representation learning, and conduct experiments on a real-world dataset to corroborate our findings.},
 author = {Han Zhao},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4287118019},
 pdf = {https://openreview.net/pdf?id=v6anjyEDVW},
 review = {https://openreview.net/forum?id=v6anjyEDVW},
 title = {Costs and Benefits of Fair Regression},
 url = {https://openreview.net/forum?id=v6anjyEDVW},
 year = {2023}
}

@article{zhao2023dual,
 abstract = {To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL constructs its complementary distribution-discriminative representation by integrating diverse representations less similar to the label-discriminative representation. Accordingly, DRL combines label- and distribution-discriminative representations to detect out-of-distribution samples. Experiments show that DRL outperforms the state-of-the-art methods for out-of-distribution detection.},
 author = {Zhilin Zhao and Longbing Cao},
 code = {https://github.com/Lawliet-zzl/DRL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283314980},
 pdf = {https://openreview.net/pdf?id=PHAr3q49h6},
 review = {https://openreview.net/forum?id=PHAr3q49h6},
 title = {Dual Representation Learning for Out-of-Distribution Detection},
 url = {https://openreview.net/forum?id=PHAr3q49h6},
 year = {2023}
}

@article{zhao2023lsvrg,
 abstract = {Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that even without prior information, the proposed adaptive sampling strategy matches, and in some cases even surpasses, the performance of the sampling scheme in Qian et al. (2021). Extensive simulations support our theory and the practical utility of the proposed sampling scheme on real data.},
 author = {Boxin Zhao and Boxiang Lyu and mladen kolar},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226197403},
 pdf = {https://openreview.net/pdf?id=9lyqt3rbDc},
 review = {https://openreview.net/forum?id=9lyqt3rbDc},
 title = {L-SVRG and L-Katyusha with Adaptive Sampling},
 url = {https://openreview.net/forum?id=9lyqt3rbDc},
 year = {2023}
}

@article{zhao2023multilabel,
 abstract = {Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, we define homophily and Cross-Class Neighborhood Similarity for the multi-label scenario and provide a thorough analyses of the collected $9$ multi-label datasets. Finally, we perform a large-scale comparative study with $8$ methods and $9$ datasets and analyse the performances of the methods to assess the progress made by current state of the art in the multi-label node classification scenario. We release our benchmark at https://github.com/Tianqi-py/MLGNC.},
 author = {Tianqi Zhao and Thi Ngan Dong and Alan Hanjalic and Megha Khosla},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4366735283},
 pdf = {https://openreview.net/pdf?id=EZhkV2BjDP},
 review = {https://openreview.net/forum?id=EZhkV2BjDP},
 title = {Multi-label Node Classification On Graph-Structured Data},
 url = {https://openreview.net/forum?id=EZhkV2BjDP},
 year = {2023}
}

@article{zhao2024automated,
 abstract = {Metaheuristics have gained great success in academia and practice because their search logic can be applied to any problem with available solution representation, solution quality evaluation, and certain notions of locality. Manually designing metaheuristic algorithms for solving a target problem is criticized for being laborious, error-prone, and requiring intensive specialized knowledge. This gives rise to increasing interest in automated design of metaheuristic algorithms. With computing power to fully explore potential design choices, the automated design could reach and even surpass human-level design and could make high-performance algorithms accessible to a much wider range of researchers and practitioners. This paper presents a broad picture of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and representative techniques in terms of design space, design strategies, performance evaluation strategies, and target problems in this field.},
 author = {Qi Zhao and Qiqi Duan and Bai Yan and Shi Cheng and Yuhui Shi},
 badge = {Survey},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Survey Certification},
 openalex = {W4324319407},
 pdf = {https://openreview.net/pdf?id=qhtHsvF5zj},
 review = {https://openreview.net/forum?id=qhtHsvF5zj},
 title = {Automated Design of Metaheuristic Algorithms: A Survey},
 url = {https://openreview.net/forum?id=qhtHsvF5zj},
 year = {2024}
}

@article{zhao2024unleashing,
 author = {Jiayu Zhao and Renyu Yang and SHENGHAO QIU and Zheng Wang},
 code = {https://github.com/gloaming2dawn/AIBO},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=0CM7Hfsy61},
 review = {https://openreview.net/forum?id=0CM7Hfsy61},
 title = {Unleashing the Potential of Acquisition Functions in High-Dimensional Bayesian Optimization},
 url = {https://openreview.net/forum?id=0CM7Hfsy61},
 year = {2024}
}

@article{zheng2023contrastive,
 abstract = {Contrastive learning (CL) methods effectively learn data representations in a self-supervision manner, where the encoder contrasts each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image data, recent CL methods have achieved promising results when pretrained on large-scale datasets, such as ImageNet. However, most of them consider the augmented views from the same instance are positive pairs, while views from other instances are negative ones. Such binary partition insufficiently considers the relation between samples and tends to yield worse performance when generalized on images in the wild. In this paper, to further improve the performance of CL and enhance its robustness on various datasets, {we propose a doubly CL strategy that separately compares positive and negative samples within their own groups, and then proceeds with a contrast between positive and negative groups}. We realize this strategy with contrastive attraction and contrastive repulsion (CACR), which makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples. Theoretical analysis reveals that CACR generalizes CL's behavior by positive attraction and negative repulsion, and it further considers the intra-contrastive relation within the positive and negative pairs to narrow the gap between the sampled and true distribution, which is important when datasets are less curated. With our extensive experiments, CACR not only demonstrates good performance on CL benchmarks, but also shows better robustness when generalized on imbalanced image datasets. Code and pre-trained checkpoints are available at https://github.com/JegZheng/CACR-SSL.},
 author = {Huangjie Zheng and Xu Chen and Jiangchao Yao and Hongxia Yang and Chunyuan Li and Ya Zhang and Hao Zhang and Ivor Tsang and Jingren Zhou and Mingyuan Zhou},
 code = {https://github.com/JegZheng/CACR-SSL},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3177400161},
 pdf = {https://openreview.net/pdf?id=f39UIDkwwc},
 review = {https://openreview.net/forum?id=f39UIDkwwc},
 title = {Contrastive Attraction and Contrastive Repulsion for Representation Learning},
 url = {https://openreview.net/forum?id=f39UIDkwwc},
 year = {2023}
}

@article{zheng2023revisiting,
 abstract = {A recent line of work in natural language processing has aimed to combine language models and topic models. These topic-guided language models augment neural language models with topic models, unsupervised learning methods that can discover document-level patterns of word use. This paper compares the effectiveness of these methods in a standardized setting. We study four topic-guided language models and two baselines, evaluating the held-out predictive performance of each model on four corpora. Surprisingly, we find that none of these methods outperform a standard LSTM language model baseline, and most fail to learn good topics. Further, we train a probe of the neural language model that shows that the baseline's hidden states already encode topic information. We make public all code used for this study.},
 author = {Carolina Zheng and Keyon Vafa and David Blei},
 code = {https://github.com/carolinazheng/revisiting-tglms},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4389469913},
 pdf = {https://openreview.net/pdf?id=lXBEwFfxpA},
 review = {https://openreview.net/forum?id=lXBEwFfxpA},
 title = {Revisiting Topic-Guided Language Models},
 url = {https://openreview.net/forum?id=lXBEwFfxpA},
 year = {2023}
}

@article{zheng2023you,
 abstract = {Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer perceptron (MLP) model in a teacher-student regime. Experimental results on proprietary e-commerce datasets and open-source citation graphs show that the proposed workflow outperforms existing transfer learning baselines that do not explicitly utilize the intersection structure.},
 author = {Wenqing Zheng and Edward W Huang and Nikhil Rao and Zhangyang Wang and Karthik Subbian},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4322759617},
 pdf = {https://openreview.net/pdf?id=Nn71AdKyYH},
 review = {https://openreview.net/forum?id=Nn71AdKyYH},
 title = {You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction},
 url = {https://openreview.net/forum?id=Nn71AdKyYH},
 year = {2023}
}

@article{zheng2024fast,
 abstract = {We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance.},
 author = {Hongkai Zheng and Weili Nie and Arash Vahdat and Anima Anandkumar},
 code = {https://github.com/Anima-Lab/MaskDiT},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4380994510},
 pdf = {https://openreview.net/pdf?id=vTBjBtGioE},
 review = {https://openreview.net/forum?id=vTBjBtGioE},
 title = {Fast Training of Diffusion Models with Masked Transformers},
 url = {https://openreview.net/forum?id=vTBjBtGioE},
 year = {2024}
}

@article{zhong2022simplifying,
 abstract = {Graph Neural Networks (GNNs) have been predominant for graph learning tasks; however, recent studies showed that a well-known graph algorithm, Label Propagation (LP), combined with a shallow neural network can achieve comparable performance to GNNs in semi-supervised node classification on graphs with high homophily. In this paper, we show that this approach falls short on graphs with low homophily, where nodes often connect to the nodes of the opposite classes. To overcome this, we carefully design a combination of a base predictor with LP algorithm that enjoys a closed-form solution as well as convergence guarantees. Our algorithm first learns the class compatibility matrix and then aggregates label predictions using LP algorithm weighted by class compatibilities. On a wide variety of benchmarks, we show that our approach achieves the leading performance on graphs with various levels of homophily. Meanwhile, it has orders of magnitude fewer parameters and requires less execution time. Empirical evaluations demonstrate that simple adaptations of LP can be competitive in semi-supervised node classification in both homophily and heterophily regimes.},
 author = {Zhiqiang Zhong and Sergei Ivanov and Jun Pang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4281259749},
 pdf = {https://openreview.net/pdf?id=JBuCfkmKYu},
 review = {https://openreview.net/forum?id=JBuCfkmKYu},
 title = {Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation},
 url = {https://openreview.net/forum?id=JBuCfkmKYu},
 year = {2022}
}

@article{zhong2022unsupervised,
 abstract = {Network embedding (NE) approaches have emerged as a predominant technique to represent complex networks and have benefited numerous tasks. However, most NE approaches rely on a homophily assumption to learn embeddings with the guidance of supervisory signals, leaving the unsupervised heterophilous scenario relatively unexplored. This problem becomes especially relevant in fields where a scarcity of labels exists. Here, we formulate the unsupervised NE task as an r-ego network discrimination problem and develop the SELENE framework for learning on networks with homophily and heterophily. Specifically, we design a dual-channel feature embedding pipeline to discriminate r-ego networks using node attributes and structural information separately. We employ heterophily adapted self-supervised learning objective functions to optimise the framework to learn intrinsic node embeddings. We show that SELENE's components improve the quality of node embeddings, facilitating the discrimination of connected heterophilous nodes. Comprehensive empirical evaluations on both synthetic and real-world datasets with varying homophily ratios validate the effectiveness of SELENE in homophilous and heterophilous settings showing an up to 12.52% clustering accuracy gain.},
 author = {Zhiqiang Zhong and Guadalupe Gonzalez and Daniele Grattarola and Jun Pang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221166995},
 pdf = {https://openreview.net/pdf?id=sRgvmXjrmg},
 review = {https://openreview.net/forum?id=sRgvmXjrmg},
 title = {Unsupervised Network Embedding Beyond Homophily},
 url = {https://openreview.net/forum?id=sRgvmXjrmg},
 year = {2022}
}

@article{zhong2023achieving,
 abstract = {We study the Pareto frontier of two archetypal objectives in multi-armed bandits, namely, regret minimization (RM) and best arm identification (BAI) with a fixed horizon. It is folklore that the balance between exploitation and exploration is crucial for both RM and BAI, but exploration is more critical in achieving the optimal performance for the latter objective. To this end, we design and analyze the BoBW-lil'UCB$(\gamma)$ algorithm. Complementarily, by establishing lower bounds on the regret achievable by any algorithm with a given BAI failure probability, we show that (i) no algorithm can simultaneously perform optimally for both the RM and BAI objectives, and (ii) BoBW-lil'UCB$(\gamma)$ achieves order-wise optimal performance for RM or BAI under different values of $\gamma$. Our work elucidates the trade-off more precisely by showing how the constants in previous works depend on certain hardness parameters. Finally, we show that BoBW-lil'UCB outperforms a close competitor UCB$_\alpha$ (Degenne et al., 2019) in terms of the time complexity and the regret on diverse datasets such as MovieLens and Published Kinase Inhibitor Set.},
 author = {Zixin Zhong and Wang Chi Cheung and Vincent Tan},
 badge = {Featured},
 code = {https://github.com/zixinzh/2023-TMLR},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W4306175763},
 pdf = {https://openreview.net/pdf?id=XXfEmIMJDm},
 review = {https://openreview.net/forum?id=XXfEmIMJDm},
 title = {Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits},
 url = {https://openreview.net/forum?id=XXfEmIMJDm},
 year = {2023}
}

@article{zhou2022dha,
 abstract = {Automated machine learning (AutoML) usually involves several crucial components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural Architecture Search (NAS). Although many strategies have been developed for automating these components in separation, joint optimization of these components remains challenging due to the largely increased search dimension and the variant input types of each component. In parallel to this, the common practice of searching for the optimal architecture first and then retraining it before deployment in NAS often suffers from low performance correlation between the searching and retraining stages. An end-to-end solution that integrates the AutoML components and returns a ready-to-use model at the end of the search is desirable. In view of these, we propose DHA, which achieves joint optimization of Data augmentation policy, Hyper-parameter and Architecture. Specifically, end-to-end NAS is achieved in a differentiable manner by optimizing a compressed lower-dimensional feature space, while DA policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update of network parameters and network architecture at the same time. Experiments show that DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the best of our knowledge, we are the first to efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end manner without retraining.},
 author = {kaichen zhou and Lanqing HONG and Shoukang Hu and Fengwei Zhou and Binxin Ru and Jiashi Feng and Zhenguo Li},
 code = {https://github.com/gitkaichenzhou/DHA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3198949132},
 pdf = {https://openreview.net/pdf?id=MHOAEiTlen},
 review = {https://openreview.net/forum?id=MHOAEiTlen},
 title = {DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture},
 url = {https://openreview.net/forum?id=MHOAEiTlen},
 year = {2022}
}

@article{zhou2022estimating,
 abstract = {Traditional causal inference approaches leverage observational study data to estimate the difference in observed and unobserved outcomes for a potential treatment, known as the Conditional Average Treatment Effect (CATE). However, CATE corresponds to the comparison on the first moment alone, and as such may be insufficient in reflecting the full picture of treatment effects. As an alternative, estimating the full potential outcome distributions could provide greater insights. However, existing methods for estimating treatment effect potential outcome distributions often impose restrictive or simplistic assumptions about these distributions. Here, we propose Collaborating Causal Networks (CCN), a novel methodology which goes beyond the estimation of CATE alone by learning the full potential outcome distributions. Estimation of outcome distributions via the CCN framework does not require restrictive assumptions of the underlying data generating process. Additionally, CCN facilitates estimation of the utility of each possible treatment and permits individual-specific variation through utility functions. CCN not only extends outcome estimation beyond traditional risk difference, but also enables a more comprehensive decision-making process through definition of flexible comparisons. Under assumptions commonly made in the causal literature, we show that CCN learns distributions that asymptotically capture the true potential outcome distributions. Furthermore, we propose an adjustment approach that is empirically effective in alleviating sample imbalance between treatment groups in observational data. Finally, we evaluate the performance of CCN in multiple synthetic and semi-synthetic experiments. We demonstrate that CCN learns improved distribution estimates compared to existing Bayesian and deep generative methods as well as improved decisions with respects to a variety of utility functions.},
 author = {Tianhui Zhou and William E Carson IV and David Carlson},
 code = {https://github.com/carlson-lab/collaborating-causal-networks},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3202114308},
 pdf = {https://openreview.net/pdf?id=q1Fey9feu7},
 review = {https://openreview.net/forum?id=q1Fey9feu7},
 title = {Estimating Potential Outcome Distributions with Collaborating Causal Networks},
 url = {https://openreview.net/forum?id=q1Fey9feu7},
 year = {2022}
}

@article{zhou2022nonstationary,
 abstract = {We consider reinforcement learning (RL) in episodic Markov decision processes (MDPs) with linear function approximation under drifting environment. Specifically, both the reward and state transition functions can evolve over time but their total variations do not exceed a $\textit{variation budget}$. We first develop $\texttt{LSVI-UCB-Restart}$ algorithm, an optimistic modification of least-squares value iteration with periodic restart, and bound its dynamic regret when variation budgets are known. Then we propose a parameter-free algorithm $\texttt{Ada-LSVI-UCB-Restart}$ that extends to unknown variation budgets. We also derive the first minimax dynamic regret lower bound for nonstationary linear MDPs and as a byproduct establish a minimax regret lower bound for linear MDPs unsolved by Jin et al. (2020). Finally, we provide numerical experiments to demonstrate the effectiveness of our proposed algorithms.},
 author = {Huozhi Zhou and Jinglin Chen and Lav R. Varshney and Ashish Jagmohan},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3091875946},
 pdf = {https://openreview.net/pdf?id=nS8A9nOrqp},
 review = {https://openreview.net/forum?id=nS8A9nOrqp},
 title = {Nonstationary Reinforcement Learning with Linear Function Approximation},
 url = {https://openreview.net/forum?id=nS8A9nOrqp},
 year = {2022}
}

@article{zhou2023krada,
 abstract = {In semantic segmentation, we aim to train a pixel-level classifier to assign category labels to all pixels in an image, where labeled training images and unlabeled test images are from the same distribution and share the same label set. However, in an open world, the unlabeled test images probably contain unknown categories and have different distributions from the labeled images. Hence, in this paper, we consider a new, more realistic, and more challenging problem setting where the pixel-level classifier has to be trained with labeled images and unlabeled open-world images -- we name it open-set domain adaptation segmentation (OSDAS). In OSDAS, the trained classifier is expected to identify unknown-class pixels and classify known-class pixels well. To solve OSDAS, we first investigate which distribution that unknown-class pixels obey. Then, motivated by the goodness-of-fit test, we use statistical measurements to show how a pixel fits the distribution of an unknown class and select highly-fitted pixels to form the unknown region in each test image. Eventually, we propose an end-to-end learning framework, known-region-aware domain alignment (KRADA), to distinguish unknown classes while aligning the distributions of known classes in labeled and unlabeled open-world images. The effectiveness of KRADA has been verified on two synthetic tasks and one COVID-19 segmentation task.},
 author = {Chenhong Zhou and Feng Liu and Chen Gong and Rongfei Zeng and Tongliang Liu and William Cheung and Bo Han},
 code = {https://github.com/chenhong-zhou/KRADA},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4321275030},
 pdf = {https://openreview.net/pdf?id=5II12ypVQo},
 review = {https://openreview.net/forum?id=5II12ypVQo},
 title = {KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation},
 url = {https://openreview.net/forum?id=5II12ypVQo},
 year = {2023}
}

@article{zhou2023understanding,
 abstract = {Over the recent years, reinforcement learning (RL) starts to show promising results in tackling combinatorial optimization (CO) problems, in particular when coupled with curriculum learning to facilitate training. Despite emerging empirical evidence, theoretical study on why RL helps is still at its early stage. This paper presents the first systematic study on policy optimization methods for online CO problems. We show that online CO problems can be naturally formulated as latent Markov Decision Processes (LMDPs), and prove convergence bounds on natural policy gradient (NPG) for solving LMDPs. Furthermore, our theory explains the benefit of curriculum learning: it can find a strong sampling policy and reduce the distribution shift, a critical quantity that governs the convergence rate in our theorem. For a canonical online CO problem, the Best Choice Problem (BCP), we formally prove that distribution shift is reduced exponentially with curriculum learning even if the curriculum is a randomly generated BCP on a smaller scale. Our theory also shows we can simplify the curriculum learning scheme used in prior work from multi-step to single-step. Lastly, we provide extensive experiments on the Best Choice Problem, Online Knapsack, and AdWords to verify our findings.},
 author = {Runlong Zhou and Zelin He and Yuandong Tian and Yi Wu and Simon Shaolei Du},
 code = {https://github.com/zhourunlong/RL-for-Combinatorial-Optimization},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4221166735},
 pdf = {https://openreview.net/pdf?id=gKEbBKRUjA},
 review = {https://openreview.net/forum?id=gKEbBKRUjA},
 title = {Understanding Curriculum Learning in Policy Optimization for Online Combinatorial Optimization},
 url = {https://openreview.net/forum?id=gKEbBKRUjA},
 year = {2023}
}

@article{zhou2024on,
 abstract = {Adaptive gradient methods are workhorses in deep learning. However, the convergence guarantees of adaptive gradient methods for nonconvex optimization have not been thoroughly studied. In this paper, we provide a fine-grained convergence analysis for a general class of adaptive gradient methods including AMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that adaptive gradient methods in expectation converge to a first-order stationary point. Our convergence rate is better than existing results for adaptive gradient methods in terms of dimension, and is strictly faster than stochastic gradient decent (SGD) when the stochastic gradients are sparse. To the best of our knowledge, this is the first result showing the advantage of adaptive gradient methods over SGD in nonconvex setting. In addition, we also prove high probability bounds on the convergence rates of AMSGrad, RMSProp as well as AdaGrad, which have not been established before. Our analyses shed light on better understanding the mechanism behind adaptive gradient methods in optimizing nonconvex objectives.},
 author = {Dongruo Zhou and Jinghui Chen and Yuan Cao and Ziyan Yang and Quanquan Gu},
 badge = {Featured},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Featured Certification},
 openalex = {W2886463271},
 pdf = {https://openreview.net/pdf?id=Gh0cxhbz3c},
 review = {https://openreview.net/forum?id=Gh0cxhbz3c},
 title = {On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization},
 url = {https://openreview.net/forum?id=Gh0cxhbz3c},
 year = {2024}
}

@article{zhu2022direct,
 abstract = {Molecular conformation generation aims to generate three-dimensional coordinates of all the atoms in a molecule and is an important task in bioinformatics and pharmacology. Previous methods usually first predict the interatomic distances, the gradients of interatomic distances or the local structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D conformation. How to directly generate the conformation without the above intermediate values is not fully explored. In this work, we propose a method that directly predicts the coordinates of atoms: (1) the loss function is invariant to roto-translation of coordinates and permutation of symmetric atoms; (2) the newly proposed model adaptively aggregates the bond and atom information and iteratively refines the coordinates of the generated conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs datasets. Further analysis shows that our generated conformations have closer properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In addition, our method improves molecular docking by providing better initial conformations. All the results demonstrate the effectiveness of our method and the great potential of the direct approach. The code is released at https://github.com/DirectMolecularConfGen/DMCG},
 author = {Jinhua Zhu and Yingce Xia and Chang Liu and Lijun Wu and Shufang Xie and Yusong Wang and Tong Wang and Tao Qin and Wengang Zhou and Houqiang Li and Haiguang Liu and Tie-Yan Liu},
 code = {https://github.com/DirectMolecularConfGen/DMCG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4226060238},
 pdf = {https://openreview.net/pdf?id=lCPOHiztuw},
 review = {https://openreview.net/forum?id=lCPOHiztuw},
 title = {Direct Molecular Conformation Generation},
 url = {https://openreview.net/forum?id=lCPOHiztuw},
 year = {2022}
}

@article{zhu2023bayesian,
 abstract = {The Bayesian transformed Gaussian process (BTG) model, proposed by Kedem and Oliviera, is a fully Bayesian counterpart to the warped Gaussian process (WGP) and marginalizes out a joint prior over input warping and kernel hyperparameters. This fully Bayesian treatment of hyperparameters often provides more accurate regression estimates and superior uncertainty propagation, but is prohibitively expensive. The BTG posterior predictive distribution, itself estimated through high-dimensional integration, must be inverted in order to perform model prediction. To make the Bayesian approach practical and comparable in speed to maximum-likelihood estimation (MLE), we propose principled and fast techniques for computing with BTG. Our framework uses doubly sparse quadrature rules, tight quantile bounds, and rank-one matrix algebra to enable both fast model prediction and model selection. These scalable methods allow us to regress over higher-dimensional datasets and apply BTG with layered transformations that greatly improve its expressibility. We demonstrate that BTG achieves superior empirical performance over MLE-based models.},
 author = {Xinran Zhu and Leo Huang and Eric Hans Lee and Cameron Alexander Ibrahim and David Bindel},
 code = {https://github.com/xinranzhu/BTG},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4307076587},
 pdf = {https://openreview.net/pdf?id=4zCgjqjzAv},
 review = {https://openreview.net/forum?id=4zCgjqjzAv},
 title = {Scalable Bayesian Transformed Gaussian Processes},
 url = {https://openreview.net/forum?id=4zCgjqjzAv},
 year = {2023}
}

@article{zhu2023improving,
 abstract = {Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in deep learning to provide rigorously defined privacy, which requires gradient clipping to bound the maximum norm of individual gradients and additive isotropic Gaussian noise. With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that randomly sparsifying gradients before clipping and noisification adjusts a trade-off between internal components of the convergence bound and leads to a smaller upper bound when the noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling noisification or gradient clipping eliminates the trade-off in the bound. This observation is indicative, as it implies DP-SGD has special inherent room for (even simply random) gradient compression. To verify the observation and utilize it, we propose an efficient and lightweight extension using random sparsification (RS) to strengthen DP-SGD. Experiments with various DP-SGD frameworks show that RS can improve performance. Additionally, the produced sparse gradients of RS exhibit advantages in reducing communication cost and strengthening privacy against reconstruction attacks, which are also key problems in private machine learning.},
 author = {Junyi Zhu and Matthew B. Blaschko},
 code = {https://github.com/JunyiZhu-AI/RandomSparsification},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3216188667},
 pdf = {https://openreview.net/pdf?id=sY35BAiIf4},
 review = {https://openreview.net/forum?id=sY35BAiIf4},
 title = {Improving Differentially Private SGD via Randomly Sparsified Gradients},
 url = {https://openreview.net/forum?id=sY35BAiIf4},
 year = {2023}
}

@article{zhu2023semisupervised,
 author = {Ronghang Zhu and Xiang Yu and Sheng Li},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=sUlbRfLijj},
 review = {https://openreview.net/forum?id=sUlbRfLijj},
 title = {Semi-Supervised Single Domain Generalization with Label-Free Adversarial Data Augmentation},
 url = {https://openreview.net/forum?id=sUlbRfLijj},
 year = {2023}
}

@article{zhu2023understanding,
 abstract = {In this paper, we are interested in understanding self-supervised pretraining through studying the capability that self-supervised representation pretraining methods learn part-aware representations. The study is mainly motivated by that random views, used in contrastive learning, and random masked (visible) patches, used in masked image modeling, are often about object parts. We explain that contrastive learning is a part-to-whole task: the projection layer hallucinates the whole object representation from the object part representation learned from the encoder, and that masked image modeling is a part-to-part task: the masked patches of the object are hallucinated from the visible patches. The explanation suggests that the self-supervised pretrained encoder is required to understand the object part. We empirically compare the off-the-shelf encoders pretrained with several representative methods on object-level recognition and part-level recognition. The results show that the fully-supervised model outperforms self-supervised models for object-level recognition, and most self-supervised contrastive learning and masked image modeling methods outperform the fully-supervised method for part-level recognition. It is observed that the combination of contrastive learning and masked image modeling further improves the performance.},
 author = {Jie Zhu and Jiyang Qi and Mingyu Ding and Xiaokang Chen and Ping Luo and Xinggang Wang and Wenyu Liu and Leye Wang and Jingdong Wang},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4318621302},
 pdf = {https://openreview.net/pdf?id=HP7Qpui5YE},
 review = {https://openreview.net/forum?id=HP7Qpui5YE},
 title = {Understanding Self-Supervised Pretraining with Part-Aware Representation Learning},
 url = {https://openreview.net/forum?id=HP7Qpui5YE},
 year = {2023}
}

@article{zhu2024chatgpt,
 author = {Deyao Zhu and Jun Chen and Kilichbek Haydarov and Xiaoqian Shen and Wenxuan Zhang and Mohamed Elhoseiny},
 code = {https://github.com/Vision-CAIR/ChatCaptioner},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 pdf = {https://openreview.net/pdf?id=1LoVwFkZNo},
 review = {https://openreview.net/forum?id=1LoVwFkZNo},
 title = {Chat{GPT} Asks, {BLIP}-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions},
 url = {https://openreview.net/forum?id=1LoVwFkZNo},
 year = {2024}
}

@article{zhuang2022understanding,
 abstract = {Adam has been widely adopted for training deep neural networks due to less hyperparameter tuning and remarkable performance. To improve generalization, Adam is typically used in tandem with a squared $\ell_2$ regularizer (referred to as Adam-$\ell_2$). However, even better performance can be obtained with AdamW, which decouples the gradient of the regularizer from the update rule of Adam-$\ell_2$. Yet, we are still lacking a complete explanation of the advantages of AdamW. In this paper, we tackle this question from both an optimization and an empirical point of view. First, we show how to re-interpret AdamW as an approximation of a proximal gradient method, which takes advantage of the closed-form proximal mapping of the regularizer instead of only utilizing its gradient information as in Adam-$\ell_2$. Next, we consider the property of "scale-freeness" enjoyed by AdamW and by its proximal counterpart: their updates are invariant to component-wise rescaling of the gradients. We provide empirical evidence across a wide range of deep learning experiments showing a correlation between the problems in which AdamW exhibits an advantage over Adam-$\ell_2$ and the degree to which we expect the gradients of the network to exhibit multiple scales, thus motivating the hypothesis that the advantage of AdamW could be due to the scale-free updates.},
 author = {Zhenxun Zhuang and Mingrui Liu and Ashok Cutkosky and Francesco Orabona},
 badge = {Written by Expert Reviewer},
 code = {https://github.com/zhenxun-zhuang/AdamW-Scale-free},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Expert Certification},
 openalex = {W4225823287},
 pdf = {https://openreview.net/pdf?id=IKhEPWGdwK},
 review = {https://openreview.net/forum?id=IKhEPWGdwK},
 title = {Understanding AdamW through Proximal Methods and Scale-Freeness},
 url = {https://openreview.net/forum?id=IKhEPWGdwK},
 year = {2022}
}

@article{zimmer2023differentiable,
 abstract = {The integration of reasoning, learning, and decision-making is key to build more general artificial intelligence systems. As a step in this direction, we propose a novel neural-logic architecture, called differentiable logic machine (DLM), that can solve both inductive logic programming (ILP) and reinforcement learning (RL) problems, where the solution can be interpreted as a first-order logic program. Our proposition includes several innovations. Firstly, our architecture defines a restricted but expressive continuous relaxation of the space of first-order logic programs by assigning weights to predicates instead of rules, in contrast to most previous neural-logic approaches. Secondly, with this differentiable architecture, we propose several (supervised and RL) training procedures, based on gradient descent, which can recover a fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL training, we also design a novel critic architecture that enables actor-critic algorithms. Fourthly, to solve hard problems, we propose an incremental training procedure that can learn a logic program progressively. Compared to state-of-the-art (SOTA) differentiable ILP methods, DLM successfully solves all the considered ILP problems with a higher percentage of successful seeds (up to 3.5$\times$). On RL problems, without requiring an interpretable solution, DLM outperforms other non-interpretable neural-logic RL approaches in terms of rewards (up to 3.9%). When enforcing interpretability, DLM can solve harder RL problems (e.g., Sorting, Path) Moreover, we show that deep logic programs can be learned via incremental supervised training. In addition to this excellent performance, DLM can scale well in terms of memory and computational time, especially during the testing phase where it can deal with much more constants ($>$2$\times$) than SOTA.},
 author = {Matthieu Zimmer and Xuening Feng and Claire Glanois and Zhaohui JIANG and Jianyi Zhang and Paul Weng and Dong Li and Jianye HAO and Wulong Liu},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W3131970035},
 pdf = {https://openreview.net/pdf?id=mXfkKtu5JA},
 review = {https://openreview.net/forum?id=mXfkKtu5JA},
 title = {Differentiable Logic Machines},
 url = {https://openreview.net/forum?id=mXfkKtu5JA},
 year = {2023}
}

@article{zimmermann2023a,
 abstract = {Generative flow networks (GFNs) are a class of models for sequential sampling of composite objects, which approximate a target distribution that is defined in terms of an energy function or a reward. GFNs are typically trained using a flow matching or trajectory balance objective, which matches forward and backward transition models over trajectories. In this work, we define variational objectives for GFNs in terms of the Kullback-Leibler (KL) divergences between the forward and backward distribution. We show that variational inference in GFNs is equivalent to minimizing the trajectory balance objective when sampling trajectories from the forward model. We generalize this approach by optimizing a convex combination of the reverse- and forward KL divergence. This insight suggests variational inference methods can serve as a means to define a more general family of objectives for training generative flow networks, for example by incorporating control variates, which are commonly used in variational inference, to reduce the variance of the gradients of the trajectory balance objective. We evaluate our findings and the performance of the proposed variational objective numerically by comparing it to the trajectory balance objective on two synthetic tasks.},
 author = {Heiko Zimmermann and Fredrik Lindsten and Jan-Willem van de Meent and Christian A Naesseth},
 code = {https://github.com/zmheiko/variational-perspective-on-gflownets},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4320536034},
 pdf = {https://openreview.net/pdf?id=AZ4GobeSLq},
 review = {https://openreview.net/forum?id=AZ4GobeSLq},
 title = {A Variational Perspective on Generative Flow Networks},
 url = {https://openreview.net/forum?id=AZ4GobeSLq},
 year = {2023}
}

@article{zwartsenberg2023conditional,
 abstract = {We present a novel, conditional generative probabilistic model of set-valued data with a tractable log density. This model is a continuous normalizing flow governed by permutation equivariant dynamics. These dynamics are driven by a learnable per-set-element term and pairwise interactions, both parametrized by deep neural networks. We illustrate the utility of this model via applications including (1) complex traffic scene generation conditioned on visually specified map information, and (2) object bounding box generation conditioned directly on images. We train our model by maximizing the expected likelihood of labeled conditional data under our flow, with the aid of a penalty that ensures the dynamics are smooth and hence efficiently solvable. Our method significantly outperforms non-permutation invariant baselines in terms of log likelihood and domain-specific metrics (offroad, collision, and combined infractions), yielding realistic samples that are difficult to distinguish from real data.},
 author = {Berend Zwartsenberg and Adam Scibior and Matthew Niedoba and Vasileios Lioutas and Justice Sefas and Yunpeng Liu and Setareh Dabiri and Jonathan Wilder Lavington and Trevor Campbell and Frank Wood},
 code = {https://github.com/inverted-ai/conditional-permutation-invariant-flows-datasets},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 openalex = {W4283331467},
 pdf = {https://openreview.net/pdf?id=DUsgPi3oCC},
 review = {https://openreview.net/forum?id=DUsgPi3oCC},
 title = {Conditional Permutation Invariant Flows},
 url = {https://openreview.net/forum?id=DUsgPi3oCC},
 year = {2023}
}
